#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Oct  2 15:07:14 2019

@author: Cretignier Michael 
@university University of Geneva
"""

# =============================================================================
# Yet Another RAssine Related Arborescence (YARARA)
# =============================================================================

import datetime
import glob as glob
import itertools as iter
import os
import pickle
import sys
import time

import astropy.coordinates as astrocoord
import astropy.time as astrotime
import calculate_RV_line_by_line3 as calculate_RV_line_by_line
import matplotlib.cm as cmx
import matplotlib.colors as mplcolors
import matplotlib.pylab as plt
import my_classes as myc
import my_functions as myf
import numpy as np
import pandas as pd
import psutil
import Rassine_functions as ras

# from scipy.optimize import curve_fit
import scipy.ndimage
from astropy import units as u

# from scipy import stats
from astropy.io import fits
from astropy.time import Time
from astroquery.simbad import Simbad
from colorama import Fore
from dace.exoplanet import Exoplanet
from dace.spectroscopy import Spectroscopy
from dace.sun import Sun
from matplotlib.ticker import MultipleLocator
from matplotlib.widgets import Slider
from scipy.interpolate import interp1d
from tqdm import tqdm

try:  # Merci Jean-Baptiste ...
    import rvmodel as rvm

    rvmodel = rvm.rvModel
    spleaf_version = "old"
except:
    spleaf_version = "new"

np.warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)


# =============================================================================
# PRODUCE THE DACE TABLE SUMMARIZING RV TIMESERIES
# =============================================================================

cwd = os.getcwd()
root = "/".join(cwd.split("/")[:-1])


def print_iter(verbose):
    if verbose == -1:
        print(
            Fore.BLUE
            + " ==============================================================================\n [INFO] Extracting data with RASSINE...\n ==============================================================================\n"
            + Fore.RESET
        )
    elif verbose == 0:
        print(
            Fore.BLUE
            + " ==============================================================================\n [INFO] Preprocessing data with RASSINE...\n ==============================================================================\n"
            + Fore.RESET
        )
    elif verbose == 1:
        print(
            Fore.GREEN
            + " ==============================================================================\n [INFO] First iteration is beginning...\n ==============================================================================\n"
            + Fore.RESET
        )
    elif verbose == 2:
        print(
            Fore.YELLOW
            + " ==============================================================================\n [INFO] Second iteration is beginning...\n ==============================================================================\n"
            + Fore.RESET
        )
    elif verbose == 42:
        print(
            Fore.YELLOW
            + " ==============================================================================\n [INFO] Merging is beginning...\n ==============================================================================\n"
            + Fore.RESET
        )
    else:
        hours = verbose // 3600 % 24
        minutes = verbose // 60 % 60
        seconds = verbose % 60
        print(
            Fore.RED
            + " ==============================================================================\n [INFO] Intermediate time : %.0fh %.0fm %.0fs \n ==============================================================================\n"
            % (hours, minutes, seconds)
            + Fore.RESET
        )


def move_extract_dace(path, instrument="HARPS"):

    os.system("mkdir " + "/".join(path.split("/")[0:-1]) + "/" + instrument + " 2>/dev/null")

    listOfFiles = [x[0] for x in os.walk(path)]
    listOfFiles = np.sort(listOfFiles)[1:]
    for j in listOfFiles:
        act = os.system(
            "mv "
            + j
            + "/*"
            + instrument
            + "*.fits "
            + "/".join(path.split("/")[0:-1])
            + "/"
            + instrument
            + " 2>/dev/null"
        )
        if not act:
            os.system("rm -r " + j)

    if not len(glob.glob(path + "/*")):
        os.system("rm -rf " + path + " 2>/dev/null")


def rw_map_pickle_fits(file, action="r"):
    """Import a fits or pickle file map, if a dictionnary map is given as action, the file is written down"""
    """Function not used since no improvement in size file"""
    file_ext = file.split(".")[-1]
    if action == "r":
        if file_ext == "p":
            f = pd.read_pickle(file)
        elif file_ext == "fits":
            f_inter = fits.open(file_ext)
            wave = f_inter[0].data
            maps = f_inter[1].data
            f = {"wave": wave, "correction_map": maps}
        return f
    else:
        if file_ext == "p":
            myf.pickle_dump(action, open(file, "wb"))
        else:
            hdu1 = fits.PrimaryHDU(action["wave"])
            hdu2 = fits.ImageHDU(action["correction_map"])
            hdul = fits.HDUList([hdu1, hdu2])
            hdul.writeto(file)


def sun_model(match_x=False):
    tab = np.genfromtxt(root + "/Python/Material/Sun_model.txt")
    if type(match_x) == bool:
        match = np.arange(len(tab))
    else:
        match = myf.find_nearest(tab[:, 0], match_x - 0.5)[0]
    tab = tab[match.astype("int")]
    model = myc.tableXY(tab[:, 0], tab[:, 1], 0 * tab[:, 0])
    return model


def yarara_artefact_suppressed(old_continuum, new_continuum, larger_than=50, lower_than=-50):
    ratio = (new_continuum / old_continuum - 1) * 100
    mask = (ratio > larger_than) | (ratio < lower_than)
    return mask


def extract_table_dace(
    starname,
    instrument=["HARPS"],
    update_table=False,
    auto_trend=False,
    degree_max=None,
    m=2,
    prefit_planet=False,
    drs_version="old",
    force_reduction=0,
):

    path = root + "/Yarara/" + starname + "/data/s1d/"

    all_files = []
    instruments = []
    for ins in instrument:
        loc_files = glob.glob(path + "*" + ins + "*.rdb")
        if (not bool(len(loc_files))) | (bool(force_reduction)):  # only redo a dace query if
            if os.path.exists(path + "/spectroDownload"):
                out_dir = "0"
            else:
                out_dir = "1"
            if ins == "HARPS":
                print(
                    " python produce_dace_summary.py -s %s -i HARPS03 -c %.1f -D %s -f %s -o %s \n"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
                os.system(
                    "python produce_dace_summary.py -s %s -i HARPS03 -c %.1f -D %s -f %s -o %s"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
                os.system(
                    "python produce_dace_summary.py -s %s -i HARPS15 -c %.1f -D %s -f %s -o %s"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
            elif ins == "HARPS03":
                print(
                    " python produce_dace_summary.py -s %s -i HARPS03 -c %.1f -D %s -f %s -o %s \n"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
                os.system(
                    "python produce_dace_summary.py -s %s -i HARPS03 -c %.1f -D %s -f %s -o %s"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
            elif ins == "HARPS15":
                print(
                    " python produce_dace_summary.py -s %s -i HARPS15 -c %.1f -D %s -f %s -o %s \n"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
                os.system(
                    "python produce_dace_summary.py -s %s -i HARPS15 -c %.1f -D %s -f %s -o %s"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
            elif ins == "CORALIE":
                print(
                    " python produce_dace_summary.py -s %s -i CORALIE14 -c %.1f -D %s -f %s -o %s \n"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
                os.system(
                    "python produce_dace_summary.py -s %s -i CORALIE98 -c %.1f -D %s -f %s -o %s"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
                os.system(
                    "python produce_dace_summary.py -s %s -i CORALIE07 -c %.1f -D %s -f %s -o %s"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
                os.system(
                    "python produce_dace_summary.py -s %s -i CORALIE14 -c %.1f -D %s -f %s -o %s"
                    % (starname, m, drs_version, str(force_reduction), out_dir)
                )
            else:
                print(
                    " python produce_dace_summary.py -s %s -i %s -c %.1f -D %s -f %s -o %s \n"
                    % (starname, ins, m, drs_version, str(force_reduction), out_dir)
                )
                os.system(
                    "python produce_dace_summary.py -s %s -i %s -c %.1f -D %s -f %s -o %s"
                    % (starname, ins, m, drs_version, str(force_reduction), out_dir)
                )
            loc_files = glob.glob(path + "*" + ins + "*.rdb")
        instruments.append([ins] * len(loc_files))
        all_files.append(loc_files)

    all_files = np.ravel(all_files)
    instruments = np.ravel(instruments)

    dico = pd.DataFrame({})
    for i, j in zip(instruments, all_files):
        sub = pd.read_csv(j, delimiter="\t", low_memory=False)
        sub["instrument"] = i
        dico = pd.concat([dico, sub[1:]])

    if not len(dico):
        print(" [ERROR] Summary table is empty or not found")

    dico = dico.sort_values(by="rjd")
    dico = dico.reset_index(drop=True)

    Dico = {}
    if "HARPS" in instruments:
        Dico["HARPS"] = dico.loc[dico["instrument"] == "HARPS"]
    if "HARPS03" in instruments:
        Dico["HARPS03"] = dico.loc[dico["instrument"] == "HARPS03"]
    if "HARPS15" in instruments:
        Dico["HARPS15"] = dico.loc[dico["instrument"] == "HARPS15"]
    if "CORALIE" in instruments:
        Dico["CORALIE"] = dico.loc[dico["instrument"] == "CORALIE"]
    if "ESPRESSO18" in instruments:
        Dico["ESPRESSO18"] = dico.loc[dico["instrument"] == "ESPRESSO18"]
    if "ESPRESSO19" in instruments:
        Dico["ESPRESSO19"] = dico.loc[dico["instrument"] == "ESPRESSO19"]
    if "HARPN" in instruments:
        Dico["HARPN"] = dico.loc[dico["instrument"] == "HARPN"]
    if "EXPRESS" in instruments:
        Dico["EXPRESS"] = dico.loc[dico["instrument"] == "EXPRESS"]
    if "CARMENES" in instruments:
        Dico["CARMENES"] = dico.loc[dico["instrument"] == "CARMENES"]

    for k in Dico.keys():

        if k == "HARPS":
            offsets = [57161.5]
        elif k == "CORALIE":
            offsets = [54200, 56900]
        else:
            offsets = [0]

        if k == "HARPS":
            berv_kw = "HIERARCH ESO DRS BERV"
            lamp_kw = "HIERARCH ESO DRS CAL TH LAMP OFFSET"
            time_kw = "MJD-OBS"
        elif k == "HARPS03":
            berv_kw = "HIERARCH ESO DRS BERV"
            lamp_kw = "HIERARCH ESO DRS CAL TH LAMP OFFSET"
            time_kw = "MJD-OBS"
        elif k == "HARPS15":
            berv_kw = "HIERARCH ESO DRS BERV"
            lamp_kw = "HIERARCH ESO DRS CAL TH LAMP OFFSET"
            time_kw = "MJD-OBS"
        elif k == "HARPN":
            berv_kw = "HIERARCH TNG DRS BERV"
            lamp_kw = "HIERARCH TNG DRS CAL TH LAMP OFFSET"
            time_kw = "MJD-OBS"
        elif k == "ESPRESSO18":
            berv_kw = "HIERARCH ESO QC BERV"
            lamp_kw = "HIERARCH ESO QC CCF RV"  # no lamp offset on espresso
            time_kw = "MJD-OBS"
        elif k == "ESPRESSO19":
            berv_kw = "HIERARCH ESO QC BERV"
            lamp_kw = "HIERARCH ESO QC CCF RV"  # no lamp offset on espresso
            time_kw = "MJD-OBS"
        elif k == "EXPRESS":
            berv_kw = "HIERARCH ESO QC BERV"
            lamp_kw = "HIERARCH ESO QC CCF RV"  # no lamp offset on espresso
            time_kw = "MJD-OBS"
        elif k == "CARMENES":
            berv_kw = "HIERARCH ESO QC BERV"
            lamp_kw = "HIERARCH ESO QC CCF RV"  # no lamp offset on espresso
            time_kw = "MJD-OBS"
        elif k == "CORALIE":
            berv_kw = "HIERARCH ESO DRS BERV"
            lamp_kw = "HIERARCH ESO DRS CAL TH LAMP OFFSET"
            time_kw = "MJD-OBS"

        if update_table:
            old_table = pd.read_csv(
                path + k + "/DACE_TABLE/Dace_extracted_table.csv", low_memory=False
            )
        else:
            old_table = {"fileroot": np.array([""])}

        old_files = np.sort(old_table["fileroot"])
        all_files = np.sort(glob.glob(path + k + "/*.fits"))
        all_files = np.sort(np.setdiff1d(all_files, old_files))
        time.sleep(1)

        stop = False
        dico_match = Dico[k].copy()
        dico_match["lamp_offset"] = 0

        if "model" not in dico_match.keys():
            dico_match["model"] = 0

        if len(all_files) != 0:
            print("Number of new files to reduce : %.0f" % (len(all_files)))
            time.sleep(1)
            berv = np.ones(len(all_files)) * np.nan
            lamp = np.ones(len(all_files)) * np.nan
            jdb = np.ones(len(all_files)) * np.nan
            num = 0
            for file in tqdm(all_files):
                f = fits.open(file)
                berv[num] = f[0].header[berv_kw]  # matching on the berv values work best
                lamp[num] = f[0].header[lamp_kw]  # matching on the berv values work best
                try:
                    jdb[num] = f[0].header[time_kw]  # matching on the berv values work best
                except:
                    jdb[num] = Time(f[0].header["DATE-OBS"], format="isot").mjd
                num += 1
            jdb += 0.5
            berv_dace = np.array(Dico[k]["berv"]).astype("float")
            matrix_berv = berv_dace - berv_dace[:, np.newaxis]
            diag_one = np.diag(np.ones(len(berv_dace)))
            matrix_berv += diag_one  # to locate two same berv value in dace
            loc_non_unique = np.where(matrix_berv == 0)
            berv_dace_unique = berv_dace
            if len(loc_non_unique[0]) != 0:
                berv_dace_unique[loc_non_unique[0]] = 50  # imposible berv value
            a = myf.match_nearest(berv, berv_dace_unique)
            if len(loc_non_unique[0]) != 0:
                sub_dico = Dico[k].loc[loc_non_unique[0], ["berv", "rjd"]]
                sub_dico = sub_dico.sort_values(by="berv")
                sub_dico["product"] = sub_dico["berv"].astype("float") * sub_dico["rjd"].astype(
                    "float"
                )
                unmatched_idx = np.setdiff1d(np.arange(len(berv)), a[:, 0].astype("int"))

                berv_to_match = berv[unmatched_idx]
                jdb_to_match = jdb[unmatched_idx]
                match = myf.match_nearest(berv_to_match * jdb_to_match, sub_dico["product"])
                match_ext = match.copy()
                match_ext[:, 0] = unmatched_idx[match[:, 0].astype("int")]
                match_ext[:, 1] = np.array(sub_dico.index)[match[:, 1].astype("int")]
                a = np.vstack([a, match_ext])
                a = a[a[:, 0].argsort()]

            dico_match = Dico[k].iloc[a[:, 1].astype("int")]
            dico_match = dico_match.reset_index(drop=True)
            dico_match["fileroot"] = all_files[a[:, 0].astype("int")]
            dico_match["lamp_offset"] = lamp[a[:, 0].astype("int")]

            # plt.figure()
            # plt.scatter(a[:,0],a[:,1])
            # plt.xlabel('New indices',fontsize=14)
            # plt.ylabel('Dace indices',fontsize=14)
            if len(a) != len(berv):
                print(" [ERROR] Matching between raw frames and DACE failed")
                stop = True
                if len(berv) == len(berv_dace_unique):
                    dico_match = Dico[k]
                    dico_match = dico_match.reset_index(drop=True)
                    dico_match["fileroot"] = all_files
                    dico_match["lamp_offset"] = lamp
                    stop = False

        if not stop:
            jdb_dace = dico_match["rjd"].astype("float")
            mjd_dace = jdb_dace - 0.5
            dico_match["mjd"] = mjd_dace

            if (
                starname == "Sun"
            ):  # special case of observation during the day (allow to avoid dbin=0.5 later)
                jdb_dace += 0.5
                mjd_dace += 0.5
                dico_match["mjd"] = mjd_dace
                dico_match["rjd"] = jdb_dace

            if update_table:
                new_table = pd.concat([old_table, dico_match])
                new_table = new_table.sort_values(by="mjd")
                new_table = new_table.reset_index(drop=True)
                dico_match = new_table

            berv_dace = dico_match["berv"].astype("float")
            vrad_dace = dico_match["vrad"].astype("float")
            svrad_dace = dico_match["svrad"].astype("float")
            # lamp_dace = dico_match['lamp_offset'].astype('float')

            vrad = myc.tableXY(mjd_dace, vrad_dace / 1000, svrad_dace / 1000)
            # lamp = myc.tableXY(mjd_dace, lamp_dace, 0*lamp_dace)

            model = vrad.y * 0

            if len(mjd_dace) < 15:  # at least 15 points for the fit
                print(" [WARNING] Not enough point for the drift RV model, detrending skipped")
            else:
                if type(prefit_planet) == np.ndarray:
                    model = prefit_planet
                    v_sys = np.nanmedian(vrad.y)
                    model += v_sys

                else:
                    table_keplerian = myf.touch_pickle(
                        root + "/Python/database/KEPLERIAN/table_keplerians.p"
                    )
                    if starname in table_keplerian.keys():
                        prefit_planet = 1  # force to use the database keplerian table if the star is found inside

                    if prefit_planet:
                        v_sys = np.nanmedian(vrad.y)
                        table_keplerian = myf.touch_pickle(
                            root + "/Python/database/KEPLERIAN/table_keplerians.p"
                        )
                        if starname in table_keplerian.keys():
                            vector = myc.tableXY(jdb_dace, 0 * jdb_dace, 0 * jdb_dace)
                            vector.evaluate_keplerian(table_keplerian[starname])
                            vector2 = vector.baseline_oversampled()
                            vector2.evaluate_keplerian(table_keplerian[starname])
                            model = vector.keplerian.y / 1000
                            offset = np.nanmedian(model - vrad.y)
                            smooth = vector2.keplerian
                            smooth.y /= 1000
                            model -= offset
                            smooth.y -= offset
                            degree_detreding = 0
                            nb_planet_fitted = len(table_keplerian[starname])
                            model_binned = model
                        else:
                            vrad.night_stack(replace=True)

                            vrad.y -= v_sys
                            vrad.y *= 1000
                            vrad.yerr *= 1000

                            species = (vrad.x > offsets).astype("int")
                            if degree_max is not None:
                                power_max = degree_max
                                degree_max = degree_max
                            else:
                                power_max = 3
                                degree_max = 2

                            model_binned, smooth, model = vrad.periodogram_auto_detrend(
                                power_max=power_max,
                                degree_max=degree_max,
                                kmin=5,
                                species=species,
                                photon_noise=0.7,
                                jitter=0.7,
                                x_export=mjd_dace,
                            )

                            offset = np.nanmedian(model_binned - vrad.y)
                            model_binned = (model_binned - offset) / 1000 + v_sys
                            model = (model - offset) / 1000 + v_sys
                            smooth.y = (smooth.y - offset) / 1000 + v_sys
                            vrad.y = vrad.y / 1000 + v_sys
                            vrad.yerr /= 1000

                            degree_detreding = vrad.auto_deg
                            nb_planet_fitted = vrad.auto_nb_planet

                        plt.figure(figsize=(10, 7))
                        plt.axes([0.1, 0.35, 0.8, 0.55])
                        vrad.plot()
                        smooth.plot(color="r", ls="-")
                        plt.title(
                            "Degree %.0f + %.0f planets" % (degree_detreding, nb_planet_fitted)
                        )
                        plt.ylabel("RV [km/s]", fontsize=14)
                        plt.axes([0.1, 0.07, 0.8, 0.23])
                        vrad.y -= model_binned
                        vrad.y *= 1000
                        vrad.yerr *= 1000
                        vrad.recenter(who="Y")
                        vrad.plot()
                        plt.axhline(y=0, color="r")
                        plt.ylabel("Residuals [m/s]", fontsize=14)
                        plt.xlabel("jdb [days]", fontsize=14)
                        plt.savefig(
                            path
                            + "/RV_model_%.0f_p%.0f_%s.jpg"
                            % (degree_detreding, nb_planet_fitted, k)
                        )
                        vrad = myc.tableXY(mjd_dace, vrad_dace / 1000, svrad_dace / 1000)
                        vrad.y -= model

                    else:
                        plt.figure(figsize=(10, 7))
                        plt.subplot(2, 2, 1)
                        plt.title("degree 0")
                        vrad.plot()
                        myf.auto_axis(vrad.y)
                        plt.ylabel("RV [km/s]", fontsize=14)
                        plt.xlabel("Time jdb [days]", fontsize=14)
                        vrad.substract_polydisc(offsets, degree=0, Draw=True)
                        plt.subplot(2, 2, 2)
                        plt.title("degree 1")
                        plt.ylabel("RV [km/s]", fontsize=14)
                        plt.xlabel("Time jdb [days]", fontsize=14)
                        vrad.plot()
                        myf.auto_axis(vrad.y)
                        vrad.substract_polydisc(offsets, degree=1, Draw=True)
                        plt.subplot(2, 2, 3)
                        plt.title("degree 2")
                        plt.ylabel("RV [km/s]", fontsize=14)
                        plt.xlabel("Time jdb [days]", fontsize=14)
                        vrad.plot()
                        myf.auto_axis(vrad.y)
                        vrad.substract_polydisc(offsets, degree=2, Draw=True)
                        plt.subplot(2, 2, 4)
                        plt.title("degree 3")
                        plt.ylabel("RV [km/s]", fontsize=14)
                        plt.xlabel("Time jdb [days]", fontsize=14)
                        vrad.plot()
                        myf.auto_axis(vrad.y)
                        vrad.substract_polydisc(offsets, degree=3, Draw=True)
                        plt.subplots_adjust(hspace=0.35)

                    if auto_trend:
                        vrad.binning(bin_width=1)
                        rv_span = np.nanpercentile(vrad.binned.y * 1000, 97.5) - np.nanpercentile(
                            vrad.binned.y * 1000, 2.5
                        )
                        if rv_span >= 50:
                            print(
                                Fore.YELLOW
                                + " [WARNING] RV span too large (binary?,large amplitude planet?, transit?), get a look on the model selected (2) : current value of %.1f m/s"
                                % (rv_span)
                                + Fore.RESET
                            )
                            degree = 2
                        elif (rv_span > 10) & (rv_span < 50):
                            print(
                                Fore.YELLOW
                                + " [WARNING] RV span large (ins.offset?,binary?,large amplitude planet?, transit?), get a look on the model selected (0) : current value of %.1f m/s"
                                % (rv_span)
                                + Fore.RESET
                            )
                            degree = 0
                        else:
                            print(
                                " [INFO] No trend detected in RV, get a look on the model selected (0) : current value of %.1f m/s"
                                % (rv_span)
                            )
                            degree = 0
                    else:
                        plt.show(block=False)
                        degree = int(
                            myf.sphinx(
                                "Which degree do you keep for the  binary detrending (99 to supress the RV)?",
                                rep=["0", "1", "2", "3", "99"],
                            )
                        )

                    plt.savefig(path + "/RV_model_%.0f_%s.jpg" % (degree, k))

                    if degree != 99:
                        vrad.substract_polydisc(offsets, degree=degree, Draw=False)
                        model += vrad.y - vrad.sub_model
                    else:
                        model += vrad.y

            if starname == "Sun":
                ephemerides = myc.tableXY(
                    np.array(jdb_dace), np.array(dico_match["model"]).astype("float")
                )
                # ephemerides = sun_model(match_x = np.array(jdb_dace))
                model += ephemerides.y / 1000

            drift_used = dico_match["drift_used"].astype("float") / 1000
            drift_used[np.isnan(drift_used)] = 0
            model += drift_used

            dico_match["model"] = model

            time.sleep(1)
            plt.close("all")
            time.sleep(1)

            if not os.path.exists(path + k + "/DACE_TABLE/"):

                os.system("mkdir " + path + k + "/DACE_TABLE/")

            dico_match.to_csv(path + k + "/DACE_TABLE/Dace_extracted_table.csv")


def find_stellar_mass_radius(Teff, sp_type="G2V"):
    """Habets 1981 calibration curve"""
    lim = 0
    for k in sp_type[::-1]:
        try:
            int(k)
            break
        except:
            lim += 1

    class_lum = sp_type[len(sp_type) - lim :]
    if class_lum == "":
        class_lum = "V"
    if class_lum != "V":
        class_lum = "IV"
    calib = pd.read_pickle(root + "/Python/Material/logT_logM_logR.p")[class_lum]
    curve_mass = myc.tableXY(10 ** calib["log(T)"], 10 ** calib["log(M/Ms)"])
    curve_radius = myc.tableXY(10 ** calib["log(T)"], 10 ** calib["log(R/Rs)"])
    curve_mass.interpolate(new_grid=np.array([Teff]))
    curve_radius.interpolate(new_grid=np.array([Teff]))
    m = curve_mass.y
    r = curve_radius.y
    log_g = 2 + np.log10(6.67e-11 * (m * 1.98e30) / (r * 696342000) ** 2)
    return m[0], r[0], log_g[0]


def split_instrument(parent_dir, instrument="HARPS"):
    """
    Split spectra in subdirectories based on the jdb values (modification of the instruments)

    Parameters
    ----------

    fiber_changed : borders of the jdb value to split the spectra in different directories
    HARPS = fiber_changed=[0,57161.5,100000]
    HARPN = fiber_changed=[0,56735.5,100000]

    """

    if instrument == "HARPS":
        fiber_changed = [0, 57161.5, 100000]
        new_dir = ["HARPS03", "HARPS15"]
    elif instrument == "HARPN2":  # no used anymore
        fiber_changed = [0, 56738, 57550, 100000]
        new_dir = ["HARPN01", "HARPN02", "HARPN03"]
    elif instrument == "CORALIE":  # no used anymore
        fiber_changed = [0, 54200, 56900, 100000]
        new_dir = ["CORALIE98", "CORALIE07", "CORALIE14"]
    elif instrument == "HARPN":  # no used anymore
        fiber_changed = [0, 100000]
        new_dir = ["HARPN"]
    elif instrument == "EXPRESS":  # no used anymore
        fiber_changed = [0, 100000]
        new_dir = ["EXPRESS"]
    elif instrument == "CARMENES":  # no used anymore
        fiber_changed = [0, 100000]
        new_dir = ["CARMENES"]
    elif instrument == "ESPRESSO18":  # no used anymore
        fiber_changed = [0, 100000]
        new_dir = ["ESPRESSO18"]
    elif instrument == "ESPRESSO19":  # no used anymore
        fiber_changed = [0, 100000]
        new_dir = ["ESPRESSO19"]

    count = -1

    dace_table = pd.read_csv(
        parent_dir + "DACE_TABLE/Dace_extracted_table.csv", index_col=0, low_memory=False
    )

    for j in range(len(fiber_changed) - 1):
        count += 1

        selection_dace = dace_table.loc[
            (dace_table["rjd"] > fiber_changed[j]) & (dace_table["rjd"] < fiber_changed[j + 1])
        ]
        selection_dace = selection_dace.reset_index(drop=True)

        if len(selection_dace) > 1:
            new_path = "/".join(parent_dir.split("/")[0:-2]) + "/" + new_dir[count]
            selection_dace["instrument"] = new_dir[count]

            if not os.path.exists(new_path):
                os.system("mkdir " + new_path)
                print("\n Directory : %s successfully created" % (new_path))
                time.sleep(1)

            if not os.path.exists(new_path + "/DACE_TABLE"):
                os.system("mkdir " + new_path + "/DACE_TABLE")

            # for idx in tqdm(selection_dace['fileroot'].index):
            #     file = selection_dace.loc[idx,'fileroot']
            #      if os.path.exists(file):
            #          new_name = new_path+'/'+file.split('/')[-1]
            #          selection_dace.loc[idx,'fileroot'] = new_name
            #          os.system('cp '+file+' '+new_name)

            selection_dace.to_csv(new_path + "/DACE_TABLE/Dace_extracted_table.csv")


class spec_time_series(object):
    def __init__(self, directory):
        if len(directory.split("/")) == 1:
            directory = root + "/Yarara/" + directory + "/data/s1d/HARPS03/WORKSPACE/"
        if directory[-1] != "/":
            directory = directory + "/"

        self.directory = directory
        self.starname = directory.split("/Yarara/")[-1].split("/")[0].split("=")[0]
        self.dir_root = directory.split("WORKSPACE/")[0]
        self.dir_yarara = directory.split("Yarara/")[0] + "Yarara/"
        self.cmap = "plasma"
        self.low_cmap = -0.005
        self.high_cmap = 0.005
        self.zoom = 1
        self.smooth_map = 1
        self.planet = False
        self.sp_type = None
        self.rv_sys = None
        self.teff = None
        self.log_g = None
        self.bv = None
        self.fwhm = None
        self.wave = None
        self.infos = {}
        self.ram = []

        self.dico_actif = "matching_diff"

        self.all_dicos = [
            "matching_diff",
            "matching_cosmics",
            "matching_fourier",
            "matching_telluric",
            "matching_oxy_bands",
            "matching_oxygen",
            "matching_pca",
            "matching_activity",
            "matching_ghost_a",
            "matching_ghost_b",
            "matching_database",
            "matching_stitching",
            "matching_berv",
            "matching_thar",
            "matching_contam",
            "matching_smooth",
            "matching_profile",
            "matching_mad",
        ]

        self.light_dicos = ["matching_diff", "matching_pca", "matching_activity", "matching_mad"]

        self.planet_fitted = {}

        if not os.path.exists(self.directory + "Analyse_ccf.p"):
            ccf_summary = {"star_info": {"name": self.starname}}
            myf.pickle_dump(ccf_summary, open(self.directory + "Analyse_ccf.p", "wb"))

        if not os.path.exists(self.directory + "Analyse_material.p"):
            file = pd.read_pickle(glob.glob(self.directory + "RASSI*.p")[0])
            wave = file["wave"]
            dico = {
                "wave": wave,
                "correction_factor": np.ones(len(wave)),
                "reference_spectrum": np.ones(len(wave)),
                "color_template": np.ones(len(wave)),
                "blaze_correction": np.ones(len(wave)),
                "rejected": np.zeros(len(wave)),
            }
            dico = pd.DataFrame(dico)
            myf.pickle_dump(dico, open(self.directory + "Analyse_material.p", "wb"))

        if os.path.exists(self.directory + "/RASSINE_Master_spectrum.p"):
            master = pd.read_pickle(self.directory + "/RASSINE_Master_spectrum.p")
            master = master["flux"] / master["output"]["continuum_linear"]
            os.system(
                "mv "
                + self.directory
                + "RASSINE_Master_spectrum.p "
                + self.directory
                + "ras_Master_spectrum.p "
            )

        if not os.path.exists(self.dir_root + "IMAGES/"):
            os.system("mkdir " + self.dir_root + "IMAGES/")

        if not os.path.exists(self.dir_root + "CCF_MASK/"):
            os.system("mkdir " + self.dir_root + "CCF_MASK/")

        if not os.path.exists(self.dir_root + "PCA/"):
            os.system("mkdir " + self.dir_root + "PCA/")

        if not os.path.exists(self.dir_root + "REDUCTION_INFO/"):
            os.system("mkdir " + self.dir_root + "REDUCTION_INFO/")

        if not os.path.exists(self.dir_root + "STAR_INFO/"):
            os.system("mkdir " + self.dir_root + "STAR_INFO/")

        if not os.path.exists(self.dir_root + "STAR_INFO/Stellar_info_" + self.starname + ".p"):
            dico = {
                "Name": self.starname,
                "Simbad_name": {"fixed": "-"},
                "Sp_type": {"fixed": "G2V"},
                "Ra": {"fixed": "00 00 00.0000"},
                "Dec": {"fixed": "00 00 00.0000"},
                "Pma": {"fixed": 0.0},
                "Pmd": {"fixed": 0.0},
                "Rv_sys": {"fixed": 0.0},
                "Mstar": {"fixed": 1.0},
                "Rstar": {"fixed": 1.0},
                "magU": {"fixed": -26.0},
                "magB": {"fixed": -26.2},
                "magV": {"fixed": -26.8},
                "magR": {"fixed": -26.8},
                "UB": {"fixed": 0.0},
                "BV": {"fixed": 0.6},
                "VR": {"fixed": 0.0},
                "Dist_pc": {"fixed": 0.0},
                "Teff": {"fixed": 5775},
                "Log_g": {"fixed": 4.5},
                "FeH": {"fixed": 0.0},
                "Vsini": {"fixed": 2.0},
                "Vmicro": {"fixed": 1.0},
                "Prot": {"fixed": 25},
                "Pmag": {"fixed": 11},
                "FWHM": {"fixed": 6.0},
                "Contrast": {"fixed": 0.5},
                "CCF_delta": {"fixed": 5},
                "stellar_template": {"fixed": "MARCS_T5750_g4.5"},
            }

            myf.pickle_dump(
                dico, open(self.dir_root + "STAR_INFO/Stellar_info_" + self.starname + ".p", "wb")
            )

        self.import_star_info()
        sp = self.star_info["Sp_type"]["fixed"][0]
        self.mask_harps = ["G2", "K5", "M2"][int((sp == "K") | (sp == "M")) + int(sp == "M")]
        try:
            self.fwhm = self.star_info["FWHM"]["YARARA"]
        except:
            self.fwhm = self.star_info["FWHM"]["fixed"]

        try:
            self.contrast = self.star_info["Contrast"]["YARARA"]
        except:
            self.contrast = self.star_info["Contrast"]["fixed"]

        if not os.path.exists(self.dir_root + "KEPLERIAN/"):
            os.system("mkdir " + self.dir_root + "KEPLERIAN/")

        if not os.path.exists(self.dir_root + "KEPLERIAN/MCMC/"):
            os.system("mkdir " + self.dir_root + "KEPLERIAN/MCMC/")

        if not os.path.exists(self.dir_root + "PERIODOGRAM/"):
            os.system("mkdir " + self.dir_root + "PERIODOGRAM/")

        if not os.path.exists(self.dir_root + "FILM"):
            os.system("mkdir " + self.dir_root + "FILM")

        if not os.path.exists(self.dir_root + "DETECTION_LIMIT/"):
            os.system("mkdir " + self.dir_root + "DETECTION_LIMIT/")

        if not os.path.exists(self.dir_root + "CORRECTION_MAP/"):
            os.system("mkdir " + self.dir_root + "CORRECTION_MAP/")

    def gen_rand(self):
        return myc.tableXY(np.linspace(0, 10, 10), np.random.randn(10))

    def flux_backup(self):
        """
        Replace the flux values to their initial values

        """

        directory = self.directory
        files = glob.glob(directory + "RASSI*.p")
        for j in tqdm(files):
            file = pd.read_pickle(j)
            file["flux"] = file["flux_backup"].copy()
            del file["flux_backup"]
            ras.save_pickle(j, file)

    def scale_cmap(self):
        a, b, c = self.yarara_map(
            reference="median", sub_dico="matching_diff", wave_min=3000, wave_max=5000, Plot=False
        )
        med = np.nanmedian(b)
        Q3 = np.nanpercentile(b, 75)
        Q1 = np.nanpercentile(b, 25)
        clim = np.max([Q3 - med, med - Q1]) + 1.5 * (Q3 - Q1)
        print(" [INFO] Zscale color scaled to : %.3f" % (clim))
        self.low_cmap = -clim
        self.high_cmap = clim

    def flux_error(self, ron=11):  # temporary solution because no error with harps s1d
        """
        Produce flux errors on the pickles files defined as the ron plus square root of flux

        Parameters
        ----------
        ron : Read-out-Noise value of the reading detector errors

        Returns
        -------

        """

        self.import_material()
        file = self.import_spectrum()
        correction = np.array(self.material["blaze_correction"])

        directory = self.directory
        files = glob.glob(directory + "RASSI*.p")
        for j in tqdm(files):
            file = pd.read_pickle(j)
            if not np.median(abs(file["flux_err"])):
                file["flux_err"] = ron + np.sqrt(abs(file["flux"])) * correction
                ras.save_pickle(j, file)

    def continuum_error(self):
        """
        Produce error on the continuum by interpolating the error flux value at anchor points

        Parameters
        ----------

        Returns
        -------

        """
        directory = self.directory
        files = glob.glob(directory + "RASSI*.p")
        for j in tqdm(files):
            file = pd.read_pickle(j)
            vec = myc.tableXY(
                file["matching_anchors"]["anchor_wave"],
                file["flux_err"][file["matching_anchors"]["anchor_index"]],
            )
            vec.interpolate(new_grid=file["wave"], method="linear", interpolate_x=False)
            file["continuum_err"] = vec.yerr
            ras.save_pickle(j, file)

    def split_mask_epochs(self, mask, ext="_0"):
        """
        Split epocs datasets in subdirectories and changing the starname path

        Parameters
        ----------

        mask: borders of the jdb value to split the spectra in different directories
        ext : extension added at the end of the starname path

        """

        self.import_table()
        self.import_material()
        self.import_ccf()

        parent_dir = self.dir_root

        dace_table = pd.read_csv(
            parent_dir + "DACE_TABLE/Dace_extracted_table.csv", index_col=0, low_memory=False
        )

        summary = self.table.loc[mask]
        summary = summary.reset_index(drop=True)

        load = self.material
        table_ccf = self.table_ccf

        liste = list(table_ccf.keys())[1:]
        for i in liste:
            for k in table_ccf[i].keys():
                table_ccf[i][k]["table"] = table_ccf[i][k]["table"].loc[mask]
                table_ccf[i][k]["table"] = table_ccf[i][k]["table"].reset_index(drop=True)

        if sum(mask) > 1:
            starname = parent_dir.split("Yarara/")[1].split("/")[0]

            new_path = parent_dir.replace(starname, starname + ext)

            sub = ["/".join(new_path.split("/")[0:i]) for i in range(2, len(new_path.split("/")))]

            for s in sub:
                if not os.path.exists(s):
                    os.system("mkdir " + s)
                    print("\n Directory : %s successfully created" % (s))
                    time.sleep(1)

            if not os.path.exists(new_path + "/DACE_TABLE"):
                os.system("mkdir " + new_path + "/DACE_TABLE")

            dace_table.to_csv(new_path + "/DACE_TABLE/Dace_extracted_table.csv")

            os.system("cp -rf " + parent_dir + "/MASTER" + " " + new_path)

            if not os.path.exists(new_path + "/WORKSPACE"):
                os.system("mkdir " + new_path + "/WORKSPACE")

            for i in summary.index:
                spec = summary.loc[i, "filename"]
                new_spec = spec.replace(starname, starname + ext)
                os.system("cp " + spec + " " + new_spec)
                print("Spectrum %s duplicated in %s " % (spec, new_path))
                summary.loc[i, "filename"] = new_spec

            summary.to_csv(new_path + "/WORKSPACE/Analyse_summary.csv")
            myf.pickle_dump(summary, open(new_path + "/WORKSPACE/Analyse_summary.p", "wb"))
            myf.pickle_dump(load, open(new_path + "/WORKSPACE/Analyse_material.p", "wb"))
            myf.pickle_dump(table_ccf, open(new_path + "/WORKSPACE/Analyse_ccf.p", "wb"))

    def spectrum(
        self, num=0, sub_dico="matching_diff", continuum="linear", norm=False, planet=False
    ):
        """
        Produce a tableXY spectrum by specifying its index number

        Parameters
        ----------
        num : index of the spectrum to extract
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        norm : True/False button to normalise the spectrum

        Returns
        -------
        Return the tableXY spectrum object

        """
        array = self.import_spectrum(num=num)
        kw = "_planet" * planet

        flux = array["flux" + kw]
        flux_std = array["flux_err"]
        wave = array["wave"]
        correction = (
            array["matching_diff"]["continuum_" + continuum]
            / array[sub_dico]["continuum_" + continuum]
        )
        spectrum = myc.tableXY(wave, flux * correction, flux_std * correction)
        if norm:
            continuum = array[sub_dico]["continuum_" + continuum]
            continuum_std = array["continuum_err"]
            flux_norm, flux_norm_std = myf.flux_norm_std(flux, flux_std, continuum, continuum_std)
            spectrum_norm = myc.tableXY(wave, flux_norm, flux_norm_std)
            return spectrum_norm
        else:
            return spectrum

    # =============================================================================
    # IMPORT ALL RASSINE DICTIONNARY
    # =============================================================================

    def import_rassine_output(self, return_name=False, kw1=None, kw2=None):
        """
        Import all the RASSINE dictionnaries in a list

        Parameters
        ----------
        return_name : True/False to also return the filenames

        Returns
        -------
        Return the list containing all thedictionnary

        """

        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        if len(files) <= 1:  # 1 when merged directory
            print("No RASSINE file found in the directory : %s" % (directory))
            if return_name:
                return [], []
            else:
                return []
        else:
            files = np.sort(files)
            file = []
            for i, j in enumerate(files):
                self.debug = j
                file.append(pd.read_pickle(j))

                if kw1 is not None:
                    file[-1] = file[-1][kw1]

                if kw2 is not None:
                    file[-1] = file[-1][kw2]

            if return_name:
                return file, files
            else:
                return file

    def import_dace_sun(self, instrument):
        data_extract = Sun.get_timeseries()
        data_extract = pd.DataFrame(data_extract)
        data_extract = data_extract.loc[data_extract["obs_quality"] > 0.90]
        data_extract["raw_file"] = data_extract["filename"]
        data_extract["sindex"] = data_extract["smw"]
        data_extract["sindex_err"] = data_extract["smw_err"]
        data_extract["drs_qc"] = data_extract["obs_quality"] > 0.90
        data_extract["spectroFluxSn50"] = data_extract["sn_order_50"]
        data_extract["rjd"] = data_extract["date_bjd"]
        data_extract["bispan"] = data_extract["bis_span"]
        data_extract["drift_used"] = 0
        data_extract["drift_noise"] = 0.10
        data_extract["berv"] /= 1000
        data_extract["ccf_noise"] = data_extract["rv_err"]

        return data_extract

    def import_mask_harps(self, stellar_frame=False):

        mask_loc = root + "/Python/MASK_CCF/" + self.mask_harps + ".txt"
        mask = np.genfromtxt(mask_loc)
        mask = np.array([0.5 * (mask[:, 0] + mask[:, 1]), mask[:, 2]]).T
        if stellar_frame:
            self.import_star_info()
            mask[:, 0] = myf.doppler_r(mask[:, 0], self.star_info["Rv_sys"]["fixed"] * 1000)[0]
        self.ccf_mask_harps = mask

    def import_photometry(self, which=None, periodogram=False):
        """which : Kepler, TESS, K2"""
        directory = self.dir_root
        directory = "/".join(directory.split("/")[0:-3]) + "/photometry"

        lightcurve = False

        if which is None:
            which = ["Kepler", "K2", "TESS"]
        else:
            which = [which]

        for w in which:
            files = glob.glob(directory + "/*" + self.starname + "*" + w + "*")

            if len(files):
                table = pd.read_csv(files[0], index_col=0)
                vec = myc.tableXY(table[table.columns[0]], table[table.columns[1]])
                try:
                    vec.y -= table[table.columns[2]]  # correction ?
                except:
                    pass
                lightcurve = True
            else:
                database = pd.read_pickle(root + "/Python/database/LIGHTCURVE/lk_database.p")
                if self.starname in database.keys():
                    database = database[self.starname]
                    if w in database.keys():
                        vec = database[w]
                        lightcurve = True
            if lightcurve:
                print("\n [INFO] Lightcurve found for instrument : %s \n" % (w))
                break

        if lightcurve:
            vec.split_seasons()
            vector = []
            for k in vec.seasons_splited:
                k.substract_polyfit(2, replace=True)
                vector.append(list(k.y))

            vector = myc.tableXY(vec.x, np.hstack(vector))

            if periodogram:
                vec.periodogram(Plot=False)

                vec.periodogram_keplerian_hierarchical(
                    Plot=True, periods=[vec.perio_max], fit_ecc=False
                )
                plt.close("all")

                plt.figure(2, figsize=(18, 6))
                plt.subplot(2, 1, 1)
                vec.plot()
                vector.plot(color="r")
                plt.xlabel("Time [days]", fontsize=14)
                plt.ylabel(r"$\Delta$ mag", fontsize=14)

                plt.subplot(2, 1, 2)
                vec.periodogram(Norm=True, legend="raw")
                if vec.perio_max > 100:
                    vec.rv_residues.periodogram(color="b", Norm=True, legend="kep fitted")
                vector.periodogram(color="r", Norm=True, legend="seasons detrending")
                plt.legend()
                plt.xlim(2, 2000)
                plt.subplots_adjust(top=0.95, left=0.10, right=0.95, hspace=0.3)
                plt.savefig(self.dir_root + "IMAGES/Photometry.pdf")

                plt.figure()
                vector.periodogram_rolling(prot_estimated=vector.perio_max)
                plt.savefig(self.dir_root + "IMAGES/Photometry2.jpg")

            self.photometry = vec
        else:
            print("No photometry found for instrument : %s" % (which))

    def import_telluric_water(self, ext="", method="cubic"):
        self.import_material()
        water_model = pd.read_pickle(root + "/Python/Material/model_telluric_water" + ext + ".p")
        water_model = myc.tableXY(
            water_model["wave"], water_model["flux_norm"], 0 * water_model["wave"]
        )
        water_model.interpolate(
            new_grid=np.array(self.material["wave"]), replace=True, method=method
        )
        self.water_model = water_model

    def import_telluric_oxygen(self, ext="", method="cubic"):
        self.import_material()
        oxy_model = pd.read_pickle(root + "/Python/Material/model_telluric_oxygen" + ext + ".p")
        oxy_model = myc.tableXY(oxy_model["wave"], oxy_model["flux_norm"], 0 * oxy_model["wave"])
        oxy_model.interpolate(
            new_grid=np.array(self.material["wave"]), replace=True, method=method
        )
        self.oxy_model = oxy_model

    def import_telluric(self, ext="", method="cubic"):
        self.import_material()
        telluric_model = pd.read_pickle(root + "/Python/Material/model_telluric" + ext + ".p")
        telluric_model = myc.tableXY(
            telluric_model["wave"], telluric_model["flux_norm"], 0 * telluric_model["wave"]
        )
        telluric_model.interpolate(
            new_grid=np.array(self.material["wave"]), replace=True, method=method
        )
        self.telluric_model = telluric_model

    def import_dace_query(self, instrument, drs_version="old"):

        if instrument == "INS_MERGED":
            instrument = ["HARPS03", "HARPS15", "HARPN"]
        else:
            instrument = [instrument]

        dico = {
            "HARPS03": ["HARPS", "3.5"],
            "HARPS15": ["HARPS", "3.5"],
            "ESPRESSO18": ["SINGLEHR11", "2.2.8-HR11"],
            "ESPRESSO19": ["SINGLEHR11", "2.2.8-HR11"],
            "HARPN": ["HARPN", ["3.7", "2.3.1"][drs_version != "old"]],
        }

        arcfiles = np.hstack(self.import_rassine_output(kw1="parameters", kw2="arcfiles"))

        df = pd.DataFrame(arcfiles)
        df[0] = df[0].str[-37:-8]

        for i in instrument:
            val = dico[i]
            if self.starname == "Sun":
                data_extract = self.import_dace_sun(instrument)
            else:
                data_extract = pd.DataFrame(
                    Spectroscopy.get_timeseries(self.starname)[i][val[1]][val[0]]
                )
                if i[0:8] == "ESPRESSO":
                    if not len(data_extract):
                        val[1] = val[1].replace("HR11", "HR21")
                        val[0] = "SINGLEHR21"
                        print(
                            "[INFO] mode %s detected for ESPRESSO drs version %s"
                            % (val[1], val[0])
                        )
                        data_extract = pd.DataFrame(
                            Spectroscopy.get_timeseries(self.starname)[i][val[1]][val[0]]
                        )
            data_extract["raw_file"] = data_extract["raw_file"].str[:-5]
            if (drs_version == "new") & (i == "HARPN"):
                vec = np.array(
                    [
                        string.split("T")[0].split("r.")[-1]
                        + "T"
                        + string.split("T")[1].replace("-", ":")
                        for string in np.array(data_extract["raw_file"])
                    ]
                )
                data_extract["raw_file"] = vec
            data_extract = data_extract.merge(
                ["", "r."][i[0:8] == "ESPRESSO"] + df, left_on="raw_file", right_on=0
            )

        return data_extract

    def import_drift_night(self, instrument, db=0, bin_length=1, drs_version="old"):
        self.import_table()
        table_dace = self.import_dace_query(instrument, drs_version=drs_version)
        drift = myc.tableXY(
            np.array(table_dace["rjd"]).astype("float"),
            np.array(table_dace["drift_used"]).astype("float"),
            np.array(table_dace["drift_noise"]).astype("float"),
        )
        drift.y[np.isnan(drift.y)] = 0
        drift.yerr[np.isnan(drift.yerr)] = 1
        if len(drift.x) == 0:
            print(" [ERROR] DACE query did not found any data (check the drs version)")
        drift.night_stack(replace=True, db=db, bin_length=bin_length)

        if len(self.table) == len(drift.x):
            self.yarara_obs_info(kw=["drift_used", drift.y])
            self.yarara_obs_info(kw=["drift_used_std", drift.yerr])

    def pickle_corrupted(self, suppress=False):
        self.import_table()
        files = self.table["filename"]
        corrupted = []
        mask = []
        for i, f in enumerate(files):
            try:
                pd.read_pickle(f)
                mask.append(False)
            except:
                corrupted.append([i, f])
                mask.append(True)
        corrupted = np.array(corrupted)
        mask = np.array(mask)
        if suppress:
            self.supress_time_spectra(liste=mask)
        else:
            return corrupted

    def pickle_protocol(self, to_protocol=3):
        all_pickles = glob.glob(self.dir_root + "/*/*.p")
        for pickle_file in tqdm(all_pickles):
            read_file = pd.read_pickle(pickle_file)
            time.sleep(0.1)  # to not underestimate pickle corruption
            myf.pickle_dump(read_file, open(pickle_file, "wb"), protocol=to_protocol)

    def import_obs_dace_night(self, obs, instrument, db=0, bin_length=1, drs_version="old"):
        self.import_table()
        table_dace = self.import_dace_query(instrument, drs_version=drs_version)
        if obs != "drift":
            vec = myc.tableXY(table_dace["rjd"], table_dace[obs], table_dace[obs + "_err"])
        else:
            vec = myc.tableXY(
                table_dace["rjd"], table_dace["drift_used"], table_dace["drift_noise"]
            )

        vec.night_stack(replace=True, db=db, bin_length=bin_length)

        return vec

    def import_rv_dace(
        self, instrument, db=0, bin_length=1, calib_std=0.7, save=True, drs_version="old"
    ):
        self.import_table()
        table_dace = self.import_dace_query(instrument, drs_version=drs_version)
        self.debug = table_dace
        # rv_err = np.nansum([np.array(table_dace['ccf_noise']).astype('float')**2,np.array(table_dace['drift_noise']).astype('float')**2],axis=0)
        # if np.sum(abs(rv_err))==0:
        rv_err = abs(
            np.array(table_dace["rv_err"]).astype("float") ** 2 - calib_std**2
        )  # take the rv_err instead of calib because absurd values 06.07.21
        vec = myc.tableXY(table_dace["rjd"], table_dace["rv"], np.sqrt(rv_err))
        vec.night_stack(replace=True, db=db, bin_length=bin_length)
        vec.yerr = np.sqrt(vec.yerr**2 + calib_std**2)
        if len(self.table) == len(vec.x):
            vec.y -= np.array(self.table["rv_sec"])
            if save:
                self.yarara_obs_info(kw=["rv_dace_std", vec.yerr])
                self.yarara_obs_info(kw=["rv_dace", vec.y])
            else:
                return vec

    # def import_rv_std_night(self,instrument,db=0,bin_length=1):
    #     self.import_table()
    #     table_dace = self.import_dace_query(instrument)
    #     drift = myc.tableXY(table_dace['rjd'],table_dace['drift_used'],np.sqrt(table_dace['ccf_noise']**2+table_dace['drift_noise']**2))
    #     drift.night_stack(replace=True, db=db, bin_length=bin_length)
    #     if len(self.table)==len(drift.x):
    #         drift.yerr = np.sqrt(drift.yerr**2+0.7**2)
    #         self.yarara_obs_info(kw=['rv_dace_std',drift.yerr])

    # =============================================================================
    # IMPORT SUMMARY TABLE
    # =============================================================================

    def import_star_info(self):
        self.star_info = pd.read_pickle(
            self.dir_root + "STAR_INFO/Stellar_info_" + self.starname + ".p"
        )

    def import_table(self):
        self.table = pd.read_pickle(self.directory + "Analyse_summary.p")

    def import_ref_spectrum(self, star=None, ins=None):

        self.import_material()
        if star is not None:
            all_files = glob.glob(
                root + "/YARARA/" + star + "/data/s1d/*/WORKSPACE/Analyse_material.p"
            )
            all_ins = [i.split("/WORK")[0].split("/")[-1] for i in all_files]
            length = np.array([2 - len(i.split("_")) for i in all_ins]).astype("bool")
            all_ins = np.array(all_ins)[length]
            all_files = np.array(all_files)[length]
            if (len(all_ins) > 1) & (ins is None):
                print(
                    "There is saveral instrument for star %s, please select one : " % (star),
                    list(all_ins),
                )
            else:
                if len(all_ins) > 1:
                    loc = np.where(all_ins == ins)[0][0]
                else:
                    loc = 0
                load = pd.read_pickle(all_files[loc])
                vec = myc.tableXY(
                    load["wave"], load["reference_spectrum"], 0 * load["reference_spectrum"]
                )
                vec.interpolate(
                    new_grid=np.array(self.material["wave"]), method="linear", replace=True
                )
                vec.y[vec.x < np.min(load["wave"])] = 0
                vec.y[vec.x > np.max(load["wave"])] = 0
                self.ref_spectrum = vec.copy()
        else:
            self.ref_spectrum = myc.tableXY(
                self.material["wave"],
                self.material["reference_spectrum"],
                0 * self.material["wave"],
            )

    def import_snr_curve(self):
        self.table_snr = pd.read_pickle(self.dir_root + "WORKSPACE/Analyse_snr.p")

    def import_berv(self):
        self.import_table()
        self.berv = myc.tableXY(self.table.jdb, self.table.berv)

    def import_proxies(self, sub_dico=None):
        self.import_table()
        self.import_ccf()

        self.ca2 = myc.tableXY(self.table["jdb"], self.table["CaII"], self.table["CaII_std"])
        self.ca2h = myc.tableXY(self.table["jdb"], self.table["CaIIH"], self.table["CaIIH_std"])
        self.ca2k = myc.tableXY(self.table["jdb"], self.table["CaIIK"], self.table["CaIIK_std"])
        self.ca1 = myc.tableXY(self.table["jdb"], self.table["CaI"], self.table["CaI_std"])
        self.rhk = myc.tableXY(self.table["jdb"], self.table["RHK"], self.table["RHK_std"])
        self.kernel_ca2 = myc.tableXY(
            self.table["jdb"], self.table["Kernel_CaII"], self.table["Kernel_CaII_std"]
        )
        self.kernel_wb = myc.tableXY(
            self.table["jdb"], self.table["Kernel_WB"], self.table["Kernel_WB_std"]
        )
        self.nad = myc.tableXY(self.table["jdb"], self.table["NaD"], self.table["NaD_std"])
        self.nad1 = myc.tableXY(self.table["jdb"], self.table["NaD1"], self.table["NaD1_std"])
        self.nad2 = myc.tableXY(self.table["jdb"], self.table["NaD2"], self.table["NaD2_std"])
        self.ha = myc.tableXY(self.table["jdb"], self.table["Ha"], self.table["Ha_std"])
        self.hb = myc.tableXY(self.table["jdb"], self.table["Hb"], self.table["Hb_std"])
        self.hc = myc.tableXY(self.table["jdb"], self.table["Hc"], self.table["Hc_std"])
        self.hd = myc.tableXY(self.table["jdb"], self.table["Hd"], self.table["Hd_std"])
        self.hed3 = myc.tableXY(self.table["jdb"], self.table["HeID3"], self.table["HeID3_std"])
        self.mg1 = myc.tableXY(self.table["jdb"], self.table["MgI"], self.table["MgI_std"])
        self.mg1a = myc.tableXY(self.table["jdb"], self.table["MgIa"], self.table["MgIa_std"])
        self.mg1b = myc.tableXY(self.table["jdb"], self.table["MgIb"], self.table["MgIb_std"])
        self.mg1c = myc.tableXY(self.table["jdb"], self.table["MgIc"], self.table["MgIc_std"])
        self.wb = myc.tableXY(self.table["jdb"], self.table["WB"], self.table["WB_std"])
        self.bis = myc.tableXY(self.table["jdb"], self.table["BIS"], self.table["BIS_std"])
        self.bis2 = myc.tableXY(self.table["jdb"], self.table["BIS2"], self.table["BIS2_std"])
        self.cb = myc.tableXY(self.table["jdb"], self.table["CB"], self.table["CB_std"])
        self.cb2 = myc.tableXY(self.table["jdb"], self.table["CB2"], self.table["CB2_std"])
        self.sas = myc.tableXY(self.table["jdb"], self.table["SAS"], self.table["SAS_std"])

        if sub_dico is None:
            try:
                self.import_dico_chain("matching_mad")
                loc = np.where(self.dico_chain == "matching_activity")[0][0]
                sb = self.dico_chain[loc + 1]
            except:
                sb = "matching_diff"
        else:
            sb = sub_dico

        ccf_masks = list(self.table_ccf.keys())
        if sb in list(self.table_ccf[ccf_masks[1]].keys()):
            ccf_mask = ccf_masks[1]
        elif sb in list(self.table_ccf[ccf_masks[2]].keys()):
            ccf_mask = ccf_masks[2]
        else:
            ccf_mask = ccf_masks[3]

        print("\n [INFO] Dictionnary selected for CCF proxy : %s" % (sb))
        print("\n [INFO] Mask selected for CCF proxy : %s" % (ccf_mask))

        fwhm = self.import_ccf_timeseries(ccf_mask, sb, "fwhm")
        contrast = self.import_ccf_timeseries(ccf_mask, sb, "contrast")
        vspan = self.import_ccf_timeseries(ccf_mask, sb, "bisspan")

        self.ccf_harps_fwhm = fwhm
        self.ccf_harps_contrast = contrast
        self.ccf_harps_vspan = vspan

        table = self.table.copy()
        table["contrast"] = contrast.y
        table["fwhm"] = fwhm.y
        table["vspan"] = vspan.y
        table["contrast_std"] = contrast.yerr
        table["fwhm_std"] = fwhm.yerr
        table["vspan_std"] = vspan.yerr

        table_proxy = table[
            [
                "jdb",
                "CaII",
                "CaIIH",
                "CaIIK",
                "CaI",
                "RHK",
                "Kernel_CaII",
                "NaD",
                "NaD1",
                "NaD2",
                "Ha",
                "Hb",
                "Hc",
                "Hd",
                "HeID3",
                "MgI",
                "MgIa",
                "MgIb",
                "MgIc",
                "WB",
                "CB",
                "CB2",
                "BIS",
                "BIS2",
            ]
        ]

        self.table_proxy2 = table[
            [
                "CaII",
                "CaIIH",
                "CaIIK",
                "CaI",
                "RHK",
                "Kernel_CaII",
                "WB",
                "NaD",
                "NaD1",
                "NaD2",
                "Ha",
                "Hb",
                "Hc",
                "Hd",
                "HeID3",
                "MgI",
                "MgIa",
                "MgIb",
                "MgIc",
                "contrast",
                "fwhm",
                "vspan",
            ]
        ]

        self.table_proxy2_std = table[
            [
                "CaII_std",
                "CaIIH_std",
                "CaIIK_std",
                "CaI_std",
                "RHK_std",
                "Kernel_CaII_std",
                "WB_std",
                "NaD_std",
                "NaD1_std",
                "NaD2_std",
                "Ha_std",
                "Hb_std",
                "Hc_std",
                "Hd_std",
                "HeID3_std",
                "MgI_std",
                "MgIa_std",
                "MgIb_std",
                "MgIc_std",
                "contrast_std",
                "fwhm_std",
                "vspan_std",
            ]
        ]

    def import_ccf(self):
        self.table_ccf = pd.read_pickle(self.directory + "Analyse_ccf.p")

    def import_ccf_profile(self):
        self.table_ccf_saved = pd.read_pickle(self.directory + "Analyse_ccf_saved.p")

    def import_ccf_timeseries(self, mask, sub_dico, kw):

        self.import_ccf()
        self.import_table()

        valid = True
        if len(mask) < 7:
            if mask not in list(self.table_ccf.keys()):
                valid = False
            else:
                if sub_dico not in self.table_ccf[mask]:
                    valid = False
        if not valid:
            all_mask = []
            for mask_iter in self.table_ccf.keys():
                if sub_dico in self.table_ccf[mask_iter]:
                    all_mask.append(mask_iter)
            print(
                " [WARNING] No CCF found for sub dico %s and CCF mask %s, switch for : %s"
                % (sub_dico, mask, all_mask[0])
            )
            mask = all_mask[0]

        self.ccf_rv = myc.tableXY(
            np.array(self.table_ccf[mask][sub_dico]["table"]["jdb"]),
            np.array(self.table_ccf[mask][sub_dico]["table"]["rv"]) * 1000,
            np.array(self.table_ccf[mask][sub_dico]["table"]["rv_std"]) * 1000,
        )
        self.ccf_fwhm = myc.tableXY(
            np.array(self.table_ccf[mask][sub_dico]["table"]["jdb"]),
            np.array(self.table_ccf[mask][sub_dico]["table"]["fwhm"]),
            np.array(self.table_ccf[mask][sub_dico]["table"]["fwhm_std"]),
        )
        self.ccf_contrast = myc.tableXY(
            np.array(self.table_ccf[mask][sub_dico]["table"]["jdb"]),
            np.array(self.table_ccf[mask][sub_dico]["table"]["contrast"]),
            np.array(self.table_ccf[mask][sub_dico]["table"]["contrast_std"]),
        )
        try:
            self.ccf_vspan = myc.tableXY(
                np.array(self.table_ccf[mask][sub_dico]["table"]["jdb"]),
                np.array(self.table_ccf[mask][sub_dico]["table"]["bisspan"]),
                np.array(self.table_ccf[mask][sub_dico]["table"]["bisspan_std"]),
            )
        except:
            self.ccf_vspan = myc.tableXY(
                np.array(self.table_ccf[mask][sub_dico]["table"]["jdb"]),
                np.array(self.table_ccf[mask][sub_dico]["table"]["bisspan"]),
                np.array(self.table_ccf[mask][sub_dico]["table"]["rv_std"]),
            )

        factor = [1, 1000][kw == "rv"]

        vec = myc.tableXY(
            np.array(self.table_ccf[mask][sub_dico]["table"]["jdb"]),
            np.array(self.table_ccf[mask][sub_dico]["table"][kw]) * factor,
            self.table.rv_dace_std,
        )

        try:
            vec = myc.tableXY(
                np.array(self.table_ccf[mask][sub_dico]["table"]["jdb"]),
                np.array(self.table_ccf[mask][sub_dico]["table"][kw]) * factor,
                np.array(self.table_ccf[mask][sub_dico]["table"][kw + "_std"]) * factor,
            )
        except:
            vec = myc.tableXY(
                np.array(self.table_ccf[mask][sub_dico]["table"]["jdb"]),
                np.array(self.table_ccf[mask][sub_dico]["table"][kw]) * factor,
            )

        return vec

    def import_dace_sts(self, substract_model=False):
        self.import_table()

        model = np.array(self.table["rv_shift"]) * 1000
        vector = np.array(self.table["rv_dace"])

        coeff = np.argmin(
            [myf.mad(vector - model), myf.mad(vector - 0 * model), myf.mad(vector + model)]
        )
        print("[INFO] Coefficient selected :", coeff)

        vec = myc.tableXY(
            self.table["jdb"],
            self.table["rv_dace"] + (coeff - 1) * model,
            self.table["rv_dace_std"],
        )

        if self.planet:
            self.import_planet()
            vec.y += self.rv_planet.y

        return vec

    def copy_dace_in_summary(self, instrument, path_table="ALL_OBSERVATIONS/", bin_length=1, db=0):
        file = glob.glob(
            "/".join(self.dir_root.split(self.instrument)[:-1])
            + path_table
            + "*"
            + instrument
            + "*"
        )
        if path_table == "":
            mapping = {
                "snr": "spectroFluxSn50",
                "jdb": "rjd",
                "berv": "berv",
                "rv_dace": "vrad",
                "rv_dace_std": "vrad_err",
                "RHK": "rhk",
                "RHK_std": "sig_rhk",
                "CaI": "ca",
                "CaI_std": "sig_ca",
                "NaD": "na",
                "NaD_std": "sig_na",
                "Ha": "ha",
                "Ha_std": "sig_ha",
                "CaII": "s_mw",
                "CaII_std": "sig_s",
                "ccf_fwhm": "fwhm",
                "ccf_fwhm_std": "sig_fwhm",
                "ccf_contrast": "contrast",
                "ccf_contrast_std": "sig_contrast",
                "ccf_vspan": "bis_span",
                "ccf_vspan_std": "sig_bispan",
            }

        elif path_table == "ALL_OBSERVATIONS/":
            mapping = {
                "snr": "spectroFluxSn50",
                "jdb": "rjd",
                "berv": "berv",
                "rv_dace": "rv",
                "rv_dace_std": "rv_err",
                "RHK": "rhk",
                "RHK_std": "rhk_err",
                "CaI": "caindex",
                "CaI_std": "caindex_err",
                "NaD": "naindex",
                "NaD_std": "naindex_err",
                "Ha": "haindex",
                "Ha_std": "haindex_err",
                "CaII": "sindex",
                "CaII_std": "sindex_err",
                "ccf_fwhm": "fwhm",
                "ccf_fwhm_std": "fwhm_err",
                "ccf_contrast": "contrast",
                "ccf_contrast_std": "contrast_err",
                "ccf_vspan": "bispan",
                "ccf_vspan_std": "bispan_err",
            }

        if len(file):
            print(" [INFO] File found : %s" % (file[0]))
            table_loaded = pd.read_csv(file[0], sep="\t")
            new_table = {}
            for name in [
                "RHK",
                "CaI",
                "NaD",
                "Ha",
                "CaII",
                "ccf_fwhm",
                "ccf_contrast",
                "ccf_vspan",
                "rv_dace",
            ]:
                table = table_loaded.copy()
                vec = myc.tableXY(
                    table[mapping["jdb"]], table[mapping[name]], table[mapping[name + "_std"]]
                )
                vec.night_stack(db=db, bin_length=bin_length, replace=True)
                new_table[name] = vec.y
                new_table[name + "_std"] = vec.yerr

            berv = myc.tableXY(
                table[mapping["jdb"]], table[mapping["berv"]], 1 / table[mapping["snr"]]
            )
            berv.night_stack(db=db, bin_length=bin_length, replace=True)
            new_table["berv"] = berv.y

            jdb = myc.tableXY(
                table[mapping["jdb"]], table[mapping["jdb"]], 1 / table[mapping["snr"]]
            )
            jdb.night_stack(db=db, bin_length=bin_length, replace=True)
            new_table["jdb"] = jdb.y

            snr = myc.tableXY(
                table[mapping["jdb"]], table[mapping["jdb"]], 1 / table[mapping["snr"]]
            )
            snr.night_stack(db=db, bin_length=bin_length, replace=True)
            new_table["snr"] = 1 / snr.yerr

            new_table = pd.DataFrame(new_table)
            new_table["ins"] = instrument

            new_table["rv_shift"] = 0

            self.yarara_update_summary(new_table)

        else:
            print(" [ERROR] File not found")

    def import_yarara_sts(self, v=1, kw_dico="lbl_iter", clean=False):

        if clean:
            clean = "cleaned_"
        else:
            clean = ""

        if v == 0:
            vec = self.import_ccf_timeseries(
                kw_dico.upper() + "_kitcat_" + clean + "mask_" + self.starname,
                "matching_diff",
                "rv",
            )
        elif v == 1:
            vec = self.import_ccf_timeseries(
                kw_dico.upper() + "_kitcat_" + clean + "mask_" + self.starname,
                "matching_mad",
                "rv",
            )
        elif v == 2:
            vec = self.import_ccf_timeseries(
                kw_dico.upper() + "_kitcat_" + clean + "mask_" + self.starname,
                "matching_empca",
                "rv",
            )

        return vec

    def import_dace_summary(self, bin_length=0):
        self.import_table()
        rv = myc.tableXY(
            np.array(self.table.jdb),
            np.array(self.table["rv_dace"]) - 1000 * np.array(self.table["rv_shift"]),
            np.array(self.table["rv_dace_std"]),
        )
        rv.recenter(who="Y")

        self.table_dace = pd.read_csv(
            self.dir_root + "DACE_TABLE/Dace_extracted_table.csv", index_col=0
        )

        tab = self.table_dace

        dace_rv = myc.tableXY(tab["rjd"], tab["vrad"] - 1000 * tab["model"], tab["svrad"])
        if bin_length:
            dace_rv.night_stack(bin_length=bin_length, replace=True)
        dace_rv, dust = dace_rv.match_x(rv, replace=False)

        matrix = []
        matrix.append(rv.x)
        matrix.append(rv.y)
        matrix.append(rv.yerr)
        matrix = np.array(matrix)

        table = pd.DataFrame(matrix.T, columns=["jdb", "vrad", "svrad"])

        # for i,j in zip(['fwhm','contrast','bis_span','s_mw','ha','na','ca','rhk'],['sig_fwhm','sig_contrast','sig_bis_span','sig_s','sig_ha','sig_na','sig_ca','sig_rhk']):
        for i, j in zip(
            ["fwhm", "contrast", "bis_span", "s_mw", "rhk"],
            ["sig_fwhm", "sig_contrast", "sig_bis_span", "sig_s", "sig_rhk"],
        ):
            dace_X = myc.tableXY(tab["rjd"].copy(), tab[i].copy(), tab[j].copy())
            if bin_length:
                dace_X.night_stack(bin_length=bin_length, replace=True)
            dace_X, dust = dace_X.match_x(rv, replace=False)
            table[i] = dace_X.y
            table[j] = dace_X.yerr

        dace_berv = myc.tableXY(tab["rjd"], tab["berv"], tab["svrad"])
        if bin_length:
            dace_berv.night_stack(bin_length=bin_length, replace=True)
        dace_berv, dust = dace_berv.match_x(rv, replace=False)

        table["berv"] = dace_berv.y

        table["sn_caii"] = self.table["snr"]
        table["ins_name"] = self.table["ins"]

        self.table_dace = table

        input_class = myc.table(table)

        input_class.export_to_dace(
            self.dir_root + "KEPLERIAN/" + self.starname + "_drs_timeseries.rdb"
        )

    def import_kitcat(self, clean=False, intermediate=False):
        if clean:
            self.kitcat = pd.read_pickle(
                self.dir_root + "KITCAT/kitcat_cleaned_mask_" + self.starname + ".p"
            )
        if intermediate:
            self.kitcat = pd.read_pickle(
                self.dir_root + "KITCAT/intermediate_kitcat_mask_" + self.starname + ".p"
            )
        else:
            self.kitcat = pd.read_pickle(
                self.dir_root + "KITCAT/kitcat_mask_" + self.starname + ".p"
            )

    def import_planet(self):
        file_test = self.import_spectrum()
        self.import_table()
        par = file_test["parameters"]["planet_injected"]
        rv = []
        rvi = np.zeros(len(self.table.jdb))
        for k in range(len(par["amp"])):
            amp, period, phi = par["amp"][k], par["period"][k], par["phase"][k]
            print("Planet %.0f : %.1f,%.2f,%.2f" % (k + 1, amp, period, phi))
            rvj = amp * np.sin(
                2 * np.pi * (self.table.jdb - np.min(self.table.jdb)) / period + phi
            )
            rvi += rvj
            rv.append(myc.tableXY(self.table.jdb, rvj, np.std(rvj) / 10 + 0 * rvj))
        self.rv_planet_i = rv
        planet = myc.tableXY(self.table.jdb, rvi, np.std(rvi) / 10 + 0 * rvi)
        self.rv_planet = planet

    def import_material(self):
        self.material = pd.read_pickle(self.directory + "Analyse_material.p")

    def update_material(self):
        myf.pickle_dump(self.material, open(self.directory + "Analyse_material.p", "wb"))

    def import_lbl(self, path=None):
        self.lbl = pd.read_pickle(self.directory + "Analyse_line_by_line.p")
        self.nb_lines = len(self.lbl[list(self.lbl.keys())[0]]["catalog"])

    def import_lbl_iter(self, path=None):
        self.lbl_iter = pd.read_pickle(self.directory + "Analyse_line_by_line_iter.p")
        self.nb_lines = len(self.lbl_iter[list(self.lbl_iter.keys())[0]]["catalog"])

    def import_dbd(self):
        self.dbd = pd.read_pickle(self.directory + "Analyse_depth_by_depth.p")

    def import_wbw(self):
        self.wbw = pd.read_pickle(self.directory + "Analyse_width_by_width.p")

    def import_bt(self):
        self.bt = pd.read_pickle(self.directory + "Analyse_bt_by_bt.p")

    def import_bbb(self):
        self.bbb = pd.read_pickle(self.directory + "Analyse_bis_by_bis.p")

    def import_aba(self):
        self.aba = pd.read_pickle(self.directory + "Analyse_asym_by_asym.p")

    # =============================================================================
    # IMPORT THE FULL DICO CHAIN
    # =============================================================================

    def import_dico_tree(self):
        file_test = self.import_spectrum()
        kw = list(file_test.keys())
        kw_kept = []
        kw_chain = []
        for k in kw:
            if len(k.split("matching_")) == 2:
                kw_kept.append(k)
        kw_kept = np.array(kw_kept)

        info = []
        for n in kw_kept:

            try:
                s = file_test[n]["parameters"]["step"]
                dico = file_test[n]["parameters"]["sub_dico_used"]
                info.append([n, s, dico])
            except:
                pass
        info = pd.DataFrame(info, columns=["dico", "step", "dico_used"])
        self.dico_tree = info.sort_values(by="step")

    def import_dico_chain(self, last_dico, method="iterative"):
        test_file = self.import_spectrum()

        if method == "iterative":
            kw = list(test_file.keys())
            kw_kept = []
            kw_chain = []
            for k in kw:
                if len(k.split("matching_")) == 2:
                    kw_kept.append(k)
            kw_kept = np.array(kw_kept)

            while last_dico != "matching_anchors":
                next_dico = test_file[last_dico]["parameters"]["sub_dico_used"]
                kw_chain.append(last_dico)
                last_dico = next_dico

            chain = np.array(kw_chain)
            chain = pd.DataFrame(
                [[n, test_file[n]["parameters"]["step"]] for n in chain], columns=["dico", "step"]
            )
            chain_sorted = np.array(chain.sort_values(by="step")["dico"])[::-1]

            self.dico_chain = chain_sorted
        else:
            self.import_dico_tree()
            tree = self.dico_tree
            last_step = np.array(tree.loc[tree["dico"] == last_dico, "step"])[0]

            tree = tree.loc[tree["step"] <= last_step].sort_values(by="step")
            chain_sorted = np.array(tree["dico"])[::-1]
            self.dico_chain = chain_sorted

    # =============================================================================
    # IMPORT a RANDOM SPECTRUM
    # =============================================================================

    def copy_spectrum(self):
        directory = self.directory
        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)[0]

        if not os.path.exists(self.dir_root + "TEMP/"):
            os.system("mkdir " + self.dir_root + "TEMP/")

        os.system("cp " + files + " " + self.dir_root + "TEMP/")

    def import_spectrum(self, num=None):
        """
        Import a pickle file of a spectrum to get fast common information shared by all spectra

        Parameters
        ----------
        num : index of the spectrum to extract (if None random selection)

        Returns
        -------
        Return the open pickle file

        """

        directory = self.directory
        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)
        if not len(files):
            files = glob.glob(directory.replace("WORKSPACE", "TEMP") + "RASSI*.p")

        if num is None:
            try:
                num = np.random.choice(np.arange(len(files)), 1)
                file = files[num][0]
            except:
                file = files[0]

        else:
            try:
                file = files[num]
            except:
                file = files[0]
        return pd.read_pickle(file)

    # =============================================================================
    #
    # =============================================================================

    def yarara_simbad_query(self, starname=None):
        self.import_star_info()
        customSimbad = Simbad()
        customSimbad.add_votable_fields(
            "sp",
            "sptype",
            "velocity",
            "plx",
            "ubv",
            "fe_h",
            "flux(U)",
            "flux(B)",
            "flux(V)",
            "flux(R)",
            "pmdec",
            "pmra",
        )
        if starname is None:
            starname = self.starname

        # myf.touch_dir(self.dir_yarara+'database/')
        # myf.touch_dir(self.dir_yarara+'database/SIMBAD/')
        # table_simbad = myf.touch_pickle(self.dir_yarara+'database/SIMBAD/table_stars.p')
        table_simbad = myf.touch_pickle(
            root + "/Python/database/SIMBAD/table_stars.p"
        )  # 05.08.21 #put the database inside Python directory instead of yarara

        dico = self.star_info

        if starname in table_simbad.keys():
            print("\n [INFO] Star found in the SIMBAD table")
            dico2 = table_simbad[starname]

            for kw in dico2.keys():
                if type(dico2[kw]) == dict:
                    for kw2 in dico2[kw].keys():
                        dico[kw][kw2] = dico2[kw][kw2]
        else:
            try:
                query = customSimbad.query_objects([starname])

                if query is None:
                    print(
                        Fore.YELLOW
                        + "\n [WARNING] Target not founded in the SIMBAD database under the speficied name : %s\n"
                        % (starname)
                        + Fore.RESET
                    )
                    print(" Cross-matching with DACE query...")

                    name = myf.crossmatch_dace_simbad(starname, direction="simbad")
                    if name is None:
                        print(" Starname not found in database, skycoord cross-matching...")
                        spectroscopy_data = Spectroscopy.query_database(
                            filters={"obj_id_catname": {"contains": [starname]}},
                            sort={},
                            limit=100,
                        )
                        if len(spectroscopy_data["obj_pos_coordinates_hms_dms"]):
                            hour = spectroscopy_data["obj_pos_coordinates_hms_dms"][0].split(
                                " / "
                            )[0]
                            dec = spectroscopy_data["obj_pos_coordinates_hms_dms"][0].split(" / ")[
                                1
                            ]
                            time.sleep(0.5)
                            second_query = Simbad.query_region(
                                astrocoord.SkyCoord(
                                    ra=hour, dec=dec, unit=(u.hourangle, u.deg), frame="fk5"
                                ),
                                radius="0d0m10s",
                            )
                            if second_query is None:
                                print(
                                    Fore.YELLOW
                                    + "\n [WARNING] Target not founded in the SIMBAD database in the sky specified region : ra=%s dec=%s\n"
                                    % (hour, dec)
                                    + Fore.RESET
                                )
                            else:
                                name = second_query["MAIN_ID"][0].decode("utf-8")
                                print(
                                    "\n [INFO] Target founded in SIMBAD database by skycoord crossmatch (10s radius) under the name : %s\n"
                                    % (name)
                                )
                        else:
                            print(
                                Fore.YELLOW
                                + "\n [WARNING] Target or sky coordinates not founded in the DACE database under the speficied name : %s\n"
                                % (starname)
                                + Fore.RESET
                            )
                            name = "Sun"
                    else:
                        print(" Starname found in database under : %s" % (name))

                    time.sleep(0.5)
                    query = customSimbad.query_objects([name])
                    simbad_name = name

                else:
                    simbad_name = self.starname
            except:
                print(Fore.YELLOW + "\n [WARNING] Simbad query failed !" + Fore.RESET)
                query = None

            if query is None:
                pass
            else:
                ra = str(np.array(query["RA"][0]))
                dec = str(np.array(query["DEC"][0]))
                sp = str(np.array(query["SP_TYPE"][0]).astype("str"))

                sp = sp.replace("+", "").replace("-", "").replace(":", "")
                sp = sp.split("V")
                sp = sp[0] + "V" * (len(sp) > 1)

                if sp == "":
                    sp = "Unfound"

                self.mask_harps = ["G2", "K5", "M2"][
                    int((sp[0] == "K") | (sp[0] == "M")) + int(sp[0] == "M")
                ]

                rv = np.round(float(query["RVZ_RADVEL"][0]), 2)
                u = np.round(float(np.array(query["FLUX_U"])), 2)
                b = np.round(float(np.array(query["FLUX_B"])), 2)
                v = np.round(float(np.array(query["FLUX_V"])), 2)
                r = np.round(float(np.array(query["FLUX_R"])), 2)
                dist_pc = np.round(float(1 / np.array(query["PLX_VALUE"]) * 1000), 2)
                t = int(np.array(query["Fe_H_Teff"])[0])
                g = np.round(float(np.array(query["Fe_H_log_g"])[0]), 2)
                feh = np.round(float(np.array(query["Fe_H_Fe_H"])[0]), 2)
                pmd = np.array(query["PMDEC"])[0]
                pma = np.array(query["PMRA"])[0]

                bv = np.round(b - v, 2)

                if not t:
                    t = np.nan

                if not np.isnan(t):
                    M, R, logg = find_stellar_mass_radius(t, sp_type=sp)
                else:
                    M = 1
                    R = 1

                if np.isnan(np.round(b - v, 2)):
                    bv = (
                        -3.684 * np.log10(t) + 14.551
                    )  # http://www.isthe.com/chongo/tech/astro/HR-temp-mass-table-byhrclass.html
                    print(
                        Fore.YELLOW
                        + " [WARNING] BV extracted from the effective temperature since not found on Simbad : BV = %.2f"
                        % (bv)
                        + Fore.RESET
                    )
                    if np.isnan(v) & (not np.isnan(b)):
                        v = np.round(b - bv, 2)
                        print(
                            Fore.YELLOW
                            + " [WARNING] magV extracted from the effective temperature since not found on Simbad : mV = %.2f"
                            % (v)
                            + Fore.RESET
                        )
                    if np.isnan(b) & (not np.isnan(v)):
                        b = np.round(bv + v, 2)
                        print(
                            Fore.YELLOW
                            + " [WARNING] magB extracted from the effective temperature since not found on Simbad : mB = %.2f"
                            % (b)
                            + Fore.RESET
                        )

                if np.isnan(t):
                    t = np.round(
                        10 ** ((bv - 14.551) / (-3.684)), 0
                    )  # http://www.isthe.com/chongo/tech/astro/HR-temp-mass-table-byhrclass.html
                    print(
                        Fore.YELLOW
                        + " [WARNING] Teff extracted from the BV since not found on Simbad : Teff = %.2f"
                        % (t)
                        + Fore.RESET
                    )

                dico = self.star_info
                dico["Simbad_name"]["fixed"] = simbad_name
                dico["Sp_type"]["fixed"] = sp
                dico["Rstar"]["fixed"] = np.round(R, 2)
                dico["Mstar"]["fixed"] = np.round(M, 2)
                dico["magU"]["fixed"] = u
                dico["magB"]["fixed"] = b
                dico["magV"]["fixed"] = v
                dico["magR"]["fixed"] = r
                dico["UB"]["fixed"] = np.round(u - b, 2)
                dico["BV"]["fixed"] = np.round(bv, 2)
                dico["VR"]["fixed"] = np.round(v - r, 2)
                dico["Dist_pc"]["fixed"] = dist_pc
                dico["Teff"]["fixed"] = t
                dico["Log_g"]["fixed"] = g
                dico["FeH"]["fixed"] = feh
                dico["Pmd"]["fixed"] = pmd
                dico["Pma"]["fixed"] = pma
                dico["Dec"]["fixed"] = dec
                dico["Ra"]["fixed"] = ra
                dico["Rv_sys"]["fixed"] = rv

                table_simbad[self.starname] = dico
                # myf.pickle_dump(table_simbad, open(self.dir_yarara+'database/SIMBAD/table_stars.p','wb'))
                myf.pickle_dump(
                    table_simbad, open(root + "/Python/database/SIMBAD/table_stars.p", "wb")
                )

        myf.pickle_dump(
            dico, open(self.dir_root + "STAR_INFO/Stellar_info_" + self.starname + ".p", "wb")
        )

        sp = dico["Sp_type"]["fixed"]
        self.mask_harps = ["G2", "K5", "M2"][
            int((sp[0] == "K") | (sp[0] == "M")) + int(sp[0] == "M")
        ]

        del dico["Name"]

        print("\nValue after SIMBAD query")
        print("----------------\n")

        dataframe = pd.DataFrame(
            [dico[i]["fixed"] for i in dico.keys()],
            index=[i for i in dico.keys()],
            columns=[self.starname],
        )
        print(dataframe)

    def yarara_star_info(
        self,
        Rv_sys=None,
        simbad_name=None,
        magB=None,
        magV=None,
        magR=None,
        BV=None,
        VR=None,
        sp_type=None,
        Mstar=None,
        Rstar=None,
        Vsini=None,
        Vmicro=None,
        Teff=None,
        log_g=None,
        FeH=None,
        Prot=None,
        Fwhm=None,
        Contrast=None,
        CCF_delta=None,
        Pmag=None,
        stellar_template=None,
    ):

        kw = [
            "Rv_sys",
            "Simbad_name",
            "Sp_type",
            "magB",
            "magV",
            "magR",
            "BV",
            "VR",
            "Mstar",
            "Rstar",
            "Vsini",
            "Vmicro",
            "Teff",
            "Log_g",
            "FeH",
            "Prot",
            "FWHM",
            "Contrast",
            "Pmag",
            "stellar_template",
            "CCF_delta",
        ]
        val = [
            Rv_sys,
            simbad_name,
            sp_type,
            magB,
            magV,
            magR,
            BV,
            VR,
            Mstar,
            Rstar,
            Vsini,
            Vmicro,
            Teff,
            log_g,
            FeH,
            Prot,
            Fwhm,
            Contrast,
            Pmag,
            stellar_template,
            CCF_delta,
        ]

        self.import_star_info()
        self.import_table()
        self.import_material()

        table = self.table
        snr = np.array(table["snr"]).argmax()
        file_test = self.import_spectrum(num=snr)

        for i, j in zip(kw, val):
            if j is not None:
                if type(j) != list:
                    j = ["fixed", j]
                if i in self.star_info.keys():
                    self.star_info[i][j[0]] = j[1]
                else:
                    self.star_info[i] = {j[0]: j[1]}

        try:
            self.star_info["Teff"]["Gray"] = file_test["parameters"]["Teff_gray"]
            M, R, logg = find_stellar_mass_radius(
                file_test["parameters"]["Teff_gray"], sp_type=self.star_info["Sp_type"]["fixed"]
            )
            self.star_info["Mstar"]["Gray"] = np.round(M, 2)
            self.star_info["Rstar"]["Gray"] = np.round(R, 2)
            self.star_info["Log_g"]["Gray"] = np.round(logg, 2)

        except:
            pass

        try:
            m = self.model_atmos["MARCS"]
            a = self.model_atmos["ATLAS"]

            self.star_info["Teff"]["MARCS"] = int(m[0].split("_")[0][1:])
            self.star_info["Log_g"]["MARCS"] = np.round(float(m[0].split("_")[1][1:]), 2)

            M, R, logg = find_stellar_mass_radius(
                int(m[0].split("_")[0][1:]), sp_type=self.star_info["Sp_type"]["fixed"]
            )
            if abs(logg - np.round(m[0].split("_")[1][1:], 2)) / logg < 0.2:
                self.star_info["Mstar"]["MARCS"] = np.round(M, 2)
                self.star_info["Rstar"]["MARCS"] = np.round(R, 2)

            self.star_info["Teff"]["ATLAS"] = int(a[0].split("_")[0][1:])
            self.star_info["Log_g"]["ATLAS"] = np.round(float(a[0].split("_")[1][1:]), 2)

            M, R, logg = find_stellar_mass_radius(
                int(a[0].split("_")[0][1:]), sp_type=self.star_info["Sp_type"]["fixed"]
            )
            if abs(logg - np.round(m[0].split("_")[1][1:], 2)) / logg < 0.2:
                self.star_info["Mstar"]["ATLAS"] = np.round(M, 2)
                self.star_info["Rstar"]["ATLAS"] = np.round(R, 2)
        except:
            pass

        myf.pickle_dump(
            self.star_info,
            open(self.dir_root + "STAR_INFO/Stellar_info_" + self.starname + ".p", "wb"),
        )

    def yarara_master_ccf(self, sub_dico="matching_diff", name_ext="", rvs=None):
        self.import_table()

        vrad, ccfs = (self.all_ccf_saved[sub_dico][0], self.all_ccf_saved[sub_dico][1])

        if rvs is None:
            rvs = self.ccf_rv.y.copy()

        med_rv = np.nanmedian(rvs)
        rvs -= med_rv

        new_ccf = []
        for j in range(len(ccfs.T)):
            ccf = myc.tableXY(vrad - rvs[j], ccfs[:, j])
            ccf.interpolate(new_grid=vrad, method="linear", fill_value=np.nan)
            new_ccf.append(ccf.y)
        new_ccf = np.array(new_ccf)
        new_vrad = vrad - med_rv
        stack = np.sum(new_ccf, axis=0)
        stack /= np.nanpercentile(stack, 95)
        half = 0.5 * (1 + np.nanmin(stack))

        master_ccf = myc.tableXY(new_vrad, stack)
        master_ccf.supress_nan()
        master_ccf.interpolate(replace=True, method="cubic")

        new_vrad = master_ccf.x
        stack = master_ccf.y

        v1 = new_vrad[new_vrad < 0][myf.find_nearest(stack[new_vrad < 0], half)[0][0]]
        v2 = new_vrad[new_vrad > 0][myf.find_nearest(stack[new_vrad > 0], half)[0][0]]

        vmin = np.nanmin(new_vrad[~np.isnan(stack)])
        vmax = np.nanmax(new_vrad[~np.isnan(stack)])

        vlim = np.min([abs(vmin), abs(vmax)])
        vmin = -vlim
        vmax = vlim

        contrast = 1 - np.nanmin(stack)

        plt.figure()
        plt.plot(new_vrad, stack, color="k", label="Contrast = %.1f %%" % (100 * contrast))

        extension = ["YARARA", "HARPS", ""][int(name_ext != "") + int(name_ext == "_telluric")]

        if extension == "YARARA":
            self.fwhm = np.round((v2 - v1) / 1000, 2)
            myf.pickle_dump(
                {"vrad": new_vrad, "ccf_power": stack},
                open(self.dir_root + "MASTER/MASTER_CCF_KITCAT.p", "wb"),
            )
            try:
                old = pd.read_pickle(self.dir_root + "MASTER/MASTER_CCF_HARPS.p")
                plt.plot(old["vrad"], old["ccf_power"], alpha=0.5, color="k", ls="--")
            except:
                pass
        elif extension == "HARPS":
            myf.pickle_dump(
                {"vrad": new_vrad, "ccf_power": stack},
                open(self.dir_root + "MASTER/MASTER_CCF_HARPS.p", "wb"),
            )
        else:
            try:
                old = pd.read_pickle(self.dir_root + "MASTER/MASTER_CCF" + name_ext + ".p")
                plt.plot(old["vrad"], old["ccf_power"], alpha=0.5, color="k", ls="--")
            except:
                pass
            myf.pickle_dump(
                {"vrad": new_vrad, "ccf_power": stack},
                open(self.dir_root + "MASTER/MASTER_CCF" + name_ext + ".p", "wb"),
            )

        plt.xlim(vmin, vmax)

        plt.plot([v1, v2], [half, half], color="r", label="FHWM = %.2f kms" % ((v2 - v1) / 1000))
        plt.scatter([v1, v2], [half, half], color="r", edgecolor="k", zorder=10)
        plt.scatter([0], [np.nanmin(stack)], color="k", edgecolor="k", zorder=10)
        plt.axvline(x=0, ls=":", color="k", alpha=0.5)
        plt.legend()
        plt.grid()
        plt.xlabel("RV [m/s]", fontsize=13)
        plt.ylabel("Flux normalised", fontsize=13)
        plt.title("%s" % (self.starname), fontsize=14)

        self.yarara_star_info(Contrast=[extension, np.round(contrast, 3)])
        self.yarara_star_info(Fwhm=[extension, np.round((v2 - v1) / 1000, 2)])

        plt.savefig(self.dir_root + "IMAGES/MASTER_CCF" + name_ext + ".pdf")

    def yarara_vsin(self, fwhm_star=None, fwhm_inst=None, vmicro=None):
        """ "According to Strassmeier 1990"""
        self.import_table()
        self.import_star_info()

        tab = self.table

        if fwhm_star is None:
            fwhm_star = np.nanpercentile(tab["ccf_fwhm"], 5)
        if fwhm_inst is None:
            fwhm_inst = np.nanpercentile(tab["telluric_fwhm"], 5)
        if vmicro is None:
            vmicro = self.star_info["Vmicro"]["fixed"]

        conv_fwhm_v = 1 / (2 * np.sqrt(np.log(2)))

        vsini = (
            np.sqrt((fwhm_star**2 - fwhm_inst**2)) - vmicro
        ) * conv_fwhm_v  # constant of calibration (need to better understand why this constant also work for HARPS)

        plt.figure(figsize=(12, 6))
        plt.scatter(
            tab["jdb"],
            conv_fwhm_v * (np.sqrt(tab["ccf_fwhm"] ** 2 - tab["telluric_fwhm"] ** 2) - vmicro),
        )
        plt.xlabel("Time [days]", fontsize=14)
        plt.ylabel("Vsin(i) [km/s]", fontsize=14)
        plt.axhline(y=vsini, color="k")
        plt.title(
            r"Vsin(i) = %.2f km/s   ($V_{star}$ = %.2f km/s,   $V_{inst}$ = %.2f km/s,   $V_{micro}$ = %.2f km/s) "
            % (vsini, fwhm_star, fwhm_inst, vmicro),
            fontsize=14,
        )

        print(
            "\n FWHM star : %.2f km/s \n FWHM inst : %.2f km/s \n V_microturb : %.2f km/s \n Vsini : %.2f km/s"
            % (fwhm_star, fwhm_inst, vmicro, vsini)
        )

        self.yarara_star_info(
            Vsini=["YARARA", np.round(vsini, 2)], Vmicro=["YARARA", np.round(vmicro, 2)]
        )

    # =============================================================================
    #
    # =============================================================================

    def yarara_clone_dico(
        self, sub_dico="matching_diff", new_dico="matching_mad", continuum="linear"
    ):
        """Duplicate a sub_dico to clone it into a new sub_dico name"""
        self.import_table()
        table = self.table
        filename = table["filename"]
        for f in tqdm(filename):
            file = pd.read_pickle(f)
            try:
                file[new_dico]["continuum_" + continuum] = file[sub_dico]["continuum_" + continuum]
            except:
                file[new_dico] = file[sub_dico]
            myf.pickle_dump(file, open(f, "wb"))

    def yarara_add_step_dico(self, sub_dico, step, chain=False):
        """Add the step kw for the dico chain, if chain is set to True numbered the full chain"""
        self.import_table()
        table = self.table
        if chain:
            self.import_dico_chain(sub_dico)
            sub_dico = self.dico_chain[::-1]
            step = np.arange(len(sub_dico))
        else:
            sub_dico = [sub_dico]
            step = [step]

        filename = table["filename"]
        for f in filename:
            file = pd.read_pickle(f)
            for sub, s in zip(sub_dico, step):
                file[sub]["parameters"]["step"] = s
            myf.pickle_dump(file, open(f, "wb"))

    def yarara_add_ccf_entry(self, kw, default_value=1):
        self.import_ccf()
        for mask in list(self.table_ccf.keys()):
            if mask != "star_info":
                for sb in list(self.table_ccf[mask].keys()):
                    self.table_ccf[mask][sb]["table"][kw] = default_value
        myf.pickle_dump(self.table_ccf, open(self.directory + "Analyse_ccf.p", "wb"))

    # =============================================================================
    #     EXPORT RV TIMESERIES TO DACE
    # =============================================================================

    def export_to_dace(self, input_class, ext="", substract_model=True):
        """
        Export both the DACE RV and the RV from the input myc.tableXY class into KEPLERIAN directory

        Parameters
        ----------
        input_class : the myc.tableXY object to export under DACE format

        Returns
        -------
        Return the tableXY spectrum object

        """
        self.import_table()
        tab = self.table

        planet = np.zeros(len(tab.jdb))
        if self.planet:
            print("\n---- PLANET ACTIVATED ----")
            planet += tab.rv_planet

        model2 = np.array(self.table["rv_shift"]) * 1000
        model = int(substract_model) * model2

        dace = myc.tableXY(tab["jdb"], tab["rv_dace"] + planet - model, tab["rv_dace_std"])
        # input_class.yerr = dace.yerr

        table = self.table.copy()
        table["model"] = model.copy()

        model = int(not substract_model) * model2

        table["vrad"] = input_class.y + model
        table["svrad"] = input_class.yerr

        matching = {
            "fwhm": "ccf_fwhm",
            "sig_fwhm": "ccf_fwhm_std",
            "contrast": "ccf_contrast",
            "sig_contrast": "ccf_contrast_std",
            "bis_span": "BIS",
            "sig_bis_span": "BIS_std",
            "s_mw": "CaII",
            "sig_s": "CaII_std",
            "ha": "Ha",
            "sig_ha": "Ha_std",
            "na": "NaD",
            "sig_na": "NaD_std",
            "ca": "CaI",
            "sig_ca": "CaI_std",
            "rhk": "RHK",
            "sig_rhk": "RHK_std",
            "sn_caii": "snr",
            "ins_name": "ins",
            "model": "model",
        }

        for i in matching.keys():
            table[i] = table[matching[i]]

        input_class = myc.table(table)

        dace.export_to_dace(self.dir_root + "KEPLERIAN/" + self.starname + "_drs_timeseries.rdb")
        input_class.export_to_dace(
            self.dir_root + "KEPLERIAN/" + self.starname + "_yarara" + ext + "_timeseries.rdb"
        )

    def export_public_yarara_reduction(
        self, sub_dico="matching_mad", substract_map=[], add_map=["activity"], planet=False
    ):

        self.import_ccf()

        instrument = self.instrument
        starname = self.starname
        directory_root = self.dir_root.split("/data/")[0] + "_public_release/"
        directory = directory_root + datetime.datetime.now().isoformat()[0:10] + "/"

        myf.touch_dir(directory_root)
        myf.touch_dir(directory)
        myf.touch_dir(directory + instrument)
        for subdir in ["CORRECTION_MAP", "S1D", "UTILITIES", "IMAGES"]:
            myf.touch_dir(directory + instrument + "/" + subdir)

        os.system(
            "cp "
            + self.dir_root.split("/Yarara")[0]
            + "/Python/docstrings/Docstring_release.pdf "
            + directory
            + instrument
            + "/Docstring_public_release.pdf"
        )
        os.system(
            "cp "
            + self.dir_root.split("/Yarara")[0]
            + "/Python/docstrings/Examples_release_code.py "
            + directory
            + instrument
        )
        os.system(
            "cp "
            + self.dir_root.split("/Yarara")[0]
            + "/Python/docstrings/Cretignier* "
            + directory
            + instrument
        )

        for maps in substract_map:
            os.system(
                "cp "
                + self.directory.replace("WORKSPACE", "CORRECTION_MAP")
                + "map_matching_"
                + maps
                + ".p "
                + directory
                + instrument
                + "/CORRECTION_MAP"
            )

        for maps in add_map:
            os.system(
                "cp "
                + self.directory.replace("WORKSPACE", "CORRECTION_MAP")
                + "map_matching_"
                + maps
                + ".p "
                + directory
                + instrument
                + "/CORRECTION_MAP"
            )

        # summary simbad + yarara star info
        os.system(
            "cp "
            + self.directory.replace("WORKSPACE", "STAR_INFO")
            + "Stellar_info_%s.p " % (starname)
            + directory
            + instrument
            + "/UTILITIES"
        )

        # summary frames selections
        os.system(
            "cp "
            + "/".join(self.dir_root.split("/")[:-2])
            + "/Selections_observations_"
            + instrument
            + ".jpg "
            + directory
            + instrument
            + "/IMAGES"
        )

        # RASSINE normalisation
        os.system(
            "cp "
            + self.directory.replace("WORKSPACE", "MASTER")
            + "Master*.png "
            + directory
            + instrument
            + "/IMAGES"
        )

        # comparison before after STS
        os.system(
            "cp "
            + self.directory.replace("WORKSPACE", "IMAGES")
            + "/3920_3980/Before_after.png "
            + directory
            + instrument
            + "/IMAGES/Before_after_blue.png"
        )
        os.system(
            "cp "
            + self.directory.replace("WORKSPACE", "IMAGES")
            + "/5730_5800/Before_after.png "
            + directory
            + instrument
            + "/IMAGES/Before_after_red.png"
        )

        for images in [
            "berv_statistic_summary.pdf",
            "MASTER_CCF.pdf",
            "MASTER_CCF_mask_HARPS.pdf",
            "RV_statistic.pdf",
            "snr_statistic_2.pdf",
            "telluric_detection.pdf",
            "telluric_control_check.pdf",
            "Correction_1d_h2o_1.png",
            "Correction_1d_h2o_2.png",
            "Correction_1d_ha.png",
            "Correction_1d_o2_1.png",
            "Correction_cosmics.png",
            "LBL_ITER_polar_periodogram.png",
            "Chunck_investigation_YARARA_V1.png",
            "KitCat_CCF_mask.pdf",
            "all_proxies.pdf",
        ]:
            os.system(
                "cp "
                + self.directory.replace("WORKSPACE", "IMAGES")
                + images
                + " "
                + directory
                + instrument
                + "/IMAGES"
            )

        # material
        self.import_material()
        material = self.material[
            [
                "wave",
                "correction_factor",
                "reference_spectrum",
                "color_template",
                "master_snr_curve",
                "borders_pxl",
                "merged",
            ]
        ].copy()
        material["color_template"] /= np.nanmean(material["color_template"])
        material["master_snr_curve"] /= np.nanmean(material["master_snr_curve"])
        material["reference_spectrum"] *= material["correction_factor"]
        myf.pickle_dump(
            material, open(directory + instrument + "/UTILITIES/Analyse_material.p", "wb")
        )

        # ccf
        self.import_ccf_profile()
        ccf = {}
        ccf["CCF_" + self.mask_harps] = self.table_ccf_saved["CCF_" + self.mask_harps][sub_dico]
        ccf["CCF_kitcat_mask_" + self.starname] = self.table_ccf_saved[
            "CCF_kitcat_mask_" + self.starname
        ][sub_dico]
        myf.pickle_dump(ccf, open(directory + instrument + "/UTILITIES/Analyse_ccf.p", "wb"))

        # rdb table
        os.system(
            "cp "
            + self.directory.replace("WORKSPACE", "KEPLERIAN")
            + "*drs*.rdb "
            + directory
            + instrument
            + "/UTILITIES"
        )
        os.system(
            "cp "
            + self.directory.replace("WORKSPACE", "KEPLERIAN")
            + "*yarara_v1*.rdb "
            + directory
            + instrument
            + "/UTILITIES"
        )
        os.system(
            "cp "
            + self.directory.replace("WORKSPACE", "KEPLERIAN")
            + "*yarara_v2*.rdb "
            + directory
            + instrument
            + "/UTILITIES"
        )

        # kitcat
        self.import_kitcat()
        kitcat = self.kitcat["catalogue"][
            ["wave", "line_depth", "depth_rel", "freq_mask0", "diff_continuum", "wave_fitted"]
        ].copy()
        kitcat["weight_rv"] = self.kitcat["catalogue"]["weight_rv_sym"]
        kitcat["valid"] = self.kitcat["catalogue"]["valid"]
        myf.pickle_dump(kitcat, open(directory + instrument + "/UTILITIES/Analyse_kitcat.p", "wb"))

        # compute ccf to update the table
        if len(substract_map) | len(add_map):
            output = self.yarara_ccf(
                mask="kitcat_mask_" + self.starname + ".p",
                plot=False,
                sub_dico=sub_dico,
                ccf_oversampling=1,
                rv_range=None,
                save=False,
                substract_map=substract_map,
                add_map=add_map,
            )

        # table
        self.import_table()

        table = self.table[
            [
                "filename",
                "ins",
                "snr",
                "snr_computed",
                "jdb",
                "mjd",
                "berv",
                "rv_dace",
                "rv_dace_std",
                "rv_shift",
                "rv_sec",
                "ccf_rv",
                "ccf_rv_std",
                "ccf_fwhm",
                "ccf_fwhm_std",
                "ccf_contrast",
                "ccf_contrast_std",
                "ccf_vspan",
                "ccf_vspan_std",
                "CaIIH",
                "CaIIH_std",
                "CaIIK",
                "CaIIK_std",
                "CaII",
                "CaII_std",
                "RHK",
                "RHK_std",
                "MgI",
                "MgI_std",
                "NaD",
                "NaD_std",
                "Ha",
                "Ha_std",
                "Hb",
                "Hb_std",
                "Hc",
                "Hc_std",
                "Hd",
                "Hd_std",
            ]
        ].copy()

        c = -1
        for f in table["filename"]:
            c += 1
            file = self.import_spectrum()
            new_file = {}
            for kw in ["wave", "flux", "flux_err"]:
                new_file[kw] = file[kw]

            if planet:
                new_file["flux"] = file["flux_planet"]

            new_file["flux"] = (
                new_file["flux"]
                / file[sub_dico]["continuum_linear"]
                * np.mean(file["matching_diff"]["continuum_linear"])
                * material["color_template"]
            )

            new_file["continuum"] = (
                np.mean(file["matching_diff"]["continuum_linear"])
                * material["color_template"]
                / material["correction_factor"]
            )
            new_file["parameters"] = {
                "arcfiles": [ff.split("/")[-1] for ff in file["parameters"]["arcfiles"]]
            }
            filename = file["parameters"]["filename"]

            new_name = directory + instrument + "/S1D/" + filename
            table.loc[c, "filename"] = filename
            myf.pickle_dump(new_file, open(new_name, "wb"))

        myf.pickle_dump(table, open(directory + instrument + "/UTILITIES/Analyse_summary.p", "wb"))

    def yarara_poissonian_noise(self, noise_wanted=1 / 100, wave_ref=None, flat_snr=True, seed=9):
        self.import_table()
        self.import_material()

        if noise_wanted:
            master = np.sqrt(
                np.array(self.material.reference_spectrum * self.material.correction_factor)
            )  # used to scale the snr continuum into errors bars
            snrs = pd.read_pickle(self.dir_root + "WORKSPACE/Analyse_snr.p")

            if not flat_snr:
                if wave_ref is None:
                    current_snr = np.array(self.table.snr_computed)
                else:
                    i = myf.find_nearest(snrs["wave"], wave_ref)[0]
                    current_snr = snrs["snr_curve"][:, i]

                curves = current_snr * np.ones(len(master))[:, np.newaxis]
            else:
                curves = snrs["snr_curve"]  # snr curves representing the continuum snr

            snr_wanted = 1 / noise_wanted
            diff = 1 / snr_wanted**2 - 1 / curves**2
            diff[diff < 0] = 0
            # snr_to_degrate = 1/np.sqrt(diff)

            noise = np.sqrt(diff)
            noise[np.isnan(noise)] = 0

            noise_values = noise * master[:, np.newaxis].T
            np.random.seed(seed=seed)
            matrix_noise = np.random.randn(len(self.table.jdb), len(self.material.wave))
            matrix_noise *= noise_values
        else:
            matrix_noise = np.zeros((len(self.table.jdb), len(self.material.wave)))
            noise_values = np.zeros((len(self.table.jdb), len(self.material.wave)))

        return matrix_noise, noise_values

    def yarara_add_noise(self, sub_dico, noise=1 / 500):
        noise_matrix, noise2 = self.yarara_poissonian_noise(
            noise_wanted=noise, wave_ref=None, flat_snr=True, seed=9
        )

        directory = self.directory
        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            f = file["flux"]
            c = file[sub_dico]["continuum_linear"]
            n = noise_matrix[i]
            new_c = f / (f / c + n)
            file[sub_dico]["continuum_linear"] = new_c
            ras.save_pickle(j, file)

        print(
            "Noise successfully injected in the spectra from %s at SNR %.0f"
            % (sub_dico, 1 / noise)
        )

    def snr_statistic(self, version=1):
        self.import_table()
        if version == 1:
            print(self.table["snr"].describe())
            snr = np.array(self.table["snr"])
        else:
            print(self.table["snr_computed"].describe())
            snr = np.array(self.table["snr_computed"])
        plt.figure(figsize=(8, 7))
        plt.title(
            "\n Nb spectra : %.0f\nMin : %.0f   |   Q1 : %.0f   |   Q2 : %.0f   |   Q3 : %.0f   |   Max : %.0f\n"
            % (
                len(snr),
                np.min(snr),
                np.percentile(snr, 25),
                np.percentile(snr, 50),
                np.percentile(snr, 75),
                np.max(snr),
            ),
            fontsize=16,
        )
        plt.hist(snr, bins=40, histtype="step", color="k")
        plt.hist(snr, bins=40, alpha=0.2, color="b")
        plt.axvline(x=np.median(snr), ls="-", color="k")
        plt.axvline(
            x=np.percentile(snr, 16),
            ls=":",
            color="k",
            label=r"$16^{th}$ percentile = %.0f" % (np.percentile(snr, 16)),
        )
        plt.axvline(
            x=np.percentile(snr, 84),
            ls=":",
            color="k",
            label=r"$84^{th}$ percentile = %.0f" % (np.percentile(snr, 84)),
        )
        plt.legend(prop={"size": 14})

        crit = int(np.percentile(snr, 50) > 75)
        check = ["r", "g"][crit]  # median higher than snr 75 in at 5500 angstrom
        plt.xlabel(r"$SNR_{5500}$", fontsize=16, fontweight="bold", color=check)
        myf.plot_color_box(color=check)

        plt.savefig(self.dir_root + "IMAGES/snr_statistic_%.0f.pdf" % (version))

    def dace_statistic(self, substract_model=False):
        self.import_table()
        vec = self.import_dace_sts(substract_model=substract_model)
        # vec.recenter(who='Y')

        species = np.array(self.table["ins"])
        vec.species_recenter(species=species)

        vec.substract_polyfit(2, replace=False)
        vec.rms_w()
        vec.detrend_poly.rms_w()
        vec.night_stack()

        plt.figure(figsize=(15, 6))
        vec.plot(color="gray", alpha=0.25, capsize=0, label="rms : %.2f m/s" % (vec.rms))
        vec.detrend_poly.plot(
            color="k", capsize=0, label="rms2 : %.2f m/s" % (vec.detrend_poly.rms), species=species
        )
        plt.xlabel(r"Time BJD [days]", fontsize=16)
        plt.ylabel(r"RV [m/s]", fontsize=16)
        plt.legend(prop={"size": 14})
        myf.plot_copy_time()

        mini = np.min(vec.x)
        maxi = np.max(vec.x)
        plt.title(
            "%s\n  Nb measurements : %.0f | Nb nights : %.0f | Time span : %.0f days \n   Min : %.0f (%s)  |  Max : %.0f (%s)\n   rms : %.2f m/s   |   rms2 : %.2f m/s   |   $\sigma_{\gamma}$ : %.2f m/s\n"
            % (
                self.starname,
                len(vec.x),
                len(vec.stacked.x),
                maxi - mini,
                mini,
                Time(mini + 2400000, format="jd").iso.split(" ")[0],
                maxi,
                Time(maxi + 2400000, format="jd").iso.split(" ")[0],
                vec.rms,
                vec.detrend_poly.rms,
                np.nanmedian(vec.yerr),
            ),
            fontsize=16,
            va="top",
        )
        plt.subplots_adjust(left=0.06, right=0.96, top=0.72)
        plt.savefig(self.dir_root + "IMAGES/RV_statistic.pdf")

    # =============================================================================
    #     IMPORT DACE KEPLERIAN MODEL
    # =============================================================================

    def periodogram_l1(
        self,
        vec,
        name_ext="",
        photon_noise=0,
        max_n_significant=9,
        p_min=2,
        fap_min=-2,
        sort_val="log10faps",
        species=None,
    ):

        if photon_noise == "auto":
            save = []
            noise_liste = np.arange(0.5, 2.5, 0.1)
            for n in noise_liste:
                print("\n [INFO] Noise values set to : %.2f m/s \n" % (n))
                vec.periodogram_l1(
                    starname=self.starname,
                    dataset_name=self.starname,
                    verbose=False,
                    Plot=False,
                    p_min=p_min,
                    fap_min=fap_min,
                    photon_noise=n,
                    sort_val=sort_val,
                    max_n_significant=max_n_significant,
                    text_output=0,
                    method_signi=["fap", "evidence_laplace"],
                    species=species,
                )
                save.append([vec.l1_nb_nonull, len(vec.l1_table)])

            plt.close("all")

            save = np.array(save)
            curve = myc.tableXY(noise_liste, save[:, 0])
            knee = curve.knee_detection(Plot=False)[0]
            photon_noise = noise_liste[knee]
            print(
                Fore.CYAN
                + "\n [INFO] Optimal noise value for %s set to : %.2f m/s \n"
                % (name_ext, photon_noise)
                + Fore.RESET
            )

            # plt.figure()
            # plt.subplot(2,1,1)
            # plt.plot(noise_liste,save[:,0])
            # plt.axvline(x=noise_liste[knee])
            # plt.subplot(2,1,2)
            # plt.plot(noise_liste,save[:,1])
            # plt.axvline(x=noise_liste[knee])
            # self.debug = (noise_liste, save)

        vec.periodogram_l1(
            starname=self.starname,
            dataset_name=self.starname,
            p_min=p_min,
            fap_min=fap_min,
            photon_noise=photon_noise,
            sort_val=sort_val,
            max_n_significant=max_n_significant,
            text_output=1,
            method_signi=["fap", "evidence_laplace"],
            species=species,
        )
        os.system(
            "mv "
            + self.starname
            + "_l1_periodogram_period.pdf "
            + self.dir_root
            + "KEPLERIAN/"
            + self.starname
            + "_l1_periodogram"
            + name_ext
            + ".pdf"
        )
        self.l1_n_signals = len(vec.l1_table)
        try:
            vec.l1_table = vec.l1_table.sort_values(by="peak_amp", ascending=False)
        except:
            pass

    def add_rv_model(self, model):
        """model has to be the RV time-series to substract in kms"""

        self.import_table()
        directory = self.directory
        current_model = self.table.rv_shift
        self.yarara_obs_info(kw=["RV_shift", model + current_model])

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        all_rv = np.array(model)

        plt.figure()
        plt.scatter(
            self.table["jdb"], (self.table["ccf_rv"] - np.median(self.table["ccf_rv"])) * 1000
        )
        plt.plot(self.table["jdb"], (all_rv - np.median(all_rv)) * 1000, color="k")

        print("Modification of the files to cancel the RV model")
        time.sleep(1)
        all_rv -= np.median(all_rv)
        i = -1
        for j in tqdm(files):
            i += 1
            if all_rv[i]:
                file = pd.read_pickle(j)
                # file['parameters']['RV_model'] = all_rv[i]
                flux = file["flux"]
                wave = file["wave"]
                flux_shifted = interp1d(
                    myf.doppler_r(wave, all_rv[i] * 1000)[1],
                    flux,
                    kind="linear",
                    bounds_error=False,
                    fill_value="extrapolate",
                )(wave)
                file["flux"] = flux_shifted
                ras.save_pickle(j, file)

        self.yarara_analyse_summary()

    def periodogram_kep(
        self,
        vec2,
        name_ext="",
        name_pre="iterative_",
        photon_noise=0,
        fap=0.1,
        deg=0,
        p_min=0,
        m_out=2,
        supress_outliers=True,
        model=False,
        ms=None,
        ms_kw="Gray",
        rs=None,
        rs_kw="Gray",
        jitter=0.7,
        periods=[None],
        fit_ecc=True,
        periodic=True,
        species=None,
        eval_on_t=None,
        k_tol=0.15,
        e_tol=0.70,
        transits=None,
        known_planet=[],
        auto_long_trend=True,
    ):
        self.import_star_info()

        vec = vec2.copy()
        mask_out = myf.rm_outliers(vec.y, m=m_out, kind="inter")[0]
        if not supress_outliers:
            vec.yerr[~mask_out] = 10 * vec.yerr[~mask_out]
        else:
            vec.masked(mask_out)
        if species is not None:
            species = np.array(species)[mask_out]

        if ms is None:
            try:
                ms = self.star_info["Mstar"][ms_kw]
            except:
                ms = 1

        if rs is None:
            try:
                rs = self.star_info["Rstar"][rs_kw]
            except:
                rs = 1

        iteratif = "l1_"

        try:
            self.planet_fitted
        except:
            self.planet_fitted = {}

        if len(periods) == 0:
            periods = [None]

        if periods[0] is None:
            print("Iteratif keplerian fit")
            vec.periodogram_keplerian_hierarchical(
                photon_noise=photon_noise,
                Plot=False,
                fap=fap,
                deg=deg,
                ms=ms,
                rs=rs,
                p_min=p_min,
                jitter=jitter,
                periods=periods,
                fit_ecc=fit_ecc,
                periodic=periodic,
                species=species,
                eval_on_t=eval_on_t,
                transits=transits,
                known_planet=known_planet,
                auto_long_trend=auto_long_trend,
            )
        else:
            vec.nb_planet_fitted = len(periods)

        for j in range(5):  # 10 attempt to converge
            plt.close(1)
            vec.periodogram_keplerian_hierarchical(
                photon_noise=photon_noise,
                Plot=True,
                model=model,
                fap=fap,
                nb_planet=vec.nb_planet_fitted,
                deg=deg,
                ms=ms,
                rs=rs,
                p_min=p_min,
                jitter=jitter,
                periods=periods,
                fit_ecc=fit_ecc,
                periodic=periodic,
                species=species,
                eval_on_t=eval_on_t,
                transits=transits,
                known_planet=known_planet,
                auto_long_trend=auto_long_trend,
            )
            ks = np.min(vec.planet_fitted["k"])
            es = np.max(vec.planet_fitted["e"])
            if (ks > k_tol) & (es < e_tol):  # refuse signals smaller than 0.20 and
                break
            else:
                print(
                    " [INFO] Attempt %.0f did not converge to a good Keplerian solution (%.2f,%.2f)"
                    % (j + 1, ks, es)
                )
                if j == 4:
                    print(" [WARNING] The Keplerian fit has not converged")
        plt.savefig(
            self.dir_root
            + "KEPLERIAN/"
            + self.starname
            + "_"
            + name_pre
            + "keplerian_fit"
            + name_ext
            + ".png"
        )

        vec2.rv_residues = vec.rv_residues
        vec2.model_keplerian_i = vec.model_keplerian_i
        vec2.model_periods = vec.model_periods

        self.planet_fitted[name_ext[1:]] = vec.planet_fitted
        self.planet_fitted["info_" + name_ext[1:]] = vec.model_info

    def keplerian_estimate_df(self):
        self.import_material()
        master = self.material["reference_spectrum"] * self.material["correction_factor"]
        wave = self.material["wave"]

        rv = self.import_ccf_timeseries("CCF_" + self.mask_harps, "matching_diff", "rv")
        rv.y -= np.nanmedian(rv.y)
        rv.rms_w()

        grad = np.gradient(master) / np.gradient(wave)
        grad = abs(grad)
        grad[grad < np.percentile(grad, 16)] = 0

        try:
            self.import_snr_curve()
            mad = 1 / np.median(self.table_snr["snr_curve"], axis=0) * np.sqrt(master)
        except:
            a, b, c = self.yarara_map(
                wave_min=3000,
                wave_max=8000,
                sub_dico="matching_mad",
                Plot=False,
                reference="master",
            )
            mad = myf.mad(a, axis=0)
            mad *= self.material["correction_factor"]

        mad[master > 0.95] = 0
        mad[grad == 0] = 0
        velocity = mad / wave / grad
        velocity = velocity[velocity > 0]
        velocity *= 3e8
        v = np.log10(velocity)
        v = v[v > 0]

        histo = plt.hist(v, bins=1000, alpha=0.8, cumulative=True, density=True, histtype="step")
        plt.xlim(0, 3)
        plt.xlabel(r"$Log_{10}(RV)$ [m/s]")
        plt.axvline(
            x=np.log10(rv.rms * np.sqrt(2)),
            color="k",
            ls=":",
            label="RV raw (%.2f m/s)" % (rv.rms * np.sqrt(2)),
        )
        plt.axhline(y=0.2, color="k", ls="-.", alpha=0.5)
        k_amp = 10 ** histo[1][myf.find_nearest(histo[0], 0.2)[0][0]] / 2
        plt.axvline(
            x=np.log10(k_amp * 2),
            color="k",
            ls="-.",
            label=r"$K_{max}$ semi-amplitude (%.2f m/s)" % (k_amp),
            alpha=0.5,
        )
        plt.legend(loc=4)
        plt.title("Estimastion of the maximum allowed RV semi-amplitude")
        plt.savefig(self.dir_root + "IMAGES/Max_semi_amplitude_allowed.png")

    def keplerian_phase_test(
        self,
        vec_lbl,
        sub_dico="matching_cb",
        kw_dico="lbl_iter",
        ref_time=0,
        ref_vector=None,
        name_ext="",
        photon_noise=0,
        periods=None,
        vecs=None,
        split_vector=None,
        fit_ecc=False,
        nb_cut=3,
        deg_detrend=0,
        deg=0,
        nb_planet=3,
        bbox=(-0.25, -0.35),
        markers=["o", "x", "^", "s", "v", "<", ">"],
    ):

        if kw_dico == "lbl":
            self.import_lbl()
            imported_rv = self.lbl[sub_dico]
            kitcat = self.lbl[sub_dico]["catalog"]

        elif kw_dico == "lbl_iter":
            self.import_lbl_iter()
            imported_rv = self.lbl_iter[sub_dico]
            kitcat = self.lbl_iter[sub_dico]["catalog"]

        master = self.yarara_activity_master()
        median_wave = np.median(kitcat.loc[kitcat["valid"], "wave"])
        p33_wave = np.percentile(kitcat.loc[kitcat["valid"], "wave"], 33)
        p66_wave = np.percentile(kitcat.loc[kitcat["valid"], "wave"], 66)

        self.yarara_sas(
            sub_dico=sub_dico,
            kw_dico=kw_dico,
            g1=[["depth_rel", "<", 0.33]],
            g2=[["depth_rel", ">", 0.66]],
            Plot=False,
            save_proxy=False,
        )
        rv_g1_depth, rv_g3_depth = self.rv_g1.copy(), self.rv_g2.copy()

        self.yarara_sas(
            sub_dico=sub_dico,
            kw_dico=kw_dico,
            g1=[["depth_rel", "<", 0.33]],
            g2=[["depth_rel", ">", 0.33], ["depth_rel", "<", 0.66]],
            Plot=False,
            save_proxy=False,
        )
        rv_g1_depth, rv_g2_depth = self.rv_g1.copy(), self.rv_g2.copy()

        self.yarara_sas(
            sub_dico=sub_dico,
            kw_dico=kw_dico,
            g1=[["wave", "<", p33_wave]],
            g2=[["wave", ">", p66_wave]],
            Plot=False,
            save_proxy=False,
        )
        rv_g1_wave, rv_g3_wave = self.rv_g1.copy(), self.rv_g2.copy()

        self.yarara_sas(
            sub_dico=sub_dico,
            kw_dico=kw_dico,
            g1=[["wave", "<", p33_wave]],
            g2=[["wave", "<", p66_wave], ["wave", ">", p33_wave]],
            Plot=False,
            save_proxy=False,
        )
        rv_g1_wave, rv_g2_wave = self.rv_g1.copy(), self.rv_g2.copy()

        # time split
        st = vec_lbl.planetary_phase_test(
            Plot=True,
            periods=periods,
            vecs=vecs,
            split_vector=split_vector,
            fit_ecc=fit_ecc,
            photon_noise=photon_noise,
            deg_detrend=deg_detrend,
            deg=deg,
            nb_cut=nb_cut,
            nb_planet=nb_planet,
            bbox=bbox,
            markers=markers,
            ref_time=ref_time,
        )
        sr = vec_lbl.planetary_phase_test(
            Plot=True,
            periods=periods,
            vecs=vecs,
            split_vector=master.y,
            fit_ecc=fit_ecc,
            photon_noise=photon_noise,
            deg_detrend=deg_detrend,
            deg=deg,
            nb_cut=nb_cut,
            nb_planet=nb_planet,
            bbox=bbox,
            markers=markers,
            ref_time=ref_time,
        )
        sd = vec_lbl.planetary_phase_test(
            Plot=True,
            periods=periods,
            vecs=[rv_g1_depth, rv_g2_depth, rv_g3_depth],
            split_vector=split_vector,
            fit_ecc=fit_ecc,
            photon_noise=photon_noise,
            deg_detrend=deg_detrend,
            deg=deg,
            nb_cut=nb_cut,
            nb_planet=nb_planet,
            bbox=bbox,
            markers=markers,
            ref_time=ref_time,
        )
        sw = vec_lbl.planetary_phase_test(
            Plot=True,
            periods=periods,
            vecs=[rv_g1_wave, rv_g2_wave, rv_g3_wave],
            split_vector=split_vector,
            fit_ecc=fit_ecc,
            photon_noise=photon_noise,
            deg_detrend=deg_detrend,
            deg=deg,
            nb_cut=nb_cut,
            nb_planet=nb_planet,
            bbox=bbox,
            markers=markers,
            ref_time=ref_time,
        )
        plt.close("all")

        mat = np.array(
            [
                np.vstack([st["k"], sr["k"], sd["k"], sw["k"]]),
                np.vstack([st["k_std"], sr["k_std"], sd["k_std"], sw["k_std"]]),
                np.vstack([st["phi"], sr["phi"], sd["phi"], sw["phi"]]),
                np.vstack([st["phi_std"], sr["phi_std"], sd["phi_std"], sw["phi_std"]]),
            ]
        )

        periods = st["p"]

        rmax = (
            np.max(
                np.hstack(
                    [np.ravel(st["k"]), np.ravel(sr["k"]), np.ravel(sd["k"]), np.ravel(sw["k"])]
                )
            )
            + 0.2
        )
        xmax = np.min(
            np.hstack(
                [
                    np.ravel(st["axe_x"]),
                    np.ravel(sr["axe_x"]),
                    np.ravel(sd["axe_x"]),
                    np.ravel(sw["axe_x"]),
                ]
            )
        )
        xmin = np.max(
            np.hstack(
                [
                    np.ravel(st["axe_x2"]),
                    np.ravel(sr["axe_x2"]),
                    np.ravel(sd["axe_x2"]),
                    np.ravel(sw["axe_x2"]),
                ]
            )
        )
        ymax = np.max(
            np.hstack(
                [
                    np.ravel(st["axe_y"]),
                    np.ravel(sr["axe_y"]),
                    np.ravel(sd["axe_y"]),
                    np.ravel(sw["axe_y"]),
                ]
            )
        )

        # time split
        st = vec_lbl.planetary_phase_test(
            rmax=rmax,
            periods=periods,
            vecs=vecs,
            split_vector=split_vector,
            fit_ecc=fit_ecc,
            photon_noise=photon_noise,
            deg_detrend=deg_detrend,
            deg=deg,
            nb_cut=nb_cut,
            nb_planet=nb_planet,
            bbox=bbox,
            markers=markers,
            ref_vector=master,
            xmin=xmin,
            xmax=xmax,
            ymax=ymax,
            ref_time=ref_time,
        )
        plt.savefig(self.dir_root + "KEPLERIAN/Planetary_phase" + name_ext + "_time.png")

        # rhk split
        sr = vec_lbl.planetary_phase_test(
            rmax=rmax,
            periods=periods,
            vecs=vecs,
            split_vector=master.y,
            fit_ecc=fit_ecc,
            photon_noise=photon_noise,
            deg_detrend=deg_detrend,
            deg=deg,
            nb_cut=nb_cut,
            nb_planet=nb_planet,
            bbox=bbox,
            markers=markers,
            ref_vector=master,
            xmin=xmin,
            xmax=xmax,
            ymax=ymax,
            ref_time=ref_time,
        )
        plt.savefig(self.dir_root + "KEPLERIAN/Planetary_phase" + name_ext + "_rhk.png")

        # depth split
        sd = vec_lbl.planetary_phase_test(
            rmax=rmax,
            periods=periods,
            vecs=[rv_g1_depth, rv_g2_depth, rv_g3_depth],
            split_vector=split_vector,
            fit_ecc=fit_ecc,
            photon_noise=photon_noise,
            deg_detrend=deg_detrend,
            deg=deg,
            nb_cut=nb_cut,
            nb_planet=nb_planet,
            bbox=bbox,
            markers=markers,
            xmin=xmin,
            xmax=xmax,
            ymax=ymax,
            ref_time=ref_time,
        )
        plt.savefig(self.dir_root + "KEPLERIAN/Planetary_phase" + name_ext + "_depth.png")

        # wave split
        sw = vec_lbl.planetary_phase_test(
            rmax=rmax,
            periods=periods,
            vecs=[rv_g1_wave, rv_g2_wave, rv_g3_wave],
            split_vector=split_vector,
            fit_ecc=fit_ecc,
            photon_noise=photon_noise,
            deg_detrend=deg_detrend,
            deg=deg,
            nb_cut=nb_cut,
            nb_planet=nb_planet,
            bbox=bbox,
            markers=markers,
            xmin=xmin,
            xmax=xmax,
            ymax=ymax,
            ref_time=ref_time,
        )
        plt.savefig(self.dir_root + "KEPLERIAN/Planetary_phase" + name_ext + "_color.png")

        fig = plt.figure(figsize=(8, 7))
        ax = fig.add_subplot(111, projection="polar")
        plt.title("%.0f-Keplerian model" % (len(mat[0, 0, :])))

        colors = ["black", "blue", "red", "green", "orange", "purple", "magenta", "cyan", "brown"]
        for j in range(len(mat[0, 0, :])):
            for i in range(len(mat[0, :, 0])):
                t1 = myf.polar_err(
                    mat[2, i, j],
                    mat[0, i, j],
                    mat[3, i, j],
                    mat[1, i, j],
                    color=colors[j],
                    marker="o",
                )
            plt.polar(
                mat[2, 0, j] * np.pi / 180,
                mat[0, 0, j],
                color=colors[j],
                label="P=%.2f days" % (list(periods)[j]),
                marker="o",
            )
        plt.legend(loc=2, bbox_to_anchor=(-0.2, 1.1), ncol=1)
        plt.savefig(self.dir_root + "KEPLERIAN/Planetary_summary" + name_ext + ".png")

    def keplerian_add_model(self, starname, model_table=None, list_par_kep=None):

        if model_table is None:
            model_table = myf.format_keplerian_dace_table(planets=list_par_kep)

        if model_table is not None:
            table_keplerian = myf.touch_pickle(
                root + "/Python/database/KEPLERIAN/table_keplerians.p"
            )  # 05.08.21 #put the database inside Python directory instead of yarara
            if starname in table_keplerian.keys():
                print("\n [INFO] Old Keplerian solutions for %s : \n--------------" % (starname))
                print(table_keplerian[starname])
            table_keplerian[starname] = model_table
            print("\n [INFO] New Keplerian solutions for %s : \n--------------" % (starname))
            print(table_keplerian[starname])

            myf.pickle_dump(
                table_keplerian, open(root + "/Python/database/KEPLERIAN/table_keplerians.p", "wb")
            )
        else:
            print("\n [ERROR] Introduce a model table or keplerians parameters")

    def keplerian_import_model(self, filename=None, M_star=None):
        """
        Import DACE keplerian fit and save it in keplerian_model attribute

        Parameters
        ----------
        filename : filename of the DACE file (file must be located in the KEPLERIAN folder)
        M_star : mass of the star in solar units

        Returns
        -------
        """

        filename = np.sort(glob.glob(filename))

        try:
            self.planet_fitted
        except:
            self.planet_fitted = {}

        if M_star is None:
            self.import_star_info()
            M_star = self.star_info["Mstar"]["fixed"]

        for f in filename:
            kep = myf.import_dace_keplerian(f, Mstar=M_star)
            fname = f.split("/")[-1].split(".dace")[0]
            kep.index = ["planet %s" % (int(k)) for k in range(1, 1 + len(kep))]
            self.planet_fitted[fname] = kep

    def keplerian_clean(self):
        try:
            del self.planet_fitted
        except:
            pass

    def mcmc_import_model(self, filename=None, sigma=1):

        if filename is None:
            filename = glob.glob(self.dir_root + "KEPLERIAN/MCMC/*")[0]
            print(" [INFO] MCMC found in the directory : %s" % (filename))
            filename = filename.split("/")[-1]

        filename = np.sort(
            glob.glob(
                self.dir_root + "KEPLERIAN/MCMC/" + filename + "/results/current/tables/stats.tex"
            )
        )

        try:
            self.planet_fitted
        except:
            self.planet_fitted = {}

        for f in filename:
            kep = myf.import_dace_mcmc_gael(f, sigma=sigma)
            fname = f.split("MCMC/")[1].split("/")[0]
            kep.index = ["planet %s" % (int(k)) for k in range(1, 1 + len(kep))]
            self.planet_fitted[fname] = kep

    def mcmc_corner(
        self,
        filename=None,
        corner=True,
        kde_plot=True,
        fraction=0.20,
        alpha_s=0.1,
        alpha_c=0.1,
        nb_draw=500,
        ms=None,
        title=None,
        ext=None,
    ):

        self.mcmc_import_model(filename=filename)
        if ms is None:
            self.import_star_info()
            ms = self.star_info["Mstar"]["Gray"]
            print(" [INFO] Stellar mass found with Gray calibrations curves : %.2f Ms" % (ms))

        if filename is None:
            filename = glob.glob(self.dir_root + "KEPLERIAN/MCMC/*")[0]
            print(" [INFO] MCMC found in the directory : %s" % (filename))
            filename = filename.split("/")[-1]

        if ext is None:
            ext = "_" + filename.split("_")[-1]

        if title is None:
            title = filename.upper()

        directory = self.dir_root + "KEPLERIAN/MCMC/" + filename + "/results/current/FITS/"
        tab = self.planet_fitted[filename]
        nb_keplerian = len(tab)
        dico1 = {}
        dico2 = {}
        kw = ["P_DAY", "K_MPS", "ECC", "OMEGA_DEG", "ML0_DEG", "PLANET_M_MEARTH", "A_AU"]
        periods = []
        for index in np.arange(nb_keplerian):
            c = 0
            for f in kw:
                c += 1
                t = fits.open(directory + "KEPLERIAN" + str(index) + "_" + f + "_stat.fits")[1]
                v = t.data[t.header["TTYPE1"]]
                name = t.header["TTYPE1"]
                dico1[name.replace("KEPLERIAN", "P")[:-5]] = np.array(v).astype("float")
                if c == 1:
                    periods.append(np.median(np.array(v).astype("float")))
                if c < 6:
                    dico2[name.replace("KEPLERIAN", "P")[:-5]] = np.array(v).astype("float")

        dico1 = pd.DataFrame(dico1)
        test1 = myc.table(dico1)

        dico2 = pd.DataFrame(dico2)
        test2 = myc.table(dico2)

        periods1 = np.array(periods)
        periods2 = np.array(self.planet_fitted[filename]["P"])
        match = myf.match_nearest(periods1, periods2)[:, 0].astype("int")

        if corner:
            plt.figure(figsize=(22, 16))
            test2.pairplot(kde_plot=kde_plot, fraction=fraction, alpha_s=alpha_s)
            plt.savefig(self.dir_root + "KEPLERIAN/MCMC_Corner_plot" + ext + ".png")

        # plot of the keplerians

        plt.figure(figsize=(9, 9))
        if len(dico1) > nb_draw:
            draw_index = np.random.choice(np.arange(len(dico1)), nb_draw, replace=False)
        else:
            draw_index = np.arange(len(dico1))

        pos_x = []
        pos_y = []
        for values in tqdm(draw_index):
            new_table = tab.copy()
            for index in np.arange(nb_keplerian):
                for k1, k2 in zip(
                    ["P_DAY", "ECC", "OMEGA_DEG", "ML0_DEG", "PLANET_M_MEARTH", "A_AU"],
                    ["P", "e", "w", "L0", "m_p", "a"],
                ):
                    new_table.at["planet " + str(index + 1), k2] = dico1[
                        "P" + str(match[index]) + "_" + k1
                    ][values]

            curves = myf.fit_planet(new_table, alpha=alpha_c, legend=False)
            pos_x.append(curves[:, 0, 0])
            pos_y.append(curves[:, 0, 1])

        pos_x = np.array(pos_x)
        pos_y = np.array(pos_y)

        plt.title(title, fontsize=14)
        curves = myf.fit_planet(
            self.planet_fitted[filename]
        )  # .loc[['planet %.0f'%(i+1) for i in match]])
        hill_radii = np.array(
            myf.get_hill(
                self.planet_fitted[filename]["a"],
                self.planet_fitted[filename]["e"],
                self.planet_fitted[filename]["m_p"],
                ms,
            )
        )

        # k_amp = np.argsort(np.array(self.planet_fitted[filename]['k']))
        for j in range(nb_keplerian):
            plt.plot(curves[j, :, 0], curves[j, :, 1], color="k", lw=5, zorder=9)
            plt.plot(
                curves[j, :, 0], curves[j, :, 1], color=myf.colors_cycle_mpl[j], lw=1.5, zorder=9
            )
            all_init = myc.tableXY(pos_x[:, j], pos_y[:, j])
            all_init.kde(levels=["2d", [0.5, 1]], alpha=0.4, zorder=10)
            # all_init.kde(levels=['2d',[0.5]],alpha=0.1,contour_color='k',zorder=10)
            plt.plot(
                all_init.vertices_curve[1][:, 0],
                all_init.vertices_curve[1][:, 1],
                color="k",
                zorder=10,
            )
            myf.plot_hill(
                curves[j, 0, 0], curves[j, 0, 1], hill_radii[j], color="k", ls=":", zorder=11
            )

        plt.plot(curves[:, 0, 0], curves[:, 0, 1], "ko", zorder=20, mec="white")
        myf.plot_hz(ms=ms, ls_inf="-.", ls_sup=":", color_inf="k", color_sup="k")
        plt.legend(loc=1)
        plt.savefig(self.dir_root + "KEPLERIAN/MCMC_orbits" + ext + ".png")

    def keplerian_draw_model(self, label=None, name_ext=""):
        """
        DRAW THE DACE keplerian orbits

        Parameters
        ----------
        data_from : DACE file from which to draw the keplerian orbits

        Returns
        -------
        """

        dico2 = list(self.planet_fitted.keys())
        dico = []
        for d in dico2:
            if d[0:4] != "info":
                dico.append(d)
        nb_plot = len(dico)

        if label is None:
            label = dico

        ny = 1
        nx = nb_plot
        if nb_plot > 4:
            nx = 3
            ny = 2
        plt.figure(figsize=(8 * nx, 6 * ny))
        for k, j in enumerate(label):
            kep = self.planet_fitted[dico[k]]
            plt.subplot(ny, nx, k + 1)
            plt.title(j.upper(), fontsize=14)
            if len(kep):
                curves = myf.fit_planet(kep)
                (l,) = plt.plot(curves[:, 0, 0], curves[:, 0, 1], "ko")
        plt.subplots_adjust(left=0.06, right=0.95, top=0.93, hspace=0.3)
        plt.savefig(
            self.dir_root
            + "KEPLERIAN/"
            + self.starname
            + "_comp_traj_model_keplerian"
            + name_ext
            + ".png"
        )

    def keplerian_comp_model(
        self,
        ref="drs",
        simu_injected=False,
        label=None,
        parameters=["p", "k", "e"],
        inv=False,
        fonttitle=15,
    ):
        """
        Produce a comparison plot of the orbital parameters between DACE and YARARA

        Parameters
        ----------

        Returns
        -------
        """

        table = self.planet_fitted.copy()

        dico = np.array(list(self.planet_fitted.keys()))
        dico = [ref] + list(np.array(dico)[np.where(dico != ref)[0]])

        if label is None:
            label = dico

        mapping = {
            "P": "Period [days]",
            "K": "K [m/s]",
            "e": "Eccentricity",
            "w": "Omega [°]",
            "L0": "Periastron [°]",
            "m_p": r"Mass [$m_\oplus$]",
            "Tc": "Time conj. [BJD]",
        }

        label_par = [mapping[i] for i in parameters]

        size1 = 4 * len(table[ref])
        size2 = 4 * len(parameters)

        if not inv:
            plt.figure(figsize=(size1, size2))
        else:
            plt.figure(figsize=(size2, size1))
        idx = -1
        for model in dico[1:]:
            idx += 1
            model_drs = table[ref].copy()
            model_yarara = table[model].copy()
            model_drs = model_drs.reset_index(drop=True)
            model_yarara = model_yarara.reset_index(drop=True)
            m = myf.match_nearest(np.array(model_yarara["P"]), np.array(model_drs["P"]))
            model_yarara = model_yarara.loc[m[:, 0].astype("int")]
            model_drs = model_drs.loc[m[:, 1].astype("int")]
            model_yarara = model_yarara.reset_index(drop=True)
            model_drs = model_drs.reset_index(drop=True)

            exoplanets = model_yarara.index
            nb_planet = len(exoplanets)
            label_planet = np.arange(1, 1 + nb_planet)

            for indice in exoplanets:
                peri = np.array([-1, 0, 1]) * 360 + np.array(model_yarara.loc[indice, "w"])
                node = np.array([-1, 0, 1]) * 360 + np.array(model_yarara.loc[indice, "L0"])
                tc = np.array([-1, 0, 1]) * np.array(model_yarara.loc[indice, "P"]) + np.array(
                    model_yarara.loc[indice, "Tc"]
                )

                model_yarara.loc[indice, "w"] = peri[
                    np.argmin(abs(np.array(model_drs.loc[indice, "w"]) - peri))
                ]
                model_yarara.loc[indice, "L0"] = node[
                    np.argmin(abs(np.array(model_drs.loc[indice, "L0"]) - node))
                ]
                model_yarara.loc[indice, "Tc"] = tc[
                    np.argmin(abs(np.array(model_drs.loc[indice, "Tc"]) - tc))
                ]

            for num1, exo in enumerate(exoplanets):
                for num2, col in enumerate(parameters):
                    if inv:
                        plt.subplot(
                            nb_planet, len(parameters), len(parameters) * (num1) + num2 + 1
                        )
                    else:
                        plt.subplot(len(parameters), nb_planet, nb_planet * (num2) + num1 + 1)
                    if not idx:
                        if simu_injected:
                            plt.axhline(y=model_drs.loc[exo, col], color="k", ls=":")
                        else:
                            plt.errorbar(
                                0,
                                model_drs.loc[exo, col],
                                yerr=[
                                    [model_drs.loc[exo, col + "_std_inf"]],
                                    [model_drs.loc[exo, col + "_std_sup"]],
                                ],
                                fmt="ko",
                                label=label[0],
                            )
                    plt.errorbar(
                        1 + idx,
                        model_yarara.loc[exo, col],
                        yerr=[
                            [model_yarara.loc[exo, col + "_std_inf"]],
                            [model_yarara.loc[exo, col + "_std_sup"]],
                        ],
                        color=None,
                        fmt="o",
                        label=label[idx + 1],
                    )
                    plt.tick_params(labelbottom=False, direction="in", top=True)
                    plt.xticks(np.arange(len(label)), label, rotation="vertical", fontsize=15)
                    plt.xlim(-1 + int(simu_injected), 2 + idx)
                    if (num1 == 0) & (not inv):
                        plt.ylabel(label_par[num2], fontsize=15)
                    if inv:
                        plt.ylabel(label_par[num2], fontsize=15)
                    if (num2 == 0) & (not inv):
                        plt.title("Planet %.0f" % (label_planet[num1]), fontsize=fonttitle)
                    if (((num1) == nb_planet - 1) & (inv)) | (
                        ((num2) == len(parameters) - 1) & (not inv)
                    ):
                        plt.tick_params(labelbottom=True)

        plt.subplots_adjust(left=0.07, top=0.95, right=0.97, bottom=0.17, hspace=0.10, wspace=0.35)
        plt.savefig(self.dir_root + "KEPLERIAN/comp_par_model_keplerian.pdf")

    def simu_detection_limit(
        self,
        rv,
        rv_ref=None,
        add_noise=0.0,
        degree_detrending=2,
        nb_phase=36,
        nb_perm=1000,
        ofac=3,
        fap=0.99,
        k=np.arange(0.2, 2.1, 0.2),
        p=myf.my_ruler(4.9, 149.9, 0.1, 3),
        multi_cpu=4,
    ):
        """multi_cpu = [nb parallel process, index of the cpu]"""
        self.import_table
        table = self.table

        if fap is not None:
            if fap * nb_perm < 3:
                print("The fap level will be not trustworthy")

        phi = np.linspace(-np.pi, np.pi, nb_phase + 1)[:-1]

        k_large = k[::4]
        p_large = p[::10]
        phi_large = phi[::6]

        if multi_cpu < 1:
            multi_cpu = 1

        for cpu in range(multi_cpu):

            if rv_ref is None:
                rv_ref = myc.tableXY(table["jdb"], table["rv_dace"], table["rv_dace_std"])
            rv.yerr = np.sqrt((add_noise) ** 2 + (rv.yerr) ** 2)
            rv_ref.yerr = np.sqrt((add_noise) ** 2 + (rv_ref.yerr) ** 2)

            rv.substract_polyfit(degree_detrending, replace=True)
            rv_ref.substract_polyfit(degree_detrending, replace=True)

            permu = np.array(list(iter.product(k, p, phi)))
            print(
                "\n Number of simulations = %.0f \n Number of different phases = %.0f "
                % (len(permu), len(phi))
            )

            permu = np.split(permu, multi_cpu)[cpu]
            print("\n Number of simulations for chunck %.0f = %.0f  " % (cpu + 1, len(permu)))

            matrix_ref = []
            matrix = []
            fap_levels_ref = []
            fap_levels = []

            permu = np.hstack([permu])

            time.sleep(1)

            rv.x -= np.min(rv.x)
            rv_ref.x -= np.min(rv_ref.x)

            for items in tqdm(permu):
                amp = items[0]
                periode = items[1]
                pulsation = 2 * np.pi / periode
                dephasage = items[2]

                planet_rv = amp * np.sin(pulsation * rv.x + dephasage)
                rv.y += planet_rv
                rv_ref.y += planet_rv

                rv_ref.periodogram(
                    nb_perm=nb_perm,
                    all_outputs=True,
                    Plot=False,
                    p_min=0.7 * np.min(p),
                    ofac=ofac,
                    level=fap,
                )
                rv.periodogram(
                    nb_perm=nb_perm,
                    all_outputs=True,
                    Plot=False,
                    p_min=0.7 * np.min(p),
                    ofac=ofac,
                    level=fap,
                )

                matrix_ref.append([1 / rv_ref.freq, rv_ref.power, rv_ref.amplitude, rv_ref.phase])
                matrix.append([1 / rv.freq, rv.power, rv.amplitude, rv.phase])

                fap_levels_ref.append([rv_ref.fap10, rv_ref.fap1, rv_ref.fap01])
                fap_levels.append([rv.fap10, rv.fap1, rv.fap01])

                # fap_levels_ref.append(rv_ref.fap)
                # fap_levels.append(rv.fap)

                rv.y -= planet_rv
                rv_ref.y -= planet_rv

            matrix = np.array(matrix)
            matrix_ref = np.array(matrix_ref)

            fap_levels = np.array(fap_levels)[:, np.newaxis]
            fap_levels_ref = np.array(fap_levels_ref)[:, np.newaxis]

            dico = {
                "permu": permu,
                "simu_perio": matrix,
                "simu_perio_ref": matrix_ref,
                "fap": fap_levels,
                "fap_ref": fap_levels_ref,
            }

            myf.pickle_dump(
                dico,
                open(
                    self.dir_root
                    + "DETECTION_LIMIT/Simulation_K_%.1f_%.1f_P_%.0f_%.0f_%.0fp_%.0f.p"
                    % (np.min(k), np.max(k), np.min(p), np.max(p), nb_phase, cpu + 1),
                    "wb",
                ),
            )
            print(
                "\nFile"
                + self.dir_root
                + "DETECTION_LIMIT/Simulation_K_%.1f_%.1f_P_%.0f_%.0f_%.0fp_%.0f.p"
                % (np.min(k), np.max(k), np.min(p), np.max(p), nb_phase, cpu + 1)
                + " saved"
            )

    def plot_basis_fitted(self):
        file = pd.read_pickle(self.dir_root + "KEPLERIAN/Vectors_fitted.p")
        sb = list(file.keys())

        self.import_table()
        time = np.array(self.table["jdb"])

        self.import_dico_tree()
        dico_tree = self.dico_tree
        tree = dico_tree.loc[np.in1d(dico_tree["dico"], sb)].sort_values(by="step")["dico"].values

        maxi = len(file[tree[-1]])
        rows = int((maxi + 1) / 2)
        print("Nb vec fitted between V1 and V2 : %.0f" % (maxi))

        color_cycle = ["k", "b", "r", "g", "purple", "magenta", "limegreen"]

        k = 0
        old = 0
        plt.figure(figsize=(18, 9))
        for n, t in enumerate(tree[1:]):
            basis = file[t][old:]
            old += len(basis)
            print("stage : %s, nb comp : %.0f" % (t, len(basis)))
            for c in np.arange(len(basis)):
                k += 1
                vec = myc.tableXY(time, basis[c])
                plt.subplot(rows, 4, 2 * k - 1)
                vec.periodogram(Norm=True, color=color_cycle[n])
                plt.subplot(rows, 4, 2 * k)
                vec.null()
                plt.scatter(vec.x, vec.y, color=color_cycle[n], s=10)
                plt.title("%s : %.0f" % (t.split("_")[1], c + 1))

        plt.subplots_adjust(left=0.05, right=0.96, top=0.96, bottom=0.09, hspace=0.5, wspace=0.35)
        plt.show(block=False)
        plt.savefig(self.dir_root + "KEPLERIAN/All_vectors_YARARA_V2_basis.png")

    def plot_detection_limit(
        self,
        files="",
        faps=1,
        Mstar=1,
        tresh_diff=0.1,
        tresh_phase=0.2,
        cmap="gist_earth",
        color_line="k",
        xscale="linear",
        overfit=1,
        button_phi=1,
        button_k=1,
        highest=True,
    ):

        files = np.sort(glob.glob(self.dir_root + "DETECTION_LIMIT/Simu*" + files + "*"))
        for i, f in enumerate(files):
            print("File used for the detection limit : %s" % (f))
            if not i:
                dico = pd.read_pickle(f)
            else:
                dico2 = pd.read_pickle(f)
                for k in dico.keys():
                    dico[k] = np.vstack([dico[k], dico2[k]])

        time.sleep(1)

        permu = dico["permu"]
        mass_true = myf.AmpStar(Mstar, 0, permu[:, 1] / 365.25, permu[:, 0], code="Sun-Earth")
        permu = np.hstack([permu, mass_true[:, np.newaxis]])

        matrix = dico["simu_perio"]
        matrix_ref = dico["simu_perio_ref"]
        fap_levels = dico["fap"]
        fap_levels_ref = dico["fap_ref"]
        grid_period = matrix[0, 0, :]

        if len(np.shape(fap_levels)) > 2:
            fap_levels = fap_levels[:, 0, :]
        if len(np.shape(fap_levels_ref)) > 2:
            fap_levels_ref = fap_levels_ref[:, 0, :]

        k = np.unique(permu[:, 0])
        p = np.unique(permu[:, 1])
        K, P = np.meshgrid(k, p)

        m = np.zeros(np.shape(K))
        detection = np.zeros((np.shape(K)[0], np.shape(K)[1], len(fap_levels[0])))
        detection_ref = np.zeros((np.shape(K)[0], np.shape(K)[1], len(fap_levels[0])))

        err_p = np.zeros((np.shape(K)[0], np.shape(K)[1], len(fap_levels[0])))
        err_k = np.zeros((np.shape(K)[0], np.shape(K)[1], len(fap_levels[0])))
        err_phi = np.zeros((np.shape(K)[0], np.shape(K)[1], len(fap_levels[0])))
        err_m = np.zeros((np.shape(K)[0], np.shape(K)[1], len(fap_levels[0])))

        err_p_ref = np.zeros((np.shape(K)[0], np.shape(K)[1], len(fap_levels[0])))
        err_k_ref = np.zeros((np.shape(K)[0], np.shape(K)[1], len(fap_levels[0])))
        err_phi_ref = np.zeros((np.shape(K)[0], np.shape(K)[1], len(fap_levels[0])))
        err_m_ref = np.zeros((np.shape(K)[0], np.shape(K)[1], len(fap_levels[0])))

        mask = np.zeros(np.shape(matrix[:, 1, :])).astype("bool")
        if not highest:
            mask = (
                abs(grid_period - permu[:, 1][:, np.newaxis]) / permu[:, 1][:, np.newaxis]
            ) > tresh_diff

        matrix[:, 1, :][mask] = 0
        matrix_ref[:, 1, :][mask] = 0

        perio_max, power_max, ampli_max, phase_max = np.array(
            [matrix[i, :, k] for i, k in enumerate(np.argmax(matrix[:, 1, :], axis=1))]
        ).T
        # dphase_max = abs(np.array([np.gradient(matrix[i][3])[k] for i,k in enumerate(np.argmax(matrix[:,1,:],axis=1))]))
        mass_max = myf.AmpStar(Mstar, 0, perio_max / 365.25, ampli_max, code="Sun-Earth")

        err_rel_p = abs(perio_max - permu[:, 1]) / permu[:, 1]
        err_rel_k = abs(ampli_max - permu[:, 0]) / permu[:, 0]
        err_rel_m = abs(mass_max - permu[:, 3]) / permu[:, 3]
        err_rel_phi = abs(myf.dist_modulo(phase_max, permu[:, 2]) / 180)

        valid = power_max > fap_levels.T
        valid_period = err_rel_p < tresh_diff
        valid_amplitude = button_k * err_rel_k < tresh_diff
        valid_phase = (button_phi * err_rel_phi) < (tresh_phase)
        all_valid = valid * valid_period * valid_amplitude * valid_phase

        perio_max, power_max, ampli_max, phase_max = np.array(
            [matrix_ref[i, :, k] for i, k in enumerate(np.argmax(matrix_ref[:, 1, :], axis=1))]
        ).T
        mass_max = myf.AmpStar(Mstar, 0, perio_max / 365.25, ampli_max, code="Sun-Earth")

        err_rel_p_ref = abs(perio_max - permu[:, 1]) / permu[:, 1]
        err_rel_k_ref = abs(ampli_max - permu[:, 0]) / permu[:, 0]
        err_rel_m_ref = abs(mass_max - permu[:, 3]) / permu[:, 3]
        err_rel_phi_ref = abs(myf.dist_modulo(phase_max, permu[:, 2]) / 180)

        valid = power_max > fap_levels_ref.T
        valid_period = err_rel_p_ref < tresh_diff
        valid_amplitude = button_k * err_rel_k_ref < tresh_diff
        valid_phase = (button_phi * err_rel_phi_ref) < (tresh_phase)
        all_valid_ref = valid * valid_period * valid_amplitude * valid_phase

        c = -1
        for period in tqdm(p):
            c += 1
            for r, amp in enumerate(k):
                loc = np.where((permu[:, 0] == amp) & (permu[:, 1] == period))[0]
                m[c, r] = permu[loc[0], 3]
                det = np.sum(all_valid[:, loc], axis=1) / len(loc) * 100
                det_ref = np.sum(all_valid_ref[:, loc], axis=1) / len(loc) * 100

                detection[c, r, :] = det
                detection_ref[c, r, :] = det_ref

                err_p[c, r, :] = np.nanmean(err_rel_p[loc]) * 100
                err_k[c, r, :] = np.nanmean(err_rel_k[loc]) * 100
                err_phi[c, r, :] = np.nanmean(err_rel_phi[loc]) * 100
                err_m[c, r, :] = np.nanmean(err_rel_m[loc]) * 100

                err_p_ref[c, r, :] = np.nanmean(err_rel_p_ref[loc]) * 100
                err_k_ref[c, r, :] = np.nanmean(err_rel_k_ref[loc]) * 100
                err_phi_ref[c, r, :] = np.nanmean(err_rel_phi_ref[loc]) * 100
                err_m_ref[c, r, :] = np.nanmean(err_rel_m_ref[loc]) * 100

        def plot_contour(
            x,
            y,
            z,
            cmap="seismic",
            vmin=None,
            vmax=None,
            ticks="abs",
            clabel=r"Detection rate [%]",
        ):

            if vmin is None:
                vmin = 0
            if vmax is None:
                vmax = 100

            z = scipy.ndimage.zoom(z, overfit)
            x = scipy.ndimage.zoom(x, overfit)
            y = scipy.ndimage.zoom(y, overfit)

            x2 = np.hstack(
                [
                    x[:, 0][0] - 0.5 * np.diff(x[:, 0])[0],
                    x[:, 0][:-1] + 0.5 * np.diff(x[:, 0]),
                    x[:, 0][-1] + 0.5 * np.diff(x[:, 0])[-1],
                ]
            )
            x2 = (x2 * np.ones(len(x[0]) + 1)[:, np.newaxis]).T

            y2 = np.hstack(
                [
                    y[0][0] - 0.5 * np.diff(y[0])[0],
                    y[0][:-1] + 0.5 * np.diff(y[0]),
                    y[0][-1] + 0.5 * np.diff(y[0])[-1],
                ]
            )
            y2 = y2 * np.ones(len(y) + 1)[:, np.newaxis]

            plt.pcolormesh(x2, y2, z, shading="auto", cmap=cmap, vmin=vmin, vmax=vmax)
            plt.ylabel("Semi-amplitude $K$ [m/s]", fontsize=13)
            plt.xlabel("Period $P$ [days]", fontsize=13)
            if ticks:
                ax = plt.colorbar(
                    ticks=[[-100, -50, 0, 50, 100], [0, 25, 50, 75, 100]][ticks == "abs"]
                )  # ,extend='both')
            else:
                ax = plt.colorbar()
            ax.ax.set_ylabel(clabel, fontsize=13)

            levels = np.arange(2, 22, 3)
            C = plt.contour(
                x,
                y,
                myf.AmpStar(Mstar, 0, x / 365.25, y, code="Sun-Earth"),
                colors=color_line,
                levels=levels,
            )
            strs = [r"" + np.str(mot) + "$M_{\oplus}$" for mot in levels]
            fmt = {}
            for l, s in zip(C.levels, strs):
                fmt[l] = s
            plt.clabel(C, inline=1, fontsize=13, fmt=fmt)

        if faps is None:
            faps = np.arange(len(fap_levels[0]))
        else:
            if faps >= len(fap_levels[0]):
                print("The fap column specified do not exist, column=0 selected")
                faps = [0]
            else:
                faps = [faps]

        plt.figure(figsize=(24, 6))
        for j, i in enumerate(faps):
            plt.subplot(len(faps), 3, j * 3 + 1)
            plt.title("DRS", fontsize=13)
            plot_contour(P, K, detection_ref[:, :, i], cmap=cmap, vmin=-5, vmax=105)
            plt.xscale(xscale)
            ax = plt.gca()
            plt.subplot(len(faps), 3, j * 3 + 2, sharex=ax, sharey=ax)
            plt.title("YARARA", fontsize=13)
            plot_contour(P, K, detection[:, :, i], cmap=cmap, vmin=-5, vmax=105)
            plt.xscale(xscale)
            plt.subplot(len(faps), 3, j * 3 + 3, sharex=ax, sharey=ax)
            plt.title("YARARA - DRS", fontsize=13)
            plot_contour(
                P,
                K,
                detection[:, :, i] - detection_ref[:, :, i],
                vmin=-100,
                vmax=100,
                ticks="diff",
            )
            plt.xscale(xscale)

        plt.subplots_adjust(wspace=0.35, hspace=0.35, top=0.95, bottom=0.13, left=0.05, right=0.95)
        plt.savefig(self.dir_root + "IMAGES/Detection_limits.png")

        plt.figure(figsize=(20, 20))
        plt.subplot(3, 3, 1)
        ax = plt.gca()
        plot_contour(
            P,
            K,
            err_p_ref[:, :, i],
            cmap=cmap + "_r",
            vmin=0,
            vmax=tresh_diff * 100,
            ticks=False,
            clabel=r"$\Delta P /P$ [%]",
        )
        # plt.subplot(3,3,2, sharex=ax, sharey=ax)
        # plot_contour(P,K,err_k_ref[:,:,i],cmap=cmap+'_r',vmin=0,vmax=tresh_diff*100,ticks=False, clabel=r'$\Delta K /K$ [%]')
        plt.subplot(3, 3, 2, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_m_ref[:, :, i],
            cmap=cmap + "_r",
            vmin=0,
            vmax=tresh_diff * 100,
            ticks=False,
            clabel=r"$\Delta M /M$ [%]",
        )
        plt.subplot(3, 3, 3, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_phi_ref[:, :, i],
            cmap=cmap + "_r",
            vmin=0,
            vmax=tresh_phase * 100,
            ticks=False,
            clabel=r"$\Delta \phi $ [%]",
        )
        plt.subplot(3, 3, 4, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_p[:, :, i],
            cmap=cmap + "_r",
            vmin=0,
            vmax=tresh_diff * 100,
            ticks=False,
            clabel=r"$\Delta P /P$ [%]",
        )
        # plt.subplot(3,3,6, sharex=ax, sharey=ax)
        # plot_contour(P,K,err_k[:,:,i],cmap=cmap+'_r',vmin=0,vmax=tresh_diff*100,ticks=False, clabel=r'$\Delta K /K$ [%]')
        plt.subplot(3, 3, 5, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_m[:, :, i],
            cmap=cmap + "_r",
            vmin=0,
            vmax=tresh_diff * 100,
            ticks=False,
            clabel=r"$\Delta M/ M $ [%]",
        )
        plt.subplot(3, 3, 6, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_phi[:, :, i],
            cmap=cmap + "_r",
            vmin=0,
            vmax=tresh_phase * 100,
            ticks=False,
            clabel=r"$\Delta \phi $ [%]",
        )
        plt.subplot(3, 3, 7, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_p_ref[:, :, i] - err_p[:, :, i],
            vmin=-tresh_diff * 200,
            vmax=tresh_diff * 200,
            ticks=False,
            clabel=r"$\Delta P /P$ [%]",
        )
        # plt.subplot(3,3,10, sharex=ax, sharey=ax)
        # plot_contour(P,K,err_k[:,:,i] - err_k_ref[:,:,i],vmin=-tresh_diff*100,vmax=tresh_diff*100,ticks=False, clabel=r'$\Delta K /K$ [%]')
        plt.subplot(3, 3, 8, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_m_ref[:, :, i] - err_m[:, :, i],
            vmin=-tresh_diff * 200,
            vmax=tresh_diff * 200,
            ticks=False,
            clabel=r"$\Delta M/ M $ [%]",
        )
        plt.subplot(3, 3, 9, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_phi_ref[:, :, i] - err_phi[:, :, i],
            vmin=-tresh_phase * 200,
            vmax=tresh_phase * 200,
            ticks=False,
            clabel=r"$\Delta \phi $ [%]",
        )
        plt.xscale(xscale)
        plt.subplots_adjust(wspace=0.35, hspace=0.35, top=0.95, bottom=0.13, left=0.05, right=0.95)

        plt.figure(figsize=(22, 6))
        plt.subplot(1, 3, 1, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_p_ref[:, :, i] - err_p[:, :, i],
            vmin=-tresh_diff * 200,
            vmax=tresh_diff * 200,
            ticks=False,
            clabel=r"$\Delta P /P$ [%]",
        )
        plt.subplot(1, 3, 2, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_m_ref[:, :, i] - err_m[:, :, i],
            vmin=-tresh_diff * 200,
            vmax=tresh_diff * 200,
            ticks=False,
            clabel=r"$\Delta M/ M $ [%]",
        )
        plt.subplot(1, 3, 3, sharex=ax, sharey=ax)
        plot_contour(
            P,
            K,
            err_phi_ref[:, :, i] - err_phi[:, :, i],
            vmin=-tresh_phase * 200,
            vmax=tresh_phase * 200,
            ticks=False,
            clabel=r"$\Delta \phi $ [%]",
        )
        plt.xscale(xscale)
        plt.subplots_adjust(wspace=0.35, hspace=0.35, top=0.95, bottom=0.13, left=0.05, right=0.95)
        plt.savefig(self.dir_root + "IMAGES/Detection_limits_accuracy.png")

    def simu_detection_limit2(
        self,
        rv,
        ext="",
        photon_noise=0.0,
        degree_detrending=2,
        nb_phase=36,
        nb_perm=1,
        ofac=5,
        fap=0.99,
        k=np.arange(0.2, 5.1, 0.2),
        p=myf.my_ruler(2.9, 2000, 0.1, 50),
        tresh_diff=0.1,
        tresh_phase=0.2,
        button_phi=1,
        button_k=1,
        highest=True,
    ):

        baseline = 0.5 * (np.max(rv.x) - np.min(rv.x))  # minimum two cycle
        p = p[p < baseline]

        phi = np.linspace(-np.pi, np.pi, nb_phase + 1)[:-1]

        rv.yerr = np.sqrt((photon_noise) ** 2 + (rv.yerr) ** 2)
        rv.substract_polyfit(degree_detrending, replace=True)
        rv.x -= np.min(rv.x)
        rv.rms_w()

        k_large = np.linspace(np.round(0.5 * rv.rms, 1), np.round(3 * rv.rms, 1), 20)
        p_large = p[:: len(p) // 30]
        phi_large = np.linspace(-np.pi, np.pi, 11)[:-1]

        permu = np.array(list(iter.product(k_large, p_large, phi_large)))
        print(" 50/100 detection limit calibration")
        print(
            "\n Number of simulations (large grid) = %.0f \n Number of different phases = %.0f "
            % (len(permu), len(phi_large))
        )
        permu = np.hstack([permu])

        matrix = []
        faps = []
        time.sleep(1)

        for items in tqdm(permu):
            amp = items[0]
            periode = items[1]
            pulsation = 2 * np.pi / periode
            dephasage = items[2]

            planet_rv = amp * np.sin(pulsation * rv.x + dephasage)
            rv.y += planet_rv
            rv.periodogram(
                nb_perm=nb_perm,
                all_outputs=True,
                Plot=False,
                p_min=0.7 * np.min(p),
                ofac=ofac,
                level=fap,
            )
            matrix.append([1 / rv.freq, rv.power, rv.amplitude, rv.phase])
            faps.append(rv.fap)
            rv.y -= planet_rv
        matrix = np.array(matrix)
        faps = np.array(faps)

        permu = np.hstack([permu])
        grid_period = 1 / rv.freq

        K, P = np.meshgrid(k_large, p_large)

        detection = np.zeros((np.shape(K)[0], np.shape(K)[1]))

        mask = np.zeros(np.shape(matrix[:, 1, :])).astype("bool")
        if not highest:
            mask = (
                abs(grid_period - permu[:, 1][:, np.newaxis]) / permu[:, 1][:, np.newaxis]
            ) > tresh_diff

        matrix[:, 1, :][mask] = 0

        perio_max, power_max, ampli_max, phase_max = np.array(
            [matrix[i, :, k] for i, k in enumerate(np.argmax(matrix[:, 1, :], axis=1))]
        ).T

        err_rel_p = abs(perio_max - permu[:, 1]) / permu[:, 1]
        err_rel_k = abs(ampli_max - permu[:, 0]) / permu[:, 0]
        err_rel_phi = abs(myf.dist_modulo(phase_max, permu[:, 2]) / 180)

        valid = power_max > faps.T
        valid_period = err_rel_p < tresh_diff
        valid_amplitude = button_k * err_rel_k < tresh_diff
        valid_phase = (button_phi * err_rel_phi) < (tresh_phase)
        all_valid = valid * valid_period * valid_amplitude * valid_phase

        c = -1
        for period in p_large:
            c += 1
            for r, amp in enumerate(k_large):
                loc = np.where((permu[:, 0] == amp) & (permu[:, 1] == period))[0]
                det = np.sum(all_valid[loc]) / len(loc) * 100
                detection[c, r] = det

        curve = myc.tableXY(np.log10(p_large), k_large[np.argmin(abs(detection - 50), axis=1)])
        curve.interpolate(new_grid=np.log10(p), replace=True, method="linear", interpolate_x=False)
        curve.x = p

        def calib(period):
            idx_center = myf.find_nearest(k, curve.y[myf.find_nearest(curve.x, period)[0]])[0]
            return idx_center

        detection_50 = calib(p)

        permu = np.array(list(iter.product(k, p, phi)))
        print(
            "\n Number of simulations (large grid) = %.0f \n Number of different phases = %.0f "
            % (len(permu), len(phi))
        )
        permu = np.hstack([permu])

        time.sleep(1)

        count = 0
        cells = []
        save = []
        index = np.arange(len(p))
        for i in tqdm(index):
            periode = p[i]
            pulsation = 2 * np.pi / periode
            for amp in k[detection_50[i] :]:
                cells.append([periode, amp])
                count += 1
                detection = 0
                for dephasage in phi:
                    planet_rv = amp * np.sin(pulsation * rv.x + dephasage)
                    rv.y += planet_rv
                    rv.periodogram(
                        nb_perm=nb_perm,
                        all_outputs=True,
                        Plot=False,
                        p_min=0.7 * np.min(p),
                        ofac=ofac,
                        level=fap,
                    )
                    rv.y -= planet_rv

                    mask = np.zeros(len(rv.freq)).astype("bool")
                    if not highest:
                        mask = (abs(grid_period - periode) / periode) > tresh_diff
                    rv.power[mask] = 0

                    maximum = rv.power.argmax()
                    perio_max = 1 / rv.freq[maximum]
                    power_max = rv.power[maximum]
                    ampli_max = rv.amplitude[maximum]
                    phase_max = rv.phase[maximum]

                    err_rel_p = abs(perio_max - periode) / periode
                    err_rel_k = abs(ampli_max - amp) / amp
                    err_rel_phi = abs(myf.dist_modulo(phase_max, dephasage) / 180)

                    valid = power_max > rv.fap
                    valid_period = err_rel_p < tresh_diff
                    valid_amplitude = button_k * err_rel_k < tresh_diff
                    valid_phase = (button_phi * err_rel_phi) < (tresh_phase)
                    all_valid = valid * valid_period * valid_amplitude * valid_phase

                    detected = int(all_valid)
                    if detected:
                        save.append([periode, amp, dephasage, perio_max, ampli_max, phase_max])

                    detection += detected

                if detection == len(phi):
                    break

            for amp in k[0 : detection_50[i]][::-1]:
                cells.append([periode, amp])
                detection = 0
                count += 1
                for dephasage in phi:
                    planet_rv = amp * np.sin(pulsation * rv.x + dephasage)
                    rv.y += planet_rv
                    rv.periodogram(
                        nb_perm=nb_perm,
                        all_outputs=True,
                        Plot=False,
                        p_min=0.7 * np.min(p),
                        ofac=ofac,
                        level=fap,
                    )
                    rv.y -= planet_rv

                    mask = np.zeros(len(rv.freq)).astype("bool")
                    if not highest:
                        mask = (abs(grid_period - periode) / periode) > tresh_diff
                    rv.power[mask] = 0

                    maximum = rv.power.argmax()
                    perio_max = 1 / rv.freq[maximum]
                    power_max = rv.power[maximum]
                    ampli_max = rv.amplitude[maximum]
                    phase_max = rv.phase[maximum]

                    err_rel_p = abs(perio_max - periode) / periode
                    err_rel_k = abs(ampli_max - amp) / amp
                    err_rel_phi = abs(myf.dist_modulo(phase_max, dephasage) / 180)

                    valid = power_max > rv.fap
                    valid_period = err_rel_p < tresh_diff
                    valid_amplitude = button_k * err_rel_k < tresh_diff
                    valid_phase = (button_phi * err_rel_phi) < (tresh_phase)
                    all_valid = valid * valid_period * valid_amplitude * valid_phase

                    detected = int(all_valid)
                    if detected:
                        save.append([periode, amp, dephasage, perio_max, ampli_max, phase_max])

                    detection += detected
                if detection == 0:
                    break

        save = np.array(save)
        cells = np.array(cells)
        K, P = np.meshgrid(k, p)

        closest_p = myf.find_nearest(p, cells[:, 0])[0]
        closest_k = myf.find_nearest(k, cells[:, 1])[0]
        cells_explored = np.zeros(np.shape(K))
        for row, col in zip(closest_p, closest_k):
            cells_explored[row, col] = cells_explored[row, col] + 1
        cells_explored = cells_explored != 0

        closest_p = myf.find_nearest(p, save[:, 0])[0]
        closest_k = myf.find_nearest(k, save[:, 1])[0]
        detection_limit = np.zeros(np.shape(K))
        for row, col in zip(closest_p, closest_k):
            detection_limit[row, col] = detection_limit[row, col] + 1

        detection_limit /= len(phi) / 100
        detection_limit = detection_limit

        for n in range(len(cells_explored)):
            maxi_100 = np.where(detection_limit[n, :] == 100)[0]
            if len(maxi_100):
                m = np.max(maxi_100)
                detection_limit[n, m:] = 100

        print(
            "K,P grid elements probe : %.0f/%.0f (%.0f%%)"
            % (count, len(k) * len(p), count * 100 / (len(k) * len(p)))
        )

        dico = {"period": P.T, "amplitude": K.T, "detection_rate": detection_limit.T}

        myf.pickle_dump(
            dico,
            open(self.dir_root + "DETECTION_LIMIT/Simu_h%.0f" % (int(highest)) + ext + ".p", "wb"),
        )

        plt.figure(figsize=(15, 5))
        plt.subplots_adjust(left=0.08, right=0.99)
        plt.pcolormesh(P.T, K.T, detection_limit.T, shading="auto", cmap="plasma")
        plt.xscale("log")
        plt.xlabel("Period [days]", fontsize=13)
        plt.ylabel("K [m/s]", fontsize=13)
        ax = plt.colorbar()
        ax.ax.set_ylabel("Detection rate [%]", fontsize=13)

    def plot_detection_limit2(
        self, file_ref=None, file=None, Mstar=None, overfit=1, color_line="white", cmap="viridis"
    ):

        self.import_star_info()
        if Mstar is None:
            Mstar = self.star_info["Mstar"]["Gray"]

        if file_ref is None:
            file_ref = pd.read_pickle(glob.glob(self.dir_root + "DETECTION_LIMIT/Simu_h*drs*")[0])
        else:
            file_ref = pd.read_pickle(
                glob.glob(self.dir_root + "DETECTION_LIMIT/*" + file_ref + "*")[0]
            )

        if file is None:
            file = pd.read_pickle(glob.glob(self.dir_root + "DETECTION_LIMIT/Simu_h*yarara*")[0])
        else:
            file = pd.read_pickle(glob.glob(self.dir_root + "DETECTION_LIMIT/*" + file + "*")[0])

        def plot_contour(
            x,
            y,
            z,
            cmap="seismic",
            vmin=None,
            vmax=None,
            ticks="abs",
            clabel=r"Detection rate [%]",
        ):

            if vmin is None:
                vmin = 0
            if vmax is None:
                vmax = 100

            z = scipy.ndimage.zoom(z, overfit)
            x = scipy.ndimage.zoom(x, overfit)
            y = scipy.ndimage.zoom(y, overfit)

            x2 = np.hstack(
                [
                    x[:, 0][0] - 0.5 * np.diff(x[:, 0])[0],
                    x[:, 0][:-1] + 0.5 * np.diff(x[:, 0]),
                    x[:, 0][-1] + 0.5 * np.diff(x[:, 0])[-1],
                ]
            )
            x2 = (x2 * np.ones(len(x[0]) + 1)[:, np.newaxis]).T

            y2 = np.hstack(
                [
                    y[0][0] - 0.5 * np.diff(y[0])[0],
                    y[0][:-1] + 0.5 * np.diff(y[0]),
                    y[0][-1] + 0.5 * np.diff(y[0])[-1],
                ]
            )
            y2 = y2 * np.ones(len(y) + 1)[:, np.newaxis]

            plt.pcolormesh(x2, y2, z, shading="auto", cmap=cmap, vmin=vmin, vmax=vmax)

            plt.ylabel("Semi-amplitude $K$ [m/s]", fontsize=13)
            plt.xlabel("Period $P$ [days]", fontsize=13)
            if ticks:
                ax = plt.colorbar(
                    ticks=[[-100, -50, 0, 50, 100], [0, 25, 50, 75, 100]][ticks == "abs"]
                )  # ,extend='both')
            else:
                ax = plt.colorbar()
            ax.ax.set_ylabel(clabel, fontsize=13)

            levels = np.arange(1, 22, 2)
            C = plt.contour(
                x,
                y,
                myf.AmpStar(Mstar, 0, x / 365.25, y, code="Sun-Earth"),
                colors=color_line,
                levels=levels,
            )
            strs = [r"" + np.str(mot) + "$M_{\oplus}$" for mot in levels]
            fmt = {}
            for l, s in zip(C.levels, strs):
                fmt[l] = s
            plt.clabel(C, inline=1, fontsize=13, fmt=fmt)

        K_ref = file_ref["amplitude"]
        P_ref = file_ref["period"]
        detection_ref = file_ref["detection_rate"]

        K = file["amplitude"]
        P = file["period"]
        detection = file["detection_rate"]

        plt.figure(figsize=(24, 18))
        plt.subplots_adjust(left=0.06, right=0.94, top=0.9, bottom=0.1, hspace=0.3, wspace=0.3)
        plt.subplot(3, 1, 1)
        plt.title("DRS", fontsize=13)
        plot_contour(P_ref.T, K_ref.T, detection_ref.T, cmap=cmap, vmin=-5, vmax=105)
        ax = plt.gca()
        plt.subplot(3, 1, 2, sharex=ax, sharey=ax)
        plt.title("YARARA", fontsize=13)
        plot_contour(P.T, K.T, detection.T, cmap=cmap, vmin=-5, vmax=105)
        plt.subplot(3, 1, 3, sharex=ax, sharey=ax)
        plt.title("YARARA - DRS", fontsize=13)
        plot_contour(P.T, K.T, detection.T - detection_ref.T, vmin=-100, vmax=100, ticks="diff")
        plt.xscale("log")

        #        plt.subplot(3, 2, 2)
        #        plt.title('DRS', fontsize=13)
        #        plot_contour(P_ref.T,M_ref.T,detection_ref.T,cmap=cmap,vmin=-5,vmax=105)
        #        ax = plt.gca()
        #        plt.subplot(3, 2, 4, sharex=ax, sharey=ax)
        #        plt.title('YARARA', fontsize=13)
        #        plot_contour(P.T,M.T,detection.T,cmap=cmap,vmin=-5,vmax=105)
        #        plt.subplot(3, 2, 6, sharex=ax, sharey=ax)
        #        plt.title('YARARA - DRS', fontsize=13)
        #        plot_contour(P.T,M.T,detection.T - detection_ref.T, vmin=-100, vmax=100,ticks='diff')
        #        plt.xscale('log')

        plt.savefig(self.dir_root + "IMAGES/Detection_limits.png")

    # =============================================================================
    # SAVE SOME OBSERVATIONNAL INFORMATION IN RASSINE FILES
    # =============================================================================

    def import_sun_model(self, match_x=True):
        self.import_table()
        model = sun_model(match_x=np.array(self.table.jdb))
        return model

    def import_dace_table_rv(self, path=None, bin_length=1, dbin=0):
        """
        Produce a summary table based on the DACE table by restacking spectra

        Parameters
        ----------
        path : path of the dace extracted table
        bin_length : length of the binning
        dbin : shift of the binning


        Returns
        -------
        """

        directory = self.directory

        files = np.sort(glob.glob(directory + "/RASS*.p"))
        jdb_table = []
        for file in files:
            file = pd.read_pickle(file)
            jdb_table.append(file["parameters"]["jdb"])
        jdb_table = np.array(jdb_table)

        if path is None:
            dace_file = (
                "/".join(self.directory.split("/")[0:-2]) + "/DACE_TABLE/Dace_extracted_table.csv"
            )
        else:
            dace_file = path + "Dace_extracted_table.csv"
        dace = pd.read_csv(dace_file, index_col=0)
        self.dace = dace
        jdb = np.array(dace["rjd"])
        vrad = np.array(dace["vrad"])
        svrad = np.array(dace["svrad"])
        model = np.array(dace["model"]) * 1000
        drift = np.array(dace["drift_used"])
        vrad -= model
        vrad += drift
        weights = 1 / svrad**2

        if bin_length == 0:
            group = np.arange(len(jdb))
            groups = np.arange(len(jdb))
        else:
            groups = (jdb // bin_length).astype("int")
            groups -= groups[0]
            group = np.unique(groups)

        mean_jdb = []
        mean_vrad = []
        mean_svrad = []

        for j in group:
            g = np.where(groups == j)[0]
            mean_jdb.append(np.mean(jdb[g]))
            mean_svrad.append(1 / np.sqrt(np.sum(weights[g])))
            mean_vrad.append(np.sum(vrad[g] * weights[g]) / np.sum(weights[g]))

        mean_jdb = np.array(mean_jdb)
        mean_vrad = np.array(mean_vrad)
        mean_svrad = np.array(mean_svrad)

        mean_vrad -= np.nanmedian(mean_vrad)

        # plt.scatter(np.arange(len(jdb_table)),jdb_table)
        # plt.scatter(np.arange(len(mean_jdb)),mean_jdb)

        match = myf.match_nearest(jdb_table, mean_jdb)

        self.debug = (jdb_table, mean_jdb, mean_vrad, match)

        if len(match) != len(jdb_table):

            print("Matching of DACE and YARARA table failed.")

            if len(mean_jdb) == len(jdb_table):
                print("But same length for both table, so 1 per 1 index assumed")
                vrad_kept = mean_vrad
                svrad_kept = mean_svrad
                dace = myc.tableXY(mean_jdb, vrad_kept, svrad_kept)
                plt.figure()
                dace.plot()
                for k, file in enumerate(files):
                    f = pd.read_pickle(file)
                    f["parameters"]["rv_dace"] = vrad_kept[k]
                    f["parameters"]["rv_dace_std"] = svrad_kept[k]
                    ras.save_pickle(file, f)

                self.yarara_analyse_summary()
                self.import_table()

        else:
            vrad_kept = mean_vrad[match[:, 1].astype("int")]
            svrad_kept = mean_svrad[match[:, 1].astype("int")]

            dace = myc.tableXY(mean_jdb[match[:, 1].astype("int")], vrad_kept, svrad_kept)
            plt.figure()
            dace.plot()
            for k, file in enumerate(files):
                f = pd.read_pickle(file)
                f["parameters"]["rv_dace"] = vrad_kept[k]
                f["parameters"]["rv_dace_std"] = svrad_kept[k]
                ras.save_pickle(file, f)

            self.yarara_analyse_summary()
            self.import_table()

    def convert_mjd_to_jdb(self):
        """
        Add a jdb entry considering a mjd entry already exist

        Parameters
        ----------


        """

        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)
        for j in tqdm(files):
            file = pd.read_pickle(j)
            file["parameters"]["jdb"] = file["parameters"]["mjd"] + 0.5
            ras.save_pickle(j, file)

    def yarara_get_berv_info(self, keyword_rv="telluric_rv", keyword_depth="telluric_depth"):
        """
        Display some basic information about the BERV information

        Parameters
        ----------
        keyword_rv : name of the telluric rv column in summary table
        keyword_depth : name of the telluric depth column in summary table

        """
        table = pd.read_pickle(self.directory + "Analyse_summary.p")
        snr = np.array(table["snr"]).astype("float")
        berv = np.array(table[keyword_rv]).astype("float")
        depth = np.array(table[keyword_depth]).astype("float")

        mask_weight = np.genfromtxt(
            root + "/Python/MASK_CCF/" + keyword_depth.split("_")[0] + ".txt"
        )[:, -1]
        constante_ccf = 1 - np.sum(mask_weight**3) / np.sum(mask_weight**2)

        print(
            " Constante CCF : %.3f \n Scaling mean : %.3f \n Scaling min : %.3f \n Scaling max : %.3f"
            % (
                constante_ccf,
                np.nanmedian(depth) / constante_ccf,
                np.nanmin(depth) / constante_ccf,
                np.nanmax(depth) / constante_ccf,
            )
        )
        print(
            " BERV mean : %.2f [km/s] \n BERV min : %.2f [km/s] \n BERV max : %.2f [km/s]"
            % (np.nansum(snr**2 * berv) / np.nansum(snr**2), np.nanmin(berv), np.nanmax(berv))
        )

    def yarara_add_info(self, keys_table, keys_param, to_table=False):
        """
        Add some observationnal information in the RASSINE files and produce a summary table

        Parameters
        ----------
        keys_table : name in the Analayse summary table to copy
        name : Name of the field in the Rassine file

        """

        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        table = pd.read_pickle(directory + "Analyse_summary.p")
        # table[keys_table] = 0

        for j in tqdm(files):
            file = pd.read_pickle(j)
            if to_table:
                value = file["parameters"][keys_param]
                table.loc[table["filename"] == j, keys_table] = value
            else:
                value = np.array(table.loc[table["filename"] == j, keys_table])[0]
                file["parameters"][keys_param] = value
                ras.save_pickle(j, file)

        if to_table:
            myf.pickle_dump(table, open(self.directory + "Analyse_summary.p", "wb"))

        self.yarara_analyse_summary()

    def yarara_transit_def(self, period=100000, T0=55000, duration=2, auto=False):
        """period in days, T0 transits center in jdb - 2'400'000, duration in hours"""

        self.import_table()
        self.import_star_info()

        time = np.sort(np.array(self.table.jdb))

        if auto:
            table_transit = pd.read_csv(root + "/Python/Material/transits.csv", index_col=0)
            star_transit_properties = table_transit.loc[table_transit["starname"] == self.starname]
            if len(star_transit_properties):
                period = star_transit_properties["period"].values[0]
                T0 = star_transit_properties["T0"].values[0]
                duration = star_transit_properties["dt"].values[0]
                teff = star_transit_properties["Teff"].values[0]
                fwhm = star_transit_properties["FWHM"].values[0]
                if teff:
                    print(
                        "\n [INFO] Effective temperature upated from %.0f to %.0f K"
                        % (self.star_info["Teff"]["fixed"], teff)
                    )
                    self.yarara_star_info(Teff=["fixed", int(teff)])
                if fwhm:
                    print(
                        "\n [INFO] FWHM upated from %.0f to %.0f km/s"
                        % (self.star_info["FWHM"]["fixed"], fwhm)
                    )
                    self.yarara_star_info(Fwhm=["fixed", int(fwhm)])
                    self.fwhm = fwhm

                print(
                    "\n [INFO] Star %s found in the table : P = %.2f days | T0 = %.2f JDB | T14 = %.2f hours | Teff = %.0f K | FWHM = %.0f km/s"
                    % (self.starname, period, T0, duration, teff, fwhm)
                )
            else:
                print("\n [WARNING] Star %s not found in the transits.csv table" % (self.starname))

        duration /= 24

        if (period != 100000) & (period != 0):
            phases = ((time - T0) % period + period / 2) % period - period / 2

            transits = (abs(phases) < (duration / 2)).astype("int")

            plt.figure(figsize=(15, 5))
            plt.subplot(2, 1, 1)
            plt.ylabel("SNR", fontsize=14)
            plt.scatter(time, self.table.snr, c=transits, zorder=10)

            myf.transit_draw(period, T0, duration)

            length = (np.max(time) - np.min(time)) * 0.1
            plt.xlim(np.min(time) - length, np.max(time) + length)

            plt.title(
                "%s In/Out spectra : %.0f/%.0f"
                % (self.starname, sum(transits), len(transits) - sum(transits))
            )
            ax = plt.gca()

            plt.subplot(2, 1, 2, sharex=ax)
            plt.xlabel("Jdb - 2,400,000 [days]", fontsize=14)
            plt.ylabel("RV", fontsize=14)
            plt.scatter(time, self.table.rv_dace, c=transits, zorder=10)

            myf.transit_draw(period, T0, duration)

            length = (np.max(time) - np.min(time)) * 0.1
            plt.xlim(np.min(time) - length, np.max(time) + length)

            plt.savefig(self.dir_root + "IMAGES/Transit_definition.pdf")

            plt.show(block=False)
        else:
            transits = np.zeros(len(time))

        self.yarara_obs_info(kw=["transit_in", transits])

    def yarara_snr_feature(
        self,
        sub_dico="matching_diff",
        reference="master",
        analysis="snr_limit",
        wave_min=3921,
        wave_max=3979,
        substract_map=[],
        add_map=[],
        berv_shift=False,
        sigma_detection=1,
        snr_flat=700,
    ):
        self.import_material()
        self.import_table()
        load = self.material

        snr = pd.read_pickle(self.dir_root + "WORKSPACE/Analyse_snr.p")["snr_curve"]

        master = np.array(load["reference_spectrum"] * load["correction_factor"])

        wave_ref = self.material.wave
        i1 = myf.find_nearest(wave_ref, wave_min)[0][0]
        i2 = myf.find_nearest(wave_ref, wave_max)[0][0]
        snrs = np.mean(snr[:, i1:i2], axis=1)
        master = master[i1 : i2 + 1]

        dticks = int((wave_ref[i2] - wave_ref[i1]) / 50)
        if dticks < 1:
            dticks = 1
        plt.figure(figsize=(14, 10))
        plt.axes([0.15, 0.1, 0.9, 0.7])
        ax = plt.gca()
        ax.xaxis.set_minor_locator(MultipleLocator(dticks))
        ax.yaxis.set_minor_locator(MultipleLocator(10))
        a, b, c = self.yarara_map(
            sub_dico=sub_dico,
            reference=reference,
            wave_min=wave_min,
            wave_max=wave_max,
            Plot=True,
            p_noise=1 / np.inf,
            new=False,
            substract_map=substract_map,
            add_map=add_map,
            unit=100,
            berv_shift=berv_shift,
        )
        plt.tick_params(labelleft=False)
        ax1 = plt.colorbar(pad=0)
        ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)
        plt.axes([0.15, 0.8, 0.765, 0.15], sharex=ax)

        std_map = np.std(a, axis=0)
        std_features = (
            std_map**2 - (np.sqrt(master) / np.nanmedian(snrs)) ** 2
        )  # remove noise from the observations

        std_features[std_features < 0] = std_map[std_features < 0] ** 2

        if analysis == "snr_limit":
            snr_features = np.sqrt(master) / np.sqrt(std_features)
        else:
            snr_features = np.sqrt(std_features) / (np.sqrt(master) / np.nanmedian(snrs))

        snr_limit = snr_flat * np.sqrt(len(self.table))
        snr_features[np.isnan(snr_features)] = snr_limit
        snr_features[snr_features > snr_limit] = snr_limit

        if berv_shift != False:
            snr_features2 = np.sqrt(master) / abs(np.mean(a, axis=0))
            snr_features[0:50] = np.median(snr_features)
            snr_features[-50:] = np.median(snr_features)
            snr_features2[0:50] = np.median(snr_features2)
            snr_features2[-50:] = np.median(snr_features2)
            plt.plot(c, snr_features2, color="gray")

        plt.plot(c, snr_features * sigma_detection, color="k")

        plt.yscale("log")
        plt.ylim(5 * 10 ** (int(np.log10(np.min(snr_features))) - 1), 1500)
        if analysis == "snr_limit":
            plt.ylabel(r"$S/N_{cont}|_{S/N_{feat}=%.0f}$" % (sigma_detection), fontsize=14)
        else:
            plt.ylabel(r"$S/N_{feat}$", fontsize=14)
        plt.tick_params(labelbottom=False, direction="in", top=True, right=True, which="both")
        plt.axes([0.05, 0.1, 0.1, 0.7], sharey=ax)
        plt.plot(snrs, np.arange(len(snrs)), color="k")
        plt.xlabel("SNR observations", fontsize=14)
        plt.ylabel("Spectra index (Time)", fontsize=14)
        ax = plt.gca()
        ax.xaxis.set_minor_locator(MultipleLocator(100))
        val = ax.get_xlim()
        plt.xlim(val[1], val[0])
        # plt.xscale('log')
        plt.tick_params(direction="in", top=True, right=True, which="both")

    def yarara_snr_contam(
        self,
        sub_dico="matching_activity",
        slope_vec=None,
        analysis="snr_limit",
        snr_flat=700,
        sigma_detection=1,
        legend=True,
        window_ang=1,
    ):

        self.import_material()
        self.import_table()
        load = self.material
        master = np.array(load["reference_spectrum"] * load["correction_factor"])
        wave = np.array(load.wave)
        epsilon = 1e-12

        dwave = np.mean(np.diff(self.material.wave))
        snr = pd.read_pickle(self.dir_root + "WORKSPACE/Analyse_snr.p")["snr_curve"]
        snrs = np.nanmean(snr, axis=1)

        if slope_vec is None:
            correction_map = pd.read_pickle(self.dir_root + "CORRECTION_MAP/map_%s.p" % (sub_dico))
            std_map = np.std(correction_map["correction_map"], axis=0)
            std_map = np.sqrt(
                std_map**2 - (np.sqrt(master) / np.nanmedian(snrs)) ** 2
            )  # remove noise from the observations
        else:
            std_map = slope_vec

        mask_reject = np.ones(len(std_map))
        mask_reject[std_map == 0] = np.nan

        std_features = std_map**2
        # std_features = std_map**2-(np.sqrt(master)/np.nanmedian(snrs))**2 #remove noise from the observations
        std_features[std_features < 0] = std_map[std_features < 0] ** 2

        self.debug = std_features

        if analysis == "snr_limit":
            snr_features = np.sqrt(master) / np.sqrt(std_features + epsilon)
        else:
            snr_features = np.sqrt(std_features) / (np.sqrt(master) / np.nanmedian(snrs))

        snr_limit = snr_flat * np.sqrt(len(self.table))
        snr_features *= mask_reject
        snr_features[np.isnan(snr_features)] = snr_limit
        snr_features[snr_features > snr_limit] = snr_limit
        if analysis == "snr_limit":
            snr_features[snr_features < 1] = snr_limit

        if window_ang:
            plt.plot(
                wave,
                myf.smooth(snr_features * sigma_detection, box_pts=window_ang / dwave),
                color="r",
                zorder=100,
                label="%.0f-$\AA$ smoothed" % (window_ang),
            )
        snr_features[snr_features == snr_limit] = np.nan
        plt.plot(wave, snr_features * sigma_detection, color="k")
        plt.title(sub_dico, fontsize=14)
        plt.yscale("log")
        if analysis == "snr_limit":
            plt.ylabel(r"$S/N_{cont}|_{S/N_{feat}=%.0f}$" % (sigma_detection), fontsize=14)
        else:
            plt.ylabel(r"$S/N_{feat}$", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)

        if analysis == "snr_limit":
            plt.axhline(y=snr_flat, color="g", label=r"$SNR_{lim}$ spectra")
            plt.axhline(y=snr_limit, color="b", label=r"$SNR_{lim}$ timeseries")

        if legend:
            plt.legend()

        return snr_features, std_map

    def yarara_snr_curve_old(
        self,
        sub_dico="matching_mad",
        reference="master",
        window_ang=20,
        window_ang_large=200,
        wave_ref=None,
        save=True,
        snr_flat=700,
    ):
        self.import_table()
        self.import_material()

        a, b, c = self.yarara_map(
            sub_dico=sub_dico, Plot=False, reference=reference, wave_min=None, wave_max=None
        )

        a[a == 0] = np.nan
        dwave = np.mean(np.diff(c))
        window_index = int(window_ang / dwave)

        step = int(window_index / 10)
        snrs = np.zeros(np.shape(a))

        for i in tqdm(np.arange(0, len(c) - window_index, step)):
            value = np.nanstd(a[:, i : window_index + i], axis=1)
            value[value == 0] = np.nan
            snrs[:, window_index + i : window_index + i + step] = 1 / value[:, np.newaxis]
            if not i:
                snrs += 1 / value[:, np.newaxis]

        snrs *= self.material.correction_factor

        median_snr = np.mean(snrs, axis=0)
        smoothed_snr = np.ravel(
            pd.DataFrame(median_snr)
            .rolling(int(window_ang_large / dwave), min_periods=1, center=True)
            .quantile(0.50)
        )
        # smoothed_snr = myf.smooth(median_snr,box_pts=int(window_ang_large/dwave))
        ratio_correction = median_snr / smoothed_snr
        ratio_correction[np.isnan(ratio_correction)] = 1
        snrs /= ratio_correction

        if wave_ref is None:
            snr = np.nanmedian(snrs, axis=1)
            wave_ref = "ALL"
        else:
            i1 = myf.find_nearest(c, wave_ref)[0]
            snr = np.ravel(snrs[:, i1])
            wave_ref = str(int(wave_ref))

        old_snr = self.table.snr
        plt.figure()
        plt.scatter(old_snr, snr, c=self.table.jdb, cmap="jet")
        ax = plt.colorbar(pad=0)
        ax.ax.set_ylabel("Jdb - 2,400,000 [days]", fontsize=14)
        maxi = np.nanpercentile(old_snr, 75) + 2 * myf.IQ(old_snr)
        step_snr = np.linspace(1, maxi, 100)
        plt.xlim(0, maxi)
        plt.ylim(0, maxi)
        plt.plot([0, maxi], [0, maxi], color="k")
        plt.plot(
            step_snr,
            1 / np.sqrt(1 / snr_flat**2 + 1 / step_snr**2),
            color="k",
            ls=":",
            label="SNR FLAT = %.0f limitation" % (snr_flat),
        )
        plt.legend()
        plt.xlabel(r"$SNR_{5500}$ stacked", fontsize=14)
        plt.ylabel(
            r"$SNR_{%s}$ computed with $%.0f \AA$ window" % (wave_ref, window_ang), fontsize=14
        )

        plt.savefig(self.dir_root + "IMAGES/snr_comparison.pdf")

        master_curve = snrs / np.nansum(snrs, axis=1)[:, np.newaxis]
        master_curve = np.median(snrs, axis=0)
        master_curve /= np.nanmean(master_curve)

        if save:
            dico = {
                "wave": np.array(self.material.wave),
                "snr_curve": snrs,
                "ratio_factor": ratio_correction,
                "master_snr_curve": master_curve,
            }
            myf.pickle_dump(dico, open(self.dir_root + "WORKSPACE/Analyse_snr.p", "wb"))
            self.yarara_obs_info(kw=["SNR_computed", snr])
        else:
            return snr, snrs, ratio_correction

    def yarara_snr_curve(
        self,
        sub_dico="matching_mad",
        reference="master",
        window_ang=50,
        window_ang_small=5,
        window_ang_large=200,
        wave_ref=None,
        save=True,
        snr_flat=650,
    ):
        """Replace the old recipe"""
        self.import_table()
        self.import_material()

        master = np.sqrt(self.material.reference_spectrum * self.material.correction_factor)

        a, b, c = self.yarara_map(
            sub_dico=sub_dico, Plot=False, reference=reference, wave_min=None, wave_max=None
        )
        a /= master  # to transform std in std_continuum

        a[a == 0] = np.nan
        dwave = np.mean(np.diff(c))
        window_index = int(window_ang / dwave)

        sig_sts = myf.mad(a, axis=0)
        sig_sts[sig_sts == 0] = np.nan
        snr_sts = 1 / (sig_sts)
        smoothed_snr_sts = np.ravel(
            pd.DataFrame(snr_sts)
            .rolling(int(window_ang_small / dwave), min_periods=1, center=True)
            .quantile(0.50)
        )
        smoothed_snr_sts /= np.nanmedian(smoothed_snr_sts)
        vec = myc.tableXY(c, smoothed_snr_sts)
        vec.replace_outliers(m=10)  # because of the gap on HARPS and borders instability
        smoothed_snr_sts = vec.y

        a *= (
            smoothed_snr_sts * 1 + 0
        )  # to scale all the snr curve with the sts snr curve (get a approximate snr curve but fast algo)

        snrs = np.zeros(np.shape(a))

        for i in tqdm(np.arange(0, len(c) - window_index, window_index)):
            value = myf.mad(a[:, i : window_index + i], axis=1)
            value[value == 0] = np.nan
            snrs[
                :,
                window_index
                + i
                - int(window_index / 2)
                - 1 : window_index
                + i
                + int(window_index / 2)
                + 1,
            ] = (
                1 / value[:, np.newaxis]
            )
            if not i:
                snrs += 1 / value[:, np.newaxis]

        median_snr = np.nanmedian(snrs, axis=0)
        median_snr /= np.nanmedian(median_snr)
        snrs /= median_snr

        snrs *= smoothed_snr_sts * 1 + 0  # to scale all the snr curve with the sts snr curve

        if wave_ref is None:
            snr = np.nanmedian(snrs, axis=1)
            wave_ref = "ALL"
        else:
            i1 = myf.find_nearest(c, wave_ref)[0]
            snr = np.ravel(snrs[:, i1])
            wave_ref = str(int(wave_ref))

        old_snr = self.table.snr
        plt.figure()
        plt.scatter(old_snr, snr, c=self.table.jdb, cmap="jet")
        ax = plt.colorbar(pad=0)
        ax.ax.set_ylabel("Jdb - 2,400,000 [days]", fontsize=14)
        maxi = np.nanpercentile(old_snr, 75) + 2 * myf.IQ(old_snr)
        step_snr = np.linspace(1, maxi, 100)
        plt.xlim(0, maxi)
        plt.ylim(0, maxi)
        plt.plot([0, maxi], [0, maxi], color="k")
        plt.plot(
            step_snr,
            1 / np.sqrt(1 / snr_flat**2 + 1 / step_snr**2),
            color="k",
            ls=":",
            label="SNR FLAT = %.0f limitation" % (snr_flat),
        )
        plt.legend()
        plt.xlabel(r"$SNR_{5500}$ stacked", fontsize=14)
        plt.ylabel(
            r"$SNR_{%s}$ computed with $%.0f \AA$ window" % (wave_ref, window_ang), fontsize=14
        )

        plt.savefig(self.dir_root + "IMAGES/snr_comparison.pdf")

        master_curve = snrs / np.nansum(snrs, axis=1)[:, np.newaxis]
        master_curve = np.nanmedian(snrs, axis=0)
        master_curve /= np.nanmedian(master_curve)

        smoothed_master = np.ravel(
            pd.DataFrame(master_curve)
            .rolling(int(window_ang_large / dwave), min_periods=1, center=True)
            .quantile(0.50)
        )
        ratio_correction = master_curve / smoothed_master
        ratio_correction /= np.nanmean(ratio_correction)

        master_curve /= ratio_correction

        self.material["master_snr_curve"] = master_curve
        self.update_material()

        if save:
            dico = {
                "wave": np.array(self.material.wave),
                "snr_curve": snrs,
                "ratio_factor": ratio_correction,
                "master_snr_curve": master_curve,
            }
            myf.pickle_dump(dico, open(self.dir_root + "WORKSPACE/Analyse_snr.p", "wb"))
            self.yarara_obs_info(kw=["SNR_computed", snr])
        else:
            return snr, snrs, ratio_correction

    def yarara_transit_properties(self):
        self.import_table()
        table = self.table
        transit_in = table.loc[table["transit_in"] == 1]
        transit_out = table.loc[table["transit_in"] == 0]

        nb = [len(transit_in), len(transit_out)]
        snr = [
            int(np.sqrt(np.sum(transit_in["snr"] ** 2))),
            int(np.sqrt(np.sum(transit_out["snr"] ** 2))),
        ]

        print("\n [INFO] Nb of spectra (in/out) : ", nb)
        print("\n [INFO] SNR of master spectra (in/out) : ", snr)

        return nb, snr

    def yarara_transmission_spectra(
        self, sub_dico="matching_pca", sub_dico_ref="matching_diff", transit_nb=0, plot=False
    ):
        self.import_table()
        transit_night = np.array(self.table.transit_in)
        master_out_stack = self.yarara_stack(
            mask_files=(transit_night == 0),
            sub_dico=sub_dico,
            sub_dico_ref=sub_dico_ref,
            plot=False,
        )
        master_out = myc.tableXY(master_out_stack[0], master_out_stack[1], master_out_stack[2])

        if transit_nb != 0:
            val, borders = myf.clustering(transit_night, 0.5, 0.5)
            val = np.array([np.product(v) for v in val])
            borders = borders[val == 1]
            mask_in = np.zeros(len(transit_night))
            mask_in[borders[transit_nb, 0] : borders[transit_nb, 1] + 2] = 1
            mask_in = mask_in.astype("bool")
        else:
            mask_in = transit_night.astype("bool")

        master_in_stack = self.yarara_stack(
            mask_files=mask_in, sub_dico=sub_dico, sub_dico_ref=sub_dico_ref, plot=False
        )
        master_in = myc.tableXY(master_in_stack[0], master_in_stack[1], master_in_stack[2])

        self.master_in = master_in.copy()
        self.master_out = master_out.copy()

        master_in.divide(master_out, const=1)
        self.transmission_spectrum = master_in.copy()

        if plot:
            plt.figure(figsize=(18, 7))

            plt.subplot(2, 1, 1)
            self.master_in.plot(color="r")
            self.master_out.plot(color="k")
            ax = plt.gca()
            plt.ylabel("Flux normalised", fontsize=14)

            plt.subplot(2, 1, 2, sharex=ax)
            master_in.plot(color="grey")
            plt.plot(master_in.x, myf.smooth(master_in.y, box_pts=5, shape="gaussian"), color="k")
            # plt.errorbar(master_in.x,master_in.y,yerr=master_in.yerr,marker='o',capsize=0,color='k')
            plt.axhline(y=1, color="r")
            dy = 0.02
            plt.ylim(1 - dy, 1 + dy)
            plt.ylabel("Transmission spectrum [%]", fontsize=14)
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)

            plt.subplots_adjust(hspace=0, left=0.07, right=0.97, top=0.94, bottom=0.11)

    def yarara_instrumental_offset(self, instrument, smooth_box=100, weight_ccf=0.25):
        self.import_material()
        self.import_table()

        table = self.table
        load = self.material
        starname = self.starname
        ins = self.instrument
        dir_root = self.dir_root

        wave_grid = load["wave"]

        time1 = np.array(table["jdb"])
        time2 = np.array(
            pd.read_csv(dir_root.replace(ins, instrument) + "/WORKSPACE/Analyse_summary.csv")[
                "jdb"
            ]
        )
        time = np.hstack([time1, time2])

        sub_dico = "matching_mad"
        all_spec = []
        for j in tqdm(np.sort(glob.glob(dir_root + "/WORKSPACE/RASSINE*"))):
            spec1 = pd.read_pickle(j)
            spec1 = myc.tableXY(spec1["wave"], spec1["flux"] / spec1[sub_dico]["continuum_linear"])
            spec1.interpolate(new_grid=wave_grid, method="linear", replace=True)
            all_spec.append(spec1.y)

        for j in tqdm(
            np.sort(glob.glob(dir_root.replace(ins, instrument) + "/WORKSPACE/RASSINE*"))
        ):
            spec1 = pd.read_pickle(j)
            spec1 = myc.tableXY(spec1["wave"], spec1["flux"] / spec1[sub_dico]["continuum_linear"])
            spec1.interpolate(new_grid=wave_grid, method="linear", replace=True)
            all_spec.append(spec1.y)

        all_spec = np.array(all_spec)

        # ref = np.median(all_spec[0:len(time1)],axis=0)
        kitcat1 = pd.read_pickle(dir_root + "/KITCAT/kitcat_mask_" + starname + ".p")
        kitcat2 = pd.read_pickle(
            dir_root.replace(ins, instrument) + "/KITCAT/kitcat_mask_" + starname + ".p"
        )

        ref1 = myc.tableXY(
            kitcat1["spectre"]["wave"],
            kitcat1["spectre"]["flux"] / kitcat1["spectre"]["correction_factor"],
        )
        ref2 = myc.tableXY(
            kitcat2["spectre"]["wave"],
            kitcat2["spectre"]["flux"] / kitcat2["spectre"]["correction_factor"],
        )
        ref2.interpolate(new_grid=ref1.x, method="cubic", replace=True)
        ratio = myc.tableXY(ref1.x, ref1.y / ref2.y)

        continuum = abs(np.gradient(ref1.y))
        cutoff = np.percentile(continuum, 75)
        continuum[continuum > cutoff] = np.nan
        continuum[abs(ratio.y - 1) > 1] = np.nan
        continuum[continuum <= cutoff] = 1
        ratio.y *= continuum
        ratio.supress_nan()
        ratio.smooth(box_pts=20, replace=True, shape="gaussian")
        ratio.interpolate(new_grid=ref1.x, method="linear", replace=True)
        continuum_correction = ratio.y

        corr = np.zeros(len(time))
        corr[len(time1) :] = 1
        corr = continuum_correction * corr[:, np.newaxis]
        corr[corr == 0] = 1

        all_spec2 = all_spec * corr

        contamination = abs(np.mean(all_spec2[len(time1) :] - ref1.y, axis=0))

        contam_line = []
        num = -1
        for left, right in zip(
            np.array(kitcat1["catalogue"]["wave_left"]),
            np.array(kitcat1["catalogue"]["wave_right"]),
        ):
            num += 1
            l = myf.find_nearest(wave_grid, left)[0][0]
            r = myf.find_nearest(wave_grid, right)[0][0]
            contam_line.append(np.nansum(contamination[l : r + 1]))
        contam_line = np.array(contam_line)

        weight_rv = np.array(kitcat1["catalogue"]["weight_rv"])
        curve_info_rv = np.cumsum(weight_rv[np.argsort(contam_line)]) / sum(weight_rv)
        limite_contam = np.sort(contam_line)[myf.find_nearest(curve_info_rv, weight_ccf)[0][0]]

        good_lines = contam_line < limite_contam
        line_table = kitcat1["catalogue"][good_lines].reset_index(drop=True)

        ccf_output = self.yarara_ccf(
            mask=line_table,
            plot=True,
            sub_dico="matching_mad",
            save=False,
            rv_range=None,
            delta_window=5,
            ccf_oversampling=3,
        )

        # TBD

        wave_left = myf.find_nearest(wave_grid, 3800)[0][0]
        wave_right = myf.find_nearest(wave_grid, 4200)[0][0]

        order = np.argsort(time % 100000)

        plt.figure()
        plt.subplot(2, 1, 1)
        plt.imshow(
            (all_spec - ref1.y)[order, wave_left:wave_right],
            cmap="plasma",
            aspect="auto",
            vmin=-0.005,
            vmax=0.005,
        )
        plt.subplot(2, 1, 2)
        plt.imshow(
            (all_spec * corr - ref1.y)[order, wave_left:wave_right],
            cmap="plasma",
            aspect="auto",
            vmin=-0.005,
            vmax=0.005,
        )

    def yarara_inject_planet(
        self,
        sub_dico="matching_mad",
        amp=1,
        period=16.45,
        phase=0,
        berv_shift=False,
        vector=None,
        export=False,
    ):

        """
        Injected circular keplerian signals in the spectra

        Parameters
        ----------

        amp : amplitude of the signal in m/s
        period : period of the signals in days
        phase : phase of the signal in rad

        """

        if type(amp) != list:
            amp = [amp]
        if type(period) != list:
            period = [period]
        if type(phase) != list:
            phase = [phase]

        self.import_table()
        self.import_material()

        grid = np.array(self.material.wave)
        jdb = np.array(self.table["jdb"])
        berv = np.array(self.table["berv"])
        planet = np.zeros(len(jdb))
        t0 = np.min(jdb)

        if (len(amp) == len(period)) & (len(amp) == len(phase)):
            vec_lbl = self.import_ccf_timeseries("CCF_" + self.mask_harps, sub_dico, "rv")
            k_std = self.yarara_keplerian_fit(vec_lbl, periods=period)["k"]
            k_std = list(np.array(["%.3f" % (i) for i in k_std]).astype("float"))

            if vector is not None:
                planet = vector
                t = myc.tableXY(jdb - t0, planet)
                t.periodogram(Plot=False, all_outputs=True, p_min=2)
                phase = np.round(t.phase_max, 2)
                amp = np.round(t.amplitude_max, 2)
                period = np.round(t.perio_max, 2)
            else:
                for a, p, phi in zip(amp, period, phase):
                    print("Keplerian model : K=%.2f / P=%.2f / phi=%.2f" % (a, p, phi))
                    planet += a * np.sin(2 * np.pi / p * (jdb - t0) + phi)

            directory = self.directory
            files = glob.glob(directory + "RASSI*.p")
            files = np.sort(files)
            file_test = self.import_spectrum()

            self.import_dico_chain(sub_dico)
            correct_dico = self.dico_chain[:-1]
            first_dico = self.dico_chain[-1]

            fluxes = []
            fluxes_std = []
            continuum = []
            backup_fluxes = []
            for j in tqdm(files):
                file = pd.read_pickle(j)
                if not j:
                    wave = file["wave"]
                flux = file["flux"]
                flux_std = file["flux_err"]
                backup_fluxes.append(flux.copy())
                conti = file[first_dico]["continuum_linear"]
                conti_std = file["continuum_err"]
                f_norm, f_norm_std = myf.flux_norm_std(flux, flux_std, conti, conti_std)
                continuum.append(conti)
                fluxes.append(flux / conti)
                fluxes_std.append(f_norm_std)

            fluxes = np.array(fluxes)
            fluxes_std = np.array(fluxes_std)
            continuum = np.array(continuum)
            backup_fluxes = np.array(backup_fluxes)

            dico_to_correct = [
                "matching_telluric",
                "matching_oxy_bands",
                "matching_oxygen",
                "matching_pca",
                "matching_ghost_a",
                "matching_ghost_b",
                "matching_berv",
                "matching_thar",
                "matching_contam",
            ]

            correct_dico = correct_dico[np.in1d(correct_dico, dico_to_correct)]
            print("\n [INFO] Spectra will be correct of ", correct_dico)

            substract_map = [c.split("matching_")[-1] for c in correct_dico]

            flux_corrected = fluxes.copy()
            for maps in substract_map:
                flux_corrected = self.yarara_substract_map(
                    flux_corrected, maps, correction_factor=False
                )

            contamination = fluxes - flux_corrected

            if export:
                export = {
                    "wave": grid,
                    "jdb": jdb,
                    "flux": flux_corrected,
                    "flux_std": fluxes_std,
                    "contam": contamination,
                }
                if not os.path.exists(self.dir_root + "KEPLERIAN/PLANET_INJECTION"):
                    os.system("mkdir " + self.dir_root + "KEPLERIAN/PLANET_INJECTION")
                myf.pickle_dump(
                    export,
                    open(
                        self.dir_root
                        + "KEPLERIAN/PLANET_INJECTION/%s_contam_splitted.p" % (self.starname),
                        "wb",
                    ),
                )
            else:
                shifted_fluxes = flux_corrected.copy()
                idx = -1
                for j in tqdm(files):
                    idx += 1
                    wave = file["wave"]
                    flux = flux_corrected[idx].copy()
                    test = myc.tableXY(wave, flux, 0 * wave)
                    test.x = myf.doppler_r(test.x, planet[idx])[0]
                    test.interpolate(
                        new_grid=wave, method="linear", replace=True, interpolate_x=False
                    )
                    test.y += contamination[idx]
                    test.y *= continuum[idx]
                    shifted_fluxes[idx] = test.y

                if file_test["parameters"]["hole_left"] != -99.9:
                    left = myf.find_nearest(wave, file_test["parameters"]["hole_left"])[0][0]
                    right = myf.find_nearest(wave, file_test["parameters"]["hole_right"])[0][0]
                    shifted_fluxes[:, int(left) : int(right) + 1] = 0

                idx = -1
                for j in tqdm(files):
                    idx += 1
                    file = pd.read_pickle(j)
                    file["flux_planet"] = shifted_fluxes[idx]
                    file["parameters"]["rv_planet"] = planet[idx]
                    file["parameters"]["planet_injected"] = {
                        "amp": amp,
                        "amp_std": k_std,
                        "period": period,
                        "phase": phase,
                        "t0": t0,
                    }
                    file["parameters"]["berv_planet"] = berv[idx] + planet[idx] / 1000 * int(
                        berv_shift
                    )  # no more used
                    ras.save_pickle(j, file)

            self.yarara_analyse_summary()
            print(
                "\n[NOTE TO MYSELF] If you rename afterwards the root directory as X_planet, recall you to launch the yarara_analyse_summary in erasing mode. Cheers, your past yourself."
            )
        else:
            print("Not the same number of arguments")

    def get_injected_planet(self):
        file_test = self.import_spectrum()
        p = []
        if "planet_injected" in file_test["parameters"].keys():
            for j in file_test["parameters"]["planet_injected"]["period"]:
                p.append(j)
        return p

    def yarara_indicate_planet(self, color="r", alpha=0.2, ls="-"):
        file_test = self.import_spectrum()
        if "planet_injected" in file_test["parameters"].keys():
            for j in file_test["parameters"]["planet_injected"]["period"]:
                plt.axvline(x=j, color=color, alpha=alpha, ls=ls)

    def yarara_check_rv_sys(self):
        self.import_star_info()
        self.import_table()
        file_test = self.spectrum(num=np.argmax(self.table.snr), norm=True)

        rv_sys2 = self.import_spectrum(num=0)["parameters"]["RV_sys"]
        rv_sys1 = self.star_info["Rv_sys"]["fixed"]

        if 2 * abs(rv_sys2 - rv_sys1) / abs(rv_sys1 + rv_sys2 + 1e-6) * 100 > 20:
            print(
                "[WARNING] RV_sys incompatible between star_info (%.1f) and RASSINE output (%.1f)"
                % (rv_sys1, rv_sys2)
            )
            mask = np.genfromtxt(root + "/Python/MASK_CCF/" + self.mask_harps + ".txt")
            mask = np.array([0.5 * (mask[:, 0] + mask[:, 1]), mask[:, 2]]).T

            rv_range = [15, self.star_info["FWHM"]["fixed"]][
                int(self.star_info["FWHM"]["fixed"] > 15)
            ]

            file_test.ccf(
                mask, weighted=True, rv_range=rv_range + np.max([abs(rv_sys1), abs(rv_sys2)])
            )

            rv_sys_fit = file_test.ccf_params["cen"].value
            file_test.ccf(mask, weighted=True, rv_range=rv_range * 1.5, rv_sys=rv_sys_fit)

            rv_sys_fit += file_test.ccf_params["cen"].value

            rv_sys_fit = np.round(rv_sys_fit / 1000, 2)
            contrast_fit = 100 * (abs(file_test.ccf_params["amp"].value))

            y_min = 1 - 2 * contrast_fit / 100
            if y_min < 0:
                y_min = 0
            plt.ylim(y_min, 1.1)

            print("\n [INFO] RV_sys value fitted as %.2f" % (rv_sys_fit))

            plt.savefig(self.dir_root + "IMAGES/RV_sys_fitting.pdf")

            self.yarara_star_info(Rv_sys=["fixed", rv_sys_fit])
            self.yarara_star_info(Contrast=["fixed", np.round(contrast_fit / 100, 3)])
            self.yarara_obs_info(kw=["RV_sys", rv_sys_fit])
        else:
            print("[INFO] Both RV_sys match : %.2f/%.2f kms" % (rv_sys1, rv_sys2))

    def yarara_check_fwhm(self, delta_window=5):

        warning_rv_borders = True
        iteration = -1
        while warning_rv_borders:
            iteration += 1
            output = self.yarara_ccf(
                mask=self.mask_harps,
                plot=False,
                save=False,
                sub_dico="matching_diff",
                ccf_oversampling=1,
                rv_range=None,
                rv_borders=None,
                delta_window=delta_window,
                display_ccf=True,
            )

            new_fwhm = np.nanmedian(output["fwhm"].y)

            warning_rv_borders = self.warning_rv_borders
            if iteration == 4:
                warning_rv_borders = True
                new_fwhm = 6
                print(" [WARNING] The algorithm does not found a proper FWHM")

            self.fwhm = np.round(new_fwhm, 1)

        self.yarara_star_info(Fwhm=["YARARA", np.round(self.fwhm, 2)])

        print("\n [INFO] FWHM measured as %.1f kms" % (self.fwhm))

    def yarara_ccf_summary(self, mask, sub_dico):
        rv = self.import_ccf_timeseries(mask, sub_dico, "rv")

        table = {}

        for name, vec in zip(
            ["contrast", "fwhm", "vspan", "rv"],
            [self.ccf_contrast, self.ccf_fwhm, self.ccf_vspan, self.ccf_rv],
        ):
            factor = {"contrast": 1, "fwhm": 1 / 2.355, "vspan": 1, "rv": 1 / 1000}[name]
            table["ccf_" + name] = vec.y * factor
            table["ccf_" + name + "_std"] = vec.yerr * factor
        table = pd.DataFrame(table)
        self.yarara_obs_info(kw=table)

    def yarara_change_flux_unit(self, factor):
        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file["flux"] *= factor
            file["flux_used"] *= factor
            file["matching_anchors"]["continuum_linear"] *= factor
            file["matching_diff"]["continuum_linear"] *= factor
            ras.save_pickle(j, file)

    def yarara_obs_info(
        self,
        kw=[None, None],
        jdb=None,
        berv=None,
        rv=None,
        airmass=None,
        texp=None,
        seeing=None,
        humidity=None,
    ):
        """
        Add some observationnal information in the RASSINE files and produce a summary table

        Parameters
        ----------
        kw: list-like with format [keyword,array]
        jdb : array-like with same size than the number of files in the directory
        berv : array-like with same size than the number of files in the directory
        rv : array-like with same size than the number of files in the directory
        airmass : array-like with same size than the number of files in the directory
        texp : array-like with same size than the number of files in the directory
        seeing : array-like with same size than the number of files in the directory
        humidity : array-like with same size than the number of files in the directory

        """

        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        nb = len(files)

        if type(kw) == pd.core.frame.DataFrame:  # in case of a pandas dataframe
            kw = [list(kw.keys()), [i for i in np.array(kw).T]]
        else:
            try:
                if len(kw[1]) == 1:
                    kw[1] = [kw[1][0]] * nb
            except TypeError:
                kw[1] = [kw[1]] * nb

            kw[0] = [kw[0]]
            kw[1] = [kw[1]]

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            for kw1, kw2 in zip(kw[0], kw[1]):
                if kw1 is not None:
                    if len(kw1.split("ccf_")) - 1:
                        file["ccf_gaussian"][kw1.split("ccf_")[1]] = kw2[i]
                    else:
                        file["parameters"][kw1] = kw2[i]
            if jdb is not None:
                file["parameters"]["jdb"] = jdb[i]
            if berv is not None:
                file["parameters"]["berv"] = berv[i]
            if rv is not None:
                file["parameters"]["rv"] = rv[i]
            if airmass is not None:
                file["parameters"]["airmass"] = airmass[i]
            if texp is not None:
                file["parameters"]["texp"] = texp[i]
            if seeing is not None:
                file["parameters"]["seeing"] = seeing[i]
            if humidity is not None:
                file["parameters"]["humidity"] = humidity[i]
            ras.save_pickle(j, file)

        self.yarara_analyse_summary()

    def yarara_suppress_obs_info(self, kw=[None, None]):
        """
        Add some observationnal information in the RASSINE files and produce a summary table

        Parameters
        ----------
        kw: list-like with format [keyword,array]
        jdb : array-like with same size than the number of files in the directory
        berv : array-like with same size than the number of files in the directory
        rv : array-like with same size than the number of files in the directory
        airmass : array-like with same size than the number of files in the directory
        texp : array-like with same size than the number of files in the directory
        seeing : array-like with same size than the number of files in the directory
        humidity : array-like with same size than the number of files in the directory

        """

        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        nb = len(files)
        try:
            if len(kw[1]) == 1:
                kw[1] = [kw[1][0]] * nb
        except TypeError:
            kw[1] = [kw[1]] * nb

        for j in tqdm(files):
            file = pd.read_pickle(j)
            if kw[1] is not None:
                all_keys = list(file[kw[0]])
                dust, keys_to_rm = myf.string_contained_in(all_keys, kw[1])
                for k in keys_to_rm:
                    del file[kw[0]][k]
            else:
                del file[kw[0]]

            ras.save_pickle(j, file)
        print("\n [INFO] the following keys has been removed : %s : " % (kw[0]), keys_to_rm)

        self.yarara_analyse_summary()

    # =============================================================================
    #     SUPRESS LOW SNR
    # =============================================================================

    def yarara_map_detector(
        self, kitcat_table, instrument="HARPS03", coeff="r_proxy_1", minimize="laplacien"
    ):
        self.import_material()
        mat = self.material
        wave = np.array(mat["wave"])
        wave_matrix = fits.open(root + "/Python/Material/" + instrument + "_WAVE_MATRIX_A.fits")[
            0
        ].data

        val, borders = myf.clustering(np.array(self.material.merged), 0.5, 0.5)
        val = np.array([np.sum(v) for v in val])
        val = (val != 0) * np.sign(val)
        borders = borders[val == 1]
        w1 = wave[borders[:, 0]]
        w2 = wave[borders[:, 1]]

        merged = np.zeros(np.shape(wave_matrix))
        for j in range(len(wave_matrix)):
            for i in range(len(w1)):
                merged[j] += ((wave_matrix[j] < w2[i]) & (wave_matrix[j] > w1[i])).astype("int")
        merged *= merged[:, 10][:, np.newaxis]
        merged *= merged[:, -10][:, np.newaxis]

        frontier = abs(np.gradient(merged)[1])
        pxl_frontier = np.argsort(frontier[:, 10:-10], axis=1)[:, ::-1][:, 0:4] + 10
        pxl_frontier = np.array([np.min(pxl_frontier, axis=1), np.max(pxl_frontier, axis=1)]).T
        half_size = int(len(frontier[0]) / 2)
        pxl_frontier[pxl_frontier == 10] = half_size
        pxl_frontier[pxl_frontier == (len(frontier[0]) - 1 - 10)] = half_size

        for j in range(len(frontier)):
            if frontier[j, pxl_frontier[j, 0]] == 0:
                pxl_frontier[j, 0] = half_size
            elif frontier[j, pxl_frontier[j, 1]] == 0:
                pxl_frontier[j, 1] = half_size

        try:
            blaze = fits.open(root + "/Python/Material/" + instrument + "_BLAZE.fits")[0].data
        except:
            blaze = 1 * wave_matrix

        weight = []
        for i, j, k, l in zip(
            kitcat_table["pixels_l1"],
            kitcat_table["pixels_l2"],
            kitcat_table["orders_l1"],
            kitcat_table["orders_l2"],
        ):
            b1 = blaze[k - 1, i - 1] ** 2
            b2 = blaze[l - 1, j - 1] ** 2
            val = b1 * (i != 0) + b2 * (j != 0)
            weight.append(b1 * (i != 0) / val)
            weight.append(b2 * (j != 0) / val)
        weight_table = np.array(weight[::2] + weight[1::2])

        twins = np.array(
            (kitcat_table["pixels_l2"] != 0) & (kitcat_table["pixels_l1"] != 0)
        ).astype("int")

        n = (
            np.sign(np.array(kitcat_table["orders_l1"] - kitcat_table["orders_l2"]).astype("int"))
            * twins
        )

        # flattening of the vector
        num_table = np.hstack([n, -n])
        line_table = np.hstack([np.arange(len(kitcat_table)), np.arange(len(kitcat_table))])
        pixels_table = np.hstack([kitcat_table["pixels_l1"], kitcat_table["pixels_l2"]])
        orders_table = np.hstack([kitcat_table["orders_l1"], kitcat_table["orders_l2"]])
        metric_table = np.hstack([kitcat_table[coeff], kitcat_table[coeff]])
        wave_table = np.hstack([kitcat_table["wave"], kitcat_table["wave"]])
        reject = (pixels_table != 0) & (~np.isnan(metric_table))

        wave_table = wave_table[reject]
        pixels_table = pixels_table[reject]
        orders_table = orders_table[reject]
        metric_table = metric_table[reject]
        weight_table = weight_table[reject]
        line_table = line_table[reject]
        num_table = num_table[reject]
        num_table[num_table == -1] = 2

        dico = pd.DataFrame(
            {
                "wave": wave_table,
                "pixels": pixels_table,
                "orders": orders_table,
                "metric": metric_table,
                "weight": weight_table,
                "line": line_table,
                "num": num_table,
            }
        )

        def scaling(alpha, model="1"):
            new_metric = np.array(dico["metric"])
            if model == "1":
                alpha_new = alpha * np.array(dico["metric"])[np.array(dico["num"]) == 1]
            elif model == "poly":
                pass
            new_metric[np.array(dico["num"]) == 1] = alpha_new
            new_metric[np.array(dico["num"]) == 2] = (
                (np.array(dico["metric"])[np.array(dico["num"]) == 2])
                - alpha_new * np.array(dico["weight"])[np.array(dico["num"]) == 2] * alpha
            ) / (np.array(dico["weight"])[np.array(dico["num"]) == 2])
            return new_metric

        # test = (1-np.array(dico['weight'])[np.array(dico['num'])==2])/np.array(dico['weight'])[np.array(dico['num'])==1]

        save = []
        for val in np.linspace(-2, 2, 41):
            table, table_flat = myf.grid_binning(
                np.linspace(0, 4096, 18),
                np.linspace(-0.5, 71.5, 18),
                pixels_table,
                orders_table,
                scaling(val),
                Draw=False,
                cmap="seismic",
            )

            if minimize == "laplacien":
                tominimize = abs(myf.laplacien(table_flat[2]))
            elif minimize == "gradx":
                tominimize = abs(np.gradient(np.gradient(table_flat[2])[0])[0])
            elif minimize == "grady":
                tominimize = abs(np.gradient(np.gradient(table_flat[2])[1])[1])

            mean_orders = np.round(table_flat[1], 0)[:, 0].astype("int")

            valid_position = []
            for i in range(len(mean_orders)):
                valid_position.append(
                    (abs(table_flat[0][i] - pxl_frontier[mean_orders[i], 0]) < (200))
                    | (abs(table_flat[0][i] - pxl_frontier[mean_orders[i], 1]) < (200))
                )
            valid_position = np.array(valid_position).astype("int")

            final = np.sum(valid_position * tominimize)

            save.append(final)
        save = np.array(save)
        dico["new_metric"] = scaling(np.linspace(-2, 2, 41)[save.argmin()])

        vmin = np.nanpercentile(metric_table, 5)
        vmax = np.nanpercentile(metric_table, 95)

        plt.figure()
        plt.subplot(1, 1, 1)

        plt.scatter(
            pixels_table, orders_table, c=metric_table, cmap="seismic", vmin=vmin, vmax=vmax
        )
        plt.scatter(pxl_frontier[:, 0], np.arange(len(frontier)), color="k")
        plt.scatter(pxl_frontier[:, 1], np.arange(len(frontier)), color="k")
        plt.xlabel("Pixels", fontsize=14)
        plt.ylabel("Orders", fontsize=14)
        ax = plt.gca()
        # plt.subplot(1,2,2)
        # plt.scatter(pixels_table,orders_table,c=scaling(np.linspace(-2,2,41)[save.argmin()]),cmap='seismic',vmin=vmin,vmax=vmax)
        # plt.scatter(pxl_frontier[:,0],np.arange(len(frontier)),color='k')
        # plt.scatter(pxl_frontier[:,1],np.arange(len(frontier)),color='k')

        # for i,j,k in zip(pixels_table,orders_table,weight_table):
        #     merged[j-1,i-1] = merged[j-1,i-1]*(1-k)

        # merged = 1-merged

    def yarara_spectrum_detector(
        self, matrix_spectra, matrix_spectra_std=None, cmin=0, cmax=1, Plot=False
    ):

        if len(np.shape(matrix_spectra)) == 1:
            spectra = matrix_spectra.copy()[:, np.newaxis].T
        else:
            spectra = matrix_spectra.copy()

        if matrix_spectra_std is not None:
            if len(np.shape(matrix_spectra)) == 1:
                spectra_std = matrix_spectra_std.copy()[:, np.newaxis].T
            else:
                spectra_std = matrix_spectra_std.copy()

        if np.nanpercentile(spectra, 95) > 2:
            cmax = np.nanpercentile(spectra, 95)

        try:
            orders = self.orders
        except:
            orders = self.yarara_get_orders()

        try:
            pixels = self.pixels
        except:
            pixels = self.yarara_get_pixels()

        try:
            load = self.material
        except:
            self.import_material()
            load = self.material

        try:
            kitcat = self.kitcat
        except:
            self.import_kitcat()
            kitcat = self.kitcat

        kitcat = kitcat["spectre"]
        wave = np.array(load["wave"])

        merged = np.array(load["merged"])

        merged_sticked = np.hstack([merged, merged[pixels[:, 1] != 0]])
        orders_sticked = np.hstack([orders[:, 0], orders[pixels[:, 1] != 0, 1]])
        wave_sticked = np.hstack([wave, wave[pixels[:, 1] != 0]])
        pixels_sticked = np.hstack([pixels[:, 0], pixels[pixels[:, 1] != 0, 1]])

        mask = (pixels_sticked == 0) & (orders_sticked == 0)

        merged_sticked = merged_sticked[~mask]
        orders_sticked = orders_sticked[~mask]
        wave_sticked = wave_sticked[~mask]
        pixels_sticked = pixels_sticked[~mask]

        # #to order the vector by order and pixels (not necessary for the matrix production)
        # position = sorted([(i,j,k,l,m) for i,j,k,l,m in zip(orders_sticked,pixels_sticked,wave_sticked,merged_sticked,np.arange(len(wave_sticked)))])

        # orders_sticked = np.array([i[0] for i in position])
        # pixels_sticked = np.array([i[1] for i in position])
        # wave_sticked = np.array([i[2] for i in position])
        # merged_sticked = np.array([i[3] for i in position])+1
        # index_sticked = np.array([i[4] for i in position])

        merged_sticked += 1

        all_orders = myf.map_unique(orders_sticked)
        all_pixels = myf.map_unique(pixels_sticked)

        hole = 0
        matrix_wave, matrix_merged = hole * np.zeros(
            (2, len(np.unique(orders_sticked)), len(np.unique(pixels_sticked)))
        )

        for i in range(len(pixels_sticked)):
            matrix_wave[all_orders[i], all_pixels[i]] = wave_sticked[i]
            matrix_merged[all_orders[i], all_pixels[i]] = merged_sticked[i]

        matrix_wave = myf.fill_hole_mean(matrix_wave, def_hole=hole, axis=1)
        matrix_merged = myf.fill_hole_mean(matrix_merged, def_hole=hole, axis=1) - 1

        pixels_unique = np.unique(all_pixels)
        orders_unique = np.unique(all_orders) + 1

        matrix_spectrum = hole * np.zeros(
            (np.shape(spectra)[0], len(np.unique(orders_sticked)), len(np.unique(pixels_sticked)))
        )

        spectrum_sticked = np.hstack([spectra, spectra[:, pixels[:, 1] != 0]])
        spectrum_sticked = spectrum_sticked[:, ~mask]

        for i in range(len(pixels_sticked)):
            matrix_spectrum[:, all_orders[i], all_pixels[i]] = spectrum_sticked[:, i]

        for i in np.arange(len(matrix_spectrum)):
            matrix_spectrum[i] = myf.fill_hole_mean(matrix_spectrum[i], def_hole=hole, axis=1)

        if matrix_spectra_std is not None:
            matrix_spectrum_std = hole * np.zeros(
                (
                    np.shape(spectra_std)[0],
                    len(np.unique(orders_sticked)),
                    len(np.unique(pixels_sticked)),
                )
            )

            spectrum_std_sticked = np.hstack([spectra_std, spectra_std[:, pixels[:, 1] != 0]])
            spectrum_std_sticked = spectrum_std_sticked[:, ~mask]

            for i in range(len(pixels_sticked)):
                matrix_spectrum_std[:, all_orders[i], all_pixels[i]] = spectrum_std_sticked[:, i]

            for i in np.arange(len(matrix_spectrum)):
                matrix_spectrum_std[i] = myf.fill_hole_mean(
                    matrix_spectrum_std[i], def_hole=hole, axis=1
                )
        else:
            matrix_spectrum_std = np.ones(np.shape(matrix_spectrum))

        if Plot:
            myf.my_colormesh(
                pixels_unique,
                orders_unique,
                matrix_spectrum[0],
                cmap="plasma",
                vmin=cmin,
                vmax=cmax,
            )
            plt.xlabel("Pixels", fontsize=14)
            plt.ylabel("Orders", fontsize=14)

        return (
            pixels_unique,
            orders_unique,
            matrix_wave,
            matrix_merged,
            matrix_spectrum,
            matrix_spectrum_std,
        )

    def yarara_map_1d_to_2d(self, instrument="HARPS03"):
        self.import_material()
        mat = self.material
        wave = np.array(mat["wave"])
        wave_matrix = fits.open(root + "/Python/Material/" + instrument + "_WAVE_MATRIX_A.fits")[
            0
        ].data

        jdb = (
            fits.open(root + "/Python/Material/" + instrument + "_WAVE_MATRIX_A.fits")[0].header[
                "MJD-OBS"
            ]
            + 0.5
        )
        berv_file = self.yarara_get_berv_value(jdb)
        shape = np.shape(wave_matrix)
        wave_matrix = np.reshape(
            myf.doppler_r(np.ravel(wave_matrix), 0 * berv_file * 1000)[0], shape
        )
        dim1, dim2 = np.shape(wave_matrix)
        dim1 += 1
        dim2 += 1

        try:
            blaze = fits.open(root + "/Python/Material/" + instrument + "_BLAZE.fits")[0].data
        except:
            blaze = 0 * wave_matrix

        mapping_pixels = np.zeros((len(wave), len(wave_matrix)))
        mapping_orders = np.zeros((len(wave), len(wave_matrix)))
        mapping_blaze = np.zeros((len(wave), len(wave_matrix)))

        index = np.arange(len(wave))
        for order in tqdm(range(len(wave_matrix))):
            index_cut = index[
                (wave > np.min(wave_matrix[order])) & (wave < np.max(wave_matrix[order]))
            ]
            wave_cut = wave[index_cut]
            match = myf.find_nearest(wave_matrix[order], wave_cut)
            mapping_orders[index_cut, order] = order + 1
            mapping_pixels[index_cut, order] = match[0] + 1
            mapping_blaze[index_cut, order] = blaze[order, match[0]]

        max_overlapping = np.max(np.sum(mapping_orders != 0, axis=1))
        print("\nMaximum %.0f orders overlapped" % (max_overlapping))

        sort = np.argsort(mapping_pixels, axis=1)[:, ::-1]
        map_pix = np.array(
            [mapping_pixels[i, sort[i]][0:max_overlapping] for i in range(len(sort))]
        )
        map_ord = np.array(
            [mapping_orders[i, sort[i]][0:max_overlapping] for i in range(len(sort))]
        )
        map_blaze = np.array(
            [mapping_blaze[i, sort[i]][0:max_overlapping] for i in range(len(sort))]
        )
        blaze_correction = np.sqrt(map_blaze[:, 0] + map_blaze[:, 1])
        blaze_correction[blaze_correction == 0] = 1
        blaze_correction = 1 / np.sqrt(blaze_correction)

        zone_merged = np.sum(map_pix != 0, axis=1) - 1

        coded_pixels = np.zeros(len(wave))
        coded_orders = np.zeros(len(wave))

        print("\nEncoding pixels and orders by R%.0f to R mapping\n" % (max_overlapping))
        time.sleep(1)

        for i in tqdm(range(len(wave))):
            coded_pixels[i] = myf.map_rnr(map_pix[i], val_max=dim2, n=max_overlapping)
            coded_orders[i] = myf.map_rnr(map_ord[i], val_max=dim1, n=max_overlapping)

        mat["pixels_rnr"] = coded_pixels
        mat["orders_rnr"] = coded_orders
        mat["merged"] = zone_merged
        mat["blaze_correction"] = blaze_correction

        myf.pickle_dump(mat, open(self.directory + "Analyse_material.p", "wb"))

    def yarara_detector(self, wave_cut=[]):
        self.import_material()
        load = self.material
        load["detector"] = 0
        wave_cut.append(0)
        for j in wave_cut:
            load.loc[load["wave"] > j, "detector"] = load.loc[load["wave"] > j, "detector"] + 1

        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

    def yarara_get_bin_length(self):
        bin_length = int(
            "".join(
                glob.glob(self.dir_root + "WORKSPACE/RASSINE*")[0]
                .split("bin_")[1]
                .split(".")[0:-3]
            )
        )
        return bin_length

    def yarara_get_orders(self):
        self.import_material()
        mat = self.material
        orders = np.array(mat["orders_rnr"])
        orders = myf.map_rnr(orders)
        orders = np.round(orders, 0)
        self.orders = orders
        return orders

    def yarara_get_pixels(self):
        self.import_material()
        mat = self.material
        pixels = np.array(mat["pixels_rnr"])
        pixels = myf.map_rnr(pixels)
        pixels = np.round(pixels, 0)
        self.pixels = pixels
        return pixels

    def supress_low_snr_spectra(self, snr_cutoff=100, supress=False):
        """
        Supress spectra under fixed treshold in SNR

        Parameters
        ----------

        snr_cutoff : treshold value of the cutoff below which spectra are removed
        supress : True/False to delete of simply hide the files

        """

        files_to_process = np.sort(glob.glob(self.directory + "R*.p"))
        mask = np.zeros(len(files_to_process))
        c = -1
        for j in files_to_process:
            c += 1
            file = pd.read_pickle(j)
            if "parameters" not in file.keys():
                if file["SNR_5500"] < snr_cutoff:
                    mask[c] = 1
                    if supress:
                        print("File deleted : %s " % (j))
                        os.system("rm " + j)
                    else:
                        new_name = "snr_rassine_" + j.split("_")[1]
                        print("File renamed : %s as %s" % (j, new_name))
                        os.system("mv " + j + " " + new_name)
            else:
                if file["parameters"]["SNR_5500"] < snr_cutoff:
                    mask[c] = 1
                    if supress:
                        print("File deleted : %s " % (j))
                        os.system("rm " + j)
                    else:
                        new_name = "snr_rassine_" + j.split("_")[1]
                        print("File renamed : %s as %s" % (j, new_name))
                        os.system("mv " + j + " " + new_name)

        os.system("rm " + self.directory + "Analyse_summary.p")
        os.system("rm " + self.directory + "Analyse_summary.csv")
        self.yarara_analyse_summary()
        mask = mask.astype("bool")
        self.supress_time_RV(mask)

    # =============================================================================
    #     SUPRESS SPECTRA ACCORDING TO TIME
    # =============================================================================

    def supress_time_RV(self, liste):

        self.import_ccf()

        if sum(liste):
            mask = list(self.table_ccf.keys())[1:]
            try:
                for m in mask:
                    for d in self.table_ccf[m].keys():
                        self.table_ccf[m][d]["table"] = self.table_ccf[m][d]["table"][
                            ~liste
                        ].reset_index(drop=True)

                myf.pickle_dump(self.table_ccf, open(self.directory + "Analyse_ccf.p", "wb"))

                print(" [INFO] CCF table modifed")
            except:
                print(" [ERROR] CCF cannot be modified")

            for i in ["lbl", "lbl_iter", "dbd", "wbw", "aba", "bt"]:
                try:
                    if i == "lbl":
                        self.import_lbl()
                        tab = self.lbl
                    elif i == "lbl_iter":
                        self.import_lbl_iter()
                        tab = self.lbl_iter
                    elif i == "dbd":
                        self.import_dbd()
                        tab = self.dbd
                    elif i == "wbw":
                        self.import_wbw()
                        tab = self.wbw
                    elif i == "aba":
                        self.import_aba()
                        tab = self.aba
                    elif i == "bt":
                        self.import_bt()
                        tab = self.bt

                    name = i[0:3]
                    for d in tab.keys():
                        tab[d]["jdb"] = tab[d]["jdb"][~liste]
                        tab[d][name] = tab[d][name][:, :, ~liste]
                        tab[d][name + "_std"] = tab[d][name + "_std"][:, :, ~liste]

                    if i == "lbl":
                        self.lbl = tab
                        myf.pickle_dump(
                            self.lbl, open(self.directory + "Analyse_line_by_line.p", "wb")
                        )
                        print(" [INFO] LBL table modifed")
                    elif i == "lbl_iter":
                        self.lbl_iter = tab
                        myf.pickle_dump(
                            self.lbl_iter,
                            open(self.directory + "Analyse_line_by_line_iter.p", "wb"),
                        )
                        print(" [INFO] LBL_ITER table modifed")
                    elif i == "dbd":
                        self.dbd = tab
                        myf.pickle_dump(
                            self.dbd, open(self.directory + "Analyse_depth_by_depth.p", "wb")
                        )
                        print(" [INFO] DBD table modifed")
                    elif i == "wbw":
                        self.wbw = tab
                        myf.pickle_dump(
                            self.wbw, open(self.directory + "Analyse_width_by_width.p", "wb")
                        )
                        print(" [INFO] WBW table modifed")
                    elif i == "aba":
                        self.aba = tab
                        myf.pickle_dump(
                            self.aba, open(self.directory + "Analyse_asym_by_asym.p", "wb")
                        )
                        print(" [INFO] ABA table modifed")
                    elif i == "bt":
                        self.bt = tab
                        myf.pickle_dump(self.bt, open(self.directory + "Analyse_bt_by_bt.p", "wb"))
                        print(" [INFO] BT table modifed")
                except:
                    print(" [ERROR] %s cannot be modified" % (i.upper()))

    def supress_time_spectra(
        self,
        liste=None,
        jdb_min=None,
        jdb_max=None,
        num_min=None,
        num_max=None,
        supress=False,
        name_ext="temp",
    ):
        """
        Supress spectra according to time

        Parameters
        ----------

        jdb or num are both inclusive in lower and upper limit
        snr_cutoff : treshold value of the cutoff below which spectra are removed
        supress : True/False to delete of simply hide the files

        """

        self.import_table()
        jdb = self.table["jdb"]
        name = self.table["filename"]
        directory = "/".join(np.array(name)[0].split("/")[:-1])

        if num_min is not None:
            jdb_min = jdb[num_min]

        if num_max is not None:
            jdb_max = jdb[num_max]

        if jdb_min is None:
            jdb_min = jdb_max

        if jdb_max is None:
            jdb_max = jdb_min

        if (num_min is None) & (num_max is None) & (jdb_min is None) & (jdb_max is None):
            jdb_min = 1e9
            jdb_max = -1e9

        if liste is None:
            mask = np.array((jdb >= jdb_min) & (jdb <= jdb_max))
        else:
            if type(liste[0]) == np.bool_:
                mask = liste
            else:
                mask = np.in1d(np.arange(len(jdb)), liste)

        if sum(mask):
            idx = np.arange(len(jdb))[mask]
            print(" [INFO] Following spectrum indices will be supressed : ", idx)
            print(" [INFO] Number of spectrum supressed : %.0f \n" % (sum(mask)))
            maps = glob.glob(self.dir_root + "CORRECTION_MAP/*.p")
            if len(maps):
                for names in maps:
                    correction_map = pd.read_pickle(names)
                    correction_map["correction_map"] = np.delete(
                        correction_map["correction_map"], idx, axis=0
                    )
                    myf.pickle_dump(correction_map, open(names, "wb"))
                    print("%s modified" % (names.split("/")[-1]))

            name = name[mask]

            files_to_process = np.sort(np.array(name))
            for j in files_to_process:
                if supress:
                    print("File deleted : %s " % (j))
                    os.system("rm " + j)
                else:
                    new_name = name_ext + "_" + j.split("/")[-1]
                    print("File renamed : %s " % (directory + "/" + new_name))
                    os.system("mv " + j + " " + directory + "/" + new_name)

            os.system("rm " + directory + "/Analyse_summary.p")
            os.system("rm " + directory + "/Analyse_summary.csv")
            self.yarara_analyse_summary()

            self.supress_time_RV(mask)

    def add_spectra(self, name_ext):
        directory = self.directory
        name = glob.glob(directory + name_ext + "*.p")
        files_to_process = np.sort(np.array(name))
        for j in files_to_process:
            new_name = "".join(j.split(name_ext))
            print("File renamed : %s " % (new_name))
            os.system("mv " + j + " " + new_name)
        os.system("rm " + directory + "/Analyse_summary.p")
        os.system("rm " + directory + "/Analyse_summary.csv")
        self.yarara_analyse_summary()

    # =============================================================================
    #     SPLIT REDUCTION IF CHANGE OF INSTRUMENT
    # =============================================================================

    def split_instrument(self, instrument="HARPS"):
        """
        Split spectra in subdirectories based on the jdb values (modification of the instruments)

        Parameters
        ----------

        fiber_changed : borders of the jdb value to split the spectra in different directories
        HARPS = fiber_changed=[0,57161.5,100000]
        HARPN = fiber_changed=[0,56735.5,100000]

        """

        if instrument == "HARPS":
            fiber_changed = [0, 57161.5, 100000]
            new_dir = ["HARPS03", "HARPS15"]
        if instrument == "HARPN":
            fiber_changed = [0, 56750, 57550, 100000]
            new_dir = ["HARPN01", "HARPN02", "HARPN03"]

        self.import_table()
        tab = self.table
        parent_dir = self.directory
        count = -1

        dace_table = pd.read_csv(
            self.dir_root + "DACE_TABLE/Dace_extracted_table.csv", index_col=0
        )

        for j in range(len(fiber_changed) - 1):
            count += 1

            selection = tab.loc[
                (tab["jdb"] > fiber_changed[j]) & (tab["jdb"] < fiber_changed[j + 1])
            ]
            selection_dace = dace_table.loc[
                (dace_table["rjd"] > fiber_changed[j]) & (dace_table["rjd"] < fiber_changed[j + 1])
            ]
            selection_dace = selection_dace.reset_index(drop=True)

            if len(selection) > 1:
                new_path = "/".join(parent_dir.split("/")[0:-3]) + "/" + new_dir[count]

                if not os.path.exists(new_path):
                    os.system("mkdir " + new_path)

                selection_dace.to_csv(new_path + "/DACE_TABLE/Dace_extracted_table.csv")

                for idx in tqdm(selection["filename"].index):
                    file = selection.loc[idx, "filename"]
                    new_name = file.replace(instrument, new_dir[count])
                    selection.loc[idx, "filename"] = new_name
                    os.system("cp " + file + " " + new_name)

                print("\n Directory : %s successfully created" % (new_path))

    def yarara_merge_instrument(
        self,
        instruments=["HARPS03", "HARPS15", "HARPN", "ESPRESSO18", "ESPRESSO19"],
        main_ins=None,
    ):
        """
        Remerge spectra time-series that were splitted

        Parameters
        ----------

        """

        parent = "/".join(self.dir_root.split("/")[:-2])
        if not os.path.exists(parent + "/INS_merged"):
            os.system("mkdir " + parent + "/INS_merged")

        if not os.path.exists(parent + "/INS_merged/KITCAT"):
            os.system("mkdir " + parent + "/INS_merged/KITCAT")

        if not os.path.exists(parent + "/INS_merged/IMAGES"):
            os.system("mkdir " + parent + "/INS_merged/IMAGES")

        if not os.path.exists(parent + "/INS_merged/KEPLERIAN"):
            os.system("mkdir " + parent + "/INS_merged/KEPLERIAN")

        if not os.path.exists(parent + "/INS_merged/WORKSPACE"):
            os.system("mkdir " + parent + "/INS_merged/WORKSPACE")

        if not os.path.exists(parent + "/INS_merged/STAR_INFO"):
            os.system("mkdir " + parent + "/INS_merged/STAR_INFO")

        if not os.path.exists(parent + "/INS_merged/DACE_TABLE"):
            os.system("mkdir " + parent + "/INS_merged/DACE_TABLE")

        if not os.path.exists(parent + "/INS_merged/TEMP"):
            os.system("mkdir " + parent + "/INS_merged/TEMP")

        # summary merging
        dir_found = []
        for i in instruments:
            if len(glob.glob(parent + "/" + i + "/*")):
                dir_found.append("/".join(glob.glob(parent + "/" + i + "/*")[0].split("/")[:-1]))

        valid_data = []
        for d in dir_found:
            if os.path.exists(d + "/WORKSPACE/Analyse_line_by_line.p"):
                valid_data.append(d)
        dir_found = valid_data

        if not len(dir_found):
            print(" [INFO] Any instrument found")
        else:
            for j, path in enumerate(dir_found):
                if not j:
                    summary = pd.read_pickle(path + "/WORKSPACE/Analyse_summary.p")
                else:
                    summary = pd.concat(
                        [summary, pd.read_pickle(path + "/WORKSPACE/Analyse_summary.p")]
                    )

            summary = summary.sort_values(by="jdb")
            summary = summary.reset_index(drop=True)
            myf.pickle_dump(
                summary, open(parent + "/INS_merged/WORKSPACE/Analyse_summary.p", "wb")
            )
            summary.to_csv(parent + "/INS_merged/WORKSPACE/Analyse_summary.csv")

            # star_info
            if main_ins is None:
                main_ins = summary["ins"].value_counts().index[0]
                print(" [INFO] The main instrument is %s" % (main_ins))
            os.system(
                "cp "
                + parent
                + "/"
                + main_ins
                + "/STAR_INFO/Stellar_info_"
                + self.starname
                + ".p "
                + parent
                + "/INS_merged/STAR_INFO/Stellar_info_"
                + self.starname
                + ".p"
            )

            star_template = pd.read_pickle(
                parent + "/" + main_ins + "/STAR_INFO/Stellar_info_" + self.starname + ".p"
            )["stellar_template"]["YARARA"]

            file_test = summary["filename"][0]
            os.system("cp " + file_test + " " + parent + "/INS_merged/WORKSPACE/")

            all_instrument = np.array([i.split("/")[-1] for i in dir_found])
            loc = np.where(all_instrument == main_ins)[0][0]

            print(" [INFO] All instrument found : ", all_instrument)

            summary = pd.read_pickle(parent + "/" + main_ins + "/WORKSPACE/Analyse_summary.p")

            dir_found = np.delete(dir_found, loc)

            # homogeneity of stellar_template for activity proxies
            for path in dir_found:
                star_inf = pd.read_pickle(path + "/STAR_INFO/Stellar_info_" + self.starname + ".p")
                star_inf["stellar_template"]["YARARA"] = star_template
                myf.pickle_dump(
                    star_inf, open(path + "/STAR_INFO/Stellar_info_" + self.starname + ".p", "wb")
                )
                star_name_dir = path.split("Yarara/")[1].split("/")[0]
                ins_dir = path.split("s1d/")[1].split("/")[0].lower()
                # print(' python trigger_yarara_%s.py -s %s -b 38 -r master'%(ins_dir,star_name_dir))
                # os.system('python trigger_yarara_%s.py -s %s -b 38 -r master'%(ins_dir,star_name_dir))
                plt.close("all")

            # material
            material_file = pd.read_pickle(
                parent + "/" + main_ins + "/WORKSPACE/Analyse_material.p"
            )
            for path in dir_found:
                material = pd.read_pickle(path + "/WORKSPACE/Analyse_material.p")
                material_file["mask_brute"] = material_file["mask_brute"] | material["mask_brute"]
            myf.pickle_dump(
                material_file, open(parent + "/INS_merged/WORKSPACE/Analyse_material.p", "wb")
            )

            # kitcat cleaned merging
            kitcat_file = pd.read_pickle(
                parent + "/" + main_ins + "/KITCAT/kitcat_cleaned_mask_" + self.starname + ".p"
            )
            kitcat = kitcat_file["catalogue"]
            kitcat_wave = np.array(kitcat["freq_mask0"].copy())

            for j, path in enumerate(dir_found):
                tab = pd.read_pickle(path + "/KITCAT/kitcat_cleaned_mask_" + self.starname + ".p")[
                    "catalogue"
                ]
                match = myf.match_nearest(kitcat_wave, np.array(tab["freq_mask0"]))
                match = match[abs(match[:, -1]) < 0.03]
                kitcat_wave = kitcat_wave[match[:, 0].astype("int")]

            kitcat = pd.merge(
                kitcat,
                pd.DataFrame({"freq_mask0": kitcat_wave}),
                how="inner",
                left_on="freq_mask0",
                right_on="freq_mask0",
            )
            kitcat = kitcat.reset_index(drop=True)
            kitcat_file["catalogue"] = kitcat

            myf.pickle_dump(
                kitcat_file,
                open(
                    parent + "/INS_merged/KITCAT/kitcat_cleaned_mask_" + self.starname + ".p", "wb"
                ),
            )

            # kitcat merging
            kitcat_file = pd.read_pickle(
                parent + "/" + main_ins + "/KITCAT/kitcat_mask_" + self.starname + ".p"
            )
            kitcat = kitcat_file["catalogue"]
            kitcat_wave = np.array(kitcat["freq_mask0"].copy())

            for j, path in enumerate(dir_found):
                tab = pd.read_pickle(path + "/KITCAT/kitcat_mask_" + self.starname + ".p")[
                    "catalogue"
                ]
                match = myf.match_nearest(kitcat_wave, np.array(tab["freq_mask0"]))
                match = match[abs(match[:, -1]) < 0.03]
                kitcat_wave = kitcat_wave[match[:, 0].astype("int")]

            kitcat = pd.merge(
                kitcat,
                pd.DataFrame({"freq_mask0": kitcat_wave}),
                how="inner",
                left_on="freq_mask0",
                right_on="freq_mask0",
            )
            kitcat = kitcat.reset_index(drop=True)
            kitcat_file["catalogue"] = kitcat

            os.system(
                "cp "
                + parent
                + "/"
                + main_ins
                + "/KITCAT/kitcat_spectrum.p "
                + parent
                + "/INS_merged/KITCAT/kitcat_spectrum.p"
            )
            myf.pickle_dump(
                kitcat_file,
                open(parent + "/INS_merged/KITCAT/kitcat_mask_" + self.starname + ".p", "wb"),
            )

            # dace table
            dace_file = pd.read_csv(
                parent + "/" + main_ins + "/DACE_TABLE/Dace_extracted_table.csv", index_col=0
            )
            for j, path in enumerate(dir_found):
                try:
                    tab = pd.read_csv(path + "/DACE_TABLE/Dace_extracted_table.csv", index_col=0)
                    dace_file = pd.concat([dace_file, tab])
                    dace_file = dace_file.sort_values(by="jdb").reset_index(drop=True)
                except:
                    pass
            dace_file.to_csv(parent + "/INS_merged/DACE_TABLE/Dace_extracted_table.csv")

            # ccf
            ccf_file = pd.read_pickle(parent + "/" + main_ins + "/WORKSPACE/Analyse_ccf.p")
            masks = np.array(list(ccf_file.keys()))
            masks = masks[masks != "star_info"]
            for mask in masks:
                for sub_dico in ccf_file[mask].keys():
                    for j, path in enumerate(dir_found):
                        try:
                            tab = pd.read_pickle(path + "/WORKSPACE/Analyse_ccf.p")[mask][
                                sub_dico
                            ]["table"]
                            ccf_file[mask][sub_dico]["table"] = pd.concat(
                                [ccf_file[mask][sub_dico]["table"], tab]
                            )
                            ccf_file[mask][sub_dico]["table"] = (
                                ccf_file[mask][sub_dico]["table"]
                                .sort_values(by="jdb")
                                .reset_index(drop=True)
                            )
                        except KeyError:
                            pass

            myf.pickle_dump(ccf_file, open(parent + "/INS_merged/WORKSPACE/Analyse_ccf.p", "wb"))

            # lbl
            for fname in ["line_by_line", "line_by_line_iter"]:
                lbl_file = pd.read_pickle(
                    parent + "/" + main_ins + "/WORKSPACE/Analyse_" + fname + ".p"
                )
                for sub_dico in lbl_file.keys():
                    valid_lines = np.array(lbl_file[sub_dico]["catalog"]["valid"])
                    for j, path in enumerate(dir_found):
                        try:
                            tab = pd.read_pickle(path + "/WORKSPACE/Analyse_" + fname + ".p")[
                                sub_dico
                            ]
                            match = myf.match_nearest(
                                kitcat_wave, np.array(tab["catalog"]["freq_mask0"])
                            )

                            valid_lines = (valid_lines[match[:, 0].astype("int")]) & (
                                np.array(tab["catalog"]["valid"])[match[:, 1].astype("int")]
                            )
                            lbl_file[sub_dico]["catalog"] = lbl_file[sub_dico][
                                "catalog"
                            ].reset_index(drop=True)
                            lbl_file[sub_dico]["catalog"] = lbl_file[sub_dico]["catalog"].iloc[
                                match[:, 0].astype("int")
                            ]

                            lbl_file[sub_dico]["lbl"] = lbl_file[sub_dico]["lbl"][
                                :, match[:, 0].astype("int"), :
                            ]
                            lbl_file[sub_dico]["lbl_std"] = lbl_file[sub_dico]["lbl_std"][
                                :, match[:, 0].astype("int"), :
                            ]

                            tab["lbl"] = tab["lbl"][:, match[:, 1].astype("int"), :]
                            tab["lbl_std"] = tab["lbl_std"][:, match[:, 1].astype("int"), :]

                            lbl_file[sub_dico]["jdb"] = np.vstack(
                                [lbl_file[sub_dico]["jdb"], tab["jdb"]]
                            )
                            lbl_file[sub_dico]["lbl"] = np.concatenate(
                                [lbl_file[sub_dico]["lbl"], tab["lbl"]], axis=2
                            )
                            lbl_file[sub_dico]["lbl_std"] = np.concatenate(
                                [lbl_file[sub_dico]["lbl_std"], tab["lbl_std"]], axis=2
                            )
                        except:
                            pass
                    sorting = np.argsort(lbl_file[sub_dico]["jdb"])
                    lbl_file[sub_dico]["jdb"] = lbl_file[sub_dico]["jdb"][sorting]
                    lbl_file[sub_dico]["lbl"] = lbl_file[sub_dico]["lbl"][:, :, sorting]
                    lbl_file[sub_dico]["lbl_std"] = lbl_file[sub_dico]["lbl_std"][:, :, sorting]

                    lbl_file[sub_dico]["catalog"]["valid"] = valid_lines

                    t = myc.table(lbl_file[sub_dico]["lbl"][0])
                    t.rms_w(1 / lbl_file[sub_dico]["lbl_std"][0] ** 2, axis=1)
                    lbl_file[sub_dico]["catalog"]["rms"] = t.rms
                    lbl_file[sub_dico]["catalog"]["med_err"] = np.median(
                        lbl_file[sub_dico]["lbl_std"][0], axis=1
                    )
                    lbl_file[sub_dico]["catalog"]["quality"] = (
                        lbl_file[sub_dico]["catalog"]["rms"]
                        / lbl_file[sub_dico]["catalog"]["med_err"]
                    )

                myf.pickle_dump(
                    lbl_file, open(parent + "/INS_merged/WORKSPACE/Analyse_" + fname + ".p", "wb")
                )

            # morpho
            for fname, kw in zip(
                ["depth_by_depth", "asym_by_asym", "width_by_width", "bt_by_bt"],
                ["dbd", "aba", "wbw", "bt"],
            ):
                lbl_file = pd.read_pickle(
                    parent + "/" + main_ins + "/WORKSPACE/Analyse_" + fname + ".p"
                )
                dic = np.array(list(lbl_file.keys())).copy()
                for sub_dico in dic:
                    valid_lines = np.array(lbl_file[sub_dico]["catalog"]["valid"])
                    try:
                        for j, path in enumerate(dir_found):
                            tab = pd.read_pickle(path + "/WORKSPACE/Analyse_" + fname + ".p")[
                                sub_dico
                            ]
                            match = myf.match_nearest(
                                kitcat_wave, np.array(tab["catalog"]["freq_mask0"])
                            )

                            valid_lines = (valid_lines[match[:, 0].astype("int")]) & (
                                np.array(tab["catalog"]["valid"])[match[:, 1].astype("int")]
                            )
                            lbl_file[sub_dico]["catalog"] = lbl_file[sub_dico][
                                "catalog"
                            ].reset_index(drop=True)
                            lbl_file[sub_dico]["catalog"] = lbl_file[sub_dico]["catalog"].iloc[
                                match[:, 0].astype("int")
                            ]

                            lbl_file[sub_dico][kw] = lbl_file[sub_dico][kw][
                                :, match[:, 0].astype("int"), :
                            ]
                            lbl_file[sub_dico][kw + "_std"] = lbl_file[sub_dico][kw + "_std"][
                                :, match[:, 0].astype("int"), :
                            ]

                            tab[kw] = tab[kw][:, match[:, 1].astype("int"), :]
                            tab[kw + "_std"] = tab[kw + "_std"][:, match[:, 1].astype("int"), :]
                            try:
                                lbl_file[sub_dico]["jdb"] = np.vstack(
                                    [lbl_file[sub_dico]["jdb"], tab["jdb"]]
                                )
                                lbl_file[sub_dico][kw] = np.concatenate(
                                    [lbl_file[sub_dico][kw], tab[kw]], axis=2
                                )
                                lbl_file[sub_dico][kw + "_std"] = np.concatenate(
                                    [lbl_file[sub_dico][kw + "_std"], tab[kw + "_std"]], axis=2
                                )
                            except:
                                pass
                        sorting = np.argsort(lbl_file[sub_dico]["jdb"])
                        lbl_file[sub_dico]["jdb"] = lbl_file[sub_dico]["jdb"][sorting]
                        lbl_file[sub_dico][kw] = lbl_file[sub_dico][kw][:, :, sorting]
                        lbl_file[sub_dico][kw + "_std"] = lbl_file[sub_dico][kw + "_std"][
                            :, :, sorting
                        ]

                        lbl_file[sub_dico]["catalog"]["valid"] = valid_lines
                    except KeyError:
                        del lbl_file[sub_dico]

                myf.pickle_dump(
                    lbl_file, open(parent + "/INS_merged/WORKSPACE/Analyse_" + fname + ".p", "wb")
                )

    # =============================================================================
    # MAKE SUMMARY
    # =============================================================================

    def yarara_update_summary(self, table_to_merge):
        self.import_table()
        directory = self.directory

        table = self.table
        if "jdb" not in list(table_to_merge.keys()):
            print("[ERROR] The keyword 'jdb' must at least be present in the input table")
        else:
            tab = pd.concat([table, table_to_merge])
            tab = tab.sort_values(by="jdb").reset_index(drop=True)
            ras.save_pickle(directory + "/Analyse_summary.p", tab)
            tab.to_csv(directory + "/Analyse_summary.csv")
            print(" [INFO] Analyse summary updated !")

    def yarara_analyse_summary(self, rm_old=False):
        """
        Produce a summary table with the RASSINE files of the specified directory

        """

        directory = self.directory
        if rm_old:
            os.system("rm " + directory + "Analyse_summary.p")
            os.system("rm " + directory + "Analyse_summary.csv")

        test, names = self.import_rassine_output(return_name=True)

        # info obs

        ins = np.array(
            [ras.try_field(test[j]["parameters"], "instrument") for j in range(len(test))]
        )
        snr = np.array(
            [ras.try_field(test[j]["parameters"], "SNR_5500") for j in range(len(test))]
        )
        snr2 = np.array(
            [ras.try_field(test[j]["parameters"], "SNR_computed") for j in range(len(test))]
        )
        jdb = np.array([ras.try_field(test[j]["parameters"], "jdb") for j in range(len(test))])
        mjd = np.array([ras.try_field(test[j]["parameters"], "mjd") for j in range(len(test))])
        berv = np.array([ras.try_field(test[j]["parameters"], "berv") for j in range(len(test))])
        berv_planet = np.array(
            [ras.try_field(test[j]["parameters"], "berv_planet") for j in range(len(test))]
        )
        lamp_offset = np.array(
            [ras.try_field(test[j]["parameters"], "lamp_offset") for j in range(len(test))]
        )
        rv = np.array([ras.try_field(test[j]["parameters"], "rv") for j in range(len(test))])
        rv_shift = np.array(
            [ras.try_field(test[j]["parameters"], "RV_shift") for j in range(len(test))]
        )
        rv_sec = np.array(
            [ras.try_field(test[j]["parameters"], "RV_sec") for j in range(len(test))]
        )
        rv_moon = np.array(
            [ras.try_field(test[j]["parameters"], "RV_moon") for j in range(len(test))]
        )
        airmass = np.array(
            [ras.try_field(test[j]["parameters"], "airmass") for j in range(len(test))]
        )
        texp = np.array([ras.try_field(test[j]["parameters"], "texp") for j in range(len(test))])
        seeing = np.array(
            [ras.try_field(test[j]["parameters"], "seeing") for j in range(len(test))]
        )
        humidity = np.array(
            [ras.try_field(test[j]["parameters"], "humidity") for j in range(len(test))]
        )
        rv_planet = np.array(
            [ras.try_field(test[j]["parameters"], "rv_planet") for j in range(len(test))]
        )
        rv_dace = np.array(
            [ras.try_field(test[j]["parameters"], "rv_dace") for j in range(len(test))]
        )
        rv_dace_std = np.array(
            [ras.try_field(test[j]["parameters"], "rv_dace_std") for j in range(len(test))]
        )
        flux_balance = np.array(
            [ras.try_field(test[j]["parameters"], "flux_balance") for j in range(len(test))]
        )
        transit = np.array(
            [ras.try_field(test[j]["parameters"], "transit_in") for j in range(len(test))]
        )
        night_drift = np.array(
            [ras.try_field(test[j]["parameters"], "drift_used") for j in range(len(test))]
        )
        night_drift_std = np.array(
            [ras.try_field(test[j]["parameters"], "drift_used_std") for j in range(len(test))]
        )

        # info activity

        rhk = np.array([ras.try_field(test[j]["parameters"], "RHK") for j in range(len(test))])
        rhk_std = np.array(
            [ras.try_field(test[j]["parameters"], "RHK_std") for j in range(len(test))]
        )
        kernel_rhk = np.array(
            [ras.try_field(test[j]["parameters"], "Kernel_CaII") for j in range(len(test))]
        )
        kernel_rhk_std = np.array(
            [ras.try_field(test[j]["parameters"], "Kernel_CaII_std") for j in range(len(test))]
        )
        ca2k = np.array([ras.try_field(test[j]["parameters"], "CaIIK") for j in range(len(test))])
        ca2k_std = np.array(
            [ras.try_field(test[j]["parameters"], "CaIIK_std") for j in range(len(test))]
        )
        ca2h = np.array([ras.try_field(test[j]["parameters"], "CaIIH") for j in range(len(test))])
        ca2h_std = np.array(
            [ras.try_field(test[j]["parameters"], "CaIIH_std") for j in range(len(test))]
        )
        ca2 = np.array([ras.try_field(test[j]["parameters"], "CaII") for j in range(len(test))])
        ca2_std = np.array(
            [ras.try_field(test[j]["parameters"], "CaII_std") for j in range(len(test))]
        )
        ca1 = np.array([ras.try_field(test[j]["parameters"], "CaI") for j in range(len(test))])
        ca1_std = np.array(
            [ras.try_field(test[j]["parameters"], "CaI_std") for j in range(len(test))]
        )
        mg1 = np.array([ras.try_field(test[j]["parameters"], "MgI") for j in range(len(test))])
        mg1_std = np.array(
            [ras.try_field(test[j]["parameters"], "MgI_std") for j in range(len(test))]
        )
        mg1a = np.array([ras.try_field(test[j]["parameters"], "MgIa") for j in range(len(test))])
        mg1a_std = np.array(
            [ras.try_field(test[j]["parameters"], "MgIa_std") for j in range(len(test))]
        )
        mg1b = np.array([ras.try_field(test[j]["parameters"], "MgIb") for j in range(len(test))])
        mg1b_std = np.array(
            [ras.try_field(test[j]["parameters"], "MgIb_std") for j in range(len(test))]
        )
        mg1c = np.array([ras.try_field(test[j]["parameters"], "MgIc") for j in range(len(test))])
        mg1c_std = np.array(
            [ras.try_field(test[j]["parameters"], "MgIc_std") for j in range(len(test))]
        )
        d3 = np.array([ras.try_field(test[j]["parameters"], "HeID3") for j in range(len(test))])
        d3_std = np.array(
            [ras.try_field(test[j]["parameters"], "HeID3_std") for j in range(len(test))]
        )
        nad = np.array([ras.try_field(test[j]["parameters"], "NaD") for j in range(len(test))])
        nad_std = np.array(
            [ras.try_field(test[j]["parameters"], "NaD_std") for j in range(len(test))]
        )
        nad1 = np.array([ras.try_field(test[j]["parameters"], "NaD1") for j in range(len(test))])
        nad1_std = np.array(
            [ras.try_field(test[j]["parameters"], "NaD1_std") for j in range(len(test))]
        )
        nad2 = np.array([ras.try_field(test[j]["parameters"], "NaD2") for j in range(len(test))])
        nad2_std = np.array(
            [ras.try_field(test[j]["parameters"], "NaD2_std") for j in range(len(test))]
        )
        ha = np.array([ras.try_field(test[j]["parameters"], "Ha") for j in range(len(test))])
        ha_std = np.array(
            [ras.try_field(test[j]["parameters"], "Ha_std") for j in range(len(test))]
        )
        hb = np.array([ras.try_field(test[j]["parameters"], "Hb") for j in range(len(test))])
        hb_std = np.array(
            [ras.try_field(test[j]["parameters"], "Hb_std") for j in range(len(test))]
        )
        hc = np.array([ras.try_field(test[j]["parameters"], "Hc") for j in range(len(test))])
        hc_std = np.array(
            [ras.try_field(test[j]["parameters"], "Hc_std") for j in range(len(test))]
        )
        hd = np.array([ras.try_field(test[j]["parameters"], "Hd") for j in range(len(test))])
        hd_std = np.array(
            [ras.try_field(test[j]["parameters"], "Hd_std") for j in range(len(test))]
        )
        heps = np.array([ras.try_field(test[j]["parameters"], "Heps") for j in range(len(test))])
        heps_std = np.array(
            [ras.try_field(test[j]["parameters"], "Heps_std") for j in range(len(test))]
        )

        wbk = np.array([ras.try_field(test[j]["parameters"], "WB_K") for j in range(len(test))])
        wbk_std = np.array(
            [ras.try_field(test[j]["parameters"], "WB_K_std") for j in range(len(test))]
        )
        wbh = np.array([ras.try_field(test[j]["parameters"], "WB_H") for j in range(len(test))])
        wbh_std = np.array(
            [ras.try_field(test[j]["parameters"], "WB_H_std") for j in range(len(test))]
        )
        wb = np.array([ras.try_field(test[j]["parameters"], "WB") for j in range(len(test))])
        wb_std = np.array(
            [ras.try_field(test[j]["parameters"], "WB_std") for j in range(len(test))]
        )
        wb_h1 = np.array([ras.try_field(test[j]["parameters"], "WB_H1") for j in range(len(test))])
        wb_h1_std = np.array(
            [ras.try_field(test[j]["parameters"], "WB_H1_std") for j in range(len(test))]
        )
        kernel_wb = np.array(
            [ras.try_field(test[j]["parameters"], "Kernel_WB") for j in range(len(test))]
        )
        kernel_wb_std = np.array(
            [ras.try_field(test[j]["parameters"], "Kernel_WB_std") for j in range(len(test))]
        )

        cb = np.array([ras.try_field(test[j]["parameters"], "CB") for j in range(len(test))])
        cb_std = np.array(
            [ras.try_field(test[j]["parameters"], "CB_std") for j in range(len(test))]
        )
        cb2 = np.array([ras.try_field(test[j]["parameters"], "CB2") for j in range(len(test))])
        cb2_std = np.array(
            [ras.try_field(test[j]["parameters"], "CB2_std") for j in range(len(test))]
        )

        # pca shells coefficient

        shell_compo = np.array(
            [ras.try_field(test[j]["parameters"], "shell_fitted") for j in range(len(test))]
        )
        shell1 = np.array(
            [ras.try_field(test[j]["parameters"], "shell_v1") for j in range(len(test))]
        )
        shell2 = np.array(
            [ras.try_field(test[j]["parameters"], "shell_v2") for j in range(len(test))]
        )
        shell3 = np.array(
            [ras.try_field(test[j]["parameters"], "shell_v3") for j in range(len(test))]
        )
        shell4 = np.array(
            [ras.try_field(test[j]["parameters"], "shell_v4") for j in range(len(test))]
        )
        shell5 = np.array(
            [ras.try_field(test[j]["parameters"], "shell_v5") for j in range(len(test))]
        )

        # pca rv vectors
        pca_compo = np.array(
            [ras.try_field(test[j]["parameters"], "pca_fitted") for j in range(len(test))]
        )
        pca1 = np.array([ras.try_field(test[j]["parameters"], "pca_v1") for j in range(len(test))])
        pca2 = np.array([ras.try_field(test[j]["parameters"], "pca_v2") for j in range(len(test))])
        pca3 = np.array([ras.try_field(test[j]["parameters"], "pca_v3") for j in range(len(test))])
        pca4 = np.array([ras.try_field(test[j]["parameters"], "pca_v4") for j in range(len(test))])
        pca5 = np.array([ras.try_field(test[j]["parameters"], "pca_v5") for j in range(len(test))])

        # telluric ccf

        vec_background = np.array(
            [ras.try_field(test[j]["parameters"], "proxy_background") for j in range(len(test))]
        )

        # telluric ccf

        tell_ew = np.array(
            [ras.try_field(test[j]["parameters"], "telluric_ew") for j in range(len(test))]
        )
        tell_contrast = np.array(
            [ras.try_field(test[j]["parameters"], "telluric_contrast") for j in range(len(test))]
        )
        tell_rv = np.array(
            [ras.try_field(test[j]["parameters"], "telluric_rv") for j in range(len(test))]
        )
        tell_fwhm = np.array(
            [ras.try_field(test[j]["parameters"], "telluric_fwhm") for j in range(len(test))]
        )
        tell_center = np.array(
            [ras.try_field(test[j]["parameters"], "telluric_center") for j in range(len(test))]
        )
        tell_depth = np.array(
            [ras.try_field(test[j]["parameters"], "telluric_depth") for j in range(len(test))]
        )

        # telluric ccf h2o

        h2o_ew = np.array(
            [ras.try_field(test[j]["parameters"], "h2o_ew") for j in range(len(test))]
        )
        h2o_contrast = np.array(
            [ras.try_field(test[j]["parameters"], "h2o_contrast") for j in range(len(test))]
        )
        h2o_rv = np.array(
            [ras.try_field(test[j]["parameters"], "h2o_rv") for j in range(len(test))]
        )
        h2o_fwhm = np.array(
            [ras.try_field(test[j]["parameters"], "h2o_fwhm") for j in range(len(test))]
        )
        h2o_center = np.array(
            [ras.try_field(test[j]["parameters"], "h2o_center") for j in range(len(test))]
        )
        h2o_depth = np.array(
            [ras.try_field(test[j]["parameters"], "h2o_depth") for j in range(len(test))]
        )

        # telluric ccf o2

        o2_ew = np.array([ras.try_field(test[j]["parameters"], "o2_ew") for j in range(len(test))])
        o2_contrast = np.array(
            [ras.try_field(test[j]["parameters"], "o2_contrast") for j in range(len(test))]
        )
        o2_rv = np.array([ras.try_field(test[j]["parameters"], "o2_rv") for j in range(len(test))])
        o2_fwhm = np.array(
            [ras.try_field(test[j]["parameters"], "o2_fwhm") for j in range(len(test))]
        )
        o2_center = np.array(
            [ras.try_field(test[j]["parameters"], "o2_center") for j in range(len(test))]
        )
        o2_depth = np.array(
            [ras.try_field(test[j]["parameters"], "o2_depth") for j in range(len(test))]
        )

        # teff

        t_eff = np.array([ras.try_field(test[j]["parameters"], "Teff") for j in range(len(test))])
        t_eff_std = np.array(
            [ras.try_field(test[j]["parameters"], "Teff_std") for j in range(len(test))]
        )

        # ghost

        ghost = np.array([ras.try_field(test[j]["parameters"], "ghost") for j in range(len(test))])

        # sas
        sas = np.array([ras.try_field(test[j]["parameters"], "SAS") for j in range(len(test))])
        sas_std = np.array(
            [ras.try_field(test[j]["parameters"], "SAS_std") for j in range(len(test))]
        )
        sas1y = np.array([ras.try_field(test[j]["parameters"], "SAS1Y") for j in range(len(test))])
        sas1y_std = np.array(
            [ras.try_field(test[j]["parameters"], "SAS1Y_std") for j in range(len(test))]
        )
        bis = np.array([ras.try_field(test[j]["parameters"], "BIS") for j in range(len(test))])
        bis_std = np.array(
            [ras.try_field(test[j]["parameters"], "BIS_std") for j in range(len(test))]
        )
        bis2 = np.array([ras.try_field(test[j]["parameters"], "BIS2") for j in range(len(test))])
        bis2_std = np.array(
            [ras.try_field(test[j]["parameters"], "BIS2_std") for j in range(len(test))]
        )

        # parabola ccf

        try:
            para_rv = np.array(
                [ras.try_field(test[j]["ccf_parabola"], "para_rv") for j in range(len(test))]
            )
            para_depth = np.array(
                [ras.try_field(test[j]["ccf_parabola"], "para_depth") for j in range(len(test))]
            )
        except KeyError:
            para_rv = np.zeros(len(snr))
            para_depth = np.zeros(len(snr))

        # gaussian ccf
        try:
            ccf_ew = np.array([ras.try_field(test[j]["ccf"], "ew") for j in range(len(test))])
            ccf_contrast = np.array(
                [ras.try_field(test[j]["ccf_gaussian"], "contrast") for j in range(len(test))]
            )
            ccf_contrast_std = np.array(
                [ras.try_field(test[j]["ccf_gaussian"], "contrast_std") for j in range(len(test))]
            )
            ccf_rv = np.array(
                [ras.try_field(test[j]["ccf_gaussian"], "rv") for j in range(len(test))]
            )
            ccf_rv_std = np.array(
                [ras.try_field(test[j]["ccf_gaussian"], "rv_std") for j in range(len(test))]
            )
            ccf_fwhm = (
                np.array(
                    [ras.try_field(test[j]["ccf_gaussian"], "fwhm") for j in range(len(test))]
                )
                * 2.355
            )
            ccf_fwhm_std = (
                np.array(
                    [ras.try_field(test[j]["ccf_gaussian"], "fwhm_std") for j in range(len(test))]
                )
                * 2.355
            )
            ccf_offset = np.array(
                [ras.try_field(test[j]["ccf_gaussian"], "offset") for j in range(len(test))]
            )
            ccf_offset_std = np.array(
                [ras.try_field(test[j]["ccf_gaussian"], "offset_std") for j in range(len(test))]
            )
            ccf_vspan = np.array(
                [ras.try_field(test[j]["ccf_gaussian"], "vspan") for j in range(len(test))]
            )
            ccf_vspan_std = np.array(
                [ras.try_field(test[j]["ccf_gaussian"], "rv_std") for j in range(len(test))]
            )
            ccf_svrad_phot = np.array(
                [ras.try_field(test[j]["ccf_gaussian"], "rv_std_phot") for j in range(len(test))]
            )

        except KeyError:
            ccf_ew = np.zeros(len(snr))
            ccf_contrast = np.zeros(len(snr))
            ccf_contrast_std = np.zeros(len(snr))
            ccf_rv = np.zeros(len(snr))
            ccf_rv_std = np.zeros(len(snr))
            ccf_fwhm = np.zeros(len(snr))
            ccf_fwhm_std = np.zeros(len(snr))
            ccf_offset = np.zeros(len(snr))
            ccf_offset_std = np.zeros(len(snr))
            ccf_vspan = np.zeros(len(snr))
            ccf_vspan_std = np.zeros(len(snr))
            ccf_svrad_phot = np.zeros(len(snr))

        dico = pd.DataFrame(
            {
                "filename": names,
                "ins": ins,
                "snr": snr,
                "snr_computed": snr2,
                "flux_balance": flux_balance,
                "jdb": jdb,
                "mjd": mjd,
                "berv": berv,
                "berv_planet": berv_planet,
                "lamp_offset": lamp_offset,
                "rv": rv,
                "rv_shift": rv_shift,
                "rv_sec": rv_sec,
                "rv_moon": rv_moon,
                "rv_planet": rv_planet,
                "rv_dace": rv_dace,
                "rv_dace_std": rv_dace_std,
                "rv_drift": night_drift,
                "rv_drift_std": night_drift_std,
                "airmass": airmass,
                "texp": texp,
                "seeing": seeing,
                "humidity": humidity,
                "transit_in": transit,
                "RHK": rhk,
                "RHK_std": rhk_std,
                "Kernel_CaII": kernel_rhk,
                "Kernel_CaII_std": kernel_rhk_std,
                "CaIIK": ca2k,
                "CaIIK_std": ca2k_std,
                "CaIIH": ca2h,
                "CaIIH_std": ca2h_std,
                "CaII": ca2,
                "CaII_std": ca2_std,
                "CaI": ca1,
                "CaI_std": ca1_std,
                "MgI": mg1,
                "MgI_std": mg1_std,
                "MgIa": mg1a,
                "MgIa_std": mg1a_std,
                "MgIb": mg1b,
                "MgIb_std": mg1b_std,
                "MgIc": mg1c,
                "MgIc_std": mg1c_std,
                "HeID3": d3,
                "HeID3_std": d3_std,
                "NaD": nad,
                "NaD_std": nad_std,
                "NaD1": nad1,
                "NaD1_std": nad1_std,
                "NaD2": nad2,
                "NaD2_std": nad2_std,
                "Ha": ha,
                "Ha_std": ha_std,
                "Hb": hb,
                "Hb_std": hb_std,
                "Hc": hc,
                "Hc_std": hc_std,
                "Hd": hd,
                "Hd_std": hd_std,
                "Heps": heps,
                "Heps_std": heps_std,
                "WBK": wbk,
                "WBK_std": wbk_std,
                "WBH": wbh,
                "WBH_std": wbh_std,
                "WB": wb,
                "WB_std": wb_std,
                "WB_H1": wb_h1,
                "WB_H1_std": wb_h1_std,
                "Kernel_WB": kernel_wb,
                "Kernel_WB_std": kernel_wb_std,
                "CB": cb,
                "CB_std": cb_std,
                "CB2": cb2,
                "CB2_std": cb2_std,
                "shell_fitted": shell_compo,
                "shell1": shell1,
                "shell2": shell2,
                "shell3": shell3,
                "shell4": shell4,
                "shell5": shell5,
                "pca_fitted": pca_compo,
                "pca1": pca1,
                "pca2": pca2,
                "pca3": pca3,
                "pca4": pca4,
                "pca5": pca5,
                "telluric_ew": tell_ew,
                "telluric_contrast": tell_contrast,
                "telluric_rv": tell_rv,
                "telluric_fwhm": tell_fwhm,
                "telluric_center": tell_center,
                "telluric_depth": tell_depth,
                "h2o_ew": h2o_ew,
                "h2o_contrast": h2o_contrast,
                "h2o_rv": h2o_rv,
                "h2o_fwhm": h2o_fwhm,
                "h2o_center": h2o_center,
                "h2o_depth": h2o_depth,
                "o2_ew": o2_ew,
                "o2_contrast": o2_contrast,
                "o2_rv": o2_rv,
                "o2_fwhm": o2_fwhm,
                "o2_center": o2_center,
                "o2_depth": o2_depth,
                "proxy_background": vec_background,
                "ccf_rv_para": para_rv,
                "ccf_depth_para": para_depth,
                "ccf_ew": ccf_ew,
                "ccf_contrast": ccf_contrast,
                "ccf_contrast_std": ccf_contrast_std,
                "ccf_vspan": ccf_vspan,
                "ccf_vspan_std": ccf_vspan_std,
                "ccf_rv": ccf_rv,
                "ccf_rv_std": ccf_rv_std,
                "ccf_rv_std_phot": ccf_svrad_phot,
                "ccf_fwhm": ccf_fwhm,
                "ccf_fwhm_std": ccf_fwhm_std,
                "ccf_offset": ccf_offset,
                "ccf_offset_std": ccf_offset_std,
                "Teff": t_eff,
                "Teff_std": t_eff_std,
                "ghost": ghost,
                "SAS": sas,
                "SAS_std": sas_std,
                "SAS1Y": sas1y,
                "SAS1Y_std": sas1y_std,
                "BIS": bis,
                "BIS_std": bis_std,
                "BIS2": bis2,
                "BIS2_std": bis2_std,
            }
        )

        if os.path.exists(directory + "Analyse_summary.p"):
            summary = pd.read_pickle(directory + "Analyse_summary.p")
            merge = summary.merge(dico, on="filename", how="outer", indicator=True)
            update = pd.DataFrame({"filename": np.array(merge["filename"])})
            merge = merge.drop(columns="filename")
            summary = summary.drop(columns="filename")
            dico = dico.drop(columns="filename")

            all_keys = np.array(summary.keys())
            for j in all_keys:
                try:
                    merge.loc[merge["_merge"] != "left_only", j + "_x"] = merge.loc[
                        merge["_merge"] != "left_only", j + "_y"
                    ]
                    update[j] = merge[j + "_x"]
                except KeyError:
                    update[j] = merge[j]

            all_keys = np.setdiff1d(np.array(dico.keys()), np.array(summary.keys()))
            if len(all_keys) != 0:
                for j in all_keys:
                    update[j] = merge[j]
            dico = update

        dico = dico.sort_values(by="jdb").reset_index(drop=True)

        ras.save_pickle(directory + "/Analyse_summary.p", dico)
        dico.to_csv(directory + "/Analyse_summary.csv")

        print("\n [INFO] Summary table updated")

    # =============================================================================
    # EXTRACT BERV AT A SPECIFIC TIME
    # =============================================================================

    def yarara_get_berv_value(
        self, time_value, Draw=False, new=True, light_graphic=False, save_fig=True
    ):
        """Return the berv value for a given jdb date"""

        self.import_table()
        tab = self.table
        berv = myc.tableXY(tab["jdb"], tab["berv"], tab["berv"] * 0 + 0.3)
        berv.fit_sinus(guess=[30, 365.25, 0, 0, 0, 0], Draw=False)
        amp = berv.lmfit.params["amp"].value
        period = berv.lmfit.params["period"].value
        phase = berv.lmfit.params["phase"].value
        offset = berv.lmfit.params["c"].value

        berv_value = amp * np.sin(2 * np.pi * time_value / period + phase) + offset
        if Draw == True:
            t = np.linspace(0, period, 365)
            b = amp * np.sin(2 * np.pi * t / period + phase) + offset

            if new:
                plt.figure(figsize=(8.5, 7))
            plt.title(
                "BERV min : %.1f | BERV max : %.1f | BERV mean : %.1f"
                % (np.min(berv.y), np.max(berv.y), np.mean(berv.y)),
                fontsize=13,
            )
            berv.plot(modulo=period)
            plt.plot(t, b, color="k")
            if not light_graphic:
                plt.axvline(x=time_value % period, color="gray")
                plt.axhline(y=berv_value, color="gray", label="BERV = %.1f [km/s]" % (berv_value))
                plt.axhline(y=0, ls=":", color="k")
                plt.legend()
            plt.xlabel("Time %% %.2f" % (period), fontsize=16)
            plt.ylabel("BERV [km/s]", fontsize=16)
            if save_fig:
                plt.savefig(self.dir_root + "IMAGES/berv_values_summary.pdf")
        return berv_value

    def yarara_berv_summary(
        self, sub_dico="matching_diff", continuum="linear", dbin_berv=0.3, nb_plot=3
    ):
        """
        Produce a berv summary

        Parameters
        ----------

        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        telluric_tresh : Treshold used to cover the position of the contaminated wavelength
        wave_min : The minimum xlim axis
        wave_max : The maximum xlim axis

        """

        myf.print_box("\n---- RECIPE : PRODUCE BERV SUMMARY ----\n")

        self.import_table()
        self.import_material()
        tab = self.table

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("---- DICO %s used ----" % (sub_dico))

        all_flux = []
        all_conti = []
        snr = np.array(tab["snr"])
        for i, name in enumerate(np.array(tab["filename"])):
            file = pd.read_pickle(name)
            if not i:
                wavelength = file["wave"]
                self.wave = wavelength

            f = file["flux"]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]

            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)

            all_flux.append(f_norm)
            all_conti.append(file["matching_diff"]["continuum_" + continuum])

        all_conti = np.array(all_conti)
        all_flux = np.array(all_flux) * all_conti.copy()

        berv = np.array(tab["berv"])
        rv_shift = np.array(tab["rv_shift"])
        berv = berv - rv_shift

        berv_bin = np.arange(
            np.min(np.round(berv, 0)) - dbin_berv,
            np.max(np.round(berv, 0)) + dbin_berv + 0.01,
            dbin_berv,
        )
        mask_bin = (berv > berv_bin[0:-1][:, np.newaxis]) & (berv < berv_bin[1:][:, np.newaxis])
        sum_mask = mask_bin[np.sum(mask_bin, axis=1) != 0]
        if sum(np.sum(sum_mask, axis=1) != 0) < 5:  # need ast least 5 bins
            dbin_berv = np.round((np.max(berv) - np.min(berv)) / 15, 1)
            print("Bin size changed for %.1f km/s because not enough bins" % (dbin_berv))
            if not dbin_berv:
                dbin_berv = 0.05
            berv_bin = np.arange(
                np.min(np.round(berv, 0)) - dbin_berv,
                np.max(np.round(berv, 0)) + dbin_berv + 0.01,
                dbin_berv,
            )
            mask_bin = (berv > berv_bin[0:-1][:, np.newaxis]) & (
                berv < berv_bin[1:][:, np.newaxis]
            )

        plt.figure(figsize=(6 * nb_plot, 6))
        plt.subplot(1, nb_plot, 1)
        self.yarara_get_berv_value(0, Draw=True, new=False, save_fig=False, light_graphic=True)
        ax = plt.gca()
        for j in berv_bin:
            plt.axhline(y=j, color="k", alpha=0.2)
        plt.axhline(y=0, color="k", ls=":")

        berv_bin = berv_bin[:-1][np.sum(mask_bin, axis=1) != 0] + dbin_berv / 2
        mask_bin = mask_bin[np.sum(mask_bin, axis=1) != 0]
        print("[INFO] Nb bins full : %.0f" % (sum(np.sum(mask_bin, axis=1) != 0)))

        all_conti_ref = []
        snr_binned = []
        snr_stacked = []
        all_snr = []
        for j in range(len(mask_bin)):
            all_flux[j] = np.sum(all_flux[mask_bin[j]], axis=0)
            all_conti[j] = np.sum(all_conti[mask_bin[j]], axis=0)
            all_conti_ref.append(np.mean(all_conti[mask_bin[j]], axis=0))
            snr_binned.append(np.sqrt(np.mean((snr[mask_bin[j]]) ** 2)))
            snr_stacked.append(np.sqrt(np.sum((snr[mask_bin[j]]) ** 2)))
            all_snr.append(list(snr[mask_bin[j]]))
        all_flux = all_flux[0 : len(mask_bin)]
        all_conti = all_conti[0 : len(mask_bin)]
        all_conti_ref = np.array(all_conti_ref)
        snr_binned = np.array(snr_binned)

        if nb_plot == 3:
            plt.subplot(1, nb_plot, 2)
            plt.boxplot(
                all_snr,
                positions=berv_bin,
                widths=dbin_berv / 2,
                vert=False,
                labels=[len(all_snr[j]) for j in range(len(all_snr))],
            )
            plt.scatter(snr_binned, berv_bin, marker="x", color="r")
            plt.ylabel("Nb spectra", fontsize=16)

            # plt.tick_params(labelleft=False)
            plt.xlabel("SNR", fontsize=16)
            plt.ylim(ax.get_ylim())

        plt.subplot(1, nb_plot, nb_plot, sharey=ax)
        plt.axhline(y=0, color="k", ls=":")

        plt.plot(snr_stacked, berv_bin, "bo-", alpha=0.3)
        curve = myc.tableXY(snr_stacked, berv_bin)
        curve.myscatter(
            num=False, liste=[len(all_snr[j]) for j in range(len(all_snr))], color="k", factor=50
        )
        plt.xlabel("SNR stacked", fontsize=16)
        plt.ylabel("BERV [km/s]", fontsize=16)

        plt.subplots_adjust(left=0.07, right=0.97, top=0.93, bottom=0.10)

        plt.savefig(self.dir_root + "IMAGES/berv_statistic_summary.pdf")

    # =============================================================================
    # PREPROCESSING MASK
    # =============================================================================

    def yarara_prestacking(self, rv_bin=2):
        """
        Prestacking function to identifying suitable jdb value where the BERV is rather fixed

        Parameters
        ----------

        rv_bin : length of the binning for the plot

        """

        self.import_table()
        berv = np.array(self.table["berv"])
        snr = np.array(self.table["snr"])
        time = np.array(self.table["jdb"])

        snr = snr[berv.argsort()]
        time = time[berv.argsort()]
        berv = berv[berv.argsort()]

        all_berv = np.abs(berv)
        limits = np.arange(-np.max(all_berv), -np.min(all_berv), rv_bin)
        limits = np.hstack([limits, -limits[::-1]])

        bins = []
        for j in range(len(limits) - 1):
            sup = myf.find_nearest(berv, limits[j + 1])[0]
            inf = myf.find_nearest(berv, limits[j])[0]
            bins.append(np.sqrt(np.cumsum(snr**2))[sup] - np.sqrt(np.cumsum(snr**2))[inf])

        best = np.array(bins).argmax()
        print(
            "Best BERV interval around : %.1f and %.1f producing SNR : %.0f"
            % (limits[best], limits[best + 1], np.array(bins).max())
        )
        plt.subplot(1, 2, 1)
        plt.plot(limits[:-1] + rv_bin / 2, bins)
        plt.subplot(1, 2, 2)
        plt.plot(berv, np.sqrt(np.cumsum(snr**2)))

    def yarara_create_kernel(
        self,
        slope_vec=None,
        sub_dico="matching_activity",
        mask=None,
        wave_min=None,
        wave_max=None,
        save=False,
        ext="",
    ):
        self.import_material()
        load = self.material
        epsilon = 1e-6
        master = np.array(load["reference_spectrum"] * load["correction_factor"])
        wave = np.array(load["wave"])

        plt.figure(figsize=(15, 9))
        plt.subplot(3, 1, 1)
        snr_feature, slope_vec2 = self.yarara_snr_contam(sub_dico=sub_dico, slope_vec=slope_vec)
        plt.tick_params(labelbottom=False, top=True, direction="in", which="both", right=True)
        plt.xlabel("")

        weight = (
            1 / snr_feature**1
        )  # power_snr fixed to 1 and the power is taken in the integral now
        weight[np.isnan(weight)] = 0

        if slope_vec is None:
            diff = pd.read_pickle(self.dir_root + "CORRECTION_MAP/map_%s.p" % (sub_dico))[
                "correction_map"
            ]

            proxy = diff[:, weight.argmax()]

            proxy_rms = np.std(proxy)
            rslope = np.median(
                (diff - np.mean(diff, axis=0)) / ((proxy - np.mean(proxy))[:, np.newaxis]), axis=0
            )
            t = myc.table(diff)
            t.rms_w(diff * 0 + 1, axis=0)
            # rcorr = rslope*np.std(proxy)/np.std(diff,axis=0) #old version unweighted
            rcorr = (
                rslope * proxy_rms / (t.rms + epsilon)
            )  # need good weighting of the proxy and the flux

            sign_corr = np.sign(rslope)
            slope_vec = sign_corr * slope_vec2
        else:
            sign_corr = np.sign(slope_vec)

        if mask is None:
            mask = np.ones(len(master))

        kernel = weight * sign_corr * mask

        if wave_min is not None:
            i1 = myf.find_nearest(wave, wave_min)[0][0]
            kernel[0:i1] = 0

        if wave_max is not None:
            i1 = myf.find_nearest(wave, wave_max)[0][0]
            kernel[i1:] = 0

        mask_flux = master > 0.95
        kernel[mask_flux] = 0
        kernel /= np.nansum(abs(kernel))
        kernel *= 100

        ax = plt.gca()
        plt.subplot(3, 1, 2, sharex=ax)
        plt.plot(wave, kernel, color="k")
        plt.ylabel("Kernel weights [%]", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.tick_params(labelbottom=False, top=True, direction="in", which="both", right=True)
        plt.xlabel("")

        plt.subplot(3, 1, 3, sharex=ax)
        plt.plot(wave, abs(kernel), color="k")
        plt.ylabel("| Kernel weights | [%]", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.yscale("log")
        plt.ylim(1e-3, None)
        plt.tick_params(top=True, direction="in", which="both", right=True)

        plt.subplots_adjust(left=0.07, right=0.94, top=0.95, bottom=0.1, hspace=0)

        slope_vec[kernel == 0] = 0

        if save:
            root = self.dir_root.split("Yarara")[0]
            self.import_star_info()
            rv_sys = self.star_info["Rv_sys"]["fixed"]
            kernel = myc.tableXY(wave, kernel)
            kernel.rv_shift(-rv_sys)
            slope_vec = myc.tableXY(wave, slope_vec)
            slope_vec.rv_shift(-rv_sys)
            table = np.array([kernel.x, kernel.x, kernel.y, slope_vec.y]).T
            if ext == "":
                ext = " %s_%s" % (sub_dico, self.starname)
            np.savetxt(
                root + "Python/Material/Kernel_%s.txt" % (ext),
                table,
                "%.6e",
                header="Wave Wave Weight Strength",
            )
        else:
            return kernel

    def yarara_kernel_from_proxy(
        self,
        proxy_corr,
        r_min=0.4,
        sub_dico="matching_pca",
        substract_map=["stitching", "ghost_a", "ghost_b", "thar", "smooth"],
        wave_min=None,
        wave_max=None,
        power_snr=1,
        smooth_corr=1,
        time_min=None,
        time_max=None,
        save=True,
    ):
        self.import_table()

        self.yarara_time_variations(
            sub_dico=sub_dico,
            reference="master",
            substract_map=substract_map,
            Plot=False,
            proxy_corr=proxy_corr,
            proxy_detrending=0,
            smooth_corr=smooth_corr,
            time_min=None,
            time_max=None,
        )
        slope_vec = self.slope_corr * self.std_norm_corr
        slope_vec[abs(self.r_corr) < r_min] = 0
        self.yarara_create_kernel(
            slope_vec=slope_vec,
            sub_dico=sub_dico,
            mask=None,
            wave_min=wave_min,
            wave_max=wave_max,
            save=save,
            ext=proxy_corr + "_" + self.starname,
        )

        test, test_std = self.yarara_kernel_integral(
            sub_dico,
            "Kernel_" + proxy_corr + "_" + self.starname + ".txt",
            doppler_free=False,
            substract_map=substract_map,
            contam=True,
            power_snr=power_snr,
        )
        test = test.T[0]
        test_std = test_std.T[0]

        test = myc.tableXY(self.table.jdb, test, test_std)

        proxy = myc.tableXY(
            self.table.jdb, self.table[proxy_corr], self.table[proxy_corr + "_std"]
        )
        calib = myc.tableXY(test.y, proxy.y, test.yerr, proxy.yerr)
        calib.fit_line_weighted()

        magnetic_cycle = myc.tableXY(
            test.x,
            test.y * calib.lin_slope_w + calib.lin_intercept_w,
            test.yerr * calib.lin_slope_w,
        )
        ratio_std = 100 * np.median(magnetic_cycle.yerr) / np.median(proxy.yerr)

        plt.figure(figsize=(16, 6))
        plt.axes([0.06, 0.76, 0.91, 0.19])
        ax = plt.gca()
        proxy.plot(color="r", label="%s (%.2e)" % (proxy_corr, np.median(proxy.yerr)))
        magnetic_cycle.plot(
            label="Kernel %s (%.2e=%.0f%%)"
            % (proxy_corr, np.median(magnetic_cycle.yerr), ratio_std)
        )
        plt.legend()
        plt.ylabel("%s" % (proxy_corr), fontsize=14)
        plt.tick_params(direction="in", top=True, labelbottom=False)
        myf.plot_copy_time()

        scaling = 100 / (np.nanmax(proxy.y) - np.nanmin(proxy.y))
        plt.axes([0.06, 0.57, 0.91, 0.19], sharex=ax)
        diff = myc.tableXY(
            proxy.x,
            (proxy.y - magnetic_cycle.y) * scaling,
            np.sqrt(proxy.yerr**2 + magnetic_cycle.yerr**2) * scaling,
        )
        diff.plot(color="purple")
        plt.xlabel("Time jdb [days]", fontsize=14)
        plt.ylabel(r"$\Delta$ proxy [%]", fontsize=14)
        plt.tick_params(direction="in", top=True)

        plt.axes([0.06, 0.28, 0.91, 0.19])
        ax2 = plt.gca()
        proxy.substract_polyfit(5)
        proxy.detrend_poly.periodogram(color="r", Norm=True)
        magnetic_cycle.substract_polyfit(5)
        magnetic_cycle.detrend_poly.periodogram(color="k", Norm=True)
        plt.ylim(0, None)
        plt.tick_params(direction="inout", top=True, which="both", labelbottom=False)

        plt.axes([0.06, 0.09, 0.91, 0.19], sharey=ax2)
        # diff.periodogram(color='gray',Norm=True)

        diff.substract_polyfit(5)
        diff.detrend_poly.min_noise()
        diff.detrend_poly.periodogram(color="purple", Norm=True)

        plt.ylim(0, None)
        plt.tick_params(direction="inout", top=True, which="both")

    def yarara_kernel_integral(
        self,
        sub_dico,
        kernel,
        noise_kernel="unique",
        reference="master",
        p_noise=1 / np.inf,
        doppler_free=False,
        substract_map=[],
        add_map=[],
        wave_min=None,
        wave_max=None,
        contam=True,
        power_snr=1,
    ):
        self.import_table()
        self.import_material()
        self.import_ccf()
        wave = np.array(self.material.wave)
        master = np.array(self.material.reference_spectrum * self.material.correction_factor)
        if doppler_free:
            rv = self.import_ccf_timeseries("CCF_" + self.mask_harps, "matching_mad", "rv")
            doppler_free = rv.y
            doppler_free -= np.median(doppler_free)

        maps, maps_std, wave = self.yarara_map(
            sub_dico=sub_dico,
            wave_min=None,
            wave_max=None,
            Plot=False,
            p_noise=p_noise,
            reference=reference,
            rv_shift=doppler_free,
            substract_map=substract_map,
            add_map=add_map,
        )

        if type(kernel) == str:
            self.import_star_info()
            rv_sys = self.star_info["Rv_sys"]["fixed"]
            root = self.dir_root.split("Yarara")[0]
            tab = np.genfromtxt(root + "Python/Material/" + kernel)
            if noise_kernel == "fix":
                kernel_coeff = tab[:, 2]
            else:
                self.import_snr_curve()
                snr_master = self.table_snr["master_snr_curve"]  # *self.table_snr['ratio_factor']
                master = np.array(
                    self.material.reference_spectrum * self.material.correction_factor
                )
                master[master < 0] = 0
                noise = np.sqrt(master) / snr_master
                noise[np.isnan(noise)] = 0
                noise = myc.tableXY(np.array(self.material.wave), noise)
                noise.interpolate(new_grid=tab[:, 0], method="linear")
                noise.rv_shift(-rv_sys)
                noise.y[abs(noise.y) == 0] = np.inf
                kernel_coeff = tab[:, 3] / noise.y
                kernel_coeff[np.isnan(kernel_coeff)] = 0
                kernel_coeff[abs(kernel_coeff) == np.inf] = 0

            kernel = myc.tableXY(tab[:, 0], kernel_coeff)
            if wave_min is None:
                wave_min = np.min(tab[:, 0])
            if wave_max is None:
                wave_max = np.max(tab[:, 0])

            kernel.rv_shift(rv_sys)
            kernel.interpolate(new_grid=wave)
            kernel = kernel.y

        if wave_min is not None:
            print("\n [INFO] Wavelength cut below %.2f" % (wave_min))
            kernel[wave <= wave_min] = 0
        if wave_max is not None:
            print("\n [INFO] Wavelength cut above %.2f" % (wave_max))
            kernel[wave >= wave_max] = 0

        if not contam:
            mask = np.ones(len(wave))
            for j in ["stitching", "ghost_a", "ghost_b", "thar", "rejected", "borders_pxl"]:
                try:
                    mask = mask & (~self.material[j].astype("bool"))
                except:
                    pass
            kernel *= np.array(mask).astype("float")

        kernel[(abs(kernel) < 1e-4) | (master > 0.95)] = 0

        if type(power_snr) != np.ndarray:
            power_snr = np.array([power_snr])

        kernel = abs(kernel) ** (power_snr[:, np.newaxis]) * np.sign(kernel)
        kernel = kernel.T
        kernel /= np.nansum(abs(kernel), axis=0)
        kernel *= 100

        region_kept = (kernel[:, 0] != 0).astype("bool")
        maps = maps[:, region_kept]
        maps_std = maps_std[:, region_kept]
        kernel = kernel[region_kept]

        print(
            "\n [INFO] Percentage of the spectrum used = %.0f%%"
            % (sum(region_kept) * 100 / len(region_kept))
        )

        product = np.sum(kernel * maps[:, :, np.newaxis], axis=1)
        product_std = np.sqrt(np.sum((kernel * maps_std[:, :, np.newaxis]) ** 2, axis=1))

        normalisation = np.std(product, axis=0)

        product /= normalisation
        product_std /= normalisation

        product -= np.mean(product, axis=0)

        return product, product_std

    def yarara_kernel_caii(
        self,
        mask="CaII_HD128621",
        rvm_corr=None,
        p_noise=1 / np.inf,
        sub_dico=None,
        substract_map=None,
        wave_min=None,
        wave_max=None,
        contam=True,
        power_snr=None,
        noise_kernel="unique",
        doppler_free=False,
        save=True,
    ):
        self.import_table()
        self.import_dico_chain("matching_mad")
        self.import_proxies()
        directory = self.directory

        kernel = "Kernel_" + mask + ".txt"

        if sub_dico is None:
            loc = np.where(self.dico_chain == "matching_activity")[0][0]

        if substract_map is None:
            substract_map = [i.split("matching_")[1] for i in self.dico_chain[0:loc]]

        if rvm_corr is None:
            rvm_corr = np.array(self.table.ccf_rv)

        if power_snr is None:
            power_snr = np.arange(0.2, 3.01, 0.1)
        else:
            if type(power_snr) != np.ndarray:
                power_snr = np.array([power_snr])

        integral, integral_std = self.yarara_kernel_integral(
            self.dico_chain[loc + 1],
            kernel,
            doppler_free=doppler_free,
            substract_map=substract_map,
            p_noise=p_noise,
            wave_min=wave_min,
            wave_max=wave_max,
            contam=contam,
            power_snr=power_snr,
            noise_kernel=noise_kernel,
        )

        proxy = self.ca2.copy()
        proxy.znorm(recenter=True)

        def corr_plot(tab, tab_std, vec, label):
            stat = myc.table(np.array([tab.T, tab_std.T]))
            stat.fit_corr(vec)
            r_corr_proxy = abs(stat.rpearson)
            plt.plot(power_snr, r_corr_proxy, label=label)
            plt.ylim(-0.01, 1.01)
            plt.xlabel(r"$\alpha$ exponent", fontsize=14)
            plt.ylabel(r"$|\mathcal{R}_{pearson}|$", fontsize=14)

        plt.figure(figsize=(12, 14))
        plt.axes([0.1, 0.58, 0.37, 0.35])
        plt.title("Kernel integral")
        corr_plot(integral, integral_std, proxy.y, "CaII")
        corr_plot(integral, integral_std, rvm_corr, "RV")
        corr_plot(integral, integral_std, self.wb.y, "WB")
        corr_plot(integral, integral_std, self.ccf_harps_fwhm.y, "FWHM")
        corr_plot(integral, integral_std, self.ccf_harps_fwhm.y, "Contrast")
        plt.legend()
        plt.axes([0.53, 0.58, 0.37, 0.35])
        plt.title("Kernel integral - CaII")
        corr_plot(integral - proxy.y[:, np.newaxis], integral_std, proxy.y, "CaII")
        corr_plot(integral - proxy.y[:, np.newaxis], integral_std, rvm_corr, "RV")
        corr_plot(integral - proxy.y[:, np.newaxis], integral_std, self.wb.y, "WB")

        kernel_scaled = integral.copy()
        kernel_std_scaled = integral_std.copy()

        for j in range(len(power_snr)):
            test = myc.tableXY(
                self.table.jdb, kernel_scaled[:, j].copy(), kernel_std_scaled[:, j].copy()
            )
            test.znorm(recenter=True)
            test.y *= np.std(self.ca2.y)
            test.y += np.mean(self.ca2.y)
            test.yerr *= np.std(self.ca2.y)

            calib = myc.tableXY(test.y, self.ca2.y, test.yerr, self.ca2.yerr)
            calib.fit_line_weighted()
            calib.lin_slope_w = 1
            calib.lin_intercept_w = 0

            magnetic_cycle = myc.tableXY(
                test.x,
                test.y * calib.lin_slope_w + calib.lin_intercept_w,
                test.yerr * calib.lin_slope_w,
            )
            kernel_scaled[:, j] = magnetic_cycle.y
            kernel_std_scaled[:, j] = magnetic_cycle.yerr

        ratio_std = 100 * np.median(kernel_std_scaled, axis=0) / np.median(self.ca2.yerr)
        self.proxy_gain_std = myc.tableXY(power_snr, ratio_std)
        self.proxy_gain_std.null()

        scaling = 100 / (np.nanmax(self.ca2.y) - np.nanmin(self.ca2.y))
        stat = myc.table((kernel_scaled - self.ca2.y[:, np.newaxis]) * scaling)
        stat.table = stat.table.T
        stat.rms_w(
            1 / ((kernel_std_scaled**2 + self.ca2.yerr[:, np.newaxis] ** 2) * scaling**2).T
        )

        self.proxy_diff_std = myc.tableXY(power_snr, stat.rms)
        self.proxy_diff_std.null()

        plt.axes([0.1, 0.12, 0.37, 0.35])
        self.proxy_gain_std.plot(ls="-")
        plt.xlabel(r"$\alpha$ exponent", fontsize=14)
        plt.ylabel(r"$\sigma_{med}$(K)/$\sigma_{med}$(P) [%]", fontsize=14)
        plt.axhline(y=100, ls=":", color="k")

        plt.axes([0.53, 0.12, 0.37, 0.35])
        self.proxy_diff_std.plot(ls="-")
        plt.xlabel(r"$\alpha$ exponent", fontsize=14)
        plt.ylabel(r"std(K-P)[%]", fontsize=14)
        if len(power_snr) > 1:
            plt.savefig(
                self.dir_root
                + "IMAGES/Kernel_curve_CaII_%s.pdf" % (["n1", "n0"][noise_kernel == "fix"])
            )
        else:
            plt.close()

        loc = myf.find_nearest(power_snr, 1)[0][0]
        magnetic_cycle = myc.tableXY(test.x, kernel_scaled[:, loc], kernel_std_scaled[:, loc])

        plt.figure(figsize=(16, 6))
        plt.axes([0.06, 0.76, 0.91, 0.19])
        ax = plt.gca()

        self.ca2.plot(
            color="r", label=r"CaIIH&K ($\sigma_{med}$ = %.2e)" % (np.median(self.ca2.yerr))
        )
        magnetic_cycle.plot(
            label=r"Kernel CaIIH&K ($\sigma_{med}$ = %.2e = %.0f%%)"
            % (np.median(magnetic_cycle.yerr), ratio_std[loc])
        )
        plt.legend()
        plt.ylabel(r"CaIIH&K []", fontsize=14)
        plt.tick_params(direction="in", top=True, labelbottom=False)
        myf.plot_copy_time()

        plt.axes([0.06, 0.57, 0.91, 0.19], sharex=ax)
        diff = myc.tableXY(
            self.ca2.x,
            (self.ca2.y - magnetic_cycle.y) * scaling,
            np.sqrt(self.ca2.yerr**2 + magnetic_cycle.yerr**2) * scaling,
        )
        diff.rms_w()
        diff.plot(color="purple", label="std(P-K) = %.2f %%" % (diff.rms))
        plt.xlabel("Time jdb [days]", fontsize=14)
        plt.ylabel(r"$\Delta$proxy [%]", fontsize=14)
        plt.legend()
        plt.tick_params(direction="in", top=True)

        plt.axes([0.06, 0.28, 0.91, 0.19])
        ax2 = plt.gca()
        self.ca2.periodogram(color="r", Norm=True, detrending=5)
        magnetic_cycle.periodogram(color="k", Norm=True, detrending=5)
        ymax = 3 * self.ca2.fap
        plt.ylim(0, ymax)
        plt.tick_params(direction="inout", top=True, which="both", labelbottom=False)

        plt.axes([0.06, 0.09, 0.91, 0.19], sharey=ax2)
        # diff.periodogram(color='gray',Norm=True)

        diff.substract_polyfit(5)
        diff.detrend_poly.min_noise()
        diff.detrend_poly.periodogram(color="purple", Norm=True)

        plt.ylim(0, ymax)
        plt.tick_params(direction="inout", top=True, which="both")

        power_snr = power_snr[loc]
        plt.savefig(
            self.dir_root
            + "IMAGES/Kernel_CaII_%s_%.0f.pdf"
            % (["n1", "n0"][noise_kernel == "fix"], int(power_snr * 100))
        )

        self.ca2_kernel = magnetic_cycle

        if save:
            files = glob.glob(directory + "RASSI*.p")
            files = np.sort(files)
            for i, j in enumerate(files):
                file = pd.read_pickle(j)
                file["parameters"]["Kernel_CaII"] = magnetic_cycle.y[i]
                file["parameters"]["Kernel_CaII_std"] = magnetic_cycle.yerr[i]
                myf.pickle_dump(file, open(j, "wb"))

            self.yarara_analyse_summary()

    def yarara_kernel_wb(
        self,
        mask="WB_HD128621",
        p_noise=1 / np.inf,
        rvm_corr=None,
        sub_dico=None,
        substract_map=None,
        wave_min=None,
        wave_max=None,
        contam=True,
        power_snr=None,
        noise_kernel="unique",
        doppler_free=False,
        save=True,
    ):

        self.import_dico_chain("matching_mad")
        self.import_proxies()
        self.import_table()
        directory = self.directory

        kernel = "Kernel_" + mask + ".txt"

        if sub_dico is None:
            loc = np.where(self.dico_chain == "matching_mad")[0][0]
            print(" [INFO] Dico selected %s \n" % (self.dico_chain[loc + 1]))
            sub_dico = self.dico_chain[loc + 1]
            a = self.import_spectrum()
            if "continuum_linear" not in list(a[sub_dico].keys()):
                sub_dico = "matching_mad"

        print("---- DICO %s used ----\n" % (sub_dico))

        if substract_map is None:
            substract_map = []

        if rvm_corr is None:
            rvm_corr = np.array(self.table.ccf_rv)

        if power_snr is None:
            power_snr = np.arange(0.2, 3.01, 0.1)
        else:
            if type(power_snr) != np.ndarray:
                power_snr = np.array([power_snr])

        integral, integral_std = self.yarara_kernel_integral(
            sub_dico,
            kernel,
            doppler_free=doppler_free,
            substract_map=substract_map,
            p_noise=p_noise,
            wave_min=wave_min,
            wave_max=wave_max,
            contam=contam,
            power_snr=power_snr,
            noise_kernel=noise_kernel,
        )

        proxy = self.wb.copy()
        proxy.znorm(recenter=True)

        def corr_plot(tab, tab_std, vec, label):
            stat = myc.table(np.array([tab.T, tab_std.T]))
            stat.fit_corr(vec)
            r_corr_proxy = abs(stat.rpearson)
            plt.plot(power_snr, r_corr_proxy, label=label)
            plt.ylim(-0.01, 1.01)
            plt.xlabel(r"$\alpha$ exponent", fontsize=14)
            plt.ylabel(r"$|\mathcal{R}_{pearson}|$", fontsize=14)

        plt.figure(figsize=(12, 14))
        plt.axes([0.1, 0.58, 0.37, 0.35])
        plt.title("Kernel integral")
        corr_plot(integral, integral_std, proxy.y, "CaII")
        corr_plot(integral, integral_std, rvm_corr, "RV")
        corr_plot(integral, integral_std, self.wb.y, "WB")
        corr_plot(integral, integral_std, self.ccf_harps_fwhm.y, "FWHM")
        corr_plot(integral, integral_std, self.ccf_harps_fwhm.y, "Contrast")
        plt.legend()
        plt.axes([0.53, 0.58, 0.37, 0.35])
        plt.title("Kernel integral - WB")
        corr_plot(integral - proxy.y[:, np.newaxis], integral_std, proxy.y, "CaII")
        corr_plot(integral - proxy.y[:, np.newaxis], integral_std, rvm_corr, "RV")
        corr_plot(integral - proxy.y[:, np.newaxis], integral_std, self.wb.y, "WB")

        kernel_scaled = integral.copy()
        kernel_std_scaled = integral_std.copy()

        for j in range(len(power_snr)):
            test = myc.tableXY(
                self.table.jdb, kernel_scaled[:, j].copy(), kernel_std_scaled[:, j].copy()
            )
            test.znorm(recenter=True)
            test.y *= np.std(self.wb.y)
            test.y += np.mean(self.wb.y)
            test.yerr *= np.std(self.wb.y)

            calib = myc.tableXY(test.y, self.wb.y, test.yerr, self.wb.yerr)
            calib.fit_line_weighted()
            calib.lin_slope_w = 1
            calib.lin_intercept_w = 0

            magnetic_cycle = myc.tableXY(
                test.x,
                test.y * calib.lin_slope_w + calib.lin_intercept_w,
                test.yerr * calib.lin_slope_w,
            )
            kernel_scaled[:, j] = magnetic_cycle.y
            kernel_std_scaled[:, j] = magnetic_cycle.yerr

        ratio_std = 100 * np.median(kernel_std_scaled, axis=0) / np.median(self.wb.yerr)
        self.proxy_gain_std = myc.tableXY(power_snr, ratio_std)
        self.proxy_gain_std.null()

        scaling = 100 / (np.nanmax(self.wb.y) - np.nanmin(self.wb.y))
        stat = myc.table((kernel_scaled - self.wb.y[:, np.newaxis]) * scaling)
        stat.table = stat.table.T
        stat.rms_w(
            1 / ((kernel_std_scaled**2 + self.wb.yerr[:, np.newaxis] ** 2) * scaling**2).T
        )

        self.proxy_diff_std = myc.tableXY(power_snr, stat.rms)
        self.proxy_diff_std.null()

        plt.axes([0.1, 0.12, 0.37, 0.35])
        self.proxy_gain_std.plot(ls="-")
        plt.xlabel(r"$\alpha$ exponent", fontsize=14)
        plt.ylabel(r"$\sigma_{med}$(K)/$\sigma_{med}$(P) [%]", fontsize=14)
        plt.axhline(y=100, ls=":", color="k")

        plt.axes([0.53, 0.12, 0.37, 0.35])
        self.proxy_diff_std.plot(ls="-")
        plt.xlabel(r"$\alpha$ exponent", fontsize=14)
        plt.ylabel(r"std(K-P)[%]", fontsize=14)
        if len(power_snr) > 1:
            plt.savefig(
                self.dir_root
                + "IMAGES/Kernel_curve_WB_%s.pdf" % (["n1", "n0"][noise_kernel == "fix"])
            )
        else:
            plt.close()

        loc = myf.find_nearest(power_snr, 1)[0][0]
        magnetic_cycle = myc.tableXY(test.x, kernel_scaled[:, loc], kernel_std_scaled[:, loc])

        plt.figure(figsize=(16, 6))
        plt.axes([0.06, 0.76, 0.91, 0.19])
        ax = plt.gca()

        self.wb.plot(color="r", label=r"WB ($\sigma_{med}$ = %.2e)" % (np.median(self.wb.yerr)))
        magnetic_cycle.plot(
            label=r"Kernel WB ($\sigma_{med}$ = %.2e = %.0f%%)"
            % (np.median(magnetic_cycle.yerr), ratio_std[loc])
        )
        plt.legend()
        plt.ylabel(r"WB []", fontsize=14)
        plt.tick_params(direction="in", top=True, labelbottom=False)
        myf.plot_copy_time()

        plt.axes([0.06, 0.57, 0.91, 0.19], sharex=ax)
        diff = myc.tableXY(
            self.wb.x,
            (self.wb.y - magnetic_cycle.y) * scaling,
            np.sqrt(self.wb.yerr**2 + magnetic_cycle.yerr**2) * scaling,
        )
        diff.rms_w()
        diff.plot(color="purple", label="std(P-K) = %.2f %%" % (diff.rms))
        plt.xlabel("Time jdb [days]", fontsize=14)
        plt.ylabel(r"$\Delta$proxy [%]", fontsize=14)
        plt.legend()
        plt.tick_params(direction="in", top=True)

        plt.axes([0.06, 0.28, 0.91, 0.19])
        ax2 = plt.gca()
        self.wb.periodogram(color="r", Norm=True, detrending=5)
        magnetic_cycle.periodogram(color="k", Norm=True, detrending=5)
        ymax = 3 * self.wb.fap
        plt.ylim(0, ymax)
        plt.tick_params(direction="inout", top=True, which="both", labelbottom=False)

        plt.axes([0.06, 0.09, 0.91, 0.19], sharey=ax2, sharex=ax2)
        # diff.periodogram(color='gray',Norm=True)

        diff.substract_polyfit(5)
        diff.detrend_poly.min_noise()
        diff.detrend_poly.periodogram(color="purple", Norm=True)
        plt.xlim(None, 700)
        plt.ylim(0, ymax)
        plt.tick_params(direction="inout", top=True, which="both")

        power_snr = power_snr[loc]
        plt.savefig(
            self.dir_root
            + "IMAGES/Kernel_WB_%s_%.0f.pdf"
            % (["n1", "n0"][noise_kernel == "fix"], int(power_snr * 100))
        )

        self.wb_kernel = magnetic_cycle

        if save:
            files = glob.glob(directory + "RASSI*.p")
            files = np.sort(files)
            for i, j in enumerate(files):
                file = pd.read_pickle(j)
                file["parameters"]["Kernel_WB"] = magnetic_cycle.y[i]
                file["parameters"]["Kernel_WB_std"] = magnetic_cycle.yerr[i]
                myf.pickle_dump(file, open(j, "wb"))

            self.yarara_analyse_summary()

    def yarara_clean_proxies(self, algo="pca", nb_comp=3):
        self.import_proxies()
        table = myc.table(np.array(self.table_proxy2).T)
        table_std = np.array(self.table_proxy2_std).T

        zscore, phi, base_vec = table.dim_reduction(algo, nb_comp, 1 / table_std**2)

        base_vec -= np.nanmedian(base_vec, axis=0)
        base_vec /= np.nanstd(base_vec, axis=0)

        proxies = base_vec.T

        count = -1
        plt.figure(figsize=(16, 8))
        plt.subplot(len(proxies), 1, 1)
        ax = plt.gca()
        for j in proxies:
            plt.tick_params(top=True, labelbottom=False, direction="in", right=True)
            count += 1
            if count:
                plt.subplot(len(proxies), 1, count + 1, sharex=ax)
            plt.scatter(self.table.jdb, proxies[count], color="k")
        plt.tick_params(top=True, labelbottom=True, direction="in", right=True)

        plt.subplots_adjust(left=0.05, right=0.96, top=0.96, bottom=0.10, hspace=0)

        plt.savefig(self.dir_root + "IMAGES/all_proxies_cleaned.pdf")

        clean_proxies = pd.DataFrame(
            {"Proxies_pca%.0f" % (i + 1): proxies[i] for i in range(len(proxies))}
        )

        clean_proxies["jdb"] = self.table.jdb
        self.clean_proxies = clean_proxies

    def yarara_preprocess_mask(
        self,
        sub_dico_ref="matching_diff",
        sub_dico="matching_mad",
        continuum="linear",
        method="stack",
        jdb_min=None,
        jdb_max=None,
        berv_min=None,
        berv_max=None,
        plot=True,
        template_telluric=False,
        shift_spectrum=False,
        wave_min=None,
        wave_max=None,
        fixed_berv_span=0,
        rejected=None,
        transit_out=True,
        adjust_fwhm=False,
        kernel_smooth=None,
    ):
        """
        Plot all the RASSINE spectra in the same plot

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)

        """

        self.import_table()
        self.import_material()
        self.import_star_info()

        mask_files = None
        if transit_out:
            mask_files = np.array(self.table["transit_in"] == 0).astype("bool")

        if sub_dico is None:
            sub_dico = self.dico_actif

        all_flux = []
        for i, name in enumerate(np.array(self.table["filename"])):
            file = pd.read_pickle(name)
            f = file["flux"]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]

            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)

            all_flux.append(f_norm)

        all_flux = np.array(all_flux)
        if rejected is None:
            rejected = (np.nanstd(all_flux, axis=0) < 1e-5).astype("int")
            rejected = (rejected) | np.array(self.material.rejected).astype("int")
            rejected = rejected.astype("int")

        del all_flux

        span_berv = np.max(self.table.berv) - np.min(self.table.berv)
        if span_berv < 5:
            print(
                Fore.YELLOW
                + " [WARNING] The berv span is %.1f which is not sufficient to provide reliable master telluric"
                % (span_berv)
                + Fore.RESET
            )
            myf.make_sound("Warning")
            template_telluric = True

        if template_telluric:
            print(" [INFO] A Template telluric will be used")

        snr = np.argmax(np.array(self.table["snr"]))

        file_test = self.import_spectrum(num=snr)

        rv_sys = file_test["parameters"]["RV_sys"]

        if rv_sys is None:
            print(Fore.YELLOW + " [WARNING] No systemics velocity RV_sys found" + Fore.RESET)
            myf.make_sound("Warning")
        try:
            Teff = file_test["parameters"]["Teff_gray"]
        except KeyError:
            print(Fore.YELLOW + " [WARNING] No effective temperature found" + Fore.RESET)
            myf.make_sound("Warning")
            Teff = np.nan

        (
            wave,
            flux_telluric_free,
            flux_telluric_free_std,
            conti,
            b_mean,
            b_min,
            b_max,
        ) = self.yarara_stack(
            sub_dico_ref=sub_dico_ref,
            method=method,
            sub_dico=sub_dico,
            continuum=continuum,
            jdb_min=jdb_min,
            jdb_max=jdb_max,
            berv_min=berv_min,
            berv_max=berv_max,
            plot=False,
            shift_spectrum=shift_spectrum,
            mask_files=mask_files,
        )

        wave, flux_telluric, flux_telluric_std, conti, b_mean, b_min, b_max = self.yarara_stack(
            sub_dico_ref=sub_dico_ref,
            method=method,
            sub_dico=sub_dico_ref,
            continuum=continuum,
            jdb_min=jdb_min,
            jdb_max=jdb_max,
            berv_min=berv_min,
            berv_max=berv_max,
            plot=False,
            shift_spectrum=shift_spectrum,
            mask_files=mask_files,
        )

        telluric_width_kms = (
            np.min(pd.read_pickle(self.directory + "Analyse_summary.p")["telluric_fwhm"]) * 4
        )  # 1 sigma width
        telluric_width_ang = wave * telluric_width_kms / 3e5
        dwave_dindex = np.gradient(wave)
        telluric_width_idx = telluric_width_ang / dwave_dindex
        telluric_width_idx = telluric_width_idx.astype("int")

        telluric = (flux_telluric - flux_telluric_free) + 1
        telluric[wave < 5000] = 1  # do not consider telluric below 5000 Angstrom
        telluric[telluric > 1] = 1  # not emission telluric line
        spike_loc = np.arange(1, len(telluric) - 1)[
            (telluric[2:] == 1) & (telluric[1:-1] != 1) & (telluric[:-2] == 1)
        ]
        telluric[spike_loc] = 1

        model = self.telluric_model.copy()
        model.x = myf.doppler_r(model.x, b_mean * 1000)[0]
        model.interpolate(wave, interpolate_x=False)

        if template_telluric:
            telluric = model.y.copy()

        factor = np.array(self.material["correction_factor"])

        self.material["reference_spectrum"] = flux_telluric_free
        myf.pickle_dump(self.material, open(self.directory + "Analyse_material.p", "wb"))

        dico = {
            "wave": wave,
            "flux": flux_telluric_free,
            "correction_factor": factor,
            "flux_telluric": telluric,
            "flux_uncorrected": flux_telluric,
            "continuum": conti,
            "berv_mean": b_mean,
            "berv_min": [b_min, -fixed_berv_span][int(fixed_berv_span != 0)],
            "berv_max": [b_max, fixed_berv_span][int(fixed_berv_span != 0)],
            "rejected": rejected,
            "rv_sys": rv_sys,
            "t_eff": Teff,
        }

        if not os.path.exists(self.dir_root + "KITCAT"):
            os.system("mkdir " + self.dir_root + "KITCAT")
        myf.pickle_dump(dico, open(self.dir_root + "KITCAT/kitcat_spectrum.p", "wb"))
        print(" Kitcat spectrum for star %s created" % (self.starname))

        name = self.starname
        ins = self.dir_root.split("/")[-2]
        fwhm_ccf = self.star_info["FWHM"]["YARARA"]
        teff = Teff

        print("\n Run the following code :\n")

        command_line = "python KitCat.py -s %s -i %s -t %.0f" % (name, ins, teff)
        if wave_min is not None:
            command_line += " -b " + str(wave_min)
        if wave_max is not None:
            command_line += " -e " + str(wave_max)
        if adjust_fwhm is not None:
            command_line += " -F " + str(fwhm_ccf)
            command_line += " -v " + str(int(fwhm_ccf / 2 + 5))
        if kernel_smooth is not None:
            command_line += " -k " + str(kernel_smooth)
        print(" [CODE LINE] " + command_line + "\n")

        if plot:
            plt.figure()
            plt.plot(dico["wave"], dico["flux_uncorrected"], label="master spectrum uncorrected")
            plt.plot(dico["wave"], dico["flux"] * factor, label="master spectrum")
            plt.plot(dico["wave"], dico["flux_telluric"], color="k", label="telluric model")
            plt.legend()
            plt.xlabel(r"Wavelength $[\AA]$]", fontsize=14)
            plt.ylabel("Flux normalised", fontsize=14)
        return command_line

    # =============================================================================
    # YARARA alfev
    # =============================================================================

    def yarara_binned_rms_curve(self, table_xy, label=""):
        t, output, output_std, simu, simu_std = table_xy.binned_rms_curve(
            nb_db=100, num_sim=100, nb_time=100
        )

        plt.subplot(1, 2, 1)
        plt.plot(t, output)
        plt.fill_between(t, output - output_std, output + output_std, alpha=0.3)
        # plt.plot(t,output[0]/np.sqrt(t),color='k',ls=':')
        plt.xlabel("Window bins [days]", fontsize=14)
        plt.ylabel("RV rms [m/s]", fontsize=14)
        plt.xscale("log")
        plt.yscale("log")

        output_std = np.sqrt(output_std**2 + simu_std**2)
        output -= simu
        output -= output[0]

        plt.subplot(1, 2, 2)
        plt.plot(t, output, label=label)
        plt.fill_between(t, output - output_std, output + output_std, alpha=0.3)

        # plt.plot(np.arange(1,1+len(output)),output[0]/np.sqrt(np.arange(1,1+len(output))),ls=':',color='k')
        plt.xlabel("Window bins [days]", fontsize=14)
        plt.ylabel(r"$\Delta$ RV rms (white noise) [m/s]", fontsize=14)
        plt.xscale("log")
        plt.axhline(y=0, ls=":", color="k")

    # =============================================================================
    # STACKING
    # =============================================================================

    def yarara_stack(
        self,
        method="stack",
        sub_dico_ref="matching_diff",
        sub_dico="matching_mad",
        continuum="linear",
        jdb_min=None,
        jdb_max=None,
        index_min=None,
        index_max=None,
        mask_files=None,
        berv_kw="berv",
        telluric_restframe=False,
        berv_min=None,
        berv_max=None,
        plot=True,
        normalised=True,
        shift_spectrum=False,
    ):
        """
        Make a stacking or a median of the spectrum

        Parameters
        ----------
        method : stack or median along the wavelength axis
        sub_dico_ref : The sub_dictionnary used to  select the continuum
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        jdb_min : minimum jdb of the spectra to stack
        jdb_max : maximum jdb of the spectra to stack
        berv_kw : keyword which represent the berv value
        telluric_restframe : True/False move in the Earth restframe
        berv_min : minimum berv of the spectra to stack
        berv_max : maximum berv of the spectra to stack
        plot : True/False display the stack spectra

        """

        directory = self.directory
        kw = "_planet" * self.planet

        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        table = pd.read_pickle(directory + "Analyse_summary.p")

        if jdb_min is None:
            jdb_min = np.min(table["jdb"])
        if jdb_max is None:
            jdb_max = np.max(table["jdb"])

        if index_min is None:
            index_min = myf.find_nearest(np.array(table["jdb"]), jdb_min)[0][0]
        if index_max is None:
            index_max = myf.find_nearest(np.array(table["jdb"]), jdb_max)[0][0]

        if berv_min is None:
            berv_min = -40  # can never be higher than 30 kms
        if berv_max is None:
            berv_max = 40

        if mask_files is None:
            mask_berv = (table[berv_kw] >= berv_min) & (table[berv_kw] <= berv_max)
            mask_time = np.zeros(len(table)).astype("bool")
            mask_time[index_min : index_max + 1] = True
            mask_files = mask_berv & mask_time

        print(
            "\n [INFO] Number of files used for the stacking : %.0f over %.0f"
            % (sum(mask_files), len(mask_files))
        )

        files = np.array(table.loc[mask_files, "filename"])
        # jdb = np.array(table.loc[(table['jdb']>=jdb_min)&(table['jdb']<=jdb_max)&(table[berv_kw]>=berv_min)&(table[berv_kw]<=berv_max),'jdb'])
        snr = np.array(table.loc[mask_files, "snr"])
        berv = np.array(table.loc[mask_files, berv_kw])
        rv_shift = np.array(table.loc[mask_files, "rv_shift"])

        table = table.loc[mask_files]

        print("\n [INFO] Master spectra snr : %.0f" % (np.sqrt(sum(snr**2))))

        berv_mean = np.nansum(snr**2 * berv) / np.nansum(snr**2)

        berv = berv - berv_mean - rv_shift
        print("\n BERV mean : %.2f [km/s]" % (berv_mean))

        files = np.sort(files)
        flux = []
        flux_err = []
        conti = []
        wave = []

        for i, j in enumerate(files):
            f = pd.read_pickle(j)
            if not i:
                wave = f["wave"]
            flux_step = (
                f["flux" + kw]
                / f[sub_dico]["continuum_" + continuum]
                * f[sub_dico_ref]["continuum_" + continuum]
            )
            flux_step_err = (
                f["flux_err"]
                / f[sub_dico]["continuum_" + continuum]
                * f[sub_dico_ref]["continuum_" + continuum]
            )
            flux.append(flux_step)
            flux_err.append(flux_step_err)
            conti.append(f[sub_dico_ref]["continuum_" + continuum])

        flux = np.array(flux)
        flux_err = np.array(flux_err)
        conti = np.array(conti)

        if telluric_restframe:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, flux[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                flux[j] = test.y

                test = myc.tableXY(wave, conti[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                conti[j] = test.y

        if shift_spectrum:
            rv_star = np.array(table["ccf_rv"])
            rv_star[np.isnan(rv_star)] = np.nanmedian(rv_star)
            rv_star -= np.median(rv_star)
            i = -1
            for j in tqdm(rv_star):
                i += 1
                test = myc.tableXY(wave, flux[i], 0 * wave)
                test.x = myf.doppler_r(wave, j * 1000)[1]
                test.interpolate(new_grid=wave, method="linear", replace=True, interpolate_x=False)
                flux[i] = test.y.copy()

                test = myc.tableXY(wave, conti[i], 0 * wave)
                test.x = myf.doppler_r(wave, j * 1000)[1]
                test.interpolate(new_grid=wave, method="linear", replace=True, interpolate_x=False)
                conti[i] = test.y.copy()

        if method == "stack":
            flux = np.sum(flux, axis=0)
            conti = np.sum(conti, axis=0)
            flux_err = np.sqrt(np.sum(flux_err**2, axis=0))

            flux_norm = (normalised * flux / conti) + (1 - normalised) * flux
            flux_norm_err = (normalised * flux_err / conti) + (1 - normalised) * flux_err

        else:
            flux_norm = np.median(flux / conti, axis=0)
            flux_norm_err = np.median(flux_err / conti, axis=0)

        if plot:
            plt.plot(wave, flux_norm)

        flux_norm[flux_norm < 0] = 0
        flux_norm_err[flux_norm_err > flux_norm] = flux_norm[flux_norm_err > flux_norm]

        return (
            wave,
            flux_norm,
            flux_norm_err,
            conti,
            berv_mean,
            np.min(table[berv_kw] - table["rv_shift"]),
            np.max(table[berv_kw] - table["rv_shift"]),
        )

    # =============================================================================
    #  SUPRESS SUB DICO
    # =============================================================================

    def yarara_supress_ccf_mask(self, mask="kitcat"):
        os.system("rm " + self.dir_root + "CCF_MASK/*" + mask + "*")

    def yarara_supress_ccf(self, sub_dico, mask=None):
        """
        Supress a specific dictionnary in the pickle files

        Parameters
        ----------
        mask : Name of the mask in the table ccf
        sub_dico : Name of the field to supress in the main arborescence of the pickle file

        """
        self.import_ccf()
        table = self.table_ccf

        if mask is None:
            m = np.array(list(self.table_ccf.keys()))[1:]
        else:
            m = [mask]

        if type(sub_dico) is not list:
            sub_dico = [sub_dico]

        for sub_dicos in sub_dico:
            for masks in m:
                if sub_dicos in np.array(list(table[masks].keys())):
                    print(" [INFO] Dictionnary %s deleted from %s" % (sub_dicos, masks))
                    del table[masks][sub_dicos]

        myf.pickle_dump(table, open(self.directory + "/Analyse_ccf.p", "wb"))

    def yarara_recreate_dico(self, sub_dico, continuum="linear", force_creation=False):
        self.import_table()
        self.import_material()
        table = self.table
        load = self.material
        file_test = self.import_spectrum()

        try:
            self.import_dico_chain(sub_dico)
            chain = self.dico_chain[:-1]
        except:
            chain = []

        files = np.array(table["filename"])

        do_reduction = "continuum_" + continuum not in file_test[sub_dico]
        do_reduction = do_reduction | force_creation

        if (len(chain) != 0) & (do_reduction):
            print("\n [INFO] Recreating continuum for step : %s\n" % (sub_dico))

            maps = np.zeros((len(table["jdb"]), len(load["wave"])))
            for m in chain:
                maps += pd.read_pickle(self.dir_root + "CORRECTION_MAP/map_" + m + ".p")[
                    "correction_map"
                ]

            fluxes = []
            conti = []
            for i, name in enumerate(files):
                file = pd.read_pickle(name)
                f = file["flux"]
                c = file["matching_diff"]["continuum_" + continuum]
                fluxes.append(f)
                conti.append(c)
            fluxes = np.array(fluxes)
            conti = np.array(conti)

            new_conti = fluxes / (fluxes / conti - maps)
            new_conti[np.isnan(new_conti)] = conti[np.isnan(new_conti)]

            i = -1
            for j in tqdm(files):
                i += 1
                file = pd.read_pickle(j)
                file[sub_dico]["continuum_" + continuum] = new_conti[i]
                ras.save_pickle(j, file)
        else:
            print(
                "\n [INFO] No stage detected in the chain to build the continuum or continuum already exist\n"
            )

    def yarara_supress_dico(self, sub_dico, only_continuum=False):
        """
        Supress a specific dictionnary in the pickle files

        Parameters
        ----------

        sub_dico : Name of the field to supress in the main arborescence of the pickle file

        """

        directory = self.directory
        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        def try_supress(objects, kw, kw2=None):
            if kw2 is None:
                try:
                    del objects[kw]
                    return objects
                except:
                    return None
            else:
                try:
                    del objects[kw][kw2]
                    return objects
                except:
                    return None

        if only_continuum:
            for j in tqdm(files):
                file = pd.read_pickle(j)
                file = try_supress(file, sub_dico, "continuum_linear")
                if file is not None:
                    myf.pickle_dump(file, open(j, "wb"))
        else:
            for j in tqdm(files):
                file = pd.read_pickle(j)
                file = try_supress(file, sub_dico)
                if file is not None:
                    myf.pickle_dump(file, open(j, "wb"))

            try:
                self.import_ccf()
                table_ccf = self.table_ccf
            except:
                table_ccf = None

            try:
                self.import_lbl()
                table_lbl = self.lbl
            except:
                table_lbl = None

            if table_lbl is not None:
                table_lbl = try_supress(table_lbl, sub_dico)
                if table_lbl is not None:
                    myf.pickle_dump(
                        table_lbl, open(self.directory + "/Analyse_line_by_line.p", "wb")
                    )

            if table_ccf is not None:
                for mask in table_ccf:
                    output = try_supress(table_ccf[mask], sub_dico)
                    if output is not None:
                        table_ccf[mask] = output
                myf.pickle_dump(table_ccf, open(self.directory + "/Analyse_ccf.p", "wb"))

    # =============================================================================
    # YARARA NONE ZERO FLUX
    # =============================================================================

    def yarara_non_zero_flux(self, spectrum=None, min_value=None):
        file_test = self.import_spectrum()
        hole_left = file_test["parameters"]["hole_left"]
        hole_right = file_test["parameters"]["hole_right"]
        grid = file_test["wave"]
        mask = (grid < hole_left) | (grid > hole_right)

        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        if spectrum is None:
            for i, j in enumerate(files):
                file = pd.read_pickle(j)
                flux = file["flux"]
                zero = flux == 0
                if min_value is None:
                    min_value = np.min(flux[flux != 0])
                flux[mask & zero] = min_value
                myf.pickle_dump(file, open(j, "wb"))
        else:
            print("[INFO] Removing null values of the spectrum")
            zero = spectrum <= 0
            if min_value is None:
                min_value = np.min(spectrum[spectrum > 0])
            spectrum[mask & zero] = min_value
            return spectrum

    # =============================================================================
    # COMPUTE CONSTANT FLUX
    # =============================================================================

    def yarara_flag_outliers(self, num, wave_min=None, wave_max=None, m=1.5, analysis=False):

        self.import_material()

        if wave_min is None:
            wave_min_label = np.array(self.material.wave)[0]
        else:
            wave_min_label = wave_min

        if wave_max is None:
            wave_max_label = np.array(self.material.wave)[-1]
        else:
            wave_max_label = wave_max

        wave_min_label = int(wave_min_label)
        wave_max_label = int(wave_max_label)

        try:
            save = self.maps["%.0f-%.0f" % (wave_min_label, wave_max_label)]
        except:

            a, b, c = self.yarara_map(
                "matching_mad",
                reference="median",
                wave_min=wave_min,
                wave_max=wave_max,
                Plot=False,
            )
            iq = myf.IQ(a, axis=0)
            inf = np.nanpercentile(a, 25, axis=0) - m * iq
            sup = np.nanpercentile(a, 75, axis=0) + m * iq
            self.maps = {"%.0f-%.0f" % (wave_min_label, wave_max_label): [inf, sup, a, c]}
            save = self.maps["%.0f-%.0f" % (wave_min_label, wave_max_label)]

            if analysis:
                plt.figure(111, figsize=(18, 6))

                outliers = np.sum((a > sup) | (a < inf), axis=1)
                t = myc.tableXY(np.arange(len(outliers)), outliers)
                t.myscatter()

        plt.fill_between(save[3], save[0], save[1], color="k", alpha=0.3)

        curve = save[2][num].copy()
        plt.plot(save[3], curve, color="k", alpha=0.1)

        outliers = (save[2][num] > save[1]) | (save[2][num] < save[0])
        curve[~outliers] = np.nan
        plt.plot(save[3], curve, color=None)

        print("[REMINDER] Supress sts.maps as soon as you finished your analys")

    # =============================================================================
    # COMPUTE CONSTANT FLUX
    # =============================================================================

    def yarara_flux_constant(self):
        """
        Compute the flux constant which allow to preserve the bolometric integrated flux

        Parameters
        ----------

        """

        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        all_flux = []
        snr = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            all_flux.append(file["flux"])
            snr.append(file["parameters"]["SNR_5500"])

        all_flux = np.array(all_flux)
        snr = np.array(snr)

        constants = np.sum(all_flux, axis=1)
        constants = constants / constants[snr.argmax()]

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file["parameters"]["flux_balance"] = constants[i]
            myf.pickle_dump(file, open(j, "wb"))

        self.yarara_analyse_summary()

    # =============================================================================
    #  SECULAR ACCELERATION CORRECTION
    # =============================================================================

    def yarara_correct_secular_acc(self, update_rv=False):
        """
        Compute the secular drift and correction the fluxes to cancel the drift

        Parameters
        ----------

        update_rv : True/False to update the flux value in order to cancel the secular drift

        """

        directory = self.directory
        self.import_table()
        self.import_star_info()

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        dist = self.star_info["Dist_pc"]["fixed"]
        pma = self.star_info["Pma"]["fixed"]
        pmd = self.star_info["Pmd"]["fixed"]

        distance_m = dist * 3.08567758e16
        mu_radps = (
            np.sqrt(pma**2 + pmd**2) * 2 * np.pi / (360.0 * 1000.0 * 3600.0 * 86400.0 * 365.25)
        )
        acc_sec = distance_m * 86400.0 * mu_radps**2  # rv secular drift in m/s per days

        # file_random = self.import_spectrum()
        all_acc_sec = []
        all_rv_sec = []
        for j in tqdm(files):
            file = pd.read_pickle(j)
            # acc_sec = file_random['parameters']['acc_sec']
            all_acc_sec.append(acc_sec)
            file["parameters"]["RV_sec"] = file["parameters"]["jdb"] * acc_sec
            all_rv_sec.append(file["parameters"]["RV_sec"])
            ras.save_pickle(j, file)
        all_rv = np.array(all_rv_sec)

        plt.figure()
        plt.scatter(
            self.table["jdb"], (self.table["ccf_rv"] - np.median(self.table["ccf_rv"])) * 1000
        )
        plt.plot(self.table["jdb"], all_rv - np.median(all_rv), color="k")

        if update_rv:
            print("Modification of the files to cancel the secular acceleration")
            time.sleep(1)
            all_rv -= np.median(all_rv)
            i = -1
            for j in tqdm(files):
                i += 1
                file = pd.read_pickle(j)
                file["parameters"]["RV_sec"] = all_rv[i]
                try:
                    flux = file["flux_backup"]
                except KeyError:
                    flux = file["flux"]
                wave = file["wave"]
                file["flux_backup"] = flux.copy()
                if all_rv[i]:
                    flux_shifted = interp1d(
                        myf.doppler_r(wave, all_rv[i])[1],
                        flux,
                        kind="linear",
                        bounds_error=False,
                        fill_value="extrapolate",
                    )(wave)
                    file["flux"] = flux_shifted
                ras.save_pickle(j, file)

        self.yarara_analyse_summary()

    # =============================================================================
    #  LAMP OFFSET CORRECTION
    # =============================================================================

    def yarara_supress_doubtful_spectra(self, supress=False):
        file_test = self.import_spectrum()
        grid = file_test["wave"]
        maps, dust, dust = self.yarara_map(
            wave_min=np.min(grid), wave_max=np.max(grid), reference=None, Plot=False
        )

        nb_out = np.sum(maps < 0.05, axis=1).astype("float")
        m, dust = myf.rm_outliers(nb_out, m=5, kind="inter")
        m = np.where(~m)[0]

        if len(m):
            print("\n [INFO] %.0f Spectra are doubtful and will be deleted : " % (len(m)), m, "\n")
            m = m - np.arange(len(m))
            m = np.sort(m)
            for i in m:
                self.supress_time_spectra(num_min=i, num_max=i, supress=supress)

    def yarara_correct_lamp_offset(
        self, lamp_changed_dates=np.array([0, 52942.5, 54312.5, 56169.5, 57303.5, 58450.5, 100000])
    ):
        """
        Correct the day to day lamp offset jitter by fitting a discontinuted polynomial curve

        Parameters
        ----------

        lamp_changed_dates : borders of the jdb value to split the data in chunck of lamp
        """

        self.import_table()
        tab = self.table
        time = np.array(tab["jdb"])

        lamp_offset = myc.tableXY(time, np.array(tab["lamp_offset"]))
        lamp_offset.null()

        lamp_changed = lamp_changed_dates[
            np.where(lamp_changed_dates < time.min())[0][-1] : np.where(
                lamp_changed_dates > time.max()
            )[0][0]
            + 1
        ]

        plt.figure(figsize=(14, 7))
        plt.subplot(2, 1, 1)
        plt.xlabel("Time [days]", fontsize=14)
        plt.ylabel("Lamp  offset [m/s]", fontsize=14)

        lamp_offset_detrend = []
        all_values = []
        for j in range(len(lamp_changed) - 1):
            lamp_offset.clip(
                min=[lamp_changed[j], None], max=[lamp_changed[j + 1], None], replace=False
            )
            if len(lamp_offset.clipped.x) > 10:
                lamp_offset.clipped.plot()
                lamp_offset.clipped.substract_polyfit(3, replace=True, Draw=True)
                lamp_offset_detrend.append(lamp_offset.clipped.y)
                val1 = np.polyval(lamp_offset.clipped.poly_coefficient, lamp_changed[j])
                val2 = np.polyval(lamp_offset.clipped.poly_coefficient, lamp_changed[j + 1])
                all_values.append([val1, val2])
            else:
                if j != 0:
                    lamp_offset.clip(
                        min=[lamp_changed[j - 1], None],
                        max=[lamp_changed[j + 1], None],
                        replace=False,
                    )
                    lamp_offset.clipped.plot()
                    lamp_offset.clipped.fit_discontinuity(lamp_changed[j], Draw=True, degree=2)
                    lamp_offset.clipped.substract_polydisc(lamp_changed[j], degree=2, replace=True)
                    lamp_offset_detrend = lamp_offset_detrend[0:-1]
                    lamp_offset_detrend.append(lamp_offset.clipped.y)
                else:
                    lamp_offset.clip(
                        min=[lamp_changed[j], None], max=[lamp_changed[j + 2], None], replace=False
                    )
                    lamp_offset.clipped.plot()
                    lamp_offset.clipped.fit_discontinuity(lamp_changed[j], Draw=True, degree=2)
                    lamp_offset.clipped.substract_polydisc(lamp_changed[j], degree=2, replace=True)
                    lamp_offset_detrend = lamp_offset_detrend[0:-1]
                    lamp_offset_detrend.append(lamp_offset.clipped.y)

        lamp_offset_detrended = np.hstack(lamp_offset_detrend)
        lamp_offset_detrend = myc.tableXY(time, lamp_offset_detrended)
        lamp_offset_detrend.null()
        ax = plt.gca()

        rms = np.std(lamp_offset_detrend.y)
        p2p = lamp_offset_detrend.y.max() - lamp_offset_detrend.y.min()
        plt.subplot(2, 1, 2, sharex=ax)
        lamp_offset_detrend.plot(label="rms : %.2f \np2p : %.2f" % (rms, p2p))
        plt.axhline(y=0, color="r")
        plt.legend()
        plt.xlabel("Time [days]", fontsize=14)
        plt.ylabel("Residues [m/s]", fontsize=14)
        self.lamp_offset_detrend = lamp_offset_detrend

    # =============================================================================
    # PLOT ALL THE RASSINE NORMALISED SPECTRUM
    # =============================================================================

    def yarara_plot_all(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        wave_min=4400,
        wave_max=4500,
        plot_median=False,
        berv_keys="none",
        cmap="brg",
        color="CaII",
        relatif=False,
        new=True,
        substract_map=[],
        p_noise=1 / np.inf,
    ):
        """
        Plot all the RASSINE spectra in the same plot

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : The minimum xlim axis
        wave_max : The maximum xlim axis
        plot_median : True/False, Plot the median of the spectrum
        berv_keys : The berv keyword from RASSINE dictionnary to remove the berv from the spectra (berv in kms)
        planet : True/False to use the flux containing the injected planet or not

        """

        directory = self.directory
        planet = self.planet
        self.import_table()
        self.import_material()
        load = self.material

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)
        median_spec = []

        jet = plt.get_cmap(cmap)
        index = self.table[color]
        cNorm = mplcolors.Normalize(vmin=np.percentile(index, 16), vmax=np.percentile(index, 84))
        scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)

        maps = np.zeros((len(self.table["jdb"]), len(load["wave"])))
        for m in substract_map:
            maps += pd.read_pickle(self.dir_root + "CORRECTION_MAP/map_matching_" + m + ".p")[
                "correction_map"
            ]

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        noise_matrix, noise_values = self.yarara_poissonian_noise(
            noise_wanted=p_noise, wave_ref=None, flat_snr=True
        )

        if new:
            plt.figure()
        for i, j in enumerate(files):
            f = pd.read_pickle(j)
            try:
                berv = f["parameters"][berv_keys]
            except:
                berv = 0

            colorVal = scalarMap.to_rgba(index[i])

            begin = int(myf.find_nearest(f["wave"], wave_min)[0][0])
            end = int(myf.find_nearest(f["wave"], wave_max)[0][0])

            median = np.array(load["reference_spectrum"])[begin:end]

            flux = f["flux" + kw][begin:end]
            conti = f[sub_dico]["continuum_" + continuum][begin:end]

            plt.plot(
                myf.doppler_r(f["wave"][begin:end], berv * 1000)[1],
                (flux / conti)
                - relatif * median
                - maps[i, begin:end]
                + noise_matrix[i, begin:end],
                color=colorVal,
                alpha=0.4,
            )
            if plot_median:
                median_spec.append(
                    f["flux" + kw][begin:end] / f[sub_dico]["continuum_" + continuum][begin:end]
                )

        if plot_median:
            median_spec = np.array(median_spec)
            plt.plot(f["wave"][begin:end], np.median(median_spec, axis=0), color="k")

    def yarara_comp_all(
        self, sub_dico1="matching_cosmics", sub_dico2="matching_mad", analysis="h2o_1"
    ):
        if analysis == "h2o_1":
            wave_min = 5874.9
            wave_max = 5925.1
            color = "telluric_contrast"
            cmap = "brg"
        elif analysis == "h2o_2":
            wave_min = 6459.9
            wave_max = 6500.1
            color = "telluric_contrast"
            cmap = "brg"
        elif analysis == "h2o_3":
            wave_min = 7159.9
            wave_max = 7250.1
            color = "telluric_contrast"
            cmap = "brg"
        elif analysis == "o2_1":
            wave_min = 6274.9
            wave_max = 6310.1
            color = "telluric_contrast"
            cmap = "brg"
        elif analysis == "o2_2":
            wave_min = 6859.9
            wave_max = 7050.1
            color = "telluric_contrast"
            cmap = "brg"
        elif analysis == "o2_3":
            wave_min = 7584.9
            wave_max = 7760.1
            color = "telluric_contrast"
            cmap = "brg"
        elif analysis == "activity":
            wave_min = 4199.9
            wave_max = 4240.1
            color = "CaII"
            cmap = "brg"
        elif analysis == "ha":
            wave_min = 6549.9
            wave_max = 6580.1
            color = "telluric_contrast"
            cmap = "brg"

        wave = self.spectrum(norm=True, num=1).x
        if wave_min < np.min(wave):
            wave_min = np.min(wave)
        if wave_min > np.max(wave):
            wave_min = np.max(wave)
        if wave_max > np.max(wave):
            wave_max = np.max(wave)
        if wave_max < np.min(wave):
            wave_max = np.min(wave)

        if wave_min != wave_max:
            plt.figure(figsize=(18, 7))
            plt.subplot(2, 1, 1)
            plt.title("Before YARARA (%s)" % (sub_dico1), fontsize=16)
            self.yarara_plot_all(
                wave_min=wave_min,
                wave_max=wave_max,
                sub_dico=sub_dico1,
                color=color,
                new=False,
                cmap=cmap,
                plot_median=True,
            )
            ax = plt.gca()
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=16)
            plt.ylabel(r"Flux", fontsize=16)

            plt.subplot(2, 1, 2, sharex=ax, sharey=ax)
            plt.title("After YARARA (%s)" % (sub_dico2), fontsize=16)
            self.yarara_plot_all(
                wave_min=wave_min,
                wave_max=wave_max,
                sub_dico=sub_dico2,
                color=color,
                new=False,
                cmap=cmap,
                plot_median=True,
            )
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=16)
            plt.ylabel(r"Flux", fontsize=16)
            plt.xlim(wave_min, wave_max)
            plt.ylim(-0.01, 1.09)
            plt.subplots_adjust(left=0.07, right=0.97, top=0.95, bottom=0.10, hspace=0.40)

            plt.savefig(self.dir_root + "IMAGES/Correction_1d_%s.png" % (analysis))

    # =============================================================================
    #     CREATE MEDIAN SPECTRUM TELLURIC SUPRESSED
    # =============================================================================

    def yarara_median_master_backup(
        self,
        sub_dico="matching_diff",
        method="mean",
        continuum="linear",
        supress_telluric=True,
        shift_spectrum=False,
        telluric_tresh=0.001,
        wave_min=5750,
        wave_max=5900,
        jdb_range=[-100000, 100000, 1],
        mask_percentile=[None, 50],
        save=True,
    ):
        """
        Produce a median master by masking region of the spectrum

        Parameters
        ----------

        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        telluric_tresh : Treshold used to cover the position of the contaminated wavelength
        wave_min : The minimum xlim axis
        wave_max : The maximum xlim axis

        """

        mask_percentile = [None, 50]

        myf.print_box("\n---- RECIPE : PRODUCE MASTER MEDIAN SPECTRUM ----\n")

        self.import_table()
        self.import_material()
        load = self.material
        epsilon = 1e-6

        planet = self.planet

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----" % (sub_dico))

        if self.table["telluric_fwhm"][0] is None:
            fwhm = np.array([3.0] * len(self.table.jdb))
        else:
            try:
                fwhm = np.array(self.table["telluric_fwhm"])
            except:
                fwhm = np.array([3.0] * len(self.table.jdb))

        fwhm_max = [5, 8][self.instrument[:-2] == "CORALIE"]
        fwhm_min = [2, 3][self.instrument[:-2] == "CORALIE"]
        fwhm_default = [3, 5][self.instrument[:-2] == "CORALIE"]
        if np.percentile(fwhm, 95) > fwhm_max:
            print(
                Fore.YELLOW
                + "\n [WARNING] FWHM of tellurics larger than %.0f km/s (%.1f), reduced to default value of %.0f km/s"
                % (fwhm_max, np.percentile(fwhm, 95), fwhm_default)
                + Fore.RESET
            )
            myf.make_sound("warning")
            fwhm = np.array([fwhm_default] * len(self.table.jdb))
        if np.percentile(fwhm, 95) < fwhm_min:
            print(
                Fore.YELLOW
                + "\n [WARNING] FWHM of tellurics smaller than %.0f km/s (%.1f), increased to default value of %.0f km/s"
                % (fwhm_min, np.percentile(fwhm, 95), fwhm_default)
                + Fore.RESET
            )
            myf.make_sound("warning")
            fwhm = np.array([fwhm_default] * len(self.table.jdb))

        print("\n [INFO] FWHM of tellurics : %.1f km/s" % (np.percentile(fwhm, 95)))

        all_flux = []
        all_conti = []
        for i, name in enumerate(np.array(self.table["filename"])):
            file = pd.read_pickle(name)
            self.debug = name
            if not i:
                wavelength = file["wave"]
                self.wave = wavelength

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]

            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)

            all_flux.append(f)
            all_conti.append(c)

        all_flux = np.array(all_flux)
        all_conti = np.array(all_conti)
        all_flux_norm = all_flux / all_conti

        mask = np.ones(len(self.table.jdb)).astype("bool")
        if jdb_range[2]:
            mask = (np.array(self.table.jdb) > jdb_range[0]) & (
                np.array(self.table.jdb) < jdb_range[1]
            )
        else:
            mask = (np.array(self.table.jdb) < jdb_range[0]) | (
                np.array(self.table.jdb) > jdb_range[1]
            )

        if sum(mask) < 40:
            print(
                Fore.YELLOW
                + "\n [WARNING] Not enough spectra %s the specified temporal range"
                % (["inside", "outside"][jdb_range[2] == 0])
                + Fore.RESET
            )
            mask = np.ones(len(self.table.jdb)).astype("bool")
        else:
            print(
                "\n [INFO] %.0f spectra %s the specified temporal range can be used for the median\n"
                % (sum(mask), ["inside", "outside"][jdb_range[2] == 0])
            )

        all_flux = all_flux[mask]
        all_conti = all_conti[mask]
        all_flux_norm = all_flux_norm[mask]

        berv = np.array(self.table["berv" + kw])[mask]
        rv_shift = np.array(self.table["rv_shift"])[mask]
        berv = berv - rv_shift

        model = pd.read_pickle(root + "/Python/Material/model_telluric.p")
        grid = model["wave"]
        spectre = model["flux_norm"]
        telluric = myc.tableXY(grid, spectre)
        telluric.find_min()

        all_min = np.array([telluric.x_min, telluric.y_min, telluric.index_min]).T
        all_min = all_min[1 - all_min[:, 1] > telluric_tresh]
        all_width = np.round(
            all_min[:, 0] * fwhm[:, np.newaxis] / 3e5 / np.median(np.diff(wavelength)), 0
        )
        all_width = np.nanpercentile(all_width, 95, axis=0)

        if supress_telluric:
            borders = np.array([all_min[:, 2] - all_width, all_min[:, 2] + all_width]).T
            telluric_mask = myf.flat_clustering(len(grid), borders) != 0
            all_mask2 = []
            for j in tqdm(berv):
                mask = myc.tableXY(myf.doppler_r(grid, j * 1000)[0], telluric_mask, 0 * grid)
                mask.interpolate(new_grid=wavelength, method="linear", interpolate_x=False)
                all_mask2.append(mask.y != 0)
            all_mask2 = np.array(all_mask2).astype("float")
        else:
            all_mask2 = np.zeros(np.shape(all_flux_norm))

        #        i=-1
        #        for j in tqdm(berv):
        #            i+=1
        #            borders = np.array([all_min[:,2]-all_width[i],all_min[:,2]+all_width[i]]).T
        #            telluric_mask = myf.flat_clustering(len(grid),borders)!=0
        #            mask = myc.tableXY(myf.doppler_r(grid,j*1000)[0],telluric_mask)
        #            mask.interpolate(new_grid=wavelength,method='linear')
        #            all_mask2.append(mask.y!=0)
        #         all_mask2 = np.array(all_mask2).astype('float')

        if supress_telluric:
            telluric_mask = telluric.y < (1 - telluric_tresh)
            all_mask = []
            for j in tqdm(berv):
                mask = myc.tableXY(myf.doppler_r(grid, j * 1000)[0], telluric_mask, 0 * grid)
                mask.interpolate(new_grid=wavelength, method="linear", interpolate_x=False)
                all_mask.append(mask.y != 0)
            all_mask = np.array(all_mask).astype("float")
        else:
            all_mask = np.zeros(np.shape(all_flux_norm))

        if shift_spectrum:
            rv_star = np.array(self.table["ccf_rv"])
            rv_star[np.isnan(rv_star)] = np.nanmedian(rv_star)
            rv_star -= np.median(rv_star)
            i = -1
            if method == "median":
                for j in tqdm(rv_star):
                    i += 1
                    mask = myc.tableXY(
                        myf.doppler_r(wavelength, j * 1000)[1], all_flux_norm[i], 0 * wavelength
                    )
                    mask.interpolate(new_grid=wavelength, method="linear", interpolate_x=False)
                    all_flux_norm[i] = mask.y.copy()
            else:
                # print(len(rv))
                # print(np.shape(all_flux))
                for j in tqdm(rv_star):
                    i += 1
                    mask = myc.tableXY(
                        myf.doppler_r(wavelength, j * 1000)[1], all_flux[i], 0 * wavelength
                    )
                    mask.interpolate(new_grid=wavelength, method="linear", interpolate_x=False)
                    all_flux[i] = mask.y.copy()
                    mask = myc.tableXY(
                        myf.doppler_r(wavelength, j * 1000)[1], all_conti[i], 0 * wavelength
                    )
                    mask.interpolate(new_grid=wavelength, method="linear", interpolate_x=False)
                    all_conti[i] = mask.y.copy()

        # plt.plot(wavelength,np.product(all_mask,axis=0))
        # plt.plot(wavelength,np.product(all_mask2,axis=0))
        print(
            "Percent always contaminated metric1 : %.3f %%"
            % (np.sum(np.product(all_mask, axis=0)) / len(wavelength) * 100)
        )
        print(
            "Percent always contaminated metric2 : %.3f %%"
            % (np.sum(np.product(all_mask2, axis=0)) / len(wavelength) * 100)
        )

        all_mask_nan1 = 1 - all_mask
        all_mask_nan1[all_mask_nan1 == 0] = np.nan

        all_mask_nan2 = 1 - all_mask2
        all_mask_nan2[all_mask_nan2 == 0] = np.nan

        print(mask_percentile)
        if mask_percentile[0] is None:
            mask_percentile[0] = np.ones(len(wavelength)).astype("bool")

        print(
            np.shape(wavelength),
            np.shape(all_flux_norm),
            np.shape(mask_percentile[0]),
            np.shape(mask_percentile[1]),
        )

        med = np.zeros(len(wavelength))
        med[mask_percentile[0]] = np.nanpercentile(
            all_flux_norm[:, mask_percentile[0]], mask_percentile[1], axis=0
        )
        med[~mask_percentile[0]] = np.nanpercentile(
            all_flux_norm[:, ~mask_percentile[0]], 50, axis=0
        )

        del mask_percentile

        # med1 = np.nanmedian(all_flux_norm*all_mask_nan1,axis=0)
        # med2 = np.nanmedian(all_flux_norm*all_mask_nan2,axis=0)

        mean = np.nansum(all_flux, axis=0) / np.nansum(all_conti, axis=0)
        mean1 = np.nansum(all_flux * all_mask_nan1, axis=0) / (
            np.nansum(all_conti * all_mask_nan1, axis=0) + epsilon
        )
        mean2 = np.nansum(all_flux * all_mask_nan2, axis=0) / (
            np.nansum(all_conti * all_mask_nan2, axis=0) + epsilon
        )
        mean2[mean2 == 0] = np.nan
        mean1[mean1 == 0] = np.nan
        mean1[mean1 != mean1] = mean2[mean1 != mean1]
        mean1[mean1 != mean1] = med[mean1 != mean1]
        mean2[mean2 != mean2] = mean1[mean2 != mean2]
        # med1[med1!=med1] = mean1[med1!=med1]
        # med2[med2!=med2] = mean1[med2!=med2]
        all_flux_diff_med = all_flux_norm - med
        tresh = 1.5 * myf.IQ(np.ravel(all_flux_diff_med)) + np.nanpercentile(all_flux_diff_med, 75)

        mean1[mean1 > (1 + tresh)] = 1

        if method != "median":
            self.reference = (wavelength, mean1)
        else:
            self.reference = (wavelength, med)

        plt.figure(figsize=(16, 8))
        plt.plot(wavelength, med, color="b", ls="-", label="median")
        plt.plot(wavelength, mean1, color="g", ls="-", label="mean1")
        plt.plot(wavelength, mean2, color="r", ls="-", label="mean2")
        plt.legend(loc=2)
        # plt.plot(wavelength,med,color='b',ls='-.')
        # plt.plot(wavelength,med1,color='g',ls='-.')
        # plt.plot(wavelength,med2,color='r',ls='-.')

        all_flux_diff_mean = all_flux_norm - mean
        all_flux_diff_med = all_flux_norm - med
        all_flux_diff1_mean = all_flux_norm - mean1
        # all_flux_diff1_med = all_flux_norm - med1
        # all_flux_diff2_mean = all_flux_norm - mean2
        # all_flux_diff2_med = all_flux_norm - med2

        idx_min = int(myf.find_nearest(wavelength, wave_min)[0])
        idx_max = int(myf.find_nearest(wavelength, wave_max)[0])

        plt.figure(figsize=(16, 8))
        plt.subplot(2, 1, 2)
        plt.imshow(
            all_flux_diff_mean[::-1, idx_min:idx_max],
            aspect="auto",
            vmin=-0.005,
            vmax=0.005,
            cmap="plasma",
        )
        plt.imshow(all_mask2[::-1, idx_min:idx_max], aspect="auto", alpha=0.2, cmap="Reds")
        ax = plt.gca()
        plt.subplot(2, 1, 1, sharex=ax, sharey=ax)
        plt.imshow(
            all_flux_diff_mean[::-1, idx_min:idx_max],
            aspect="auto",
            vmin=-0.005,
            vmax=0.005,
            cmap="plasma",
        )
        plt.imshow(all_mask[::-1, idx_min:idx_max], aspect="auto", alpha=0.2, cmap="Reds")

        if len(berv) > 15:
            plt.figure(figsize=(16, 8))
            plt.subplot(2, 1, 1)

            plt.title("Median")
            myf.my_colormesh(
                wavelength[idx_min : idx_max + 1],
                np.arange(len(all_mask)),
                all_flux_diff_med[:, idx_min : idx_max + 1],
                vmin=-0.005,
                vmax=0.005,
                cmap="plasma",
            )
            ax = plt.gca()

            plt.subplot(2, 1, 2, sharex=ax, sharey=ax)
            plt.title("Masked mean")
            myf.my_colormesh(
                wavelength[idx_min : idx_max + 1],
                np.arange(len(all_mask)),
                all_flux_diff1_mean[:, idx_min : idx_max + 1],
                vmin=-0.005,
                vmax=0.005,
                cmap="plasma",
            )

        load["wave"] = wavelength
        if method != "median":
            load["reference_spectrum"] = mean1
        else:
            load["reference_spectrum"] = med - np.median(all_flux_diff_med)

        ref = np.array(load["reference_spectrum"])
        ref = self.yarara_non_zero_flux(spectrum=ref, min_value=None)
        load["reference_spectrum"] = ref

        load.loc[load["reference_spectrum"] < 0, "reference_spectrum"] = 0

        if save:
            myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))
        else:
            return np.array(load["reference_spectrum"])

    def yarara_median_master(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        method="max",
        smooth_box=7,
        supress_telluric=True,
        shift_spectrum=False,
        wave_min=5750,
        wave_max=5900,
        bin_berv=10,
        bin_snr=None,
        telluric_tresh=0.001,
        jdb_range=[-100000, 100000, 1],
        mask_percentile=[None, 50],
        save=True,
    ):
        """
        Produce a median master by masking region of the spectrum

        Parameters
        ----------

        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        telluric_tresh : Treshold used to cover the position of the contaminated wavelength
        wave_min : The minimum xlim axis
        wave_max : The maximum xlim axis

        """
        if method:
            self.yarara_median_master_backup(
                sub_dico=sub_dico,
                method=method,
                continuum=continuum,
                telluric_tresh=telluric_tresh,
                wave_min=wave_min,
                wave_max=wave_max,
                jdb_range=jdb_range,
                supress_telluric=supress_telluric,
                shift_spectrum=shift_spectrum,
                mask_percentile=mask_percentile,
                save=save,
            )

        if method == "max":
            myf.print_box("\n---- RECIPE : PRODUCE MASTER MAX SPECTRUM ----\n")

            self.import_table()
            self.import_material()
            load = self.material
            tab = self.table
            planet = self.planet

            epsilon = 1e-12

            kw = "_planet" * planet
            if kw != "":
                print("\n---- PLANET ACTIVATED ----")

            if sub_dico is None:
                sub_dico = self.dico_actif
            print("---- DICO %s used ----" % (sub_dico))

            all_flux = []
            all_conti = []
            snr = np.array(tab["snr"])
            for i, name in enumerate(np.array(tab["filename"])):
                file = pd.read_pickle(name)
                if not i:
                    wavelength = file["wave"]
                    self.wave = wavelength

                f = file["flux" + kw]
                f_std = file["flux_err"]
                c = file[sub_dico]["continuum_" + continuum]
                c_std = file["continuum_err"]

                f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)

                all_flux.append(f_norm)
                all_conti.append(file["matching_diff"]["continuum_" + continuum])

            all_conti = np.array(all_conti)
            all_flux = np.array(all_flux) * all_conti.copy()
            all_flux_backup = all_flux / all_conti
            med = np.nanmedian(all_flux_backup, axis=0)

            berv = np.array(tab["berv"])
            rv_shift = np.array(tab["rv_shift"])
            berv = berv - rv_shift

            sort = np.argsort(berv)
            snr = np.array(tab["snr"])
            snr_sorted = snr[sort]

            if bin_snr is not None:
                val = int(np.sum(snr_sorted**2) // (bin_snr**2))
                if val > 4:
                    bin_berv = val
                else:
                    print(
                        "The expected SNR cannot be reached since the total SNR is about : %.0f"
                        % (np.sqrt(np.sum(snr_sorted**2)))
                    )
                    print(
                        "The maximum value allowed : %.0f" % (np.sqrt(np.sum(snr_sorted**2) / 5))
                    )
                    bin_snr = np.sqrt(np.sum(snr_sorted**2) / 5) - 50
                    bin_berv = int(np.sum(snr_sorted**2) // (bin_snr**2))

            # plt.plot(np.sqrt(np.cumsum(snr_sorted**2)))
            snr_lim = np.linspace(0, np.sum(snr_sorted**2), bin_berv + 1)
            berv_bin = berv[sort][myf.find_nearest(np.cumsum(snr_sorted**2), snr_lim)[0]]
            berv_bin[0] -= 1  # to ensure the first point to be selected

            mask_bin = (berv > berv_bin[0:-1][:, np.newaxis]) & (
                berv <= berv_bin[1:][:, np.newaxis]
            )
            berv_bin = (
                berv_bin[:-1][np.sum(mask_bin, axis=1) != 0]
                + np.diff(berv_bin)[np.sum(mask_bin, axis=1) != 0] / 2
            )
            mask_bin = mask_bin[np.sum(mask_bin, axis=1) != 0]

            snr_stacked = []
            all_flux_norm = []
            all_snr = []
            for j in range(len(mask_bin)):
                all_flux_norm.append(
                    np.sum(all_flux[mask_bin[j]], axis=0)
                    / (np.sum(all_conti[mask_bin[j]], axis=0) + epsilon)
                )
                snr_stacked.append(np.sqrt(np.sum((snr[mask_bin[j]]) ** 2)))
                all_snr.append(snr[mask_bin[j]])
            all_flux_norm = np.array(all_flux_norm)

            plt.figure(figsize=(12, 6))
            plt.subplot(1, 2, 1)
            self.yarara_get_berv_value(0, Draw=True, new=False, save_fig=False, light_graphic=True)
            ax = plt.gca()
            for j in berv_bin:
                plt.axhline(y=j, color="k", alpha=0.2)
            plt.axhline(y=0, color="k", ls=":")

            plt.subplot(1, 2, 2, sharey=ax)
            plt.axhline(y=0, color="k", ls=":")

            plt.plot(snr_stacked, berv_bin, "bo-", alpha=0.3)
            curve = myc.tableXY(snr_stacked, berv_bin)
            curve.myscatter(
                num=False,
                liste=[len(all_snr[j]) for j in range(len(all_snr))],
                color="k",
                factor=50,
            )
            plt.xlabel("SNR stacked", fontsize=13)
            plt.ylabel("BERV [km/s]", fontsize=13)

            print("SNR of binned spetcra around %.0f" % (np.mean(snr_stacked)))

            for j in range(len(all_flux_norm)):
                all_flux_norm[j] = myf.smooth(all_flux_norm[j], shape="savgol", box_pts=smooth_box)

            mean1 = np.max(all_flux_norm, axis=0)

            self.reference_max = (wavelength, mean1)

            mean1 -= np.median(mean1 - self.reference[1])

            all_flux_diff_mean = all_flux_backup - mean1
            all_flux_diff_med = all_flux_backup - med
            all_flux_diff1_mean = all_flux_backup - self.reference[1]

            idx_min = int(myf.find_nearest(wavelength, wave_min)[0])
            idx_max = int(myf.find_nearest(wavelength, wave_max)[0])

            plt.figure(figsize=(16, 8))
            plt.subplot(3, 1, 1)

            plt.title("Median")
            myf.my_colormesh(
                wavelength[idx_min : idx_max + 1],
                np.arange(len(berv)),
                all_flux_diff_med[sort][:, idx_min : idx_max + 1],
                vmin=-0.005,
                vmax=0.005,
                cmap="plasma",
            )
            ax = plt.gca()

            plt.subplot(3, 1, 2, sharex=ax, sharey=ax)
            plt.title("Max")
            myf.my_colormesh(
                wavelength[idx_min : idx_max + 1],
                np.arange(len(berv)),
                all_flux_diff_mean[sort][:, idx_min : idx_max + 1],
                vmin=-0.005,
                vmax=0.005,
                cmap="plasma",
            )

            plt.subplot(3, 1, 3, sharex=ax, sharey=ax)
            plt.title("Masked weighted mean")
            myf.my_colormesh(
                wavelength[idx_min : idx_max + 1],
                np.arange(len(berv)),
                all_flux_diff1_mean[sort][:, idx_min : idx_max + 1],
                vmin=-0.005,
                vmax=0.005,
                cmap="plasma",
            )

            load["wave"] = wavelength
            load["reference_spectrum"] = mean1
            myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

    def yarara_color_template(self, sub_dico="matching_anchors", continuum="linear"):
        """
        Define the color template used in the weighting of the lines for the CCF.
        The color is defined as the matching_anchors continuum of the best SNR spectra.
        The product is saved in the Material file.

        Parameters
        ----------

        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)

        """

        self.import_table()
        self.import_material()
        load = self.material
        tab = self.table
        snr = np.array(tab["snr"]).argmax()

        file_ref = self.import_spectrum(num=snr)
        wave = file_ref["wave"]
        continuum_ref = file_ref[sub_dico]["continuum_" + continuum]

        load["wave"] = wave
        load["color_template"] = continuum_ref
        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

    # =============================================================================
    #
    # =============================================================================
    def yarara_get_moon_rv(self, obs_loc):
        self.import_table()
        self.import_star_info()

        jdb_time = np.array(self.table["jdb"])
        alpha = self.star_info["Ra"]["fixed"].split(" ")
        decli = self.star_info["Dec"]["fixed"].split(" ")
        ra = 0
        dec = 0
        for j in range(3):
            ra += np.float(alpha[j]) / (60**j)
            dec += np.float(decli[j]) / (60**j)
        alpha = ra * np.pi / 180
        delta = dec * np.pi / 180

        time_obs = astrotime.Time(jdb_time - 0.5, format="mjd")
        moon_t1 = astrocoord.get_moon(time_obs, location=obs_loc)
        moon_t2 = astrocoord.get_moon(time_obs + 1 / 24 / 3600, location=obs_loc)

        moon_t1_cartesian = moon_t1.cartesian
        moon_t2_cartesian = moon_t2.cartesian

        dra = (moon_t2.ra - moon_t1.ra) / (1 * u.s)
        ddec = (moon_t2.dec - moon_t1.dec) / (1 * u.s)
        radvel = (moon_t2.distance - moon_t1.distance) / (1 * u.s)

        vx = (moon_t2_cartesian.x - moon_t1_cartesian.x) / (1 * u.s)
        vy = (moon_t2_cartesian.y - moon_t1_cartesian.y) / (1 * u.s)
        vz = (moon_t2_cartesian.z - moon_t1_cartesian.z) / (1 * u.s)

        vrad = (
            vx * np.cos(alpha) * np.cos(delta)
            + vy * np.cos(delta) * np.sin(alpha)
            + vz * np.sin(delta)
        )
        vrad_moon = vrad.value

        self.yarara_obs_info(kw=["RV_moon", vrad_moon])

        self.yarara_analyse_summary()

    # =============================================================================
    #     MAKE A GIF OF SPECTRUM VARIATION
    # =============================================================================

    def yarara_gif(
        self,
        files_index=None,
        label="",
        continuum="linear",
        sub_dico1="matching_diff",
        sub_dico2=None,
        wave_min=5870,
        wave_max=5930,
        berv_keys="none",
        color="k",
        ymin=-0.01,
        ymax=1.01,
        loop=0,
        delay=10,
        figsize=(20, 5),
        create_images=True,
        delete_dir=False,
        pre_setup="",
    ):
        """
        Make a gif of a spectra-timeseries.

        Parameters
        ----------

        to do :  to do

        """

        self.import_table()

        kw = "_planet" * self.planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if pre_setup == "CaII":
            wave_min = 3928
            wave_max = 3938
            ymax = 0.41
            label = "CaIIK : "
            files_index = self.table["CaII"]
            delay = 5

        elif pre_setup == "Na":
            wave_min = 5880
            wave_max = 5905
            ymax = 1.01
            label = "BERV : "
            files_index = self.table["berv" + kw]
            delay = 10

        elif pre_setup == "Ha":
            wave_min = 6559
            wave_max = 6566
            ymax = 1.01
            label = "Ha : "
            files_index = self.table["Ha"]
            delay = 5

        elif pre_setup == "Gband":
            wave_min = 4210
            wave_max = 4265
            ymax = 1.01
            label = "Time : "
            files_index = self.table["jdb"]
            delay = 5

        values = np.array(files_index)
        files_index = np.argsort(values)

        files_names = np.array(self.table["filename"])[files_index]

        directory = self.directory.split("WORKSPACE/")[0]
        if not os.path.exists(directory + "GIFS/"):
            os.system("mkdir " + directory + "GIFS/")

        if not os.path.exists(directory + "GIFS/workspace/"):
            os.system("mkdir " + directory + "GIFS/workspace/")

        count = 0
        f0 = pd.read_pickle(files_names[0])
        if create_images:
            for j in tqdm(files_names):
                count += 1
                f = pd.read_pickle(j)
                try:
                    berv = f["parameters"][berv_keys]
                except:
                    berv = 0

                begin = int(myf.find_nearest(f["wave"], wave_min)[0][0])
                end = int(myf.find_nearest(f["wave"], wave_max)[0][0])

                plt.figure(figsize=figsize)
                if sub_dico2 is not None:
                    plt.subplot(2, 1, 1)
                if label != "":
                    plt.title(label + "%.3f" % (values[files_index[count - 1]]))

                plt.plot(
                    myf.doppler_r(f["wave"][begin:end], berv * 1000)[1],
                    f["flux"][begin:end] / f[sub_dico1]["continuum_" + continuum][begin:end],
                    color="r",
                )
                plt.plot(
                    myf.doppler_r(f0["wave"][begin:end], berv * 1000)[1],
                    f0["flux"][begin:end] / f0[sub_dico1]["continuum_" + continuum][begin:end],
                    color="k",
                )

                plt.xlim(f0["wave"][begin], f0["wave"][end])
                plt.ylim(ymin, ymax)
                plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
                plt.ylabel("Flux normalised", fontsize=14)

                if sub_dico2 is not None:
                    plt.subplot(2, 1, 2)
                    plt.plot(
                        myf.doppler_r(f["wave"][begin:end], berv * 1000)[1],
                        f["flux"][begin:end] / f[sub_dico2]["continuum_" + continuum][begin:end],
                        color="r",
                    )
                    plt.plot(
                        myf.doppler_r(f0["wave"][begin:end], berv * 1000)[1],
                        f0["flux"][begin:end] / f0[sub_dico2]["continuum_" + continuum][begin:end],
                        color="k",
                    )

                    plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
                    plt.ylabel("Flux normalised", fontsize=14)

                    plt.xlim(f0["wave"][begin], f0["wave"][end])
                    plt.ylim(ymin, ymax)

                plt.subplots_adjust(left=0.07, right=0.93)
                plt.savefig(directory + "GIFS/workspace/im" + str(count).zfill(4) + ".png")
                plt.close()

        os.system(
            "convert -delay "
            + str(delay)
            + " -loop "
            + str(loop)
            + " "
            + directory
            + "GIFS/workspace/im*.png "
            + directory
            + "GIFS/%s_%s.gif" % (str(wave_min), str(wave_max))
        )
        if delete_dir:
            os.system("rm -rf " + directory + "GIFS/workspace/")

    # =============================================================================
    # CUT SPECTRUM
    # =============================================================================

    def yarara_cut_spectrum(self, wave_min=None, wave_max=None):
        """Cut the spectrum time-series borders to reach the specified wavelength limits (included)
        There is no way to cancel this step ! Use it wisely."""

        myf.print_box("\n---- RECIPE : SPECTRA CROPING ----\n")

        directory = self.directory
        self.import_material()
        load = self.material

        if wave_min == "auto":
            w0 = np.min(load["wave"])
            a, b, c = self.yarara_map(
                "matching_diff", reference="norm", wave_min=w0, wave_max=w0 + 1000, Plot=False
            )
            a = a[
                np.sum(a < 1e-5, axis=1) > 100
            ]  # only kept spectra with more than 10 values below 0

            if len(a):
                t = np.nanmean(np.cumsum(a < 1e-5, axis=1).T / np.sum(a < 1e-5, axis=1), axis=1)
                i0 = myf.find_nearest(t, 0.99)[0][
                    0
                ]  # supress wavelength range until 99% of nan values
                if i0:
                    wave_min = load["wave"][i0]
                    print(
                        " [INFO] Automatic detection of the blue edge spectrum found at %.2f AA\n"
                        % (wave_min)
                    )
                else:
                    wave_min = None
            else:
                wave_min = None

        maps = glob.glob(self.dir_root + "CORRECTION_MAP/*.p")
        if len(maps):
            for name in maps:
                correction_map = pd.read_pickle(name)
                old_wave = correction_map["wave"]
                length = len(old_wave)
                idx_min = 0
                idx_max = len(old_wave)
                if wave_min is not None:
                    idx_min = int(myf.find_nearest(old_wave, wave_min)[0])
                if wave_max is not None:
                    idx_max = int(myf.find_nearest(old_wave, wave_max)[0] + 1)
                correction_map["wave"] = old_wave[idx_min:idx_max]
                correction_map["correction_map"] = correction_map["correction_map"][
                    :, idx_min:idx_max
                ]
                myf.pickle_dump(correction_map, open(name, "wb"))
                print("%s modified" % (name.split("/")[-1]))

        time.sleep(1)

        old_wave = np.array(load["wave"])
        length = len(old_wave)
        idx_min = 0
        idx_max = len(old_wave)
        if wave_min is not None:
            idx_min = int(myf.find_nearest(old_wave, wave_min)[0])
        if wave_max is not None:
            idx_max = int(myf.find_nearest(old_wave, wave_max)[0] + 1)

        new_wave = old_wave[idx_min:idx_max]
        wave_min = np.min(new_wave)
        wave_max = np.max(new_wave)

        load = load[idx_min:idx_max]
        load = load.reset_index(drop=True)
        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

        try:
            file_kitcat = pd.read_pickle(self.dir_root + "KITCAT/kitcat_spectrum.p")
            wave_kit = file_kitcat["wave"]
            idx_min = 0
            idx_max = len(wave_kit)
            if wave_min is not None:
                idx_min = int(myf.find_nearest(wave_kit, wave_min)[0])
            if wave_max is not None:
                idx_max = int(myf.find_nearest(wave_kit, wave_max)[0] + 1)
            for kw in [
                "wave",
                "flux",
                "correction_factor",
                "flux_telluric",
                "flux_uncorrected",
                "continuum",
            ]:
                file_kitcat[kw] = np.array(file_kitcat[kw])[idx_min:idx_max]
            myf.pickle_dump(file_kitcat, open(self.dir_root + "KITCAT/kitcat_spectrum.p", "wb"))
        except:
            pass

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        file_ref = self.import_spectrum()
        old_wave = np.array(file_ref["wave"])
        length = len(old_wave)
        idx_min = 0
        idx_max = len(old_wave)
        if wave_min is not None:
            idx_min = int(myf.find_nearest(old_wave, wave_min)[0])
        if wave_max is not None:
            idx_max = int(myf.find_nearest(old_wave, wave_max)[0] + 1)

        new_wave = old_wave[idx_min:idx_max]
        wave_min = np.min(new_wave)
        wave_max = np.max(new_wave)

        for j in tqdm(files):
            file = pd.read_pickle(j)
            file["parameters"]["wave_min"] = wave_min
            file["parameters"]["wave_max"] = wave_max

            anchors_wave = file["matching_anchors"]["anchor_wave"]
            mask = (anchors_wave >= wave_min) & (anchors_wave <= wave_max)
            file["matching_anchors"]["anchor_index"] = (
                file["matching_anchors"]["anchor_index"][mask] - idx_min
            )
            file["matching_anchors"]["anchor_flux"] = file["matching_anchors"]["anchor_flux"][mask]
            file["matching_anchors"]["anchor_wave"] = file["matching_anchors"]["anchor_wave"][mask]

            anchors_wave = file["output"]["anchor_wave"]
            mask = (anchors_wave >= wave_min) & (anchors_wave <= wave_max)
            file["output"]["anchor_index"] = file["output"]["anchor_index"][mask] - idx_min
            file["output"]["anchor_flux"] = file["output"]["anchor_flux"][mask]
            file["output"]["anchor_wave"] = file["output"]["anchor_wave"][mask]

            fields = file.keys()
            for field in fields:
                if type(file[field]) == dict:
                    sub_fields = file[field].keys()
                    for sfield in sub_fields:
                        if type(file[field][sfield]) == np.ndarray:
                            if len(file[field][sfield]) == length:
                                file[field][sfield] = file[field][sfield][idx_min:idx_max]
                elif type(file[field]) == np.ndarray:
                    if len(file[field]) == length:
                        file[field] = file[field][idx_min:idx_max]
            ras.save_pickle(j, file)

    # =============================================================================
    # COMPUTE ALLTHE TEMPERATURE SENSITIVE RATIO
    # =============================================================================

    def yarara_temperature_variation(
        self, sub_dico="matching_diff", continuum="linear", Teff=0, plot=False, debug=False
    ):
        """
        Produce the temperature time-series based on ratio lines Gray 1989. Need to cancel the RV systemic of the star

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        rv : The RV of the star system
        Teff : Effective  temperature of the star (if not specified a value is derived based on Gray calibration)
        plot : True/False, Plot the proxies time-series
        debug : True/False, Plot the intermediate graphicwith the spectrum and area extraction for the proxies
        """

        myf.print_box("\n---- RECIPE : TEFF DETERMINATION ----\n")

        self.import_table()
        self.import_star_info()

        directory = self.directory
        rv_sys = self.rv_sys
        kw = "_planet" * self.planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)
        file_ref = pd.read_pickle(files[0])
        wave_ref = file_ref["wave"]
        grid = wave_ref
        flux_ref = file_ref["flux" + kw]
        conti_ref = file_ref[sub_dico]["continuum_" + continuum]

        line_gray_temp = np.array(
            [
                6150.15,
                6220.48,
                6151.62,
                6221.34,
                6223.99,
                6224.51,
                6226.74,
                6229.23,
                6233.20,
                6242.84,
                6243.83,
                6244.48,
                6247.56,
                6251.83,
                6252.57,
                6253.83,
                6256.89,
                5379.58,
                5380.32,
                5381.03,
                6125.03,
                6126.22,
            ]
        )

        calib_curve = np.array(
            [
                [2.4, 4.7, 13, 35],
                [2.8, 5.5, 15, 36],
                [1.3, 2.6, 7.4, 20],
                [2.2, 4.2, 11, 26],
                [4.2, 7.7, 19, 46],
                [0.8, 1.6, 5.3, 20],
                [0.3, 1.0, 6.8, 19],
                [0.3, 0.9, 5.1, 12],
                [1.8, 2.5, 4.3, 6.9],
                [12, 14, 29],
            ]
        )

        calib_teff = [[4500, 5000, 5500, 5800]] * 9 + [[5000, 5500, 5800]]
        calib_std = 5.28e-4

        snr = np.argmax(np.array(self.table["snr"]))
        file_random = self.import_spectrum(num=snr)

        if rv_sys is None:
            if file_random["parameters"]["RV_sys"] is not None:
                rv_sys = 1000 * file_random["parameters"]["RV_sys"]
            else:
                rv_sys = 0
        else:
            rv_sys *= 1000

        lines = myf.find_nearest(wave_ref, myf.doppler_r(line_gray_temp, rv_sys)[0])[0]

        dl1, dl2, std_dl1, std_dl2, r1, std_r1, std2_r1 = myf.ratio_line(
            lines[13], lines[14], wave_ref, flux_ref, conti_ref
        )
        t = (
            6660.5
            - 9941.7 * r1
            + 35297.7 * r1**2
            - 67336.1 * r1**3
            + 61565 * r1**4
            - 21767 * r1**5
        )
        t_std = (
            abs(
                -9941.7
                + 2 * 35297.7 * r1
                - 3 * 67336.1 * r1**2
                + 4 * 61565 * r1**3
                - 5 * 21767 * r1**4
            )
            * std_r1
        )

        if t < 0:
            t = self.star_info["Teff"]["fixed"]
            t_std = 200

        print("\n Teff based on Gray 1981 calibration : %.0f +/- %.0f \n " % (t, t_std))
        time.sleep(0.5)

        self.teff = t
        t_gray = int(t)
        t_gray_std = int(t_std)

        if (t_gray < 2500) | (t_gray > 10000):
            database = pd.read_pickle(root + "/Python/Material/logT_logM_logR_Gray.p")["V"]
            database["class"] = database["Spec"].str[0]
            database["num"] = database["Spec"].str[1]

            sp = self.star_info["Sp_type"]["fixed"]
            if sp == "Unfound":
                sp = "G2V"

            database = database.loc[database["class"] == sp[0]]
            database = database.reset_index(drop=True)
            match = database.loc[
                myf.find_nearest(np.array(database["num"]).astype("int"), int(sp[1]))[0][0]
            ]
            t_gray2 = int(np.round(10 ** match["log(T)"], -2))

            print(
                Fore.YELLOW
                + " [WARNING] Effective temperature not reliable (%.0f K), matching with spectral type %s provide Teff = %.0f K"
                % (t_gray, sp, t_gray2)
                + Fore.RESET
            )
            t_gray = t_gray2
            time.sleep(1)

        b = self.star_info["magB"]["fixed"]
        v = self.star_info["magV"]["fixed"]
        if np.isnan(self.star_info["BV"]["fixed"]):
            bv = (
                -3.684 * np.log10(t_gray) + 14.551
            )  # http://www.isthe.com/chongo/tech/astro/HR-temp-mass-table-byhrclass.html
            print(
                Fore.YELLOW
                + " [WARNING] BV extracted from the effective temperature since not found on Simbad : BV = %.2f"
                % (bv)
                + Fore.RESET
            )
            self.yarara_star_info(BV=["fixed", np.round(bv, 2)])

        if np.isnan(v) & (not np.isnan(b)):
            v = np.round(b - bv, 2)
            print(
                Fore.YELLOW
                + " [WARNING] magV extracted from the effective temperature since not found on Simbad : mV = %.2f"
                % (v)
                + Fore.RESET
            )
            self.yarara_star_info(magV=["fixed", np.round(v, 2)])
        if np.isnan(b) & (not np.isnan(v)):
            b = np.round(bv + v, 2)
            print(
                Fore.YELLOW
                + " [WARNING] magB extracted from the effective temperature since not found on Simbad : mB = %.2f"
                % (b)
                + Fore.RESET
            )
            self.yarara_star_info(magB=["fixed", np.round(b, 2)])

        if debug:
            plt.plot(
                file_ref["wave"],
                file_ref["flux" + kw] / file_ref[sub_dico]["continuum_" + continuum],
            )
            for j in line_gray_temp:
                plt.axvline(x=myf.doppler_r(j, rv_sys)[0], color="k")

        teff = []
        std_teff = []
        all_teff = []
        all_teff_std = []
        jdb = []

        count = -1
        for k in tqdm(files):
            count += 1
            file = pd.read_pickle(k)
            spectre = file["flux" + kw]
            conti = file[sub_dico]["continuum_" + continuum]

            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(count)

            lines = myf.find_nearest(grid, myf.doppler_r(line_gray_temp, rv_sys)[0])[0]

            dl1, dl2, std_dl1, std_dl2, r1, std_r1, std2_r1 = myf.ratio_line(
                lines[13], lines[14], grid, spectre, conti
            )
            t = (
                6660.5
                - 9941.7 * r1
                + 35297.7 * r1**2
                - 67336.1 * r1**3
                + 61565 * r1**4
                - 21767 * r1**5
            )
            t_std = (
                abs(
                    -9941.7
                    + 2 * 35297.7 * r1
                    - 3 * 67336.1 * r1**2
                    + 4 * 61565 * r1**3
                    - 5 * 21767 * r1**4
                )
                * std_r1
            )

            if not Teff:
                Teff = t

            teff.append(t)
            std_teff.append(t_std)

            file["parameters"]["Teff"] = t
            file["parameters"]["Teff_std"] = t_std

            file["parameters"]["Teff_gray"] = t_gray
            file["parameters"]["Teff_gray_std"] = t_gray_std

            line_depth = []

            for num in [
                [1, 4],
                [3, 4],
                [5, 4],
                [5, 6],
                [8, 7],
                [9, 10],
                [9, 12],
                [13, 12],
                [13, 15],
                [13, 14],
                [0, 2],
                [20, 21],
            ]:
                sub = myf.ratio_line(lines[num[0]], lines[num[1]], grid, spectre, conti)
                sub = [sub[element] for element in range(7)] + [
                    line_gray_temp[num[0]],
                    line_gray_temp[num[1]],
                ]
                line_depth.append(sub)

            ratio_lines = pd.DataFrame(
                line_depth,
                columns=[
                    "d1",
                    "d2",
                    "std_d1",
                    "std_d2",
                    "ratio",
                    "std_ratio",
                    "std_ratio2",
                    "wave1",
                    "wave2",
                ],
            )
            ratio_lines = np.array(ratio_lines)
            if count == 0:
                save = ratio_lines.copy()

            save_teff = []
            save_teff_std = []
            for j, num1, num2 in zip(
                range(10),
                [1, 3, 5, 5, 8, 9, 9, 13, 13, 13, 0, 20],
                [4, 4, 4, 6, 7, 10, 12, 12, 15, 14, 2, 21],
            ):
                coeff = np.polyfit(calib_teff[j], -0.01 / np.array(calib_curve[j]), 2)
                coeff_int = np.hstack(
                    [[coeff[j] / (len(coeff) - j) for j in range(len(coeff))], [0]]
                )
                fit = myc.tableXY(
                    np.linspace(Teff - 50, Teff + 50, 100),
                    np.polyval(coeff_int, np.linspace(Teff - 50, Teff + 50, 100))
                    - np.polyval(coeff_int, Teff)
                    + save[j, 4],
                )
                fit.switch()
                polynome = np.polyfit(fit.x, fit.y, 2)
                poly_deri = np.array(
                    [polynome[j] * (len(polynome) - j - 1) for j in range(len(polynome) - 1)]
                )

                dt = np.polyval(polynome, ratio_lines[j, 4]) - Teff
                dt_std = np.sqrt(ratio_lines[j, 6] ** 2 + calib_std**2) * np.polyval(
                    poly_deri, ratio_lines[j, 4]
                )

                save_teff.append(dt)
                save_teff_std.append(dt_std)

                file["parameters"]["Teff_" + str(num1) + "/" + str(num2)] = dt
                file["parameters"]["Teff_" + str(num1) + "/" + str(num2) + "_std"] = dt_std

            file["parameters"]["Teff"] = np.sum(
                np.array(save_teff) / np.array(save_teff_std) ** 2
            ) / np.sum(1 / np.array(save_teff_std) ** 2)
            file["parameters"]["Teff_std"] = np.sqrt(1 / np.sum(1 / np.array(save_teff_std) ** 2))

            all_teff.append(save_teff)
            all_teff_std.append(save_teff_std)

            myf.pickle_dump(file, open(k, "wb"))

        all_teff = np.array(all_teff)
        all_teff_std = np.array(all_teff_std)

        if plot:
            plt.figure(figsize=(12, 16))
            mean = np.zeros(len(jdb))
            for j in range(10):
                a = myc.tableXY(jdb, all_teff[:, j], all_teff_std[:, j])
                a.y -= np.median(a.y)
                if not j:
                    plt.subplot(5, 2, j + 1)
                    ax = plt.gca()
                else:
                    plt.subplot(5, 2, j + 1, sharex=ax, sharey=ax)
                a.plot()
                plt.xlabel("Jdb - 2,400,000 [days]", fontsize=13)
                plt.ylabel(r"$\Delta$ T [K]", fontsize=13)
                plt.ylim(-15, 15)
                mean += a.y / a.yerr**2
            mean /= np.sum(1 / all_teff_std**2, axis=1)
            plt.figure(figsize=(12, 6))
            plt.xlabel("Jdb - 2,400,000 [days]", fontsize=13)
            plt.ylabel(r"$\Delta$ T [K]", fontsize=13)
            b = myc.tableXY(jdb, mean, np.sqrt(1 / np.sum(1 / all_teff_std**2, axis=1)))
            b.plot()
            self.teff_mean = b

        self.yarara_analyse_summary()
        self.all_teff = all_teff
        self.all_teff_std = all_teff_std

    def yarara_stellar_atmos(
        self,
        sub_dico="matching_anchors",
        continuum="linear",
        reference="snr",
        metric="l1",
        debug=False,
    ):
        """
        Compute the best Teff and log value by fitting Ha, NaD and MgIb.
        The best model is then used to get a model of the spectrum.

        Parameters
        ----------

        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        metric : metric used to determine the best model
        debug : True/False button to save the intermediate products

        """

        myf.print_box("\n---- RECIPE : STELLAR ATMOSPHERIC PARAMETERS DETERMINATION ----\n")

        NaD = [5892.94, 30]  # center and width
        Ha = [6562.79, 30]
        Mg1b = [5178.5, 15]

        if metric == "mad":

            def score(array, yerr):
                return myf.mad(array, axis=1)

        elif metric == "std":

            def score(array, yerr):
                return np.nanstd(array, axis=1)

        elif metric == "median":

            def score(array, yerr):
                return np.nanmedian(array, axis=1)

        elif metric == "mean":

            def score(array, yerr):
                return np.nanmean(array, axis=1)

        elif metric == "l1":

            def score(array, yerr):
                return np.nansum(abs(array) / yerr, axis=1) / len(yerr)

        self.import_table()
        tab = self.table

        kw = "_planet" * self.planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("---- DICO %s used ----" % (sub_dico))

        snr = np.array(tab["snr"]).argmax()

        file_test = self.import_spectrum(num=snr)
        if reference == "snr":
            f_norm, f_norm_std = myf.flux_norm_std(
                file_test["flux" + kw],
                file_test["flux_err"],
                file_test[sub_dico]["continuum_" + continuum],
                file_test["continuum_err"],
            )
        elif reference == "master":
            self.import_material()
            load = self.material
            ref = np.array(load["reference_spectrum"])
            f_norm, f_norm_std = myf.flux_norm_std(
                ref * file_test[sub_dico]["continuum_" + continuum],
                file_test["flux_err"],
                file_test[sub_dico]["continuum_" + continuum],
                file_test["continuum_err"],
            )
        else:
            ref = reference
            f_norm, f_norm_std = myf.flux_norm_std(
                ref * file_test[sub_dico]["continuum_" + continuum],
                file_test["flux_err"],
                file_test[sub_dico]["continuum_" + continuum],
                file_test["continuum_err"],
            )

        grid = file_test["wave"]
        if np.min(grid) > (Mg1b[0] - Mg1b[1]):
            print("MgI not found in the spectrum, replaced by NaD")
            Mg1b = NaD

        Teff = file_test["parameters"]["Teff_gray"]
        rv_sys = file_test["parameters"]["RV_sys"]
        print("\nTeff Gray : %.0f [K]" % (Teff))
        print("-------------------")

        self.atmos_model = {}
        self.atmos_metric = []

        count = 0
        for model in ["ATLAS", "MARCS"]:
            count += 1
            print("Analysis of %s models" % (model))
            print("-------------------")
            table_atlas = pd.read_pickle(root + "/Python/Material/template_star_" + model + ".p")
            table_atlas = table_atlas.loc[
                (table_atlas["wave"] >= grid[0]) & (table_atlas["wave"] <= grid[-1])
            ]
            table_atlas = table_atlas.reset_index(drop=True)

            spectrum = myc.tableXY(myf.doppler_r(grid, rv_sys * 1000)[1], f_norm, f_norm_std)
            spectrum.interpolate(new_grid=grid, replace=True, interpolate_x=False)

            teff_model = np.array([j.split("_")[0][1:] for j in table_atlas.keys()[1:]]).astype(
                "int"
            )
            gravity_model = np.array([j.split("_")[1][1:] for j in table_atlas.keys()[1:]]).astype(
                "float"
            )

            stack = []
            stack_yerr = []
            s = 1
            plt.figure()

            if debug:
                all_model = []

            for name, center, windows in zip(
                ["ha", "nad", "mg1b"], [Ha[0], NaD[0], Mg1b[0]], [Ha[1], NaD[1], Mg1b[1]]
            ):
                left_line = int(myf.find_nearest(grid, center - windows)[0])
                right_line = int(myf.find_nearest(grid, center + windows)[0])

                table_line = np.array(table_atlas[left_line : right_line + 1])[:, 1:]
                table_line = table_line.T

                region = np.ones(len(grid[left_line : right_line + 1]))
                for index in range(len(table_line)):
                    star = np.gradient(table_line[index])
                    exclusion_region = myf.rm_outliers(star, kind="inter")[0]
                    val, cluster = myf.clustering(exclusion_region, 0.5, num=0)
                    val = np.array([np.product(v) for v in val]).astype("bool")
                    cluster = cluster[val]
                    cluster = cluster[cluster[:, -1] > 5]
                    region *= myf.flat_clustering(len(star), cluster)

                star = np.gradient(spectrum.y[left_line : right_line + 1])
                exclusion_region = myf.rm_outliers(star, kind="inter")[0]
                val, cluster = myf.clustering(exclusion_region, 0.5, num=0)
                val = np.array([np.product(v) for v in val]).astype("bool")
                cluster = cluster[val]
                cluster = cluster[cluster[:, -1] > 5]
                region *= myf.flat_clustering(len(star), cluster)
                region = region.astype("bool")

                line_rms = table_line - spectrum.y[left_line : right_line + 1]
                line_rms[:, ~region] = np.nan
                line_yerr = spectrum.yerr[left_line : right_line + 1]

                plt.subplot(3, 2, 2 * s - 1)
                plt.xlabel(r"Wavelength [$\AA$]", fontsize=13)
                plt.ylabel(r"$\Delta$ flux", fontsize=13)
                plt.plot(spectrum.x[left_line : right_line + 1], line_rms.T)
                plt.axhline(y=0, color="k", alpha=0.5)
                if s == 1:
                    plt.subplot(3, 2, 2 * s)
                    ax = plt.gca()
                else:
                    plt.subplot(3, 2, 2 * s, sharex=ax, sharey=ax)

                metric = score(line_rms, line_yerr)
                plt.scatter(
                    teff_model, gravity_model, c=metric, s=50, vmax=np.nanpercentile(metric, 25)
                )
                plt.axvline(x=Teff, color="r")
                plt.xlabel(r"Teff [K]", fontsize=13)
                plt.ylabel(r"log(g)", fontsize=13)
                s += 1

                stack.append(line_rms)
                stack_yerr.append(line_yerr)

                if debug:
                    all_model.append(line_rms)

            if debug:
                dico = {k: j for k, j in zip(["ha", "nad", "mg1b"], all_model)}
                if count == 1:
                    self.all_model = {model: dico}
                else:
                    self.all_model[model] = dico

            stack2 = np.hstack(stack)
            stack2_yerr = np.hstack(stack_yerr)

            metric = score(stack2, stack2_yerr)
            atmos = myc.tableXY(teff_model, gravity_model)

            X, Y = np.meshgrid(np.unique(teff_model), np.unique(gravity_model))
            metric_matrix = np.reshape(metric, np.shape(X))

            self.atmos_metric.append([atmos.x, atmos.y, metric])

            plt.figure(figsize=(15, 5))
            plt.subplot(1, 3, 1)
            atmos.fit_poly2d(
                z=metric,
                Draw=True,
                vmax=np.nanpercentile(metric, 25),
                cmap="plasma",
                maximum=False,
                expo1=2,
                expo2=2,
            )
            plt.axvline(x=Teff, color="r")
            plt.xlabel(r"Teff [K]", fontsize=13)
            plt.ylabel(r"log(g)", fontsize=13)
            plt.subplot(1, 3, 2)
            plt.plot(np.min(X, axis=0), np.min(metric_matrix, axis=0))
            plt.xlabel(r"Teff [K]", fontsize=13)
            plt.ylabel("Score", fontsize=13)
            plt.subplot(1, 3, 3)
            plt.plot(np.min(Y, axis=1), np.min(metric_matrix, axis=1))
            plt.xlabel(r"log(g)", fontsize=13)
            plt.ylabel("Score", fontsize=13)

            print("Best score %s : %.2f" % (model, metric.min()))
            print("Teff %s : %.0f [K]" % (model, teff_model[metric.argmin()]))
            print("log(g) %s : %.1f" % (model, gravity_model[metric.argmin()]))

            t = teff_model[metric.argmin()]
            if (t == 4000) & (model == "ATLAS"):
                print(
                    Fore.YELLOW
                    + "\n [WARNING] Minimum temperature of the ATLAS stellar template grid reached \n"
                    + Fore.RESET
                )

            if (t == 5000) & (model == "MARCS"):
                print(
                    Fore.YELLOW
                    + "\n [WARNING] Minimum temperature of the MARCS stellar template grid reached \n"
                    + Fore.RESET
                )

            if (t == 7000) & (model == "ATLAS"):
                print(
                    Fore.YELLOW
                    + "\n [WARNING] Maximum temperature of the ATLAS stellar template grid reached \n"
                    + Fore.RESET
                )

            if (t == 7000) & (model == "MARCS"):
                print(
                    Fore.YELLOW
                    + "\n [WARNING] Maximum temperature of the MARCS stellar template grid reached \n"
                    + Fore.RESET
                )

            if count == 1:
                self.model_atmos = {
                    model: [
                        "T"
                        + str(teff_model[metric.argmin()])
                        + "_g"
                        + str(gravity_model[metric.argmin()]),
                        metric.min(),
                    ]
                }
            else:
                self.model_atmos[model] = [
                    "T"
                    + str(teff_model[metric.argmin()])
                    + "_g"
                    + str(gravity_model[metric.argmin()]),
                    metric.min(),
                ]
                if metric.min() < self.model_atmos["ATLAS"][1]:
                    self.model_atmos["best"] = [
                        "MARCS",
                        self.model_atmos["MARCS"][0],
                        self.model_atmos["MARCS"][1],
                    ]
                else:
                    self.model_atmos["best"] = [
                        "ATLAS",
                        self.model_atmos["ATLAS"][0],
                        self.model_atmos["ATLAS"][1],
                    ]

        self.yarara_star_info()

        mask_atlas = abs(Teff - self.atmos_metric[0][0]) < 500
        mask_marcs = abs(Teff - self.atmos_metric[1][0]) < 500
        if sum(mask_atlas) == 0:
            mask_atlas = abs(Teff - self.atmos_metric[0][0]) < 5000
        if sum(mask_marcs) == 0:
            mask_marcs = abs(Teff - self.atmos_metric[1][0]) < 5000

        atlas = myc.tableXY(
            self.atmos_metric[0][0][mask_atlas], self.atmos_metric[0][1][mask_atlas]
        )
        marcs = myc.tableXY(
            self.atmos_metric[1][0][mask_marcs], self.atmos_metric[1][1][mask_marcs]
        )

        plt.figure(figsize=(15, 5))
        plt.subplot(1, 2, 1)
        plt.xlim(np.min(atlas.x), np.max(atlas.x))
        plt.ylim(np.min(atlas.y), np.max(atlas.y))
        plt.title(
            "ATLAS (best metric %.1f : %s)"
            % (self.model_atmos["ATLAS"][1], self.model_atmos["ATLAS"][0]),
            fontsize=14,
        )
        plt.xlabel(r"Teff [K]", fontsize=13)
        plt.ylabel(r"log(g)", fontsize=13)
        plt.axvline(x=Teff, color="k")
        atlas.fit_poly2d(
            z=self.atmos_metric[0][2][mask_atlas],
            Draw=True,
            vmax=np.nanpercentile(self.atmos_metric[0][2][mask_atlas], 25),
            cmap="plasma",
            maximum=True,
            expo1=2,
            expo2=2,
        )
        plt.subplot(1, 2, 2)
        plt.xlim(np.min(marcs.x), np.max(marcs.x))
        plt.ylim(np.min(marcs.y), np.max(marcs.y))
        plt.title(
            "MARCS (best metric %.1f : %s)"
            % (self.model_atmos["MARCS"][1], self.model_atmos["MARCS"][0]),
            fontsize=14,
        )
        plt.xlabel(r"Teff [K]", fontsize=13)
        plt.ylabel(r"log(g)", fontsize=13)
        plt.axvline(x=Teff, color="k")
        marcs.fit_poly2d(
            z=self.atmos_metric[1][2][mask_marcs],
            Draw=True,
            vmax=np.nanpercentile(self.atmos_metric[1][2][mask_marcs], 25),
            cmap="plasma",
            maximum=True,
            expo1=2,
            expo2=2,
        )

    def yarara_correct_continuum_absorption(self, model=None, T=None, g=None):
        """
        Use the stellar template to correct for continuum absorption.
        The product is saved in Material.
        If astellar atmospheric parameters unspecified, value of the best fit selected.

        Parameters
        ----------

        model : MARCS or ATLAS
        T : Teff of the star
        g : Log g of the star

        """

        myf.print_box("\n---- RECIPE : CORRECT ABSORPTION CONTINUUM ----\n")

        reject_zones = [[5875, 5910]]

        self.import_table()
        self.import_material()
        self.import_star_info()

        load = self.material
        tab = self.table

        snr = np.array(tab["snr"]).argmax()
        file_test = self.import_spectrum(num=snr)
        rv_sys = file_test["parameters"]["RV_sys"]
        grid = file_test["wave"]
        reference = np.array(load["reference_spectrum"])

        if (model is None) | (T is None) | (g is None):
            try:
                model = self.star_info["stellar_template"]["YARARA"].split("_")[0]
                parameter = "_".join(self.star_info["stellar_template"]["YARARA"].split("_")[1:])
            except:
                model = self.model_atmos["best"][0]
                parameter = self.model_atmos["best"][1]
        else:
            parameter = "T" + str(T) + "_g" + str(g)

        print("Model selected : %s (%s)" % (model, parameter))

        suffixe = model + "_" + parameter
        self.yarara_star_info(stellar_template=["YARARA", suffixe])

        table_atlas = pd.read_pickle(root + "/Python/Material/template_star_" + model + ".p")
        table_atlas = table_atlas.loc[
            (table_atlas["wave"] >= grid[0]) & (table_atlas["wave"] <= grid[-1])
        ]
        table_atlas = table_atlas.reset_index(drop=True)
        template = myc.tableXY(
            myf.doppler_r(np.array(table_atlas["wave"]), rv_sys * 1000)[0],
            np.array(table_atlas[parameter]),
            0 * table_atlas["wave"],
        )
        template.interpolate(new_grid=grid, replace=True, method="cubic", interpolate_x=False)
        template.y[template.y > 1] = 1
        template.y[template.y < 0] = 0
        template_flux = template.y.copy()
        smooth = pd.DataFrame(template.y).rolling(1000, min_periods=1, center=True).quantile(0.9)
        smooth = np.array(smooth).T[0]
        template.y[template.y < np.array(smooth)] = 0
        template.find_max(vicinity=10)

        mask = np.zeros(len(template.x_max)).astype("bool")
        for zone in reject_zones:
            mask = mask | ((template.x_max > zone[0]) & (template.x_max < zone[1]))
            template.x_max = template.x_max[~mask]
            template.y_max = template.y_max[~mask]
            template.index_max = template.index_max[~mask]

        anchor_idx = np.sort(file_test["matching_anchors"]["anchor_index"])
        match = myf.match_nearest(anchor_idx, template.index_max)

        median_spectrum = myc.tableXY(grid, reference)

        parameter = parameter.split("_")

        plt.figure(figsize=(15, 8))
        plt.subplot(2, 1, 1)
        plt.title("Before correction", fontsize=16)
        plt.xlabel(r"Wavelength $\lambda$ [$\AA$]", fontsize=16)
        plt.ylabel(r"Flux normalised", fontsize=16)
        plt.plot(median_spectrum.x, median_spectrum.y, color="k", label="RASSINE")
        plt.plot(
            template.x,
            template_flux,
            color="r",
            label="Template (%s, Teff = %s, log(g) = %s)"
            % (model, parameter[0][1:], parameter[1][1:]),
        )
        plt.legend(loc=4, prop={"size": 14})
        plt.scatter(
            template.x_max[match[:, 1]],
            template.y_max[match[:, 1]],
            color="orange",
            zorder=10,
            s=20,
        )
        ax = plt.gca()
        # plt.subplot(3,1,2,sharex=ax,sharey=ax)
        # plt.title('Fitted correction',fontsize=14)
        local = myc.tableXY(template.x_max[match[:, 1]], template.y_max[match[:, 1]])
        local.x = np.hstack([grid[0], local.x])
        local.y = np.hstack([local.y[0], local.y])
        local.xerr = np.hstack([local.xerr[0], local.xerr])
        local.yerr = np.hstack([local.yerr[0], local.yerr])
        local.interpolate(new_grid=grid, interpolate_x=False)
        local.smooth(box_pts=1000, shape="savgol", replace=True)
        correction = local.y_smoothed
        # local.binned_scatter(100,Show=False,color='b')
        # save = local.binned_data.x
        # local.binned_data.supress_nan()
        # local.binned_data.interpolate(new_grid=save,method='linear',replace=True) # draw a line if bin is empty
        # local.binned_data.interpolate(new_grid=grid,method='cubic',replace=True)
        # correction = local.binned_data.y
        plt.plot(grid, correction, color="orange")
        plt.subplot(2, 1, 2, sharex=ax, sharey=ax)
        plt.title("After correction", fontsize=16)
        plt.xlabel(r"Wavelength $\lambda$ [$\AA$]", fontsize=16)
        plt.ylabel(r"Flux normalised", fontsize=16)
        plt.plot(median_spectrum.x, median_spectrum.y * correction, color="k")
        plt.plot(grid, template_flux, color="r")
        ymin = ax.get_ylim()[0]
        ymax = ax.get_ylim()[1]
        if ymin < -1:
            plt.ylim(-1, None)
        if ymax > 1.5:
            plt.ylim(None, 1.5)
        plt.subplots_adjust(left=0.07, right=0.97, top=0.93, bottom=0.12, hspace=0.4)
        plt.savefig(self.dir_root + "IMAGES/Correction_absolute_continuum.png")

        load["stellar_template"] = template_flux
        load["correction_factor"] = correction
        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

    # =============================================================================
    # COMPUTE ALL THE PROXIES OF ACTIVITY
    # =============================================================================

    def yarara_activity_master(self, Plot=False, night_bin=True, time_detrending=2):
        self.import_proxies()
        sign = []
        master = self.ca2.y.copy()
        master_yerr = self.ca2.yerr
        N = 1
        if Plot:
            plt.figure(figsize=(18, 6))
        for name, proxy in zip(
            ["ha", "hb", "hc", "hd", "hed3", "ca1", "nad1", "nad2", "mg1a", "mg1b", "mg1c"],
            [
                self.ha,
                self.hb,
                self.hc,
                self.hd,
                self.hed3,
                self.ca1,
                self.nad1,
                self.nad2,
                self.mg1a,
                self.mg1b,
                self.mg1c,
            ],
        ):
            out = self.ca2.corr(proxy, Draw=False)
            p = proxy.y * np.sign(out.lin_slope_w)
            p -= np.median(p)
            proxy.y = p
            proxy.substract_polyfit(time_detrending, replace=True)
            master += proxy.y
            master_yerr += proxy.yerr
            N += 1
            if Plot:
                if night_bin:
                    proxy.night_stack(replace=True)
                proxy.periodogram(p_min=2, legend=name, Norm=True, color=None)

                plt.legend()
        output = myc.tableXY(self.ca2.x, master, master_yerr / N)
        if Plot:
            output.night_stack(replace=night_bin)
            output.periodogram(p_min=2, legend=name, Norm=True, p_max=150)
            plt.subplots_adjust(left=0.12, right=0.95)
            plt.savefig(self.dir_root + "IMAGES/master_proxy.png")
        return output

    def yarara_prot_photometry(self):
        self.import_photometry()
        self.photometry.periodogram(Plot=False)
        plt.figure(figsize=(6, 9))
        plt.axes([0.14, 0.1, 0.8, 0.6])
        self.photometry.periodogram_rolling()
        ax = plt.gca()
        xmax = ax.get_xlim()[1]
        plt.axvline(x=self.photometry.perio_max, color="k", alpha=0.6)
        plt.axes([0.14, 0.7, 0.8, 0.2], sharex=ax)
        self.photometry.periodogram(Plot=True)
        plt.xlabel("")
        plt.tick_params(direction="in", labelbottom=False)
        plt.xlim(None, xmax)
        plt.ylim(0, None)
        plt.title("Prot = %.2f days" % (self.photometry.perio_max))
        plt.axvline(x=self.photometry.perio_max, color="r", alpha=0.3)
        plt.savefig(self.dir_root + "IMAGES/photometry.png")

    def yarara_ghost_CaII(self, Ca2H=[3968.47, 0.45], Ca2K=[3933.66, 0.45], overestimation=0):

        self.import_material()
        self.import_star_info()

        load = self.material
        instrument = self.instrument
        rv_sys = self.star_info["Rv_sys"]["fixed"]

        dlambda = np.mean(np.diff(np.array(load.wave)))
        over = int(overestimation / dlambda)

        mask_ghost_a = myc.tableXY(
            myf.doppler_r(np.array(load.wave), rv_sys * 1000)[1], load.ghost_a
        )
        if over:
            mask_ghost_a.extend_clustering(extend=-over)

        mask_ghost_a.interpolate(new_grid=10, method="linear")

        mask_proxies_ca2h = mask_ghost_a.copy()
        mask_proxies_ca2h.y *= 0
        mask_proxies_ca2k = mask_ghost_a.copy()
        mask_proxies_ca2k.y *= 0

        mask_proxies_ca2h.y[abs(mask_proxies_ca2h.x - Ca2H[0]) <= Ca2H[1]] = 1
        mask_proxies_ca2k.y[abs(mask_proxies_ca2k.x - Ca2K[0]) <= Ca2K[1]] = 1

        mask_proxies_ca2k.clip(max=[4100, None])
        mask_proxies_ca2h.clip(max=[4100, None])
        mask_ghost_a.clip(max=[4100, None])

        contam_h = []
        contam_k = []
        rv_range = np.arange(-300, 300, 1)
        for j in tqdm(rv_range):
            mask_proxies_ca2h.rv_shift(j, replace=False)
            mask_proxies_ca2k.rv_shift(j, replace=False)

            contam_h.append(
                [
                    np.sum(mask_proxies_ca2h.shifted.y * (mask_ghost_a.y != 0).astype("int"))
                    * 100
                    / np.sum(mask_proxies_ca2h.y),
                    np.mean(mask_proxies_ca2h.shifted.y * mask_ghost_a.y),
                ]
            )
            contam_k.append(
                [
                    np.sum(mask_proxies_ca2k.shifted.y * (mask_ghost_a.y != 0).astype("int"))
                    * 100
                    / np.sum(mask_proxies_ca2k.y),
                    np.mean(mask_proxies_ca2k.shifted.y * mask_ghost_a.y),
                ]
            )

        contam_h = np.array(contam_h)
        contam_k = np.array(contam_k)
        plt.plot(rv_range, contam_h[:, 0], label="CaIIH")
        plt.plot(rv_range, contam_k[:, 0], label="CaIIK")
        plt.plot(rv_range, 0.5 * (contam_k[:, 0] + contam_h[:, 0]), label="CaII", color="k")

        plt.xlabel("RV sys [km/s]", fontsize=14)
        plt.ylabel("Contamination of line core [%]", fontsize=14)
        plt.title("Ghost A contamination of CaIIH&K line core on %s" % (instrument))
        plt.legend()

        matrix = np.array([rv_range, contam_h[:, 0], contam_k[:, 0]]).T

        np.savetxt(
            root + "/Python/Material/Ghosts_CaII_%s.txt" % (self.instrument),
            matrix,
            "%.1f",
            header="RVsys CaIIH CaIIK",
        )

    def yarara_determine_optimal_Sindex(self):
        rv_sys = self.star_info["Rv_sys"]["fixed"]

        matrix = np.genfromtxt(root + "/Python/Material/Ghosts_CaII_%s.txt" % (self.instrument))

        values = matrix[myf.find_nearest(matrix[:, 0], rv_sys)[0][0]]

        if 0.5 * (values[1] + values[2]) < 15:
            return "CaII"
        else:
            if values[1] < 30:
                return "CaIIH"
            elif values[2] < 30:
                return "CaIIK"
            else:
                return "CaII"

    def yarara_activity_index(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        plot=True,
        debug=False,
        calib_std=0,
        optimize=False,
        substract_map=[],
        add_map=[],
        p_noise=1 / np.inf,
        save=True,
    ):
        """
        Produce the activity proxy time-series. Need to cancel the RV systemic of the star

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        rv : The RV of the star system
        plot : True/False, Plot the proxies time-series
        debug : True/False, Plot the intermediate graphicwith the spectrum and area extraction for the proxies
        ron : read-out-noise error injected by reading pixels
        calib_std : std error due to flat-field photon noise (5.34e-4 or 10.00e-4 Cretignier+20)
        """

        myf.print_box("\n---- RECIPE : ACTIVITY PROXIES EXTRACTION ----\n")

        directory = self.directory
        rv_sys = self.rv_sys
        self.import_table()

        self.import_material()
        load = self.material
        kw = "_planet" * self.planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")
        time.sleep(1)

        epsilon = 1e-12
        save_kw = save

        try:
            self.import_star_info()
            bv = self.star_info["BV"]["fixed"]
            if np.isnan(bv):
                bv = 0
                print(Fore.YELLOW + " [WARNING] No BV value given for the star" + Fore.RESET)
            # conv_offset = -0.60 - 5.99*bv + 2.51*bv**2 #old calib
            # conv_slope = 4.55 - 7.30*bv + 3.61*bv**2 #old calib

            conv_offset = 1.13 - 10.13 * bv + 4.97 * bv**2  # old calib
            conv_slope = 5.76 - 10.0 * bv + 5.10 * bv**2  # old calib

        except:
            conv_offset = 0
            conv_slope = 1

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        # [center, half-window, hole_size, half-window-continuum,database_kw, subplot]
        Ca2H = [3968.47, 0.45, 0, 1.5, "CaIIH", None]
        Ca2K = [3933.66, 0.45, 0, 1.5, "CaIIK", None]
        Ca1 = [4226.72, 1.50, 0, 1.5, "CaI", 2]
        Mg1a = [5167.32, 0.50, 0, 1.5, "MgIa", 6]
        Mg1b = [5172.68, 0.50, 0, 1.5, "MgIb", 7]
        Mg1c = [5183.60, 0.50, 0, 1.5, "MgIc", 8]
        NaDl = [5889.95, 0.50, 0, 0, "NaD1", 3]
        NaDr = [5895.92, 0.50, 0, 0, "NaD2", 4]
        Ha = [6562.79, 0.35, 0, 0, "Ha", 9]
        Hb = [4861.35, 0.35, 0, 1.5, "Hb", 10]
        Hc = [4340.47, 0.35, 0, 1.5, "Hc", 11]
        Hd = [4101.73, 0.15, 0, 1.5, "Hd", 12]
        Heps = [3889.04, 0.10, 0, 1.5, "Heps", None]
        He1D3 = [5875.62, 0.15, 0, 0, "HeID3", 5]

        all_proxies = [Ca2H, Ca2K, Ca1, Mg1a, Mg1b, Mg1c, NaDl, NaDr, Ha, Hb, Hc, Hd, Heps, He1D3]

        # FeXIV = 5302.86 ;

        # calib_std = 5.34e-4 #from Cretignier+20 precision linear + clustering
        # calib_std = 10.00e-4 #from Cretignier+20 precision linear + clustering (new sun) related to flat field SNR

        fluxes = []
        err_fluxes = []
        jdb = []
        snrs = []
        count = 0

        file_random = self.import_spectrum()
        waves = file_random["wave"]
        all_prox_names = np.array(all_proxies)[:, 4]
        proxy_found = ((np.array(all_proxies)[:, 0] - np.nanmin(waves[0])) > 0) & (
            (np.nanmax(waves[0])) > 0
        )
        all_proxies = list(np.array(all_proxies)[proxy_found])

        dgrid = np.mean(np.diff(waves))
        for j in tqdm(files):
            count += 1
            file = pd.read_pickle(j)
            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(count)

            snrs.append(file["parameters"]["SNR_5500"])

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]

            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            dustbin, f_norm_std = myf.flux_norm_std(
                f, f_std, file["matching_diff"]["continuum_" + continuum], c_std
            )

            fluxes.append(f_norm)
            err_fluxes.append(f_norm_std)
            self.debug = j

        waves = np.array(waves)
        flux = np.array(fluxes)
        err_flux = np.array(err_fluxes)
        flux *= np.array(load["correction_factor"])
        err_flux *= np.array(load["correction_factor"])
        snrs = np.array(snrs)

        noise_matrix, noise_values = self.yarara_poissonian_noise(
            noise_wanted=p_noise, wave_ref=None, flat_snr=True
        )
        flux += noise_matrix
        err_flux = np.sqrt(err_flux**2 + noise_values**2)

        for maps in substract_map:
            flux = self.yarara_substract_map(flux, maps, correction_factor=True)

        for maps in add_map:
            flux = self.yarara_add_map(flux, maps, correction_factor=True)

        jdb = np.array(jdb)
        ref = snrs.argmax()
        wave_ref = waves
        wave = waves

        flux_ref = np.median(flux, axis=0)
        ratio = flux / (flux_ref + epsilon)
        try:
            mask_ghost = np.array(load["ghost_a"]).astype("bool")
        except:
            mask_ghost = np.zeros(len(flux_ref)).astype("bool")
        ratio[:, mask_ghost] = np.nan

        if rv_sys is None:
            if file_random["parameters"]["RV_sys"] is not None:
                rv_sys = 1000 * file_random["parameters"]["RV_sys"]
            else:
                rv_sys = 0
        else:
            rv_sys *= 1000

        def find_proxy(vec):
            center = myf.doppler_r(vec[0], rv_sys)[0]
            left = myf.doppler_r(vec[0] - vec[1], rv_sys)[0]
            right = myf.doppler_r(vec[0] + vec[1], rv_sys)[0]

            center_idx_proxy = myf.find_nearest(wave, center)[0]
            left_idx_proxy = myf.find_nearest(wave, left)[0]
            right_idx_proxy = myf.find_nearest(wave, right)[0]

            left = myf.doppler_r(vec[0] - vec[2], rv_sys)[0]
            right = myf.doppler_r(vec[0] + vec[2], rv_sys)[0]

            left_idx_hole = myf.find_nearest(wave, left)[0]
            right_idx_hole = myf.find_nearest(wave, right)[0]

            left = myf.doppler_r(vec[0] - vec[3], rv_sys)[0]
            right = myf.doppler_r(vec[0] + vec[3], rv_sys)[0]

            left_idx_cont = myf.find_nearest(wave, left)[0]
            right_idx_cont = myf.find_nearest(wave, right)[0]

            return (
                int(center_idx_proxy),
                int(left_idx_proxy),
                int(right_idx_proxy),
                int(left_idx_hole),
                int(right_idx_hole),
                int(left_idx_cont),
                int(right_idx_cont),
            )

        def extract_proxy(vec, kernel=None):
            c, l, r, l_hole, r_hole, l_cont, r_cont = find_proxy(vec)
            if kernel is None:
                continuum = 1
                if r != l:
                    r += 1
                if l_hole != r_hole:
                    r_hole += 1
                if l_cont != l:
                    r_cont += 1
                    continuum = np.hstack([ratio[:, l_cont:l], ratio[:, r:r_cont]])
                    continuum = np.nanmedian(continuum, axis=1)
                    continuum[np.isnan(continuum)] = 1
                proxy = np.sum(flux[:, l:r], axis=1) - np.sum(flux[:, l_hole:r_hole], axis=1)
                proxy_std = np.sum((err_flux[:, l:r]) ** 2, axis=1) - np.sum(
                    (err_flux[:, l_hole:r_hole]) ** 2, axis=1
                )
                proxy_std = np.sqrt(proxy_std)
                norm_proxy = (r - l) - (r_hole - l_hole)

                proxy /= continuum
                proxy_std /= continuum
            else:
                kernel /= np.sum(abs(kernel))
                proxy = np.sum((flux - np.median(flux, axis=0)) * kernel, axis=1)
                proxy_std = np.sum((kernel * err_flux) ** 2, axis=1)
                proxy_std = np.sqrt(proxy_std)
                norm_proxy = 1

            prox = myc.tableXY(jdb, proxy, proxy_std)
            prox.rms_w()
            proxy_rms = prox.rms
            windex = int(1 / dgrid)
            mask_proxy = abs(np.arange(len(flux.T)) - c) < windex
            slope = np.median(
                (flux[:, mask_proxy] - np.mean(flux[:, mask_proxy], axis=0))
                / ((proxy - np.mean(proxy))[:, np.newaxis]),
                axis=0,
            )

            s = myc.tableXY(wave[mask_proxy], slope)
            s.smooth(box_pts=7, shape="savgol")
            s.center_symmetrise(myf.doppler_r(vec[0], rv_sys)[0], replace=True)
            slope = s.y

            t = myc.table(flux[:, mask_proxy] - np.mean(flux[:, mask_proxy], axis=0))
            t.rms_w(1 / err_flux[:, mask_proxy] ** 2, axis=0)
            rslope = np.zeros(len(flux.T))
            rslope[mask_proxy] = slope
            rms = np.ones(len(flux.T))
            rms[mask_proxy] = t.rms

            rcorr = (
                rslope * proxy_rms / (rms + epsilon)
            )  # need good weighting of the proxy and the flux

            if norm_proxy:
                proxy /= norm_proxy
                proxy_std /= norm_proxy
                return proxy, proxy_std, rcorr, rslope
            else:
                return 0 * proxy, 0 * proxy_std, 0 * wave, 0 * wave

        save = {"null": 0}
        mask_activity = np.zeros(len(waves))
        all_rcorr = []
        all_rslope = []
        for p in all_proxies:
            c, l, r, lh, rh, lc, rc = find_proxy(p)
            mask_activity[l:r] = 1
            proxy, proxy_std, rcorr, rslope = extract_proxy(p)
            all_rcorr.append(rcorr)
            all_rslope.append(rslope)
            save[p[4]] = proxy
            save[p[4] + "_std"] = proxy_std
        del save["null"]

        all_rcorr = np.array(all_rcorr)
        all_rslope = np.array(all_rslope)

        for n in all_prox_names:
            if n not in save.keys():
                save[n] = np.zeros(len(jdb))
                save[n + "_std"] = np.zeros(len(jdb))

        save_backup = save.copy()

        if optimize:
            save = {"null": 0}
            mask_activity = np.zeros(len(waves))
            for i, p in enumerate(all_proxies):
                mask_activity[all_rslope[i] != 0] = 1
                proxy, proxy_std, dust, dust = extract_proxy(p, kernel=all_rslope[i])
                save[p[4]] = proxy
                save[p[4] + "_std"] = proxy_std
            del save["null"]

            self.all_kernels = all_rslope

        del ratio

        if debug:
            plt.figure(figsize=(18, 9))
            plt.subplot(3, 1, 1)
            plt.plot(wave_ref, all_rslope.T, color="k")
            plt.axhline(y=0, color="r")
            ax = plt.gca()
            plt.subplot(3, 1, 2, sharex=ax)
            plt.plot(wave_ref, all_rcorr.T, color="k")
            plt.axhline(y=0, color="r")
            plt.ylim(-1, 1)
            plt.subplot(3, 1, 3, sharex=ax)
            plt.plot(wave_ref, flux_ref, color="k")
            for p in all_proxies:
                center = myf.doppler_r(p[0], rv_sys)[0]
                hw = p[1]
                plt.axvspan(xmin=center - hw, xmax=center + hw, alpha=0.5, color="r")
                plt.axvline(x=center, color="r")

        load["activity_proxies"] = mask_activity.astype("int")
        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

        if not np.sum(abs(save["CaIIK"])):
            save["CaIIK"] = save["Ha"] + 0.01
            save["CaIIH"] = save["Ha"] + 0.01
            save["CaIIK_std"] = save["Ha_std"]
            save["CaIIH_std"] = save["Ha_std"]
            save_backup["CaIIK"] = save_backup["Ha"]
            save_backup["CaIIH"] = save_backup["Ha"]
            save_backup["CaIIK_std"] = save_backup["Ha_std"]
            save_backup["CaIIH_std"] = save_backup["Ha_std"]

        save["CaII"] = 0.5 * (save["CaIIK"] + save["CaIIH"])
        save["CaII_std"] = 0.5 * np.sqrt((save["CaIIK_std"]) ** 2 + (save["CaIIH_std"]) ** 2)

        save["NaD"] = 0.5 * (save["NaD1"] + save["NaD2"])
        save["NaD_std"] = 0.5 * np.sqrt((save["NaD1_std"]) ** 2 + (save["NaD2_std"]) ** 2)

        save["MgI"] = 0.5 * (save["MgIa"] + save["MgIb"] + save["MgIc"])
        save["MgI_std"] = 0.5 * np.sqrt(
            (save["MgIa_std"]) ** 2 + (save["MgIb_std"]) ** 2 + (save["MgIc_std"]) ** 2
        )

        save_backup["CaII"] = 0.5 * (save_backup["CaIIK"] + save_backup["CaIIH"])
        save_backup["CaII_std"] = 0.5 * np.sqrt(
            (save_backup["CaIIK_std"]) ** 2 + (save_backup["CaIIH_std"]) ** 2
        )

        shift = np.mean(save["CaII"]) - np.mean(save_backup["CaII"])

        save["RHK"] = conv_slope * np.log10(save["CaII"].copy() - shift) + conv_offset
        save["RHK_std"] = (
            conv_slope
            * (save["CaII_std"].copy() + calib_std)
            / abs(save["CaII"].copy() - shift)
            / np.log(10)
        )

        save_backup["RHK"] = conv_slope * np.log10(save_backup["CaII"].copy()) + conv_offset
        save_backup["RHK_std"] = (
            conv_slope
            * (save_backup["CaII_std"].copy() + calib_std)
            / abs(save_backup["CaII"].copy())
            / np.log(10)
        )

        self.all_proxies = save
        self.all_proxies_name = list(save.keys())[::2]

        all_proxies.append([0, 0, 0, 0, "CaII", None])
        all_proxies.append([0, 0, 0, 0, "NaD", None])
        all_proxies.append([0, 0, 0, 0, "MgI", None])
        all_proxies.append([0, 0, 0, 0, "RHK", 1])

        for j in list(save.keys()):
            if len(j.split("_std")) == 2:
                save[j] += calib_std

        if save_kw:
            print("\n Saving activity proxies...")
            if False:  # no more used 02.07.21
                for i, j in enumerate(files):
                    file = pd.read_pickle(j)
                    # print('File (%.0f/%.0f) %s SNR %.0f reduced'%(i+1,len(files),j,snrs[i]))
                    for p in all_proxies:
                        file["parameters"][p[4]] = save[p[4]][i]
                        file["parameters"][p[4] + "_std"] = save[p[4] + "_std"][i]
                    myf.pickle_dump(file, open(j, "wb"))

                self.yarara_analyse_summary()
            else:
                self.yarara_obs_info(kw=pd.DataFrame(save))

        self.ca2k = myc.tableXY(jdb, save["CaIIK"], save["CaIIK_std"] + calib_std)
        self.ca2h = myc.tableXY(jdb, save["CaIIH"], save["CaIIH_std"] + calib_std)
        self.ca2 = myc.tableXY(jdb, save["CaII"], save["CaII_std"] + calib_std)
        self.rhk = myc.tableXY(jdb, save["RHK"], save["RHK_std"])
        self.mg1 = myc.tableXY(jdb, save["MgI"], save["MgI_std"] + calib_std)
        self.mga = myc.tableXY(jdb, save["MgIa"], save["MgIa_std"] + calib_std)
        self.mgb = myc.tableXY(jdb, save["MgIb"], save["MgIb_std"] + calib_std)
        self.mgc = myc.tableXY(jdb, save["MgIc"], save["MgIc_std"] + calib_std)
        self.nad = myc.tableXY(jdb, save["NaD"], save["NaD_std"] + calib_std)
        self.nad1 = myc.tableXY(jdb, save["NaD1"], save["NaD1_std"] + calib_std)
        self.nad2 = myc.tableXY(jdb, save["NaD2"], save["NaD2_std"] + calib_std)
        self.ha = myc.tableXY(jdb, save["Ha"], save["Ha_std"] + calib_std)
        self.hb = myc.tableXY(jdb, save["Hb"], save["Hb_std"] + calib_std)
        self.hc = myc.tableXY(jdb, save["Hc"], save["Hc_std"] + calib_std)
        self.hd = myc.tableXY(jdb, save["Hd"], save["Hd_std"] + calib_std)
        self.heps = myc.tableXY(jdb, save["Heps"], save["Heps_std"] + calib_std)
        self.hed3 = myc.tableXY(jdb, save["HeID3"], save["HeID3_std"] + calib_std)
        self.ca1 = myc.tableXY(jdb, save["CaI"], save["CaI_std"] + calib_std)

        self.infos["latest_dico_activity"] = sub_dico

        if plot:
            phase = myf.get_phase(np.array(self.table.jdb), 365.25)
            for name, modulo, phase_mod in zip(["", "_1year"], [None, 365.25], [None, phase]):
                titles = [
                    "CaII H&K",
                    "CaI",
                    "NaD_left",
                    "NaD_right",
                    "HeID3",
                    "MgIa",
                    "MgIb",
                    "MgIc",
                    r"$H_\alpha$",
                    r"$H_\beta$",
                    r"$H_\gamma$",
                    r"$H_\delta$",
                ]
                plt.figure(figsize=(20, 10))
                plt.subplot(3, 4, 1)
                ax = plt.gca()
                for p in all_proxies:
                    if p[5] is not None:
                        num = p[5]
                        plt.subplot(3, 4, num, sharex=ax)
                        plt.title(titles[num - 1])

                        vec = myc.tableXY(
                            jdb, save_backup[p[4]], save_backup[p[4] + "_std"] + calib_std
                        )
                        vec.plot(
                            capsize=0,
                            zorder=1,
                            color=["k", "r"][int(optimize)],
                            modulo=modulo,
                            phase_mod=phase,
                        )

                        if optimize:
                            vec2 = myc.tableXY(jdb, save[p[4]], save[p[4] + "_std"] + calib_std)
                            vec2.y -= np.mean(vec2.y)
                            vec2.y += np.mean(vec.y)
                            vec2.plot(
                                capsize=0, color="k", zorder=10, modulo=modulo, phase_mod=phase
                            )

                        myf.auto_axis(vec.y, m=5)
                        plt.xlabel("Time")
                        plt.ylabel("Proxy [unit arb.]")
                plt.subplots_adjust(
                    left=0.07, right=0.93, top=0.95, bottom=0.08, wspace=0.3, hspace=0.35
                )
                plt.savefig(self.dir_root + "IMAGES/all_proxies" + name + ".pdf")

    def yarara_stellar_rotation(
        self,
        windows=100,
        min_nb_points=25,
        detrending=True,
        ofac=10,
        pmax=None,
        kw_dico="lbl",
        prot_max=100,
        rhk=1,
        kernel_rhk=0,
        ca2=0,
        ha=1,
        hb=0,
        hc=0,
        h=0,
        wb=0,
        mg1=0,
        nad=0,
        bis=0,
        cb=0,
        rv=0,
    ):
        self.import_table()
        self.import_star_info()
        self.import_material()
        wave_min = np.min(np.array(self.material["wave"]))

        if wave_min > 3970:
            wb = 0
        if wave_min > 4341:
            hc = 0
        if wave_min > 4862:
            hb = 0
        if wave_min > 5185:
            mg1 = 0

        tab = self.table

        rhk_yarara = myc.tableXY(tab["jdb"], tab["RHK"], tab["RHK_std"])
        bv = self.star_info["BV"]["fixed"]
        p84, p84_std, p08, p08_std, age08, age08_std = myf.conv_rhk_prot(np.mean(rhk_yarara.y), bv)

        use = (
            np.array([rhk, kernel_rhk, ca2, ha, hb, hc, h, mg1, nad, wb, cb, bis, str(rv)]) != "0"
        )
        proxy_liste = np.array(
            [
                ["RHK"],
                ["Kernel_CaII"],
                ["CaIIH", "CaIIK"],
                ["Ha"],
                ["Hb"],
                ["Hc"],
                ["Ha", "Hb", "Hc"],
                ["MgIa", "MgIb", "MgIc"],
                ["NaD1", "NaD2"],
                ["WB"],
                ["CB"],
                ["BIS"],
                [["ccf_rv", rv][type(rv) != int]],
            ]
        )[use]
        proxy_sign = np.array(
            [[1], [1], [1, 1], [1], [-1], [-1], [1, -1, -1], [1, 1, 1], [1, 1], [1], [1], [1], [1]]
        )[use]
        proxy_name = np.array(
            ["RHK", "CaII", "CaII", "Ha", "Hb", "Hc", "H", "MgI", "NaD", "WB", "CB", "VSPAN", "RV"]
        )[use]

        proxies = []
        proxies_std = []

        for s, p in zip(proxy_sign, proxy_liste):
            py = np.zeros(len(tab["jdb"]))
            py_err = np.zeros(len(tab["jdb"]))
            for s1, p1 in zip(s, p):
                try:
                    proxy = myc.tableXY(tab["jdb"], tab[p1], tab[p1 + "_std"])
                except:
                    try:
                        proxy = self.import_ccf_timeseries(
                            kw_dico.upper() + "_kitcat_mask_" + self.starname, rv, "rv"
                        )
                    except:
                        proxy = rv
                py += proxy.y * s1
                py_err += proxy.yerr**2
            py_err = np.sqrt(py_err) / len(p)
            py = py / len(p)
            proxies.append(py)
            py_err[py_err == 0] = np.max(py_err) * 5
            proxies_std.append(py_err)
        proxies = np.array(proxies)
        proxies_std = np.array(proxies_std)

        # proxies_std/=np.std(proxies,axis=1)[:,np.newaxis]
        # proxies= (proxies - np.mean(proxies,axis=1)[:,np.newaxis])/np.std(proxies,axis=1)[:,np.newaxis]
        nb_proxy = len(proxy_liste)

        jdb = np.array(tab["jdb"])

        plt.figure(figsize=(21, 13))
        power_max = []
        chi2 = []
        for num in range(nb_proxy):

            proxy = myc.tableXY(jdb, proxies[num], proxies_std[num])
            species = np.array(self.table["ins"]).copy()

            proxy.species_recenter(species=species, ref=0)

            if not num:
                plt.subplot(3, nb_proxy, num + 1)
                ax2 = plt.gca()
            else:
                plt.subplot(3, nb_proxy, num + 1, sharex=ax2)

            proxy.rm_outliers(m=2, kind="inter")
            species = species[proxy.mask]
            proxy.rm_outliers(who="Yerr", m=2, kind="inter")
            species = species[proxy.mask]

            Pmag = -99.9
            Pmag_std = 0
            nb_seasons = (np.max(proxy.x) - np.min(proxy.x)) / 365.25

            min_seasons = 5

            if nb_seasons > min_seasons:  # to avoid long baseline with gap in between
                proxy.binned_scatter(int(nb_seasons) * 2, color="r", Plot=False)
                if np.sum(~np.isnan(proxy.biny)) <= min_seasons:
                    nb_seasons = 1

            proxy.split_species(species)
            proxy.species_recontinuity(
                dist_max=300, alpha=1
            )  # alpha allow to increase the stacking radius
            proxy.merge_species(replace=True)

            if (
                (proxy_name[num] == "RV")
                | (proxy_name[num] == "WB")
                | (proxy_name[num] == "VSPAN")
                | (proxy_name[num] == "CB")
            ):
                nb_seasons = 1
                proxy.rms_w()
                proxy.yerr /= proxy.rms
                proxy.y /= proxy.rms

            proxy.plot(species=species)
            proxy.yerr[proxy.yerr == 0] = np.nanmax(proxy.yerr) * 2

            if nb_seasons <= min_seasons:  # minimum 6 points for the periodogram
                proxy.substract_polyfit(3, replace=True, Draw=True)
            else:
                if detrending:
                    proxy.binned_scatter(int(nb_seasons) * 2, color="r", Plot=False, replace=False)
                    proxy.binned_data.supress_nan()

                    proxy.binned_data.periodogram_keplerian_hierarchical(
                        nb_planet=1,
                        Plot=False,
                        fap=10,
                        deg=1,
                        auto_long_trend=False,
                        min_nb_cycle=0.33,
                        fit_ecc=False,
                    )
                    # proxy.binned_data.periodogram_keplerian_hierarchical(nb_planet=1,Plot=False,fap=10,deg=2,auto_long_trend=False,fit_ecc=False, min_nb_cycle=0.33) # run a second time for some mystic reason ?

                    if len(proxy.binned_data.planet_fitted["p"]):
                        Pmag = proxy.binned_data.planet_fitted["p"][0] / 365.25
                        new_t = np.linspace(min(proxy.x), max(proxy.x), 500)

                        if spleaf_version == "old":
                            curve = proxy.binned_data.rv_model.kep[0].rv(
                                new_t - np.mean(proxy.binned_data.x)
                            )
                            for kpow in range(proxy.binned_data.model_deg + 1):
                                curve += proxy.binned_data.rv_model.get_params(
                                    "lin.drift_pow%.0f" % (kpow)
                                ) * (new_t - np.mean(proxy.binned_data.x)) ** (kpow)

                            model_mag = proxy.binned_data.rv_model.kep[0].rv(
                                proxy.x - np.mean(proxy.binned_data.x)
                            )
                            for kpow in range(proxy.binned_data.model_deg + 1):
                                model_mag += proxy.binned_data.rv_model.get_params(
                                    "lin.drift_pow%.0f" % (kpow)
                                ) * (proxy.x - np.mean(proxy.binned_data.x)) ** (kpow)
                        else:
                            curve = proxy.binned_data.rv_model.keplerian[str(0)].rv(
                                new_t - np.mean(proxy.binned_data.x)
                            )
                            for kpow in range(proxy.binned_data.model_deg + 1):
                                curve += proxy.binned_data.rv_model.get_param(
                                    "lin.drift_pow%.0f" % (kpow)
                                ) * (new_t - np.mean(proxy.binned_data.x)) ** (kpow)

                            model_mag = proxy.binned_data.rv_model.keplerian[str(0)].rv(
                                proxy.x - np.mean(proxy.binned_data.x)
                            )
                            for kpow in range(proxy.binned_data.model_deg + 1):
                                model_mag += proxy.binned_data.rv_model.get_param(
                                    "lin.drift_pow%.0f" % (kpow)
                                ) * (proxy.x - np.mean(proxy.binned_data.x)) ** (kpow)

                        res = proxy.y - model_mag

                        print(
                            "Magnetic cycle estimated around : %.1f years for %s"
                            % (Pmag, proxy_name[num])
                        )

                    if (
                        (not len(proxy.binned_data.planet_fitted["p"])) | (Pmag < 5) | (Pmag > 25)
                    ):  # magnetic cycle below 5 years doubtful
                        proxy.fit_sinus(
                            Draw=False, guess=[np.std(proxy.y), 3500, 0, 0, 0, 0], p_max=10000
                        )
                        if proxy.lmfit.params["period"].stderr is not None:
                            if (proxy.lmfit.params["period"].value < 1000) | (
                                proxy.lmfit.params["period"].value
                                < (2 * proxy.lmfit.params["period"].stderr)
                            ):  # the fit has not converge to a trustworthly value of cycle
                                # proxy.substract_bin(nb_seasons*365.25/(int(nb_seasons)*2),Draw=True,alpha_bin=0,color='#1f77b4',replace=True)
                                Pmag = -99.9
                                # proxy.substract_polyfit(3,replace=True,Draw=True)
                            else:
                                # proxy.y -= proxy.sinus_fitted
                                Pmag = proxy.lmfit.params["period"].value / 365.25
                                Pmag_std = proxy.lmfit.params["period"].stderr / 365.25
                                print(
                                    "Magnetic cycle estimated around : %.1f +/- %.1f years for %s"
                                    % (Pmag, Pmag_std, proxy_name[num])
                                )
                        else:
                            # proxy.substract_bin(nb_seasons*365.25/(int(nb_seasons)*2),Draw=True,alpha_bin=0,color='#1f77b4',replace=True)
                            Pmag = -99.9
                            # proxy.substract_polyfit(3,replace=True,Draw=True)
                    else:
                        # proxy.y = res
                        plt.plot(new_t, curve + np.median(res), color="r", zorder=10)
                    myf.auto_axis(proxy.y)
                    proxy.substract_bin(
                        nb_seasons * 365.25 / (int(nb_seasons) * 2),
                        Draw=True,
                        alpha_bin=0,
                        alpha_p=0,
                        color="orange",
                        replace=True,
                    )

            proxy.rm_outliers(m=2, kind="inter")
            proxy.yerr = proxy.yerr + 0.5 * (
                np.nanpercentile(proxy.y, 84) - np.nanpercentile(proxy.y, 16)
            )
            proxies_std[num] = proxies_std[num] + 0.0 * (
                np.nanpercentile(proxy.y, 84) - np.nanpercentile(proxy.y, 16)
            )

            if proxy_name[num] == "RHK":
                self.Pmag = Pmag
                plt.title(
                    "<RHK> = %.2f\n P84 = %.1f +/- %.1f days\n P08 = %.1f +/- %.1f days\nPmag = %.1f +/- %.1f years"
                    % (np.mean(rhk_yarara.y), p84, p84_std, p08, p08_std, Pmag, Pmag_std),
                    fontsize=10,
                )
            else:
                plt.title("%s (Pmag = %.1f)" % (proxy_name[num], Pmag), fontsize=14)

            plt.xlabel("Time", fontsize=14)
            plt.ylabel("Proxy [unit arb.]", fontsize=14)

            if not num:
                plt.subplot(3, nb_proxy, nb_proxy + num + 1)
                ax = plt.gca()
            else:
                plt.subplot(3, nb_proxy, nb_proxy + num + 1, sharex=ax, sharey=ax)

            proxy.periodogram(nb_perm=1, ofac=ofac, level=None, Plot=True, Norm=True, p_min=1.5)

            per_sig = proxy.period_significant[
                (proxy.period_significant < prot_max) & (proxy.period_significant > 2)
            ]
            pow_sig = proxy.power_significant[
                (proxy.period_significant < prot_max) & (proxy.period_significant > 2)
            ]

            if len(per_sig) == 0:
                per_sig = proxy.period_maxima[proxy.period_maxima < prot_max]
                pow_sig = proxy.power_maxima[proxy.period_maxima < prot_max]
            if len(per_sig) == 0:
                per_sig = np.array([1000])
                pow_sig = np.array([0])

            power_max.append(per_sig[0])
            chi2.append(pow_sig[0])
            plt.title(proxy_name[num] + " (Prot = %.1f)" % (power_max[-1]), fontsize=14)
            plt.scatter(power_max[-1], pow_sig[0], color="r")

        pmin = 1 / np.max(proxy.freq)
        xlim = [pmin, [100, pmax][pmax is not None]]
        plt.xlim(xlim[0], xlim[1])

        plt.subplots_adjust(left=0.06, right=0.94, top=0.9)

        power_max = np.array(power_max)
        chi2 = np.array(chi2)
        if abs(power_max[0] - power_max[1]) / power_max[0] < 0.20:
            prot_estimated = 0.5 * (power_max[0] + power_max[1])  # period found with CaII  and Ha
        else:
            prot_estimated = power_max[np.argmax(chi2[0:3])]  # period found with CaII

        if prot_estimated < 2:
            prot_estimated = power_max[1]  # period found with Ha

        if pmax is None:
            pmax = int(prot_estimated * 3)

        print("\nRotationnal period estimated around Prot = %.1f days " % (prot_estimated))
        self.yarara_star_info(Prot=["YARARA", np.round(prot_estimated, 1)])

        print("\nRotationnal period estimated by N84 calib around Prot = %.1f days " % (p84))
        self.yarara_star_info(Prot=["fixed", np.round(p84, 1)])

        if windows < (2 * prot_estimated):
            windows = 2 * prot_estimated
            print(
                "Windows increased because smaller than 2 times the estimated Prot at %.1f days"
                % (prot_estimated)
            )

        time.sleep(1)

        save = []

        plt.subplots_adjust(left=0.07, right=0.96, top=0.92, bottom=0.07, hspace=0.35, wspace=0.35)
        for num in range(nb_proxy):

            plt.subplot(3, nb_proxy, num + 1 + 2 * nb_proxy)
            plt.xscale("log")
            plt.xlim(xlim[0], xlim[1])

            jdb = np.array(tab["jdb"])
            # plt.title(proxy_name[num],fontsize=13)
            proxy = myc.tableXY(jdb, proxies[num], proxies_std[num])
            # proxy.rm_outliers(m=2,kind='inter')
            jdb_int = np.array(tab["jdb"]).astype("int")
            times = []
            # m2 = []
            # m = np.zeros(len(np.arange(int(jdb_int[0]),int(jdb_int[-1]))))
            time_idx = []
            all_power = []
            jdb_old_min = -1
            jdb_old_max = -1
            count = 0
            liste = np.arange(int(jdb_int[0]), int(jdb_int[-1]))
            for i in tqdm(range(len(liste))):
                j = liste[i]
                mask = (jdb >= j) & (jdb <= j + windows)
                proxy1 = proxy.copy()
                proxy1.x = proxy1.x[mask]
                proxy1.y = proxy1.y[mask]
                proxy1.yerr = proxy1.yerr[mask]
                proxy1.xerr = proxy1.xerr[mask]

                # m[i] = 0
                if len(proxy1.x) > min_nb_points:
                    # m[i] = 1
                    if (jdb_old_min != proxy1.x[0]) | (jdb_old_max != proxy1.x[-1]):
                        # m[i] = 2
                        if (np.max(proxy1.x) - np.min(proxy1.x)) > 1.5 * prot_estimated:
                            # m[i] = 3
                            jdb_old_min = proxy1.x[0].copy()
                            jdb_old_max = proxy1.x[-1].copy()
                            count += 1
                            times.append(np.mean(proxy1.x))
                            time_idx.append(count)
                            proxy1.substract_polyfit(1, replace=True)
                            proxy1.rm_outliers(m=2, kind="inter")
                            proxy1.periodogram(
                                nb_perm=1, ofac=ofac, level=None, Plot=False, p_min=1.5
                            )
                            power = myc.tableXY(
                                proxy1.freq, proxy1.power / proxy1.fap01, 0 * proxy1.freq
                            )
                            power.interpolate(
                                1 / np.linspace(pmin, pmax, 1000),
                                method="linear",
                                interpolate_x=False,
                            )
                            all_power.append(power.y)

            # m2 = np.array(m2)
            all_power = np.array(all_power)
            times = np.array(times)
            time_idx = np.array(time_idx)

            if len(all_power) > 5:
                periods = np.linspace(pmin, pmax, 1000)
                myf.my_colormesh(periods, time_idx, all_power, vmin=0, vmax=2)

                if proxy_name[num] != "RV":
                    plt.axvline(x=power_max[num], color="white", lw=3.5, alpha=0.5)
                    plt.axvline(x=power_max[num], color="r", alpha=0.5)

                mask_period = (periods > (prot_estimated / 1.75)) & (
                    periods < (prot_estimated * 1.75)
                )
                v_max = np.max(all_power[:, mask_period])
                i_max = np.where(all_power[:, mask_period] == v_max)
                if len(i_max[0]):
                    i_max1 = i_max[0][0]
                    i_max2 = i_max[1][0]
                    p_max = periods[mask_period][i_max2]
                    if v_max >= 1:
                        plt.scatter(
                            p_max,
                            time_idx[i_max1],
                            color="yellow",
                            edgecolor="k",
                            marker="*",
                            zorder=10,
                            s=100,
                        )
                        plt.title("Prot : %.1f" % (p_max), fontsize=14)
                    plt.axvline(x=p84, color="white", lw=3.5, alpha=0.5)
                    plt.axvline(x=p84, color="k", alpha=0.5)
                    plt.xlabel("Period [days]", fontsize=14)
                    plt.ylabel("%.0f-days window idx" % (windows), fontsize=14)

            save.append(all_power)

        plt.subplots_adjust(hspace=0.45, wspace=0.45)
        plt.savefig(self.dir_root + "IMAGES/Stellar_rotation.png")

        if rv:
            save = np.array(save)[::-1]
        else:
            save = np.array(save)
        save = np.mean(save, axis=0)
        if len(save) > 5:
            plt.figure(figsize=(7, 7))
            periods = np.linspace(pmin, pmax, 1000)
            plt.title("Rotational period : %.1f [days]" % (prot_estimated), fontsize=14)
            myf.my_colormesh(np.linspace(pmin, pmax, 1000), time_idx, save, vmin=0, vmax=2)
            plt.axvline(x=prot_estimated, color="white", lw=3.5)
            plt.axvline(x=prot_estimated, color="r")
            plt.axvline(x=p84, color="white", lw=3.5)
            plt.axvline(x=p84, color="k")
            mask_period = (periods > (prot_estimated / 1.75)) & (periods < (prot_estimated * 1.75))
            v_max = np.max(save[:, mask_period])
            i_max = np.where(save[:, mask_period] == v_max)
            if len(i_max[0]):
                i_max1 = i_max[0][0]
                i_max2 = i_max[1][0]
                p_max = periods[mask_period][i_max2]
                if v_max >= 1:
                    plt.scatter(
                        p_max,
                        time_idx[i_max1],
                        color="yellow",
                        edgecolor="k",
                        marker="*",
                        zorder=10,
                        s=100,
                    )
                    plt.title("Rotational period : %.1f" % (p_max), fontsize=14)
                plt.xlabel("Period [days]", fontsize=14)
                plt.ylabel("Window (%.0f days) center idx" % (windows), fontsize=14)
                plt.xscale("log")

                plt.savefig(self.dir_root + "IMAGES/Stellar_rotation_average.png")

    def lbl_weight_selection(self, mask, sub_dico="matching_mad"):
        self.import_lbl()
        wtot = np.sum(1 / self.lbl[sub_dico]["lbl_std"][0] ** 2)
        w = np.sum(1 / self.lbl[sub_dico]["lbl_std"][0][mask] ** 2)
        return 100 * w / wtot

    def yarara_sas(
        self,
        sub_dico="matching_morpho",
        kw_dico="lbl",
        g1=[["line_depth", "<", 0.3]],
        g2=[["line_depth", ">", 0.3]],
        valid_lines=True,
        blended_lines=True,
        good_morpho=False,
        telluric_tresh=0.2,
        Plot=False,
        save_proxy=True,
    ):
        """Produce a synthetic activity signal by splitting LBL RVs according to kitcat criterion"""
        self.import_material()
        self.import_kitcat()
        self.import_table()
        self.import_proxies()

        directory = self.directory
        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        if kw_dico == "lbl":
            self.import_lbl()
            imported_rv = self.lbl[sub_dico]
            kitcat = self.lbl[sub_dico]["catalog"]

        elif kw_dico == "lbl_iter":
            self.import_lbl_iter()
            imported_rv = self.lbl_iter[sub_dico]
            kitcat = self.lbl_iter[sub_dico]["catalog"]

        table_rv = myc.table(imported_rv["lbl"][0])

        jdb = np.array(self.table.jdb)

        # vec_lbl = self.import_ccf_timeseries('LBL_kitcat_mask_'+self.starname,sub_dico,'rv') ; vec_lbl.yerr+=0.7

        deep_idx = np.array(
            list(
                kitcat.loc[
                    ((kitcat["valid"]) | (not valid_lines))
                    & ((kitcat["blend_crit"].astype("bool")) | (blended_lines))
                    & ((kitcat["morpho_crit"].astype("bool")) | (not good_morpho))
                    & (kitcat["rel_contam"] < telluric_tresh)
                ].index
            )
        )
        shallow_idx = deep_idx

        backup = deep_idx.copy()

        for g in g1:
            if g[1] == "<":
                idx = np.array(list(kitcat.loc[kitcat[g[0]] < g[2]].index))
            elif g[1] == ">":
                idx = np.array(list(kitcat.loc[kitcat[g[0]] > g[2]].index))
            elif g[1] == "=":
                idx = np.array(list(kitcat.loc[kitcat[g[0]] == g[2]].index))
            deep_idx = deep_idx[np.in1d(deep_idx, idx)]

        for g in g2:
            if g[1] == "<":
                idx = np.array(list(kitcat.loc[kitcat[g[0]] < g[2]].index))
            elif g[1] == ">":
                idx = np.array(list(kitcat.loc[kitcat[g[0]] > g[2]].index))
            elif g[1] == "=":
                idx = np.array(list(kitcat.loc[kitcat[g[0]] == g[2]].index))
            shallow_idx = shallow_idx[np.in1d(shallow_idx, idx)]

        w = self.lbl_weight_selection(backup, sub_dico=sub_dico)
        w1 = self.lbl_weight_selection(deep_idx, sub_dico=sub_dico)
        w2 = self.lbl_weight_selection(shallow_idx, sub_dico=sub_dico)

        print("\nNb total valid lines : %.0f (weight=%.1f)" % (len(backup), w))
        print("Nb lines group 1 : %.0f (weight=%.1f)" % (len(deep_idx), w1))
        print("Nb lines group 2 : %.0f (weight=%.1f)\n" % (len(shallow_idx), w2))

        deep_rv = table_rv.rv_subselection(rv_std=imported_rv["lbl_std"][0], selection=deep_idx)
        deep_rv.x = jdb
        shallow_rv = table_rv.rv_subselection(
            rv_std=imported_rv["lbl_std"][0], selection=shallow_idx
        )
        shallow_rv.x = jdb
        sas = myc.tableXY(
            jdb, shallow_rv.y - deep_rv.y, np.sqrt(deep_rv.yerr**2 + shallow_rv.yerr**2)
        )

        self.rv_g1 = deep_rv
        self.rv_g2 = shallow_rv
        self.table_g1 = kitcat.loc[deep_idx]
        self.table_g2 = kitcat.loc[shallow_idx]
        self.sas = sas

        if Plot:
            plt.figure()
            plt.subplot(4, 1, 1)
            deep_rv.rms_w()
            shallow_rv.rms_w()

            deep_rv.plot(color="b", label="G1", zorder=int(deep_rv.rms < shallow_rv.rms) + 1)
            shallow_rv.plot(color="r", label="G2", zorder=int(deep_rv.rms > shallow_rv.rms) + 1)
            plt.legend()

            plt.subplot(4, 1, 2)
            deep_rv.periodogram(
                color="b", Norm=True, zorder=int(deep_rv.rms < shallow_rv.rms) + 1, p_min=0.7
            )
            shallow_rv.periodogram(
                color="r", Norm=True, zorder=int(deep_rv.rms > shallow_rv.rms) + 1, p_min=0.7
            )

            plt.subplot(4, 1, 3)
            sas.plot(color="k")

            plt.subplot(4, 1, 4)
            sas.periodogram(Norm=True, p_min=0.7)

        if save_proxy:
            for i, j in enumerate(files):
                file = pd.read_pickle(j)
                file["parameters"]["SAS"] = sas.y[i]
                file["parameters"]["SAS_std"] = sas.yerr[i]
                myf.pickle_dump(file, open(j, "wb"))

            self.yarara_analyse_summary()

    def yarara_produce_manual_master_slice(
        self, sub_dico="matching_color", var="s", smooth_box=5, name_ext="", modulo_time=0
    ):
        """To launch in an Ipython terminal"""
        database = pd.read_pickle(
            root + "/Yarara/database/%s/%s_pca_vec_fitted.p" % (self.instrument, self.instrument)
        )[sub_dico]
        all_stars = database.keys()
        all_slices = []
        counter = -1
        for s in all_stars:
            matrix = database[s]["catalog"]
            p = np.hstack([matrix["pixels_l1"], matrix["pixels_l2"]])
            w = np.hstack([matrix["wave"], matrix["wave"]])
            nb_comp = sum(myf.string_contained_in(list(matrix.keys()), "r_proxy_")[0]) / 2
            m = p != 0
            p = p[m]
            w = w[m]

            slices = []
            for i in range(int(nb_comp)):
                counter += 1
                c = np.hstack(
                    [matrix[var + "_proxy_%.0f" % (i + 1)], matrix[var + "_proxy_%.0f" % (i + 1)]]
                )
                c -= np.nanmedian(c)
                c /= np.nanstd(c)
                r = np.hstack([matrix["r_proxy_%.0f" % (i + 1)], matrix["r_proxy_%.0f" % (i + 1)]])
                c = c[m]
                r = r[m]
                order = np.argsort(abs(r))
                p2 = p[order]
                w2 = w[order]
                c2 = c[order]
                r2 = r[order]
                slices.append(np.vstack([p2, w2, c2, r2]))

            slices = np.array(slices)
            all_slices.append(slices)

        fig = plt.figure(figsize=(18, 14))
        plt.subplots_adjust(left=0.07, right=0.93, top=0.93, bottom=0.07, hspace=0.35, wspace=0.35)
        s2 = 5
        s1 = counter // s2 + int(counter % s2 != 0)
        t = 0
        l_axes = []
        save = []
        for j in range(len(all_slices)):
            for i in range(len(all_slices[j])):
                t += 1
                plt.subplot(s1, s2 * 2, 2 * t - 1)
                plt.title(list(all_stars)[j] + "(%.0f)" % (i + 1))
                plt.scatter(
                    all_slices[j][i, 0, :],
                    all_slices[j][i, 1, :],
                    c=all_slices[j][i, 2, :],
                    cmap="seismic",
                    vmin=-2,
                    vmax=2,
                    alpha=0.5,
                    s=6,
                )
                plt.xlim(np.min(p), np.max(p))
                plt.ylim(np.min(w), np.max(w))
                plt.tick_params(labelleft=False, labelbottom=False, left=False, bottom=False)
                ax = plt.subplot(s1, s2 * 2, 2 * t)
                if modulo_time:
                    plt.scatter(
                        database[list(all_stars)[j]]["jdb"] % modulo_time,
                        database[list(all_stars)[j]]["base_vec"][i + 1],
                        s=5,
                        c="k",
                    )
                else:
                    plt.scatter(
                        database[list(all_stars)[j]]["jdb"],
                        database[list(all_stars)[j]]["base_vec"][i + 1],
                        s=5,
                        c="k",
                    )
                s = ax.get_position().get_points()
                sx, sy = np.diff(s, axis=0)[0]
                ax2 = plt.axes([s[0, 0], s[1, 1], sx, 0.02])
                plt.imshow(
                    myf.produce_transparent(np.zeros((10, 10)), val1=t, val2=j, val3=i, val4=0),
                    aspect="auto",
                )
                ax2 = plt.text(5, 5, "0", ha="center", va="center")
                plt.tick_params(labelleft=False, labelbottom=False, left=False, bottom=False)
                l_axes.append(ax2)
                save.append([j, i])

        class Index:
            vector = np.zeros(counter)

        callback = Index()

        def update(rgba):
            callback.vector[rgba[0] - 1] += 1
            callback.vector = (callback.vector + 1) % 3 - 1
            callback.vector = callback.vector.astype("int")
            l_axes[rgba[0] - 1].set_text(int(callback.vector[rgba[0] - 1]))
            print(callback.vector)

        def onclick(event):
            xdata = event.xdata
            ydata = event.ydata
            x = event.x
            y = event.y
            if xdata is None:
                xdata = 0
            if ydata is None:
                ydata = 0
            if event.button:
                click_event = event.inaxes.get_images()
                if len(click_event):
                    rgba = click_event[0].get_cursor_data(
                        event
                    )  # get the filter rgba value above the image
                    update(rgba)
                    plt.draw()
                else:
                    plt.savefig(
                        root
                        + "/Yarara/database/%s/%s_%s_slice%s.png"
                        % (self.instrument, self.instrument, sub_dico, name_ext)
                    )
                    plt.close("all")

        fig.canvas.mpl_connect("button_press_event", onclick)
        plt.show(block=True)

        loc = np.where(callback.vector != 0)[0]

        complete_map = []
        for k in loc:
            j, i = save[k]
            complete_map.append(
                np.vstack(
                    [
                        all_slices[j][i, 0, :],
                        all_slices[j][i, 1, :],
                        callback.vector[k] * all_slices[j][i, 2, :],
                        callback.vector[k] * all_slices[j][i, 3, :],
                    ]
                )
            )
        complete_map = np.hstack(complete_map)
        order = np.argsort(complete_map[1])
        complete_map = complete_map[:, order]
        mask = np.isnan(complete_map[2])
        complete_map = complete_map[:, ~mask]
        self.slice_map_produced = complete_map

        test = myc.tableXY(complete_map[1].copy(), complete_map[2].copy())
        test.reject_twin()  # understand what is happening
        model_x = myf.smooth(test.x, box_pts=smooth_box, shape="rectangular")
        model_y = myf.smooth(test.y, box_pts=smooth_box, shape="rectangular")
        model = myc.tableXY(model_x, model_y)
        model.order()
        model.interpolate(new_grid=complete_map[1], replace=False, method="linear")

        scale = myf.mad(complete_map[2]) / myf.mad(model_y)

        plt.figure(figsize=(15, 7))
        order = np.argsort(abs(complete_map[3]))
        plt.subplot(1, 2, 1)
        plt.scatter(
            complete_map[0][order],
            complete_map[1][order],
            c=complete_map[2][order],
            vmin=-2,
            vmax=2,
            cmap="seismic",
            alpha=0.5,
        )
        ax2 = plt.gca()
        ax = plt.colorbar(pad=0)
        ax.ax.set_ylabel(r"Z score $(\gamma)$", fontsize=15)
        plt.xlabel("Pixels", fontsize=15)
        plt.ylabel(r"Wavelength $\lambda$ [$\AA$]", fontsize=15)
        plt.title("Master slice")

        order = np.argsort(abs(model.y_interp))
        plt.subplot(1, 2, 2, sharex=ax2, sharey=ax2)
        plt.scatter(
            complete_map[0][order],
            complete_map[1][order],
            c=model.y_interp[order],
            vmin=-2,
            vmax=2,
            cmap="seismic",
            alpha=0.5,
        )
        ax = plt.colorbar(pad=0)
        ax.ax.set_ylabel(r"Z score $(\gamma)$", fontsize=15)
        plt.xlabel("Pixels", fontsize=15)
        plt.ylabel(r"Wavelength $\lambda$ [$\AA$]", fontsize=15)
        plt.title("Smoothed model")
        plt.subplots_adjust(left=0.07, right=0.96, wspace=0.3)

        plt.show(block=False)

        plt.savefig(
            root
            + "/Yarara/database/%s/%s_slice%s.png" % (self.instrument, self.instrument, name_ext)
        )

        kernels_file = root + "/Yarara/database/%s/%s_manual_kernels.p" % (
            self.instrument,
            self.instrument,
        )
        if os.path.exists(kernels_file):
            slices = pd.read_pickle(kernels_file)
        else:
            slices = {}

        slices["kernel" + name_ext] = {"var": pd.DataFrame({"wave": model.x}), "strength": model.y}
        myf.pickle_dump(slices, open(kernels_file, "wb"))

    def yarara_produce_auto_master_slice(
        self,
        sub_dico="matching_color",
        var="s",
        smooth_box=5,
        snr_min=0,
        r_min=0.40,
        m_out=3,
        frac_affected=0.33,
        num0=1,
        modulo=0,
    ):
        """To launch in an Ipython terminal"""
        self.import_material()
        master_wave = np.array(self.material.wave)

        database = pd.read_pickle(
            root + "/Yarara/database/%s/%s_pca_vec_fitted.p" % (self.instrument, self.instrument)
        )[sub_dico]
        all_stars = database.keys()
        snr = []
        for s in all_stars:
            snr.append(database[s]["snr_med"])

        mask_snr = np.array(snr) > snr_min
        all_stars = np.array(list(all_stars))[mask_snr]

        nb_stars = len(all_stars)

        all_slices = []
        counter = -1
        for s in all_stars:
            matrix = database[s]["catalog"]
            p = np.hstack([matrix["pixels_l1"], matrix["pixels_l2"]])
            w = np.hstack([matrix["wave"], matrix["wave"]])
            nb_comp = sum(myf.string_contained_in(list(matrix.keys()), "r_proxy_")[0]) / 2
            m = p != 0
            p = p[m]
            w = w[m]

            slices = []
            for i in range(int(nb_comp)):
                counter += 1
                c = np.hstack(
                    [matrix[var + "_proxy_%.0f" % (i + 1)], matrix[var + "_proxy_%.0f" % (i + 1)]]
                )
                c -= np.nanmedian(c)
                c /= np.nanstd(c)
                r = np.hstack([matrix["r_proxy_%.0f" % (i + 1)], matrix["r_proxy_%.0f" % (i + 1)]])
                c = c[m]
                r = r[m]
                order = np.argsort(abs(r))
                p2 = p[order]
                w2 = w[order]
                c2 = c[order]
                r2 = r[order]
                slices.append(np.vstack([p2, w2, c2, r2]))

            slices = np.array(slices)
            all_slices.append(slices)

        fig = plt.figure(figsize=(18, 14))
        plt.subplots_adjust(left=0.04, right=0.97, top=0.95, bottom=0.05, hspace=0.35, wspace=0.20)
        s2 = 5
        s1 = counter // s2 + int(counter % s2 != 0)
        t = 0
        l_axes = []
        save = []
        all_vec = []
        all_vec_name = []
        for j in range(len(all_slices)):
            for i in range(len(all_slices[j])):
                t += 1
                time_jdb = np.array(database[list(all_stars)[j]]["jdb"])
                plt.subplot(s1, s2 * 2, 2 * t - 1)
                plt.scatter(
                    all_slices[j][i, 0, :],
                    all_slices[j][i, 1, :],
                    c=all_slices[j][i, 2, :],
                    cmap="seismic",
                    vmin=-2,
                    vmax=2,
                    alpha=0.5,
                    s=6,
                )
                plt.xlim(np.min(p), np.max(p))
                plt.ylim(np.min(w), np.max(w))
                plt.tick_params(labelleft=False, labelbottom=False, left=False, bottom=False)
                ax = plt.subplot(s1, s2 * 2, 2 * t)
                plt.title(list(all_stars)[j] + "(%.0f)" % (i + 1), fontsize=8)
                plt.scatter(
                    [(time_jdb - myf.get_phase(time_jdb, modulo)) % modulo, time_jdb][
                        int(modulo == 0)
                    ],
                    database[list(all_stars)[j]]["base_vec"][i + 1],
                    s=5,
                    c="k",
                )
                plt.tick_params(labelleft=False, labelbottom=False, left=False, bottom=False)
                save.append([j, i])

                test = myc.tableXY(all_slices[j][i, 1, :], all_slices[j][i, 2, :])
                test.supress_nan()
                test.order()
                if m_out:
                    test.rm_outliers(kind="inter", m=m_out)
                test.interpolate(
                    new_grid=master_wave, method="linear", replace=False, interpolate_x=False
                )
                model_x = myf.smooth(test.x, box_pts=smooth_box, shape="rectangular")
                model_y = myf.smooth(test.y, box_pts=smooth_box, shape="rectangular")
                model = myc.tableXY(model_x, model_y)
                model.interpolate(
                    new_grid=master_wave, method="linear", replace=False, interpolate_x=False
                )
                all_vec.append(model.y_interp)
                all_vec_name.append(list(all_stars)[j] + "_v%.0f" % (i + 1))

        plt.savefig(
            root
            + "/Yarara/database/%s/%s_%s_all_slices.png"
            % (self.instrument, self.instrument, sub_dico)
        )

        all_vec_name = np.array(all_vec_name)
        all_vec = np.array(all_vec)
        master_wave = master_wave[~np.isnan(np.sum(all_vec, axis=0))]
        all_vec = all_vec[:, ~np.isnan(np.sum(all_vec, axis=0))]

        coeff = myc.table(pd.DataFrame(all_vec.T, columns=all_vec_name))

        plt.figure(figsize=(12, 12))
        coeff.r_matrix(name=list(coeff.table.keys()), absolute=False, light_plot=True, angle=90)
        plt.close()
        coeff.matrix_corr *= 1 - np.diag(np.ones(len(coeff.matrix_corr)))
        var_kept = np.array(list(coeff.table.keys()))[
            np.where(np.sum(abs(coeff.matrix_corr) > r_min, axis=0) != 0)[0]
        ]

        plt.figure(figsize=(12, 12))
        coeff.r_matrix(
            name=var_kept, absolute=False, light_plot=True, angle=90, vmin=-1.2, vmax=1.2
        )
        plt.close()

        block_order, dust = myf.block_matrix2(coeff.matrix_corr.copy())  # ,r_min=r_min)
        plt.figure(figsize=(12, 12))
        coeff.r_matrix(
            name=var_kept[block_order],
            absolute=False,
            light_plot=True,
            angle=90,
            vmin=-1.2,
            vmax=1.2,
        )

        if True:  # new borders algo 13.09.21
            binary_matrix = abs(coeff.matrix_corr) > r_min
            borders = []
            i = 0
            for j in np.arange(1, len(binary_matrix)):
                # if np.sum(binary_matrix[j,i:j])<(j-i)*0.5:
                if np.sum(binary_matrix[j : j + 1, i:j]) < (j - i) * 0.5:
                    # if (j-i)>cluster_min_size:
                    borders.append([i, j])
                    i = j
                    plt.axvline(x=j - 0.5, color="k")
                    plt.axhline(y=j - 0.5, color="k")
            borders.append([borders[-1][1], len(binary_matrix) - 1])
            cluster_loc = np.unique(np.hstack(borders))
        else:
            binary_matrix = abs(coeff.matrix_corr) > r_min
            cluster_loc = []
            for j in range(1, len(binary_matrix)):
                if not np.sum(binary_matrix[j, 0:j]):
                    cluster_loc.append(j)
                    plt.axvline(x=j - 0.5, color="k")
                    plt.axhline(y=j - 0.5, color="k")
            plt.close()
            cluster_loc = [0] + cluster_loc + [len(binary_matrix)]

        mask_kept = np.ones(len(binary_matrix))
        for i, j in zip(cluster_loc[0:-1], cluster_loc[1:]):
            for k in np.arange(i, j):
                if k != i:
                    if not np.sum(binary_matrix[k, i:k]):
                        mask_kept[k] = False
                else:
                    if not np.sum(binary_matrix[k, k : j + 1]):
                        mask_kept[k] = False
        mask_kept = mask_kept.astype("bool")

        plt.figure(figsize=(12, 12))
        coeff.r_matrix(
            name=var_kept[block_order][mask_kept],
            absolute=False,
            light_plot=True,
            angle=90,
            vmin=-1.2,
            vmax=1.2,
        )

        if True:  # new borders algo 13.09.21
            binary_matrix = abs(coeff.matrix_corr) > r_min
            borders = []
            i = 0
            for j in np.arange(1, len(binary_matrix)):
                # if np.sum(binary_matrix[j,i:j])<(j-i)*0.5:
                if np.sum(binary_matrix[j : j + 1, i:j]) < (j - i) * 0.5:
                    # if (j-i)>cluster_min_size:
                    borders.append([i, j])
                    i = j
                    plt.axvline(x=j - 0.5, color="k")
                    plt.axhline(y=j - 0.5, color="k")
            borders.append([borders[-1][1], len(binary_matrix) - 1])
            cluster_loc = np.unique(np.hstack(borders))
        else:
            binary_matrix = abs(coeff.matrix_corr) > r_min
            cluster_loc = []
            for j in range(1, len(binary_matrix)):
                if not np.sum(binary_matrix[j, 0:j]):
                    cluster_loc.append(j)
                    plt.axvline(x=j - 0.5, color="k")
                    plt.axhline(y=j - 0.5, color="k")
            cluster_loc = [0] + cluster_loc + [len(binary_matrix)]

        plt.savefig(
            root
            + "/Yarara/database/%s/%s_%s_slices_ordering_cluster.png"
            % (self.instrument, self.instrument, sub_dico)
        )

        self.debug = (coeff, all_slices)

        cluster_min_size = np.round(nb_stars * frac_affected, 0)
        self.slice_map_auto_produced = []
        self.slice_vec_auto_produced = []
        counter = num0 - 1
        nb_cluster = np.sum(cluster_loc[1:] - cluster_loc[0:-1] > cluster_min_size)
        for n, m in zip(cluster_loc[0:-1], cluster_loc[1:]):
            if (m - n) > cluster_min_size:
                counter += 1

                loc = np.array(
                    [
                        np.where(np.in1d(all_vec_name, name_comp))[0][0]
                        for name_comp in var_kept[block_order][mask_kept][n:m]
                    ]
                )
                fig = plt.figure(69, figsize=(18, 6))
                plt.subplots_adjust(
                    left=0.04, right=0.97, top=0.95, bottom=0.12, hspace=0.35, wspace=0.20
                )
                vec = 0
                plt.subplot(nb_cluster, 1, counter)
                for l, k in enumerate(loc):
                    sign_component = np.sign(coeff.matrix_corr[n, n + l])
                    plt.plot(
                        master_wave,
                        sign_component * coeff.table[all_vec_name[k]],
                        color=None,
                        alpha=0.5,
                        label=all_vec_name[k].split("_")[0],
                    )
                    vec += sign_component * coeff.table[all_vec_name[k]]
                plt.axhline(y=0, color="k", ls=":")
                plt.plot(master_wave, vec / len(loc), color="k")
                plt.ylim(-4, 3)
                plt.ylabel(r"Z-score ($Z_{%.0f}$)" % (counter), fontsize=16)
                plt.xlabel(r"Wavelength $\lambda$ [$\AA$]", fontsize=16)
                plt.legend(loc=3, ncol=10)
                plt.xlim(np.min(master_wave), np.max(master_wave))
                plt.savefig(
                    root
                    + "/Yarara/database/%s/%s_Z_autoslice_%s.png"
                    % (self.instrument, self.instrument, sub_dico)
                )
                self.slice_vec_auto_produced.append(vec / len(loc))

                fig = plt.figure(figsize=(18, 14))
                plt.subplots_adjust(
                    left=0.04, right=0.97, top=0.95, bottom=0.05, hspace=0.35, wspace=0.20
                )

                s1, s2 = myf.best_grid_size(len(loc))
                t = 0
                complete_map = []
                for l, k in enumerate(loc):
                    j, i = save[k]
                    sign_component = np.sign(coeff.matrix_corr[n, n + l])
                    complete_map.append(
                        np.vstack(
                            [
                                all_slices[j][i, 0, :],
                                all_slices[j][i, 1, :],
                                sign_component * all_slices[j][i, 2, :],
                                sign_component * all_slices[j][i, 3, :],
                            ]
                        )
                    )
                    t += 1
                    time_jdb = database[list(all_stars)[j]]["jdb"]
                    plt.subplot(s1, s2 * 2, 2 * t - 1)
                    plt.scatter(
                        all_slices[j][i, 0, :],
                        all_slices[j][i, 1, :],
                        c=sign_component * all_slices[j][i, 2, :],
                        cmap="seismic",
                        vmin=-2,
                        vmax=2,
                        alpha=0.5,
                        s=6,
                    )
                    plt.xlim(np.min(p), np.max(p))
                    plt.ylim(np.min(w), np.max(w))
                    plt.tick_params(labelleft=False, labelbottom=False, left=False, bottom=False)
                    ax = plt.subplot(s1, s2 * 2, 2 * t)
                    plt.title(list(all_stars)[j] + "(%.0f)" % (i + 1), fontsize=8)
                    plt.scatter(
                        [(time_jdb - myf.get_phase(time_jdb, modulo)) % modulo, time_jdb][
                            int(modulo == 0)
                        ],
                        sign_component * database[list(all_stars)[j]]["base_vec"][i + 1],
                        s=5,
                        c="k",
                    )
                    plt.tick_params(labelleft=False, labelbottom=False, left=False, bottom=False)
                plt.savefig(
                    root
                    + "/Yarara/database/%s/%s_selection_autoslice_%.0f_%s.png"
                    % (self.instrument, self.instrument, counter, sub_dico)
                )

                complete_map = np.hstack(complete_map)
                order = np.argsort(complete_map[1])
                complete_map = complete_map[:, order]
                mask = np.isnan(complete_map[2])
                complete_map = complete_map[:, ~mask]
                self.slice_map_auto_produced.append(complete_map)

                test = myc.tableXY(complete_map[1], complete_map[2])
                test.reject_twin()  # understand what is happening
                model_x = myf.smooth(test.x, box_pts=smooth_box, shape="rectangular")
                model_y = myf.smooth(test.y, box_pts=smooth_box, shape="rectangular")
                model = myc.tableXY(model_x, model_y)
                model.order()
                model.interpolate(new_grid=complete_map[1], replace=False, method="linear")

                scale = myf.mad(complete_map[2]) / myf.mad(model_y)

                plt.figure(figsize=(15, 7))
                order = np.argsort(abs(complete_map[3]))
                plt.subplot(1, 2, 1)
                plt.scatter(
                    complete_map[0][order],
                    complete_map[1][order],
                    c=complete_map[2][order],
                    vmin=-2,
                    vmax=2,
                    cmap="seismic",
                    alpha=0.5,
                )
                ax2 = plt.gca()
                ax = plt.colorbar(pad=0)
                ax.ax.set_ylabel(r"Z score $(\gamma)$", fontsize=15)
                plt.xlabel("Pixels", fontsize=15)
                plt.ylabel(r"Wavelength $\lambda$ [$\AA$]", fontsize=15)
                plt.title("Master slice")

                scale = 1
                order = np.argsort(abs(model.y_interp))
                plt.subplot(1, 2, 2, sharex=ax2, sharey=ax2)
                plt.scatter(
                    complete_map[0][order],
                    complete_map[1][order],
                    c=model.y_interp[order],
                    vmin=-2 / scale,
                    vmax=2 / scale,
                    cmap="seismic",
                    alpha=0.5,
                )
                ax = plt.colorbar(pad=0)
                ax.ax.set_ylabel(r"Z score $(\gamma)$", fontsize=15)
                plt.xlabel("Pixels", fontsize=15)
                plt.ylabel(r"Wavelength $\lambda$ [$\AA$]", fontsize=15)
                plt.title("Smoothed model")
                plt.subplots_adjust(left=0.07, right=0.96, wspace=0.3)

                plt.show(block=False)

                plt.savefig(
                    root
                    + "/Yarara/database/%s/%s_autoslice%.0f.png"
                    % (self.instrument, self.instrument, counter)
                )

                kernels_file = root + "/Yarara/database/%s/%s_auto_kernels.p" % (
                    self.instrument,
                    self.instrument,
                )
                if os.path.exists(kernels_file):
                    slices = pd.read_pickle(kernels_file)
                else:
                    slices = {}

                slices["kernel_%.0f" % (counter)] = {
                    "var": pd.DataFrame({"wave": master_wave}),
                    "strength": vec / len(loc),
                    "var2": pd.DataFrame({"wave": model.x}),
                    "strength2": model.y,
                }
                myf.pickle_dump(slices, open(kernels_file, "wb"))

    def yarara_correct_shell(
        self,
        sub_dico="matching_mad",
        save_correction=True,
        reduction="empca",
        reference="median",
        ratio=False,
        continuum="linear",
        offset=False,
        substract_map=[],
        add_map=[],
        continuum_absorption=True,
        mask_rejection=True,
        fap=0.1,
        wave_min=None,
        wave_max=None,
        m=2,
        m_shell=3,
        kind="inter",
        nb_comp=10,
        nb_comp_kept=5,
        power_max=None,
        min_element_nb=30,
        snr_min=1,
        ordering="lbl_var",
        force_reduction=False,
        blended_lines=True,
        photon_noise=0.7,
        distortion_amplitude=0.05,
        proxies_to_corr=None,
        paper_plot=False,
        cross_validation=False,
        cv_sim=100,
        cv_percent_rm=10,
        cv_frac_affected=0.01,
        treshold_percent=95,
        p_noise=1 / np.inf,
        rv_ref=None,
        vec_binning=None,
    ):

        """lbl_var,rvm_rms,laplacien or divergence for ordering pca vectors"""
        myf.print_box("\n---- RECIPE : CORRECTION LINE PROFILE ACTIVITY ----\n")

        directory = self.directory

        rv_dace = self.import_dace_sts(substract_model=False)

        zoom = self.zoom
        smooth_map = self.smooth_map
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100
        cmap = self.cmap
        planet = self.planet

        self.import_material()
        self.import_table()
        self.import_star_info()

        load = self.material
        snr = np.array(self.table.snr)
        sigma_ccf = self.star_info["FWHM"]["YARARA"] / 2.355

        self.import_kitcat()
        kitcat = self.kitcat["catalogue"]

        level = 1 - fap / 100

        epsilon = 1e-12
        c_lum = 299.792e6

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        if cross_validation:
            if treshold_percent < (100 - cv_percent_rm):
                # treshold_percent = (100-cv_percent_rm/2)
                print(" [WARNING] Value of treshold lower that %.0f" % (100 - cv_percent_rm))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        file = self.import_spectrum()
        step = file[sub_dico]["parameters"]["step"]

        all_flux = []
        all_flux_std = []
        conti = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                grid = file["wave"]
                hole_left = file["parameters"]["hole_left"]
                hole_right = file["parameters"]["hole_right"]
            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            all_flux.append(f_norm)
            all_flux_std.append(f_norm_std)
            conti.append(c)

        step = file[sub_dico]["parameters"]["step"]

        all_flux = np.array(all_flux)
        all_flux_std = np.array(all_flux_std)
        conti = np.array(conti)

        # noise_matrix, noise_values = self.yarara_poissonian_noise(noise_wanted=p_noise, wave_ref=None, flat_snr=True)
        # all_flux+=noise_matrix
        # all_flux_std = np.sqrt(all_flux_std**2+noise_values**2)

        minimum_flux = np.nanmin(all_flux, axis=0)
        mask_neg_flux = (minimum_flux > 0).astype("bool")

        if reference == "snr":
            print(" [INFO] Reference spectrum : %.0f" % (snr.argmax()))
            ref = all_flux[snr.argmax()]
        elif reference == "master":
            print(" [INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print(" [INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = all_flux[reference]
        else:
            print(" [INFO] Reference spectrum : median")
            ref = np.median(all_flux, axis=0)

        if wave_min is None:
            wave_min = np.min(kitcat["wave"])
            print(" [INFO] Wave_min updated to a value of : %.2f" % (wave_min))
        else:
            print(" [INFO] Wave_min fixed to a value of : %.2f" % (wave_min))

        if wave_max is None:
            wave_max = np.max(kitcat["wave"])
            print(" [INFO] Wave_max updated to a value of : %.2f" % (wave_max))
        else:
            print(" [INFO] Wave_max fixed to a value of : %.2f" % (wave_max))

        diff_backup, diff_backup_std, wave = self.yarara_map(
            sub_dico=sub_dico,
            wave_min=None,
            wave_max=None,
            substract_map=substract_map,
            add_map=add_map,
            Plot=False,
            reference=reference,
            planet=bool(planet),
            p_noise=p_noise,
        )

        corr_factor = int(continuum_absorption)
        spectrum_norm = myc.tableXY(
            load["wave"],
            load["reference_spectrum"]
            * ((1 - corr_factor) + load["correction_factor"] * corr_factor),
        )

        if ratio:
            diff_backup /= spectrum_norm.y
        diff = diff_backup.copy()

        mask_wavelength = (grid >= wave_min) & (grid <= wave_max)
        mask_wavelength = mask_wavelength & mask_neg_flux

        if mask_rejection:
            load["rejected"] = load["rejected"].astype("bool")
            load["mask_brute"] = load["mask_brute"].astype("bool")
            mask_wavelength = (mask_wavelength) & (~load["mask_brute"]) & (~load["rejected"])

        s1 = spectrum_norm.y
        ds1 = np.gradient(s1) / np.gradient(spectrum_norm.x)

        spectrum_norm.masked(mask_wavelength)
        diff_backup = diff_backup[:, mask_wavelength]
        diff_backup_std = diff_backup_std[:, mask_wavelength]

        s1 = s1[mask_wavelength]
        ds1 = ds1[mask_wavelength]
        wave_kept = wave[mask_wavelength]

        save = (s1.copy(), ds1.copy())

        try:
            if force_reduction:
                print(pouet)
            new_matrix = self.shell[sub_dico]["f"]
            new_matrix_std = self.shell[sub_dico]["f_std"]
            x = self.shell[sub_dico]["x"]
            y = self.shell[sub_dico]["y"]
            x2 = self.shell[sub_dico]["x2"]
            y2 = self.shell[sub_dico]["y2"]
            x_inter = self.shell[sub_dico]["x_inter"]
            y_inter = self.shell[sub_dico]["y_inter"]
            s1 = self.shell[sub_dico]["spec"]
            ds1 = self.shell[sub_dico]["grad_spec"]
            closest_point = self.shell[sub_dico]["match"]
            closest_point_distance = self.shell[sub_dico]["match_dist"]
            index_wave = self.shell[sub_dico]["index_wave"]
            nb = self.shell[sub_dico]["nb"]
            nb_norm = self.shell[sub_dico]["nb_norm"]
        except:
            try:
                del self.shell
            except:
                pass

            diff_backup /= spectrum_norm.x / c_lum
            diff_backup_std /= spectrum_norm.x / c_lum

            mask = (s1 > 0.05) & (s1 < 0.95)

            print(
                "\n [INFO] Number of pixels used : %.0f/%.0f (%.1f %%)\n"
                % (sum(mask), len(mask), 100 * sum(mask) / len(mask))
            )
            text_info = "Wavelength used : %.1f %%" % (100 * sum(mask) / len(mask))

            bline = int(blended_lines)
            if int(bline) != 1:
                continuum = 1 - kitcat["line_depth"] + kitcat["depth_rel"]
                criterion = ((continuum > 0.9) & (kitcat["morpho_crit"] == 1)).astype("int")
                criterion = (abs(criterion + bline)).astype("bool")
                valid_lines = kitcat.loc[criterion]  # &(kitcat['blend_crit']==1)]
                liste = np.array(valid_lines.index)
                kitcat_vec = np.array(load["kitcat_vec"])[mask_wavelength]
                mask = mask & (np.in1d(kitcat_vec, liste))
                print(
                    " [INFO] Number of pixels used : %.0f/%.0f (%.1f %%)\n"
                    % (sum(mask), len(mask), 100 * sum(mask) / len(mask))
                )

                if False:
                    plt.figure()
                    plt.scatter(spectrum_norm.x, spectrum_norm.y, color="r")
                    spectrum_norm.masked(mask)
                    plt.scatter(spectrum_norm.x, spectrum_norm.y, color="b")

            ds1 = ds1[mask]
            s1 = s1[mask]
            wave_kept = wave_kept[mask]
            diff_backup = diff_backup[:, mask]
            diff_backup_std = diff_backup_std[:, mask]
            index_wave = np.arange(len(mask))[mask]

            mask = myf.rm_outliers(ds1, m=5, kind="inter")[0]  # rm unreliable gradient outliers
            ds1 = ds1[mask]
            s1 = s1[mask]
            diff_backup = diff_backup[:, mask]
            diff_backup_std = diff_backup_std[:, mask]
            index_wave = index_wave[mask]
            wave_kept = wave_kept[mask]

            new_matrix = []
            new_matrix_std = []

            grad_lim = np.min([abs(np.min(ds1)), abs(np.max(ds1))])
            x_grid = np.linspace(-grad_lim, grad_lim, 10)
            y_grid = np.linspace(np.min(s1), np.max(s1), 10)

            for l in tqdm(np.arange(np.shape(diff_backup)[0])):
                a, b = myf.grid_binning(
                    x_grid, y_grid, ds1, s1, diff_backup[l], diff_backup_std[l], Draw=False
                )
                a = a.loc[a["nb"] >= min_element_nb]

                z = np.ravel(a["binz"])
                zerr = np.ravel(a["binzerr"])

                new_matrix.append(z)
                new_matrix_std.append(zerr)

            wave_bin_table, dust = myf.grid_binning(
                x_grid, y_grid, ds1, s1, wave_kept, wave_kept * 0 + 1, Draw=False
            )
            wave_bin_table = wave_bin_table.loc[wave_bin_table["nb"] >= min_element_nb]

            x2 = np.ravel(a["x"])
            y2 = np.ravel(a["y"])
            nb = np.ravel(a["nb"])
            x = np.ravel(a["binx"])
            y = np.ravel(a["biny"])
            zcolor = np.ravel(wave_bin_table["binz"])

            new_matrix = np.array(new_matrix).T
            new_matrix_std = np.array(new_matrix_std).T

            self.debug = new_matrix, new_matrix_std

            mask_non_computed = np.where(new_matrix_std == 0)

            if len(mask_non_computed):
                med_matrix = np.nanmedian(new_matrix, axis=1)
                med_matrix_std = np.nanmedian(new_matrix_std, axis=1)
                for v1, v2 in zip(mask_non_computed[0], mask_non_computed[1]):
                    new_matrix[v1, v2] = med_matrix[v1]
                    new_matrix_std[v1, v2] = med_matrix_std[v1]

            # rm outliers
            if m_shell:
                iq = myf.IQ(new_matrix, axis=1)
                med = np.nanmedian(new_matrix, axis=1)
                sup = np.nanpercentile(new_matrix, 75, axis=1)
                inf = np.nanpercentile(new_matrix, 25, axis=1)

                for j in range(len(new_matrix[0])):
                    mask = (new_matrix[:, j] > sup + m_shell * iq) | (
                        new_matrix[:, j] < inf - m_shell * iq
                    )
                    new_matrix[:, j][mask] = med[mask]

            # extraction of the closest map element

            tt = myc.tableXY(x2, y2)
            tt.interpolate2d(np.ones(len(x2)), new_grid=100, replace=False, method="cubic")

            spectrum = np.array([save[1], save[0]])
            grid = np.array([np.ravel(tt.x_interp), np.ravel(tt.y_interp)])

            borders = np.linspace(0, len(spectrum[0]) + 1, 1000).astype("int")
            closest_point = []
            closest_point_distance = []
            for i, j in zip(borders[0:-1], borders[1:]):
                dist = (grid[0] - spectrum[0][i:j, np.newaxis]) ** 2 + (
                    grid[1] - spectrum[1][i:j, np.newaxis]
                ) ** 2
                closest_point = closest_point + list(np.argmin(dist, axis=1))
                closest_point_distance = closest_point_distance + list(np.min(dist, axis=1))

            closest_point = np.hstack([closest_point])
            closest_point_distance = np.hstack([closest_point_distance])

            nb_norm = nb / np.sum(nb) * 100

            cNorm = mplcolors.Normalize(vmin=4100, vmax=5300)
            scalarMap = cmx.ScalarMappable(norm=cNorm, cmap="jet")

            plt.figure()
            plt.title(text_info, fontsize=15)
            plt.scatter(ds1, s1, alpha=0.02, color="k")
            plt.xlabel(r"Flux gradient $df_0/d\lambda$ [$1/\AA$]", fontsize=15)
            plt.ylabel(r"Flux normalised $f_0$", fontsize=15)
            for j in x_grid:
                plt.axvline(x=j, color="r", alpha=0.5)
            for j in y_grid:
                plt.axhline(y=j, color="r", alpha=0.5)

            for i, j, k, l in zip(x2, y2, nb_norm, zcolor):
                plt.annotate(
                    "%.1f" % (k),
                    (i, j),
                    ha="center",
                    va="center",
                    color=scalarMap.to_rgba(l),
                    weight="bold",
                    bbox=dict(facecolor="gray", edgecolor="none", pad=1, alpha=0),
                )

            plt.scatter([0], [1.05], c=[zcolor[0]], cmap="jet", vmin=4100, vmax=5300)
            plt.ylim(0, 1)
            ax_color = plt.colorbar(pad=0)
            ax_color.ax.set_ylabel(r"Mean wavelength [$\AA$]", fontsize=15)
            plt.subplots_adjust(right=0.95, top=0.93, bottom=0.15)
            plt.savefig(self.dir_root + "PCA/SHELL_statistic.pdf")
            plt.savefig(self.dir_root + "PCA/SHELL_statistic.png")

            self.shell = {
                sub_dico: {
                    "f": new_matrix,
                    "f_std": new_matrix_std,
                    "x": x,
                    "y": y,
                    "x2": x2,
                    "y2": y2,
                    "x_inter": grid[0],
                    "y_inter": grid[1],
                    "spec": save[0],
                    "grad_spec": save[1],
                    "match": closest_point,
                    "match_dist": closest_point_distance,
                    "index_wave": index_wave,
                    "nb": nb,
                    "nb_norm": nb_norm,
                }
            }

        def xlim_shell():
            plt.scatter(
                self.shell[sub_dico]["x"], np.ones(len(self.shell[sub_dico]["x"])) * 0.5, alpha=0
            )

        if vec_binning is not None:
            vmin = np.percentile(new_matrix, 16)
            vmax = np.percentile(new_matrix, 84)
            plt.subplot(2, 3, 1)
            plt.imshow(new_matrix, vmin=vmin, vmax=vmax, aspect="auto")
            plt.subplot(2, 3, 4)
            plt.imshow(new_matrix_std, vmin=vmin, vmax=vmax, aspect="auto")
            new_matrix = new_matrix[:, np.argsort(vec_binning)]
            new_matrix_std = new_matrix_std[:, np.argsort(vec_binning)]
            plt.subplot(2, 3, 2)
            plt.imshow(new_matrix, vmin=vmin, vmax=vmax, aspect="auto")
            plt.subplot(2, 3, 5)
            plt.imshow(new_matrix_std, vmin=vmin, vmax=vmax, aspect="auto")
            new_matrix = new_matrix[np.argsort(sts.shell["matching_mad"]["x"])]
            new_matrix_std = new_matrix_std[np.argsort(sts.shell["matching_mad"]["x"])]
            plt.subplot(2, 3, 3)
            plt.imshow(new_matrix, vmin=vmin, vmax=vmax, aspect="auto")
            plt.subplot(2, 3, 6)
            plt.imshow(new_matrix_std, vmin=vmin, vmax=vmax, aspect="auto")

            x = x[np.argsort(sts.shell["matching_mad"]["x"])]

            base_fixed = np.array([-x / np.std(x), np.ones(len(x))])

            matrix_init = myc.table(new_matrix.T)
            w = 1 / new_matrix_std.T**2
            matrix_init.fit_base(base_fixed, weight=w)

            matrix = myc.table(matrix_init.vec_residues.T)

            zscore, phi, base_vec = matrix.dim_reduction("pca", 5, w)

        m1 = np.median(myf.mad(new_matrix, axis=1)[:, np.newaxis] / new_matrix_std, axis=0)
        m2 = np.median(myf.mad(new_matrix, axis=0) / new_matrix_std, axis=1)

        if np.nanpercentile(m1, 33) < snr_min:
            snr_min = np.nanpercentile(m1, 33)
            print(
                "\n [INFO] SNR crit updated to SNR = %.1f caused too much obs rejected" % (snr_min)
            )
        if sum(m1 > snr_min) < 20:
            new_perc = int(20 / len(m1) * 100)
            snr_min = np.nanpercentile(m1, [99, new_perc][int(new_perc < 100)])
            print("\n [INFO] SNR crit updated to SNR = %.1f caused not enough obs" % (snr_min))
        obs_kept = m1 > snr_min
        pxl_kept = m2 > snr_min

        plt.figure(figsize=(20, 5))
        plt.subplot(1, 2, 1)
        plt.plot(m1, "k.-")
        plt.title(
            "Nb of observations removed : %.1f %%" % (100 - 100 * sum(obs_kept) / len(obs_kept)),
            fontsize=15,
        )
        plt.ylabel("SNR", fontsize=15)
        plt.xlabel("Time index", fontsize=15)
        plt.yscale("log")
        if snr_min:
            plt.axhline(y=snr_min, ls=":", color="k")
        plt.subplot(1, 2, 2)
        plt.plot(m2, "k.-")
        plt.title(
            "Nb of pixels removed : %.1f %%" % (100 - 100 * sum(pxl_kept) / len(pxl_kept)),
            fontsize=15,
        )
        plt.ylabel("SNR", fontsize=15)
        plt.xlabel("Shell grid index", fontsize=15)
        plt.yscale("log")
        if snr_min:
            plt.axhline(y=snr_min, ls=":", color="k")
        plt.subplots_adjust(left=0.07, right=0.98)
        plt.savefig(self.dir_root + "PCA/Observations_removed_shell.pdf")

        print(
            "\n [INFO] %.1f%% of the observations removed with criterion SNR %.1f"
            % (100 - 100 * sum(obs_kept) / len(obs_kept), snr_min)
        )
        print(
            "\n [INFO] %.1f%% of the (f,df/dl) grid pixels removed with criterion SNR %.1f"
            % (100 - 100 * sum(pxl_kept) / len(pxl_kept), snr_min)
        )

        # plt.figure('dust')
        # distributions = []
        # for i in [2.5,16,25,50,75,84,97.5,snr_at_perc_min[1]]:
        #     a,b,c = plt.hist(np.nanpercentile(myf.mad(new_matrix,axis=1)/new_matrix_std,i,axis=0),cumulative=True,bins=np.linspace(0,100,1000),density=True,histtype='step')
        #     distributions.append([b[0:-1]+np.diff(b)/2,a*100])
        # plt.close('dust')

        # plt.figure()
        # for c,i in enumerate([2.5,16,25,50,75,84,97.5]):
        #     plt.plot(distributions[c][0],distributions[c][1],label='Perc=%.0f'%(i))
        # plt.plot(distributions[-1][0],distributions[-1][1],label='Crit_used=%.0f'%(snr_at_perc_min[1]),color='k')
        # plt.ylim(0,100) ; plt.xlim(0.5,None) ; plt.xscale('log')
        # plt.axvline(x=snr_at_perc_min[0],color='k',ls=':')
        # plt.legend()
        # plt.xlabel('S/N at percentile X',fontsize=15) ; plt.ylabel('Fraction of observations [%]',fontsize=15)
        # plt.title('Nb obs removed from PCA training = %.0f %%'%(distributions[-1][1][myf.find_nearest(distributions[-1][0],snr_at_perc_min[0])[0][0]]))
        # plt.savefig(self.dir_root+'PCA/Observatiosn removed_SHELL.pdf')

        # obs_kept = ~(np.nanpercentile(abs(new_matrix)/new_matrix_std,snr_at_perc_min[1],axis=0)<snr_at_perc_min[0])
        # print('\n [INFO] %.0f %% of the observations removed from the PCA training with criterion SNR %.1f at percentile %.0f'%(100-100*sum(obs_kept/len(obs_kept)),snr_at_perc_min[0],snr_at_perc_min[1]))

        # pxl_kept = np.ones(len(pxl_kept)).astype('bool')

        x = x[pxl_kept]
        x2 = x2[pxl_kept]
        y = y[pxl_kept]
        y2 = y2[pxl_kept]

        if offset:
            base_fixed = np.array([-x / np.std(x) / ([1, y][int(ratio)]), np.ones(len(x))])
        else:
            base_fixed = np.array(
                [-x / np.std(x) / ([1, y][int(ratio)])]
            )  # either gradient x or gradient x/y
        nb_fixed = len(base_fixed)

        matrix_init = myc.table(new_matrix.T[obs_kept][:, pxl_kept])

        matrix_init.fit_base(base_fixed, weight=1 / new_matrix_std.T[obs_kept][:, pxl_kept] ** 2)

        matrix = myc.table(matrix_init.vec_residues)

        w = 1 / new_matrix_std.T[obs_kept][:, pxl_kept] ** 2
        matrix.replace_outliers(m=m, kind=kind)

        stop = False

        zscore, phi, base_vec = matrix.dim_reduction(reduction, nb_comp, w)

        if zscore is None:
            stop = True

        percentages = np.zeros(len(base_vec.T))
        rmed = np.zeros(len(base_vec.T))
        if cross_validation:
            cv_base = [base_vec]
            for j in np.arange(cv_sim):
                selection = np.random.choice(
                    np.arange(len(matrix.table)),
                    int((1 - cv_percent_rm / 100) * len(matrix.table)),
                    replace=False,
                )
                matrix_cv = matrix.copy()
                matrix_cv.table = matrix_cv.table[selection]
                zscore, phi, base_vec = matrix_cv.dim_reduction(reduction, nb_comp, w)
                cv_base.append(base_vec)
            cv_base = np.array(cv_base)
            cv_base = np.hstack(cv_base).T
            cv_base = myc.table(cv_base)
            # cv_base.cross_validation_it(cv_sim, nb_comp, frac_affected=cv_frac_affected, fig_num=10, cv_rm=cv_percent_rm, algo_borders=[3,3])
            cv_base.cross_validation2(
                nb_comp,
                frac_affected=cv_frac_affected,
                fig_num=10,
                cv_rm=cv_percent_rm,
                overselection=3,
            )
            plt.figure(10)
            plt.savefig(self.dir_root + "PCA/Cross_validation_SHELL_matrix.png")
            plt.figure(11)
            plt.savefig(self.dir_root + "PCA/Cross_validation_SHELL_vectors.png")
            plt.figure(12)
            plt.savefig(self.dir_root + "PCA/Cross_validation_SHELL_iterations.png")
            components = cv_base.cv_components.T
            percentages = cv_base.cv_percentage_norm
            percentages[percentages > 105] = 0
            rmed = cv_base.cv_rmed

        percentages_values = np.round(percentages, 0).astype("int")
        self.shell_cv_percent = percentages_values.copy()

        comp_kept = list(np.arange(len(base_fixed)))
        if bool(treshold_percent) & cross_validation:
            nb_shell_significant = myf.first_transgression(
                percentages_values, treshold_percent, relation=1
            )
            comps_kept = np.arange(len(base_fixed), len(base_fixed) + len(percentages_values))[
                percentages_values >= treshold_percent
            ]
            if nb_shell_significant < 1:
                nb_shell_significant = 1
            if len(comps_kept) < 1:
                comps_kept = [1]
            nb_comp_kept = nb_shell_significant
        else:
            nb_shell_significant = nb_comp_kept
            comps_kept = np.arange(len(base_fixed), len(base_fixed) + nb_comp_kept)

        comp_kept = np.array(comp_kept + list(comps_kept))
        print(comp_kept)

        self.shell_cv_rmed = rmed.copy()
        self.shell_all_pca_base = base_vec
        nb_comp_100 = myf.first_transgression(percentages_values, 100, relation=1)

        self.shell_cv_comp_100 = nb_comp_100
        self.shell_var_ratio = matrix.var_ratio

        percentages = np.array(["%.0f %%" % (p) for p in percentages_values])
        percentages[percentages == "0 %"] = "<%.0f %%" % (cv_frac_affected * 100)

        t = myc.table(base_vec.T)
        new_base_ortho = t.gram_schimdt_modified(base_fixed[0], Plot=True)
        # new_base_ortho = base_vec.T

        base = np.vstack([base_fixed, new_base_ortho])
        # base[0] -= np.mean(base[0])
        base = base[: nb_fixed + nb_comp]
        matrix_init = myc.table(new_matrix.T[:, pxl_kept])
        matrix_init.fit_base(base, weight=1 / new_matrix_std.T[:, pxl_kept] ** 2)

        matrix_init.coeff_fitted[:, 0] /= np.std(x)
        matrix_init.coeff_fitted_std[:, 0] /= np.std(x)
        coeff_base = matrix_init.coeff_fitted
        coeff_base_std = matrix_init.coeff_fitted_std

        plt.figure(69, figsize=(22, 16))
        ax = plt.gca()
        plt.subplot(4, 2, 1)
        plt.xlabel("# components", fontsize=14)
        plt.ylabel("Variance explained", fontsize=14)
        plt.plot(np.arange(1, len(matrix.var_ratio) + 1), matrix.var_ratio, marker="o")
        plt.xlim(-1.5, None)
        ax2 = plt.gca()
        yspan = ax2.get_ylim()[1] - ax2.get_ylim()[0]

        for n, yplot in enumerate(matrix.var_ratio):
            if percentages[n] != "":
                plt.annotate(
                    "%s" % (percentages[n]),
                    (n + 1, yplot + 0.2 * yspan),
                    ha="center",
                    va="center",
                    color=["k", "r"][int(percentages_values[n]) < (100 - cv_percent_rm)],
                )
                plt.plot(
                    [n + 1, n + 1],
                    [matrix.var_ratio[n], yplot + 0.15 * yspan],
                    color="k",
                    alpha=0.2,
                )

        line = myc.tableXY(self.table.jdb, coeff_base[:, 0])  # ,coeff_base_std[:,0])
        line.recenter(who="Y")
        line.rms_w()
        rms = [line.rms]

        plt.figure(42, figsize=(16, 5 * (1 + int(ordering != "var_lbl"))))
        plt.subplot(1 + int(ordering != "var_lbl"), 1, 1)
        line.periodogram(
            nb_perm=1, color="k", Norm=True, legend="%.0f" % (0), p_min=0.7, lw=2, level=level
        )
        for i in range(1, len(coeff_base[0])):
            line.fit_base(coeff_base[:, 1 : 1 + i].T)
            line.vec_residues.rms_w()
            if i <= (nb_comp + 1):
                line.vec_residues.periodogram(
                    nb_perm=1,
                    color=None,
                    Norm=True,
                    legend="%.0f" % (i),
                    p_min=0.7,
                    lw=1,
                    level=level,
                )
            rms.append(line.vec_residues.rms)

        plt.legend()
        plt.ylim(0, power_max)
        self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)
        plt.subplots_adjust(top=0.94, hspace=0.35, left=0.07, right=0.96)
        plt.title(
            "Algo : %s            Reference : %s             Wave_min : %.2f             Wave_max : %.2f"
            % (reduction.upper(), reference, wave_min, wave_max)
        )
        ax68 = plt.gca()

        rms = np.array(rms)
        self.shell_rms_cumu = rms
        plt.legend()

        plt.figure(69)
        plt.subplot(4, 2, 3)
        plt.plot(np.arange(1, len(rms) + 1) - nb_fixed, rms, marker="o", label=reduction)
        plt.legend()
        plt.xlim(-1.5, None)
        plt.xlabel("#Vec fitted", fontsize=13)
        plt.ylabel("RV rms [m/s]", fontsize=13)

        plt.subplot(2, 2, 2)
        test = myc.tableXY(
            -np.diff(rms), np.hstack([0][0 : nb_fixed - 1] + [np.gradient(matrix.var_ratio)])
        )
        test.myscatter(liste=np.arange(1, len(base)) - (nb_fixed - 1), num=False, color=None)
        plt.axhline(y=0)
        plt.axvline(x=0)
        plt.xlabel(r"$\Delta$RV rms [m/s]", fontsize=13)
        plt.ylabel(r"$\Delta$Variance explained", fontsize=13)

        laplacien = []
        div = []
        # save_debug = []

        for i in range(len(base)):
            tt = myc.tableXY(x2, y2)
            tt.interpolate2d(base[i], new_grid=100, replace=False, method="cubic")
            div.append(myf.divergence(tt.z_interp / myf.IQ(tt.z_interp)))
            laplacien.append(myf.laplacien(tt.z_interp / myf.IQ(tt.z_interp)))
            # save_debug.append(tt.z_interp)

        laplacien = np.array(laplacien)
        div = np.array(div)
        div[div == 0] = np.nan
        # laplacien = laplacien - np.array([np.nanmedian(laplacien[i]) for i in range(len(laplacien))])[:,np.newaxis][:,np.newaxis]
        norm_laplacien = np.array(
            [myf.norm_laplacien(laplacien[i]) for i in range(len(laplacien))]
        )
        norm_div = np.array([np.nanstd(div[i]) for i in range(len(div))])

        self.div = div
        self.laplacien = laplacien
        plt.subplot(4, 2, 7)
        plt.plot(norm_laplacien - norm_laplacien[0], marker="o")
        plt.xlim(-1.5, None)
        plt.ylabel(r"$\Sigma |\Delta|$", fontsize=13)
        plt.xlabel("Shell #", fontsize=13)
        plt.subplot(4, 2, 5)
        plt.plot(norm_div - norm_div[0], marker="o", label=reduction)
        plt.ylabel(r"std($\nabla$)", fontsize=13)
        plt.xlabel("Shell #", fontsize=13)
        plt.legend()
        plt.xlim(-1.5, None)
        plt.subplot(2, 2, 4)
        test = myc.tableXY(norm_laplacien - norm_laplacien[0], norm_div - norm_div[0])
        test.myscatter(liste=np.arange(1, len(base) + 1) - (nb_fixed), num=False, color=None)
        plt.axhline(y=0)
        plt.axvline(x=0)
        plt.xlabel(r"$\Sigma |\Delta|$", fontsize=13)
        plt.ylabel(r"std($\nabla$)", fontsize=13)
        plt.subplots_adjust(top=0.94, hspace=0.35, left=0.07, right=0.96)
        plt.savefig(self.dir_root + "PCA/PCA_var_ratio_shell.png")

        # abs(myf.laplacien(table_flat[2]))

        vectors_indices = np.arange(1, nb_comp + 1)
        name = ["RV shift", "offset"][0:nb_fixed] + ["pca_%.0f" % (i) for i in vectors_indices]
        plt.figure(42)
        if ordering == "rvm_rms":
            print("\n [INFO] Reordering pca vector by rms improvement\n")
            vectors_indices = list(np.argsort(np.diff(self.shell_rms_cumu)) + 1)
            print(" New order : ", vectors_indices)
            base = base[[0] + vectors_indices]
            name = np.array(name)[[0] + vectors_indices]
            coeff_base = coeff_base[:, [0] + vectors_indices]
            coeff_base_std = coeff_base_std[:, [0] + vectors_indices]

            plt.subplot(2, 1, 2, sharex=ax68, sharey=ax68)
            plt.title("rms ordering")
            line.periodogram(
                nb_perm=1, color="k", Norm=True, legend="%.0f" % (0), p_min=0.7, lw=2, level=level
            )
            for i in range(1, len(coeff_base[0])):
                line.fit_base(coeff_base[:, 1 : 1 + i].T)
                if i <= (nb_comp + 1):
                    line.vec_residues.periodogram(
                        nb_perm=1,
                        color=None,
                        Norm=True,
                        legend="%.0f" % (i),
                        p_min=0.7,
                        lw=1,
                        level=level,
                    )
            plt.legend()
            plt.ylim(0, power_max)
            if kw != "":
                self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)

        elif (ordering == "laplacien") | (ordering == "divergence"):
            print("\n [INFO] Reordering pca vector by rms improvement\n")
            if ordering == "laplacien":
                vectors_indices = list(np.argsort(norm_laplacien[1:]) + 1)
            else:
                vectors_indices = list(np.argsort(norm_div[1:]) + 1)
            print(" New order : ", vectors_indices)
            base = base[[0] + vectors_indices]
            name = np.array(name)[[0] + vectors_indices]
            coeff_base = coeff_base[:, [0] + vectors_indices]
            coeff_base_std = coeff_base_std[:, [0] + vectors_indices]

            plt.subplot(2, 1, 2, sharex=ax68, sharey=ax68)
            plt.title("%s ordering" % (ordering))
            line.periodogram(
                nb_perm=1, color="k", Norm=True, legend="%.0f" % (0), p_min=0.7, lw=2, level=level
            )
            for i in range(1, len(coeff_base[0])):
                line.fit_base(coeff_base[:, 1 : 1 + i].T)
                if i <= (nb_comp + 1):
                    line.vec_residues.periodogram(
                        nb_perm=1,
                        color=None,
                        Norm=True,
                        legend="%.0f" % (i),
                        p_min=0.7,
                        lw=1,
                        level=level,
                    )
            plt.legend()
            plt.ylim(0, power_max)
            if kw != "":
                self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)

        plt.subplots_adjust(top=0.94, hspace=0.35, left=0.07, right=0.96)
        plt.savefig(self.dir_root + "PCA/SHELL_periodogram_nb_comp.pdf")

        # TBD HERE

        # base = base[:nb_comp_kept+nb_fixed]
        # coeff_base = coeff_base[:,:nb_comp_kept+nb_fixed]
        # coeff_base_std = coeff_base_std[:,:nb_comp_kept+nb_fixed]
        # name = name[:nb_comp_kept+nb_fixed]

        base = base[comp_kept]
        coeff_base = coeff_base[:, comp_kept]
        coeff_base_std = coeff_base_std[:, comp_kept]
        name = np.array(name)[comp_kept]

        plt.figure(figsize=(12, 12))
        plt.subplots_adjust(hspace=0, wspace=0, left=0.05, right=0.95, top=0.95, bottom=0.05)
        position = np.reshape(np.arange(len(base) ** 2), (len(base), len(base))) + 1
        i1, i2 = np.meshgrid(np.arange(0, len(base)), np.arange(0, len(base)))
        coeff = np.ones(len(base))
        r_corr = [1]
        r_corr = []
        rv_ccf = self.import_ccf_timeseries(
            "CCF_kitcat_mask_" + self.starname, "matching_mad", "rv"
        )

        shell_interpolated = []
        for i, j in zip(np.ravel(i1), np.ravel(i2)):
            if i <= j:
                plt.subplot(len(base), len(base), position[i, j])
                plt.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)

            if i == 0:
                # test = myc.tableXY(coeff_base[:,i],coeff_base[:,j],coeff_base_std[:,i],coeff_base_std[:,j])
                test = myc.tableXY(coeff_base[:, j], rv_ccf.y, coeff_base_std[:, j], rv_ccf.yerr)
                test.fit_line()
                if test.r_pearson_w < 0:
                    coeff[j] = -1
                plt.title(r"$\mathcal{R}=%.2f$" % (test.r_pearson_w * coeff[j]))
                r_corr.append(test.r_pearson_w * coeff[j])

            if i == j:
                base[j] *= coeff[j]
                coeff_base[j] *= coeff[j]

                vmin = np.nanpercentile(base[i], 16)
                vmax = np.nanpercentile(base[i], 84)
                tt = myc.tableXY(x2, y2)
                tt.interpolate2d(base[i], new_grid=100, replace=False, method="cubic")
                shell_interpolated.append(np.ravel(tt.z_interp))
                # myf.my_colormesh(np.ravel(tt.x_interp),np.ravel(tt.y_interp),np.ravel(tt.z_interp),vmin=vmin,vmax=vmax)
                plt.pcolormesh(
                    tt.x_interp,
                    tt.y_interp,
                    tt.z_interp,
                    vmin=vmin,
                    vmax=vmax,
                    cmap="brg",
                    shading="nearest",
                )
                plt.scatter(x, y, c=base[i], cmap="brg", vmin=vmin, vmax=vmax, edgecolor="k")
                xlim_shell()
            if i < j:
                plt.scatter(
                    coeff_base[:, j],
                    coeff_base[:, i],
                    c=self.table.jdb,
                    s=9,
                    cmap="jet",
                    edgecolor="k",
                )
                xlim_shell()

        plt.savefig(self.dir_root + "PCA/SHELL_maps.pdf")
        shell_interpolated = np.array(shell_interpolated)

        os.system("rm " + self.dir_root + "PCA/*_deprojected*")

        # GENERATE LINE PROFILE DISTORTION IN SPECTRUM SPACE

        liste_depth = [0.25, 0.5, 0.75]
        liste_wave = [4000, 5250, 6500]
        liste_color = ["b", "g", "r"]
        ft = 16
        figsize = (10, 10)

        nb_pts_profile = 100
        offset_plot = 1.15
        smooth_profile = int(0.05 * nb_pts_profile)  # smooth on 5%
        wave_max = np.max(spectrum_norm.x)
        sigma_line_profile = wave_max * sigma_ccf / 3e5
        wave_profile = np.hstack(
            [
                -np.linspace(0, 4 * sigma_line_profile, nb_pts_profile)[::-1][:-1],
                np.linspace(0, 4 * sigma_line_profile, nb_pts_profile),
            ]
        )

        all_profiles = []
        all_distortions = []
        for depth in liste_depth:
            for central_wave in liste_wave:
                line_profile = myf.gaussian(
                    wave_profile, 0, -depth, 1, sigma_line_profile * central_wave / wave_max
                )  # gaussian line centered on 0 flux norm at 1
                line_profile_gradient = np.gradient(line_profile) / np.gradient(wave_profile)
                loc_y = myf.find_nearest(tt.y_interp[:, 0], line_profile)[0]
                loc_x = myf.find_nearest(tt.x_interp[0], line_profile_gradient)[0]

                loc_ravel = loc_x + loc_y * len(tt.y_interp[0])
                distortion = (
                    shell_interpolated.T[loc_ravel]
                    * central_wave
                    * [1, line_profile[:, np.newaxis]][int(ratio)]
                )
                distortion /= np.nanstd(distortion, axis=0)
                distortion -= np.nanmean(distortion, axis=0)
                distortion *= distortion_amplitude

                distortion_smoothed = np.array(
                    [myf.smooth(i, smooth_profile, shape="rectangular") for i in distortion.T]
                ).T

                all_distortions.append(distortion_smoothed)
                all_profiles.append(line_profile)

        all_profiles = np.array(all_profiles).T
        all_distortions = np.array(all_distortions).T

        ymax = np.nanmax(offset_plot + all_distortions)
        xmin = np.min(wave_profile)
        xmax = np.max(wave_profile)

        for comp in np.arange(len(all_distortions)):
            counter = -1
            plt.figure(figsize=figsize)
            for depth in liste_depth:
                for colors, central_wave in zip(liste_color, liste_wave):
                    counter += 1

                    line_profile_modified = myc.tableXY(
                        wave_profile,
                        all_profiles[:, counter] + all_distortions[comp, :, counter] * depth,
                    )
                    if np.sum(np.isnan(line_profile_modified.y)):
                        line_profile_modified.bisector = np.ones((3, 3)) * np.nan
                    else:
                        try:
                            line_profile_modified.clip(max=[None, 1 - 0.10 * depth])
                            line_profile_modified.my_bisector()
                        except:
                            line_profile_modified.bisector = np.ones((3, 3)) * np.nan

                    plt.subplot(len(liste_depth), len(liste_wave), counter + 1)
                    plt.tick_params(direction="in")
                    if counter == int(len(liste_wave) / 2):
                        plt.title("Shell component %.0f" % (comp), fontsize=ft)
                    plt.plot(wave_profile, all_profiles[:, counter], color=colors, ls="-")
                    for j in np.arange(0, 1.01, 0.25):
                        plt.plot(
                            wave_profile,
                            all_profiles[:, counter]
                            + all_distortions[comp, :, counter] * j * depth,
                            color=colors,
                            ls="-",
                            alpha=1 - j,
                        )  # *depth : ad hoc correction to account for the error in the flux linearisation, dropping the second derivative of the flux
                    plt.axhline(y=offset_plot, color="k", ls=":")
                    plt.plot(
                        wave_profile, offset_plot + all_distortions[comp, :, counter], color=colors
                    )

                    plt.plot([0, 0], [1 - depth, 1], color=colors, ls=":")
                    plt.plot(
                        line_profile_modified.bisector[:, 0],
                        line_profile_modified.bisector[:, 1],
                        color=colors,
                        ls="-",
                    )

                    plt.ylim(-0.10, ymax)
                    plt.xlim(xmin, xmax)
                    plt.xticks(fontsize=ft)
                    plt.yticks(fontsize=ft)

                    if not counter % len(liste_wave):
                        plt.ylabel("Flux normalised", fontsize=ft)
                    else:
                        plt.tick_params(labelleft=False)

                    if counter >= ((len(liste_depth) - 1) * (len(liste_wave))):
                        plt.xlabel(r"Wavelength - %.0f [$\AA$]" % (central_wave), fontsize=ft)
                    else:
                        plt.tick_params(labelbottom=False)

                plt.subplots_adjust(
                    left=0.10, right=0.95, top=0.95, bottom=0.1, hspace=0, wspace=0
                )

            plt.savefig(self.dir_root + "PCA/SHELLS_deprojected_%.0f.pdf" % (comp))

        counter = -1
        counter2 = -1
        plt.figure(figsize=(16, 13))
        colors = "k"
        central_wave = liste_wave[1]
        print(np.shape(all_profiles))
        print(np.shape(all_distortions))
        for depth in liste_depth:
            counter2 += 1
            for comp in np.arange(1, nb_shell_significant + 1):
                counter += 1
                line_profile_modified = myc.tableXY(
                    wave_profile,
                    all_profiles[:, counter2 * 3 + 1]
                    + all_distortions[comp, :, counter2 * 3 + 1] * depth,
                )
                if np.sum(np.isnan(line_profile_modified.y)):
                    line_profile_modified.bisector = np.ones((3, 3)) * np.nan
                else:
                    try:
                        line_profile_modified.clip(max=[None, 1 - 0.10 * depth])
                        line_profile_modified.my_bisector()
                    except:
                        line_profile_modified.bisector = np.ones((3, 3)) * np.nan

                plt.subplot(3, nb_shell_significant, counter + 1)
                plt.tick_params(direction="in")
                if counter < nb_shell_significant:
                    plt.title("Shell component %.0f" % (comp), fontsize=ft)
                plt.plot(wave_profile, all_profiles[:, counter2 * 3 + 1], color=colors, ls="-")
                for j in np.arange(0, 1.01, 0.25):
                    plt.plot(
                        wave_profile,
                        all_profiles[:, counter2 * 3 + 1]
                        + all_distortions[comp, :, counter2 * 3 + 1] * j * depth,
                        color=colors,
                        ls="-",
                        alpha=1 - j,
                    )  # *depth : ad hoc correction to account for the error in the flux linearisation, dropping the second derivative of the flux
                plt.axhline(y=offset_plot, color="k", ls=":")
                plt.plot(
                    wave_profile,
                    offset_plot + all_distortions[comp, :, counter2 * 3 + 1],
                    color=colors,
                )

                plt.plot([0, 0], [1 - depth, 1], color=colors, ls=":")
                plt.plot(
                    line_profile_modified.bisector[:, 0],
                    line_profile_modified.bisector[:, 1],
                    color=colors,
                    ls="-",
                )

                plt.ylim(-0.10, ymax)
                plt.xlim(xmin, xmax)
                plt.xticks(fontsize=ft)
                plt.yticks(fontsize=ft)

                if not counter % nb_shell_significant:
                    plt.ylabel("Flux normalised", fontsize=ft)
                else:
                    plt.tick_params(labelleft=False)

                if counter >= 3 * (nb_shell_significant - 1) - 1:
                    plt.xlabel(r"Wavelength - %.0f [$\AA$]" % (central_wave), fontsize=ft)
                else:
                    plt.tick_params(labelbottom=False)

            plt.subplots_adjust(left=0.07, right=0.97, top=0.95, bottom=0.1, hspace=0, wspace=0)

        plt.savefig(self.dir_root + "PCA/SHELLS_deprojected_summary.pdf")

        # SHELL CORRELATION

        r_corr = np.array(r_corr)
        plt.figure(figsize=(22, 16))
        plt.subplots_adjust(hspace=0.4, top=0.95, left=0.07, right=0.97, bottom=0.07)
        for i in range(len(base)):
            if i == 0:
                plt.subplot(len(base) + 1, 2, 2 * i + 3)
                ax = plt.gca()
                try:
                    self.rv_planet.plot(color="r", label="planet")
                except:
                    pass
            else:
                plt.subplot(len(base) + 1, 2, 2 * i + 3, sharex=ax)
            if i != (len(base) - 1):
                plt.tick_params(labelbottom=False, direction="in")
            vec = myc.tableXY(self.table.jdb, coeff_base[:, i], coeff_base_std[:, i])

            # vec.rm_gap_seasons(replace=False)
            # vec.seasons_removed.plot()
            vec.recenter(who="Y")
            vec.znorm(who="Y")
            vec.plot(capsize=0)
            plt.annotate(
                "%s :\n R = %.2f" % (name[i], r_corr[i]),
                xy=(1.10, 0.33),
                xycoords="axes fraction",
                horizontalalignment="center",
                fontsize=13,
            )

            if i == 0:
                plt.subplot(len(base) + 1, 2, 2 * i + 4)
                ax1 = plt.gca()
            else:
                plt.subplot(len(base) + 1, 2, 2 * i + 4, sharex=ax1)
            if i != (len(base) - 1):
                plt.tick_params(labelbottom=False, direction="in")
            # vec.yerr = myf.mad(vec.y)*np.ones(len(vec.y))
            vec.periodogram(Norm=True, level=level)
            self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)
            plt.ylim(1e-4, None)
            plt.xlabel("")

        self.shell_base = base
        self.shell_base_interpolated = shell_interpolated
        self.shell_coeff = coeff_base.copy()
        self.shell_coeff_std = coeff_base_std.copy()

        if rv_ref is None:
            rv = myc.tableXY(self.table.jdb, coeff_base[:, 0], coeff_base_std[:, 0])
            rv.yerr = np.sqrt(rv_dace.yerr**2 + photon_noise**2)
        else:
            rv = rv_ref

        rv.fit_base(coeff_base[:, 1:].T)

        elem = np.dot(coeff_base[:, 1:], base[1:] + base[0] * rv.coeff_fitted[:, np.newaxis])

        master_shell = np.nanmean(elem / rv.vec_fitted.y[:, np.newaxis], axis=0)

        self.shell_master = master_shell

        rv_stat = myc.tableXY(rv.vec_fitted.y, rv.y, rv.yerr)
        rv_stat.fit_line()
        rv_stat.fit_spearman()

        rv.vec_residues.yerr = rv_dace.yerr

        rv.vec_residues.rms_w()
        rv.rms_w()

        print(
            "\n [INFO] Total R pearson between shell model and RV doppler : %.2f (%.2f)"
            % (rv_stat.r_pearson_w, rv_stat.r_pearson)
        )
        print(
            "\n [INFO] Total Rho spearman between shell model and RV doppler : %.2f"
            % (rv_stat.rho_spearman)
        )
        print("\n [INFO] RV rms : %.2f [m/s]" % (rv.rms))
        print("\n [INFO] RV residuals rms : %.2f [m/s]" % (rv.vec_residues.rms))

        plt.subplot(len(base) + 1, 2, 2, sharex=ax1)
        rv.vec_fitted.yerr = 0.7 * np.ones(len(rv.vec_fitted.x))
        rv.vec_fitted.periodogram(Norm=True, level=level)
        rv.vec_residues.yerr = rv.yerr
        plt.xlabel("")
        plt.tick_params(labelbottom=False, direction="in")
        self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)

        plt.subplot(len(base) + 1, 2, 1, sharex=ax)
        plt.scatter(rv.vec_fitted.x, rv.vec_fitted.y, color="k")

        self.shell_all_composite = rv.all_vec_fitted.copy()

        self.shell_composite = rv.vec_fitted.copy()

        plt.annotate(
            "%s :\n R = %.2f" % ("pca_comp", rv_stat.r_pearson_w),
            xy=(1.10, 0.33),
            xycoords="axes fraction",
            horizontalalignment="center",
            fontsize=13,
        )
        plt.tick_params(labelbottom=False, direction="in")
        plt.subplots_adjust(hspace=0, wspace=0.3, left=0.05)

        plt.savefig(self.dir_root + "PCA/SHELL_comp_periodogram.pdf")

        plt.figure(figsize=(22, 4 + 1 * len(base)))

        dy = (1 - 0.15) / (len(base) + 1)
        name[0] = "DS"
        name = [i.replace("pca_", "PS_") for i in name]
        self.shell_name = name
        for i in range(len(base)):
            vec = myc.tableXY(self.table.jdb, coeff_base[:, i])  # ,coeff_base_std[:,i])
            if i == 0:
                plt.axes([0.25, 0.97 - dy * (len(base)) - 0.05, 0.50, dy])
                ax1 = plt.gca()
                # rv.vec_residues.periodogram(Norm=True,color='b')
            else:
                plt.axes([0.25, 0.97 - dy * (i), 0.50, dy], sharex=ax1)
            plt.annotate(
                "$%s$ :\n $\mathcal{R}$ = %.2f" % (name[i], r_corr[i]),
                xy=(1.07, 0.33),
                xycoords="axes fraction",
                horizontalalignment="center",
                fontsize=16,
            )

            # vec.yerr = 1.48*myf.mad(vec.y)*np.ones(len(vec.y))
            vec.periodogram(Norm=True, level=level)
            if kw != "":
                self.yarara_indicate_planet(color="r", ls="-", alpha=0.3)
            plt.ylim(1e-4, None)

            plt.tick_params(labelbottom=False)
            plt.xlabel("")
            plt.ylabel("")

            plt.tick_params(direction="in", which="both", top=True)

            if i == 0:
                plt.axes([0.05, 0.97 - dy * (len(base)) - 0.05, 0.13, dy])
                plt.xlabel("Flux gradient", fontsize=16)
                plt.ylabel("Flux normalised", fontsize=16)
            else:
                plt.axes([0.05, 0.97 - dy * (i), 0.13, dy])
                plt.tick_params(labelbottom=False)

            plt.tick_params(direction="in", which="both", top=True)

            vmin = np.nanpercentile(base[i], 16)
            vmax = np.nanpercentile(base[i], 84)
            plt.pcolormesh(
                tt.x_interp,
                tt.y_interp,
                np.reshape(shell_interpolated[i], np.shape(tt.x_interp)),
                vmin=vmin,
                vmax=vmax,
                cmap="brg",
                shading="nearest",
            )
            plt.scatter(x, y, c=base[i], cmap="brg", vmin=vmin, vmax=vmax, edgecolor="k")
            ax = plt.colorbar(pad=0)
            xlim_shell()
            if i == 0:
                ax.ax.set_ylabel(r"$\delta f$", fontsize=16)

        plt.axes([0.25, 0.97 - dy * (2 + i) - 0.05, 0.50, dy])
        rv.vec_residues.periodogram(Norm=True, level=level)
        if kw != "":
            self.yarara_indicate_planet(color="r", ls="-", alpha=0.3)
        plt.ylim(1e-4, None)
        plt.xlabel("Period [days]", fontsize=16)
        plt.ylabel("Power", fontsize=16)
        plt.tick_params(direction="inout", which="both", top=True)
        # plt.annotate('%s :\n R = %.2f'%('pca_comp',rv_stat.r_pearson_w),xy=(1.07,0.33),xycoords='axes fraction',horizontalalignment='center',fontsize=13)
        ax = plt.axes([0.83, 0.97 - dy * (i), 0.12, dy * (len(base) - 1)])

        plt.ylabel("# components", fontsize=16)
        plt.xlabel("Variance explained", fontsize=16)
        plt.plot(matrix.var_ratio, np.arange(1, len(matrix.var_ratio) + 1), marker="o", color="k")

        plt.yticks(ticks=np.arange(1, len(matrix.var_ratio) + 1))
        ax.yaxis.set_label_position("right")
        ax.yaxis.tick_right()
        plt.ylim(len(matrix.var_ratio) + 1, 0)
        plt.xlim(0, None)
        ax2 = plt.gca()
        xspan = ax2.get_xlim()[1] - ax2.get_xlim()[0]
        xaxes = [ax2.get_xlim()[0] + 0.1 * (xspan), ax2.get_xlim()[0] + 0.9 * (xspan)]

        for n, x in enumerate(matrix.var_ratio):
            if x > (ax2.get_xlim()[0] + 0.5 * (xspan)):
                plt.annotate(
                    "%s" % (percentages[n]),
                    (x - 0.2 * xspan, n + 1),
                    ha="center",
                    va="center",
                    color=["k", "r"][int(percentages_values[n]) < (100 - cv_percent_rm)],
                )
            else:
                plt.annotate(
                    "%s" % (percentages[n]),
                    (x + 0.2 * xspan, n + 1),
                    ha="center",
                    va="center",
                    color=["k", "r"][int(percentages_values[n]) < (100 - cv_percent_rm)],
                )

        plt.savefig(self.dir_root + "PCA/SHELL_comp_periodogram2.pdf")

        plt.figure(figsize=(20, 14))
        plt.subplot(4, 1, 1)
        plt.title(
            "SHELL fit on sub_dico : %s             Algo : %s            Reference : %s             Wave_min : %.2f             Wave_max : %.2f"
            % (sub_dico, reduction.upper(), reference, wave_min, wave_max),
            fontsize=14,
        )
        rv.plot(label="RV before correction (rms=%.2f m/s)" % (rv.rms), color="b", capsize=0)

        rv.vec_residues.plot(
            label="RV after correction (rms=%.2f m/s)" % (rv.vec_residues.rms),
            color="g",
            capsize=0,
        )
        self.shell_vec_residues = rv.vec_residues

        try:
            self.rv_planet.plot(color="r", label="planet")
        except:
            pass
        plt.legend()
        plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=13)
        plt.ylabel("RV [m/s]", fontsize=13)

        plt.subplot(4, 1, 2)
        rv.periodogram(nb_perm=1, Norm=True, p_min=0.7, color="b", lw=1, level=level)
        rv.vec_residues.periodogram(nb_perm=1, Norm=True, p_min=0.7, color="g", lw=1, level=level)
        rv.plot_lowest_perio(rv.vec_residues)

        if kw != "":
            self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)

        plt.subplot(4, 1, 3)
        try:
            self.rv_planet.plot(color="r", label="planet")
        except:
            pass
        rv.vec_fitted.yerr = rv.yerr
        rv.vec_fitted.rms_w()
        rv.vec_fitted.plot(
            label="Contam (rms=%.2f m/s)" % (rv.vec_fitted.rms), color="k", capsize=0
        )
        plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=13)
        plt.ylabel(r"$\Delta$RV [m/s]", fontsize=13)
        plt.legend()

        plt.subplot(4, 1, 4)
        rv.vec_fitted.periodogram(nb_perm=1, Norm=True, p_min=0.7, color="k", lw=1, level=level)
        plt.ylim(0, power_max)
        if kw != "":
            self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)

        plt.subplots_adjust(top=0.95, left=0.08, right=0.96, bottom=0.09, hspace=0.35)
        plt.savefig(self.dir_root + "PCA/PCA_after_versus_before_shell.pdf")

        self.import_proxies()

        if proxies_to_corr is None:
            proxies_to_corr = ["CaII", "Ha", "WB", "CB", "BIS", "BIS2", "ccf_fwhm", "ccf_contrast"]

        matrix = np.hstack([self.shell_coeff, rv.vec_fitted.y[:, np.newaxis]])
        for element in proxies_to_corr:
            matrix = np.hstack([matrix, self.table[element][:, np.newaxis]])

        all_names_table = list(name) + [r"$\alpha$_model"] + proxies_to_corr

        if paper_plot:
            for n, val in enumerate(all_names_table):
                all_names_table[n] = all_names_table[n].replace("PS_", r"$\alpha$")
                all_names_table[n] = all_names_table[n].replace("BIS", "ccf_vspan")

        # self.ccf_fwhm = myc.tableXY(self.table.jdb,self.table.ccf_fwhm,self.table.ccf_fwhm_std)
        # self.ccf_contrast = myc.tableXY(self.table.jdb,self.table.ccf_contrast,self.table.ccf_contrast_std)

        m = myc.table(matrix.astype("float"))
        plt.figure(figsize=(11, 10))

        m.r_matrix(name=all_names_table, paper_plot=paper_plot, unit=[1, 100][int(paper_plot)])
        if not paper_plot:
            plt.title("sub_dico : %s" % (sub_dico))
        plt.subplots_adjust(left=0.11, right=1.00)
        plt.savefig(self.dir_root + "PCA/SHELL_R_matrix.pdf")

        if save_correction:

            elem2 = np.dot(
                coeff_base[:, 1:],
                shell_interpolated[1:] + shell_interpolated[0] * rv.coeff_fitted[:, np.newaxis],
            )

            correction_profile = (elem2.T[closest_point]).T * wave / c_lum
            correction_profile[np.isnan(correction_profile)] = 0
            correction_profile[:, np.setdiff1d(np.arange(len(wave)), index_wave).astype("int")] = 0

            sign = [1, -1][
                np.argmin(
                    [
                        np.nansum(abs(diff - correction_profile)),
                        np.nansum(abs(diff + correction_profile)),
                    ]
                )
            ]

            correction_profile *= sign

            diff2 = diff - correction_profile

            wave_min = 4100
            wave_max = 4500

            idx_min = myf.find_nearest(wave, wave_min)[0]
            idx_max = myf.find_nearest(wave, wave_max)[0] + 1

            new_wave = wave[int(idx_min) : int(idx_max)]

            fig = plt.figure(figsize=(21, 9))

            plt.axes([0.05, 0.66, 0.90, 0.25])
            myf.my_colormesh(
                new_wave,
                np.arange(len(diff)),
                100 * diff[:, int(idx_min) : int(idx_max)],
                zoom=zoom,
                vmin=low_cmap,
                vmax=high_cmap,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
            plt.colorbar(cax=cbaxes)

            plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
            myf.my_colormesh(
                new_wave,
                np.arange(len(diff)),
                100 * (diff2)[:, int(idx_min) : int(idx_max)],
                zoom=zoom,
                vmin=low_cmap,
                vmax=high_cmap,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
            plt.colorbar(cax=cbaxes2)

            plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
            myf.my_colormesh(
                new_wave,
                np.arange(len(diff)),
                100 * (correction_profile)[:, int(idx_min) : int(idx_max)],
                zoom=zoom,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
            plt.colorbar(cax=cbaxes3)

            plt.savefig(self.dir_root + "IMAGES/Correction_profile.png")

            to_be_saved = {"wave": wave, "correction_map": correction_profile}
            myf.pickle_dump(
                to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_profile.p", "wb")
            )

            print("\nComputation of the new continua, wait ... \n")
            time.sleep(0.5)

            new_conti = conti * (diff + ref) / (diff2 + ref + epsilon)
            new_continuum = new_conti.copy()
            new_continuum[all_flux == 0] = conti[all_flux == 0]
            new_continuum[new_continuum != new_continuum] = conti[
                new_continuum != new_continuum
            ]  # to supress mystic nan appearing
            new_continuum[np.isnan(new_continuum)] = conti[np.isnan(new_continuum)]
            new_continuum[new_continuum == 0] = conti[new_continuum == 0]
            new_continuum = self.uncorrect_hole(new_continuum, conti)

            i = -1
            for j in tqdm(files):
                i += 1
                file = pd.read_pickle(j)
                output = {"continuum_" + continuum: new_continuum[i]}
                file["matching_profile"] = output
                file["matching_profile"]["parameters"] = {
                    "reference_spectrum": reference,
                    "sub_dico_used": sub_dico,
                    "step": step + 1,
                }
                ras.save_pickle(j, file)

            self.dico_actif = "matching_profile"

    def yarara_correct_slice(
        self,
        sub_dico="matching_mad",
        save_correction=True,
        reduction="empca",
        reference="median",
        metric="median",
        continuum="linear",
        offset=False,
        substract_map=[],
        add_map=[],
        continuum_absorption=True,
        mask_rejection=True,
        wave_min=None,
        wave_max=None,
        m=2,
        kind="inter",
        nb_comp=10,
        nb_comp_kept=5,
        nb_cut=10,
        power_max=None,
        ext="",
        ordering="lbl_var",
        force_reduction=False,
        blended_lines=True,
        photon_noise=0.7,
    ):

        """lbl_var,rvm_rms,laplacien or divergence for ordering pca vectors"""
        myf.print_box("\n---- RECIPE : CORRECTION SMOOTH DETECTOR SPECTRA VARIATION ----\n")

        directory = self.directory

        rv_dace = self.import_dace_sts(substract_model=False)

        zoom = self.zoom
        smooth_map = self.smooth_map
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100
        cmap = self.cmap
        planet = self.planet

        self.import_material()
        self.import_table()
        load = self.material
        grid = np.array(load["wave"])
        snr = np.array(self.table.snr)

        self.import_kitcat()
        kitcat = self.kitcat["catalogue"]

        epsilon = 1e-12
        c_lum = 299.792e6

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        def compute_metric(array, array_std, metric):
            if metric == "median":
                return np.array([np.nanmedian(i) for i in array])
            if metric == "mean":
                return np.array([np.nanmean(i) for i in array])
            if metric == "sum":
                return np.array([np.nansum(i) for i in array])
            if type(metric) == int:
                return np.array([np.nanpercentile(i, metric) for i in array])
            if metric == "weighted":
                return np.array(
                    [
                        np.nansum(i / j**2) / np.nansum(1 / j**2)
                        for i, j in zip(array, array_std)
                    ]
                )

        if wave_min is None:
            wave_min = np.min(kitcat["wave"])
            print(" [INFO] Wave_min updated to a value of : %.2f" % (wave_min))

        if wave_max is None:
            wave_max = np.max(kitcat["wave"])
            print(" [INFO] Wave_max updated to a value of : %.2f" % (wave_max))

        diff_backup, diff_backup_std, wave = self.yarara_map(
            sub_dico=sub_dico,
            wave_min=None,
            wave_max=None,
            substract_map=substract_map,
            add_map=add_map,
            Plot=False,
            reference=reference,
            planet=bool(planet),
        )

        (
            pixels_unique,
            orders_unique,
            matrix_wave,
            matrix_merged,
            matrix_spectrum,
            matrix_spectrum_std,
        ) = self.yarara_spectrum_detector(
            diff_backup, matrix_spectra_std=diff_backup_std, Plot=False
        )

        diff = diff_backup.copy()

        corr_factor = int(continuum_absorption)
        spectrum_norm = myc.tableXY(
            load["wave"],
            load["reference_spectrum"]
            * ((1 - corr_factor) + load["correction_factor"] * corr_factor),
        )

        # mask_wavelength = (grid>=wave_min)&(grid<=wave_max)
        # if mask_rejection:
        #     load['rejected'] = load['rejected'].astype('bool')
        #     load['mask_brute'] = load['mask_brute'].astype('bool')
        #     mask_wavelength = (mask_wavelength)&(~load['mask_brute'])&(~load['rejected'])
        #     mask_wavelength = (0*mask_wavelength+1).astype('bol')

        # spectrum_norm.masked(mask_wavelength)
        # diff_backup = diff_backup[:,mask_wavelength]
        # diff_backup_std = diff_backup_std[:,mask_wavelength]

        s1 = spectrum_norm.y
        ds1 = np.gradient(s1) / np.gradient(spectrum_norm.x)
        save = (s1.copy(), ds1.copy())

        nb_cut_x = nb_cut
        nb_cut_y = nb_cut

        s = 0
        while len(matrix_spectrum) < (nb_cut_x * nb_cut_y * 3):
            s += 1
            if s % 2:
                nb_cut_y -= 2
                print(" [INFO] Nb cut in the cross dispersion direction")
            else:
                nb_cut_x -= 2
                print(" [INFO] Nb cut in the pixel direction")

        print(" [INFO] Nb cut in pixels direction : %.0f" % (nb_cut_x))
        print(" [INFO] Nb cut in orders direction : %.0f" % (nb_cut_y))

        cut_pixels = np.arange(
            0, np.shape(matrix_spectrum)[2] + 1, np.shape(matrix_spectrum)[2] / nb_cut_x
        ).astype("int")
        cut_orders = np.arange(
            0, np.shape(matrix_spectrum)[1] + 1, np.shape(matrix_spectrum)[1] / nb_cut_y
        ).astype("int")

        mask_merged = np.nan * np.ones(np.shape(matrix_merged))
        mask_merged[matrix_merged == 1] = 1

        mask_unmerged = np.nan * np.ones(np.shape(matrix_merged))
        mask_unmerged[matrix_merged == 0] = 1

        X, Y = np.meshgrid(pixels_unique, orders_unique)

        mean_nb_element = np.mean(np.diff(cut_orders)) * np.mean(np.diff(cut_pixels))

        try:
            self.slice
        except:
            self.slice = {}

        try:
            slices = self.slice[sub_dico]
            if force_reduction:
                pouet
        except:
            print(" [INFO] Computation of the slices...")
            x1 = []
            y1 = []
            x2 = []
            y2 = []
            nb = []
            flux = []
            flux_std = []
            merged = []

            for i, j in zip(cut_orders[0:-1], cut_orders[1:]):
                for k, l in zip(cut_pixels[0:-1], cut_pixels[1:]):
                    value1 = compute_metric(
                        matrix_spectrum[:, i:j, k:l] * mask_merged[i:j, k:l],
                        matrix_spectrum_std[:, i:j, k:l],
                        metric,
                    )
                    value2 = compute_metric(
                        matrix_spectrum[:, i:j, k:l] * mask_unmerged[i:j, k:l],
                        matrix_spectrum_std[:, i:j, k:l],
                        metric,
                    )
                    value1_std = 1 / np.sqrt(
                        compute_metric(
                            (1 / matrix_spectrum_std**2)[:, i:j, k:l] * mask_merged[i:j, k:l],
                            matrix_spectrum_std[:, i:j, k:l],
                            "sum",
                        )
                    )
                    value2_std = 1 / np.sqrt(
                        compute_metric(
                            (1 / matrix_spectrum_std**2)[:, i:j, k:l] * mask_unmerged[i:j, k:l],
                            matrix_spectrum_std[:, i:j, k:l],
                            "sum",
                        )
                    )
                    nb1 = np.median(
                        compute_metric(
                            1 + 0 * matrix_spectrum[:, i:j, k:l] * mask_merged[i:j, k:l],
                            matrix_spectrum_std[:, i:j, k:l],
                            "sum",
                        )
                    )
                    nb2 = np.median(
                        compute_metric(
                            1 + 0 * matrix_spectrum[:, i:j, k:l] * mask_unmerged[i:j, k:l],
                            matrix_spectrum_std[:, i:j, k:l],
                            "sum",
                        )
                    )
                    x1_m = np.nanmedian(X[i:j, k:l] * mask_merged[i:j, k:l])
                    y1_m = np.nanmedian(Y[i:j, k:l] * mask_merged[i:j, k:l])
                    x1_u = np.nanmedian(X[i:j, k:l] * mask_unmerged[i:j, k:l])
                    y1_u = np.nanmedian(Y[i:j, k:l] * mask_unmerged[i:j, k:l])
                    x2.append(np.nanmean(X[i:j, k:l]))
                    y2.append(np.nanmean(Y[i:j, k:l]))

                    if nb1 != 0:
                        x1.append(x1_m)
                        y1.append(y1_m)
                        nb.append(nb1)
                        flux.append(value1)
                        flux_std.append(value1_std)
                        merged.append(1)

                    if nb2 != 0:
                        x1.append(x1_u)
                        y1.append(y1_u)
                        nb.append(nb2)
                        flux.append(value2)
                        flux_std.append(value2_std)
                        merged.append(0)

            x1 = np.array(x1)
            y1 = np.array(y1)
            x2 = np.array(x2)
            y2 = np.array(y2)
            nb = np.array(nb)
            flux = np.array(flux)
            flux_std = np.array(flux_std)
            merged = np.array(merged)

            mask_nb = nb > (mean_nb_element) / 100 * 50  # minimum 50% of expected surface
            new_x = x1[mask_nb]
            new_y = y1[mask_nb]
            new_nb = nb[mask_nb]
            new_matrix = flux[mask_nb]
            new_matrix_std = flux_std[mask_nb]
            new_merged = merged[mask_nb]

            self.slice[sub_dico] = {
                "f": new_matrix,
                "f_std": new_matrix_std,
                "x": new_x,
                "y": new_y,
                "x2": x2,
                "y2": y2,
                "nb": new_nb,
                "merged": new_merged,
            }
            slices = self.slice[sub_dico]

        new_matrix = slices["f"]
        new_matrix_std = slices["f_std"]
        x = slices["x2"]
        y = slices["y2"]

        matrix = myc.table(new_matrix.T)

        w = 1 / new_matrix_std.T**2
        matrix.replace_outliers(m=m, kind=kind)

        stop = False
        zscore, phi, base_vec = matrix.dim_reduction(reduction, nb_comp, w)
        if zscore is None:
            stop = True

        self.slice_all_pca_base = base_vec

        for j in range(5):
            plt.subplot(2, 3, j + 1)
            plt.imshow(np.reshape(base_vec[:, j], (8, 8)), aspect="auto")

        t = myc.table(base_vec.T)
        new_base_ortho = t.gram_schimdt_modified(base_fixed[0], Plot=True)
        # new_base_ortho = base_vec.T

        base = np.vstack([base_fixed, new_base_ortho])
        # base[0] -= np.mean(base[0])
        base = base[: nb_fixed + nb_comp]
        matrix_init = myc.table(new_matrix.T)
        matrix_init.fit_base(base, weight=1 / new_matrix_std.T**2)

        coeff_base = matrix_init.coeff_fitted
        coeff_base_std = matrix_init.coeff_fitted_std

        plt.figure(69, figsize=(22, 16))
        ax = plt.gca()
        plt.subplot(4, 2, 1)
        plt.xlabel("# components", fontsize=14)
        plt.ylabel("Variance explained", fontsize=14)
        plt.plot(np.arange(1, len(matrix.var_ratio) + 1), matrix.var_ratio, marker="o")
        plt.xlim(-1.5, None)

        line = myc.tableXY(self.table.jdb, self.table.ccf_rv)  # ,coeff_base_std[:,0])
        line.recenter(who="Y")
        line.rms_w()
        rms = [line.rms]

        plt.figure(42, figsize=(16, 5 * (1 + int(ordering != "var_lbl"))))
        plt.subplot(1 + int(ordering != "var_lbl"), 1, 1)
        line.periodogram(nb_perm=1, color="k", Norm=True, legend="%.0f" % (0), p_min=0.7, lw=2)
        for i in range(1, len(coeff_base[0])):
            line.fit_base(coeff_base[:, 1 : 1 + i].T)
            line.vec_residues.rms_w()
            if i <= (nb_comp + 1):
                line.vec_residues.periodogram(
                    nb_perm=1, color=None, Norm=True, legend="%.0f" % (i), p_min=0.7, lw=1
                )
            rms.append(line.vec_residues.rms)

        plt.legend()
        plt.ylim(0, power_max)
        self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)
        plt.subplots_adjust(top=0.94, hspace=0.35, left=0.07, right=0.96)
        plt.title(
            "Algo : %s            Reference : %s             Wave_min : %.2f             Wave_max : %.2f"
            % (reduction.upper(), reference, wave_min, wave_max)
        )
        ax68 = plt.gca()

        rms = np.array(rms)
        self.shell_rms_cumu = rms
        plt.legend()

        plt.figure(69)
        plt.subplot(4, 2, 3)
        plt.plot(np.arange(1, len(rms) + 1) - nb_fixed, rms, marker="o", label=reduction)
        plt.legend()
        plt.xlim(-1.5, None)
        plt.xlabel("#Vec fitted", fontsize=13)
        plt.ylabel("RV rms [m/s]", fontsize=13)

        plt.subplot(2, 2, 2)
        test = myc.tableXY(
            -np.diff(rms), np.hstack([0][0 : nb_fixed - 1] + [np.gradient(matrix.var_ratio)])
        )
        test.myscatter(liste=np.arange(1, len(base)) - (nb_fixed - 1), num=False, color=None)
        plt.axhline(y=0)
        plt.axvline(x=0)
        plt.xlabel(r"$\Delta$RV rms [m/s]", fontsize=13)
        plt.ylabel(r"$\Delta$Variance explained", fontsize=13)

        laplacien = []
        div = []
        # save_debug = []

        for i in range(len(base)):
            tt = myc.tableXY(x2, y2)
            tt.interpolate2d(base[i], new_grid=100, replace=False, method="cubic")
            div.append(myf.divergence(tt.z_interp / myf.IQ(tt.z_interp)))
            laplacien.append(myf.laplacien(tt.z_interp / myf.IQ(tt.z_interp)))
            # save_debug.append(tt.z_interp)

        laplacien = np.array(laplacien)
        div = np.array(div)
        div[div == 0] = np.nan
        # laplacien = laplacien - np.array([np.nanmedian(laplacien[i]) for i in range(len(laplacien))])[:,np.newaxis][:,np.newaxis]
        norm_laplacien = np.array(
            [myf.norm_laplacien(laplacien[i]) for i in range(len(laplacien))]
        )
        norm_div = np.array([np.nanstd(div[i]) for i in range(len(div))])

        self.div = div
        self.laplacien = laplacien
        plt.subplot(4, 2, 7)
        plt.plot(norm_laplacien - norm_laplacien[0], marker="o")
        plt.xlim(-1.5, None)
        plt.ylabel(r"$\Sigma |\Delta|$", fontsize=13)
        plt.xlabel("Shell #", fontsize=13)
        plt.subplot(4, 2, 5)
        plt.plot(norm_div - norm_div[0], marker="o", label=reduction)
        plt.ylabel(r"std($\nabla$)", fontsize=13)
        plt.xlabel("Shell #", fontsize=13)
        plt.legend()
        plt.xlim(-1.5, None)
        plt.subplot(2, 2, 4)
        test = myc.tableXY(norm_laplacien - norm_laplacien[0], norm_div - norm_div[0])
        test.myscatter(liste=np.arange(1, len(base) + 1) - (nb_fixed), num=False, color=None)
        plt.axhline(y=0)
        plt.axvline(x=0)
        plt.xlabel(r"$\Sigma |\Delta|$", fontsize=13)
        plt.ylabel(r"std($\nabla$)", fontsize=13)
        plt.subplots_adjust(top=0.94, hspace=0.35, left=0.07, right=0.96)
        plt.savefig(self.dir_root + "PCA/PCA_var_ratio_slice.png")

        # abs(myf.laplacien(table_flat[2]))

        vectors_indices = np.arange(1, nb_comp + 1)
        name = ["RV shift", "offset"][0:nb_fixed] + ["pca_%.0f" % (i) for i in vectors_indices]
        if ordering == "rvm_rms":
            plt.figure(42)
            print("\n [INFO] Reordering pca vector by rms improvement\n")
            vectors_indices = list(np.argsort(np.diff(self.shell_rms_cumu)) + 1)
            print(" New order : ", vectors_indices)
            base = base[[0] + vectors_indices]
            name = np.array(name)[[0] + vectors_indices]
            coeff_base = coeff_base[:, [0] + vectors_indices]
            coeff_base_std = coeff_base_std[:, [0] + vectors_indices]

            plt.subplot(2, 1, 2, sharex=ax68, sharey=ax68)
            plt.title("rms ordering")
            line.periodogram(nb_perm=1, color="k", Norm=True, legend="%.0f" % (0), p_min=0.7, lw=2)
            for i in range(1, len(coeff_base[0])):
                line.fit_base(coeff_base[:, 1 : 1 + i].T)
                if i <= (nb_comp + 1):
                    line.vec_residues.periodogram(
                        nb_perm=1, color=None, Norm=True, legend="%.0f" % (i), p_min=0.7, lw=1
                    )
            plt.legend()
            plt.ylim(0, power_max)
            self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)

        elif (ordering == "laplacien") | (ordering == "divergence"):
            plt.figure(42)
            print("\n [INFO] Reordering pca vector by rms improvement\n")
            if ordering == "laplacien":
                vectors_indices = list(np.argsort(norm_laplacien[1:]) + 1)
            else:
                vectors_indices = list(np.argsort(norm_div[1:]) + 1)
            print(" New order : ", vectors_indices)
            base = base[[0] + vectors_indices]
            name = np.array(name)[[0] + vectors_indices]
            coeff_base = coeff_base[:, [0] + vectors_indices]
            coeff_base_std = coeff_base_std[:, [0] + vectors_indices]

            plt.subplot(2, 1, 2, sharex=ax68, sharey=ax68)
            plt.title("%s ordering" % (ordering))
            line.periodogram(nb_perm=1, color="k", Norm=True, legend="%.0f" % (0), p_min=0.7, lw=2)
            for i in range(1, len(coeff_base[0])):
                line.fit_base(coeff_base[:, 1 : 1 + i].T)
                if i <= (nb_comp + 1):
                    line.vec_residues.periodogram(
                        nb_perm=1, color=None, Norm=True, legend="%.0f" % (i), p_min=0.7, lw=1
                    )
            plt.legend()
            plt.ylim(0, power_max)
            self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)

        plt.subplots_adjust(top=0.94, hspace=0.35, left=0.07, right=0.96)
        plt.savefig(self.dir_root + "PCA/SHELL_periodogram_nb_comp.pdf")

        base = base[: nb_comp_kept + nb_fixed]
        coeff_base = coeff_base[:, : nb_comp_kept + nb_fixed]
        coeff_base_std = coeff_base_std[:, : nb_comp_kept + nb_fixed]
        name = name[: nb_comp_kept + nb_fixed]

        plt.figure(figsize=(12, 12))
        plt.subplots_adjust(hspace=0, wspace=0, left=0.05, right=0.95, top=0.95, bottom=0.05)
        position = np.reshape(np.arange(len(base) ** 2), (len(base), len(base))) + 1
        i1, i2 = np.meshgrid(np.arange(0, len(base)), np.arange(0, len(base)))
        coeff = np.ones(len(base))
        r_corr = [1]

        shell_interpolated = []
        for i, j in zip(np.ravel(i1), np.ravel(i2)):

            if i <= j:
                plt.subplot(len(base), len(base), position[i, j])
                plt.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)

            if (i == 0) & (j != 0):
                test = myc.tableXY(
                    coeff_base[:, j], coeff_base[:, i], coeff_base_std[:, j], coeff_base_std[:, i]
                )
                test.fit_line()
                if test.r_pearson_w < 0:
                    coeff[j] = -1
                plt.title(r"$\mathcal{R}=%.2f$" % (test.r_pearson_w * coeff[j]))
                r_corr.append(test.r_pearson_w * coeff[j])

                base[j] *= coeff[j]
                coeff_base[j] *= coeff[j]

            if i == j:
                vmin = np.nanpercentile(base[i], 16)
                vmax = np.nanpercentile(base[i], 84)
                tt = myc.tableXY(x2, y2)
                tt.interpolate2d(base[i], new_grid=100, replace=False, method="cubic")
                shell_interpolated.append(np.ravel(tt.z_interp))
                # myf.my_colormesh(np.ravel(tt.x_interp),np.ravel(tt.y_interp),np.ravel(tt.z_interp),vmin=vmin,vmax=vmax)
                plt.pcolormesh(
                    tt.x_interp,
                    tt.y_interp,
                    tt.z_interp,
                    vmin=vmin,
                    vmax=vmax,
                    cmap="brg",
                    shading="nearest",
                )
                plt.scatter(x, y, c=base[i], cmap="brg", vmin=vmin, vmax=vmax, edgecolor="k")
            if i < j:
                plt.scatter(
                    coeff_base[:, j],
                    coeff_base[:, i],
                    c=self.table.jdb,
                    s=9,
                    cmap="jet",
                    edgecolor="k",
                )

        plt.savefig(self.dir_root + "PCA/SHELL_maps.pdf")
        shell_interpolated = np.array(shell_interpolated)

        r_corr = np.array(r_corr)
        plt.figure(figsize=(22, 16))
        plt.subplots_adjust(hspace=0.4, top=0.95, left=0.07, right=0.97, bottom=0.07)
        for i in range(len(base)):
            if i == 0:
                plt.subplot(len(base) + 1, 2, 2 * i + 3)
                ax = plt.gca()
                try:
                    self.rv_planet.plot(color="r", label="planet")
                except:
                    pass
            else:
                plt.subplot(len(base) + 1, 2, 2 * i + 3, sharex=ax)
            if i != (len(base) - 1):
                plt.tick_params(labelbottom=False, direction="in")
            vec = myc.tableXY(self.table.jdb, coeff_base[:, i], coeff_base_std[:, i])

            # vec.rm_gap_seasons(replace=False)
            # vec.seasons_removed.plot()
            vec.recenter(who="Y")
            vec.znorm(who="Y")
            vec.plot(capsize=0)
            plt.annotate(
                "%s :\n R = %.2f" % (name[i], r_corr[i]),
                xy=(1.10, 0.33),
                xycoords="axes fraction",
                horizontalalignment="center",
                fontsize=13,
            )

            if i == 0:
                plt.subplot(len(base) + 1, 2, 2 * i + 4)
                ax1 = plt.gca()
            else:
                plt.subplot(len(base) + 1, 2, 2 * i + 4, sharex=ax1)
            if i != (len(base) - 1):
                plt.tick_params(labelbottom=False, direction="in")
            vec.yerr = myf.mad(vec.y) * np.ones(len(vec.y))
            vec.periodogram()
            self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)
            plt.ylim(1e-4, None)
            plt.xlabel("")

        self.shell_base = base
        self.shell_base_interpolated = shell_interpolated
        self.shell_coeff = coeff_base
        self.shell_coeff_std = coeff_base_std

        rv = myc.tableXY(self.table.jdb, coeff_base[:, 0], coeff_base_std[:, 0])

        rv.fit_base(coeff_base[:, 1:].T)

        elem = np.dot(coeff_base[:, 1:], base[1:] + base[0] * rv.coeff_fitted[:, np.newaxis])

        master_shell = np.nanmean(elem / rv.vec_fitted.y[:, np.newaxis], axis=0)

        self.shell_master = master_shell

        rv_stat = myc.tableXY(rv.vec_fitted.y, rv.y, rv.yerr)
        rv_stat.fit_line()
        rv_stat.fit_spearman()

        rv.yerr = np.sqrt(rv_dace.yerr**2 + photon_noise**2)
        rv.vec_residues.yerr = rv_dace.yerr

        rv.vec_residues.rms_w()
        rv.rms_w()

        print(
            "\n [INFO] Total R pearson between shell model and RV doppler : %.2f (%.2f)"
            % (rv_stat.r_pearson_w, rv_stat.r_pearson)
        )
        print(
            "\n [INFO] Total Rho spearman between shell model and RV doppler : %.2f"
            % (rv_stat.rho_spearman)
        )
        print("\n [INFO] RV residuals : %.2f [m/s]" % (rv.vec_residues.rms))

        plt.subplot(len(base) + 1, 2, 2, sharex=ax1)
        rv.vec_fitted.yerr = 0.7 * np.ones(len(rv.vec_fitted.x))
        rv.vec_fitted.periodogram()
        rv.vec_residues.yerr = rv.yerr
        plt.xlabel("")
        plt.tick_params(labelbottom=False, direction="in")
        self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)

        plt.subplot(len(base) + 1, 2, 1, sharex=ax)
        plt.scatter(rv.vec_fitted.x, rv.vec_fitted.y, color="k")

        self.shell_composite = rv.vec_fitted.copy()

        plt.annotate(
            "%s :\n R = %.2f" % ("pca_comp", rv_stat.r_pearson_w),
            xy=(1.10, 0.33),
            xycoords="axes fraction",
            horizontalalignment="center",
            fontsize=13,
        )
        plt.tick_params(labelbottom=False, direction="in")
        plt.subplots_adjust(hspace=0, wspace=0.3, left=0.05)

        plt.savefig(self.dir_root + "PCA/SHELL_comp_periodogram.pdf")

        plt.figure(figsize=(20, 14))
        plt.subplot(4, 1, 1)
        plt.title(
            "SHELL fit on sub_dico : %s             Algo : %s            Reference : %s             Wave_min : %.2f             Wave_max : %.2f"
            % (sub_dico, reduction.upper(), reference, wave_min, wave_max),
            fontsize=14,
        )
        rv.plot(label="RV before correction (rms=%.2f m/s)" % (rv.rms), color="b", capsize=0)

        rv.vec_residues.plot(
            label="RV after correction (rms=%.2f m/s)" % (rv.vec_residues.rms),
            color="g",
            capsize=0,
        )
        self.shell_vec_residues = rv.vec_residues

        try:
            self.rv_planet.plot(color="r", label="planet")
        except:
            pass
        plt.legend()
        plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=13)
        plt.ylabel("RV [m/s]", fontsize=13)

        plt.subplot(4, 1, 2)
        rv.periodogram(nb_perm=1, Norm=True, p_min=0.7, color="b", lw=1)
        rv.vec_residues.periodogram(nb_perm=1, Norm=True, p_min=0.7, color="g", lw=1)
        rv.plot_lowest_perio(rv.vec_residues.poly)

        self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)

        plt.subplot(4, 1, 3)
        try:
            self.rv_planet.plot(color="r", label="planet")
        except:
            pass
        rv.vec_fitted.yerr = rv.yerr
        rv.vec_fitted.rms_w()
        rv.vec_fitted.plot(
            label="Contam (rms=%.2f m/s)" % (rv.vec_fitted.rms), color="k", capsize=0
        )
        plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=13)
        plt.ylabel(r"$\Delta$RV [m/s]", fontsize=13)
        plt.legend()

        plt.subplot(4, 1, 4)
        rv.vec_fitted.periodogram(nb_perm=1, Norm=True, p_min=0.7, color="k", lw=1)
        plt.ylim(0, power_max)
        self.yarara_indicate_planet(color="r", ls="-", alpha=0.1)

        plt.subplots_adjust(top=0.95, left=0.08, right=0.96, bottom=0.09, hspace=0.35)
        plt.savefig(self.dir_root + "PCA/PCA_after_versus_before_shell.pdf")

        self.import_proxies()

        self.ccf_fwhm = myc.tableXY(self.table.jdb, self.table.ccf_fwhm, self.table.ccf_fwhm_std)
        self.ccf_contrast = myc.tableXY(
            self.table.jdb, self.table.ccf_contrast, self.table.ccf_contrast_std
        )

        matrix = np.hstack(
            [
                self.shell_coeff,
                rv.vec_fitted.y[:, np.newaxis],
                self.ca2.y[:, np.newaxis],
                self.ha.y[:, np.newaxis],
                self.wb.y[:, np.newaxis],
                self.cb.y[:, np.newaxis],
                self.bis.y[:, np.newaxis],
                self.bis2.y[:, np.newaxis],
                self.ccf_fwhm.y[:, np.newaxis],
                self.ccf_contrast.y[:, np.newaxis],
            ]
        )
        m = myc.table(matrix.astype("float"))
        plt.figure(figsize=(12, 12))
        m.r_matrix(
            name=list(name)
            + [
                "pca_composite",
                "CaII",
                "Ha",
                "WB",
                "CB",
                "BIS",
                "BIS2",
                "ccf_fwhm",
                "ccf_contrast",
            ]
        )
        plt.title("sub_dico : %s" % (sub_dico))
        plt.savefig(self.dir_root + "PCA/SHELL_R_matrix.pdf")

        if save_correction:

            elem2 = np.dot(
                coeff_base[:, 1:],
                shell_interpolated[1:] + shell_interpolated[0] * rv.coeff_fitted[:, np.newaxis],
            )

            correction_profile = (elem2.T[closest_point]).T * wave / c_lum
            correction_profile[np.isnan(correction_profile)] = 0
            correction_profile[:, np.setdiff1d(np.arange(len(wave)), index_wave).astype("int")] = 0

            sign = [1, -1][
                np.argmin(
                    [
                        np.nansum(abs(diff - correction_profile)),
                        np.nansum(abs(diff + correction_profile)),
                    ]
                )
            ]

            correction_profile *= sign

            diff2 = diff - correction_profile

            wave_min = 4100
            wave_max = 4500

            idx_min = myf.find_nearest(wave, wave_min)[0]
            idx_max = myf.find_nearest(wave, wave_max)[0] + 1

            new_wave = wave[int(idx_min) : int(idx_max)]

            fig = plt.figure(figsize=(21, 9))

            plt.axes([0.05, 0.66, 0.90, 0.25])
            myf.my_colormesh(
                new_wave,
                np.arange(len(diff)),
                100 * diff[:, int(idx_min) : int(idx_max)],
                zoom=zoom,
                vmin=low_cmap,
                vmax=high_cmap,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
            plt.colorbar(cax=cbaxes)

            plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
            myf.my_colormesh(
                new_wave,
                np.arange(len(diff)),
                100 * (diff2)[:, int(idx_min) : int(idx_max)],
                zoom=zoom,
                vmin=low_cmap,
                vmax=high_cmap,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
            plt.colorbar(cax=cbaxes2)

            plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
            myf.my_colormesh(
                new_wave,
                np.arange(len(diff)),
                100 * (correction_profile)[:, int(idx_min) : int(idx_max)],
                zoom=zoom,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
            plt.colorbar(cax=cbaxes3)

            plt.savefig(self.dir_root + "IMAGES/Correction_profile.png")

            to_be_saved = {"wave": wave, "correction_map": correction_profile}
            myf.pickle_dump(
                to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_profile.p", "wb")
            )

            print("\nComputation of the new continua, wait ... \n")
            time.sleep(0.5)

            new_conti = conti * (diff + ref) / (diff2 + ref + epsilon)
            new_continuum = new_conti.copy()
            new_continuum[all_flux == 0] = conti[all_flux == 0]
            new_continuum[new_continuum != new_continuum] = conti[
                new_continuum != new_continuum
            ]  # to supress mystic nan appearing
            new_continuum[np.isnan(new_continuum)] = conti[np.isnan(new_continuum)]
            new_continuum[new_continuum == 0] = conti[new_continuum == 0]
            new_continuum = self.uncorrect_hole(new_continuum, conti)

            i = -1
            for j in tqdm(files):
                i += 1
                file = pd.read_pickle(j)
                output = {"continuum_" + continuum: new_continuum[i]}
                file["matching_profile"] = output
                file["matching_profile"]["parameters"] = {
                    "reference_spectrum": reference,
                    "sub_dico_used": sub_dico,
                    "step": step + 1,
                }
                ras.save_pickle(j, file)

            self.dico_actif = "matching_profile"

    def yarara_correct_profile(
        self,
        sub_dico="matching_mad",
        proxies_corr="CB",
        reference="median",
        continuum="linear",
        substract_map=[],
        add_map=[],
        sort_values=False,
        expo1=3,
        expo2=3,
        maximum=True,
        smooth_corr=5,
        rmin=0,
        alpha_p=0.25,
        bins=0,
        flux_max=0.95,
        wave_min=4700,
        wave_max=4750,
    ):
        myf.print_box("\n---- RECIPE : CORRECTION LINE PROFILE ACTIVITY ----\n")

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100
        cmap = self.cmap
        planet = self.planet

        self.import_material()
        self.import_table()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        file = self.import_spectrum()
        step = file[sub_dico]["parameters"]["step"]

        all_flux = []
        all_flux_std = []
        conti = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                grid = file["wave"]
                hole_left = file["parameters"]["hole_left"]
                hole_right = file["parameters"]["hole_right"]
            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            all_flux.append(f_norm)
            all_flux_std.append(f_norm_std)
            conti.append(c)

        step = file[sub_dico]["parameters"]["step"]

        all_flux = np.array(all_flux)
        all_flux_std = np.array(all_flux_std)
        conti = np.array(conti)

        if reference == "snr":
            ref = all_flux[snr.argmax()]
        elif reference == "median":
            print(" [INFO] Reference spectrum : median")
            ref = np.median(all_flux, axis=0)
        elif reference == "master":
            print(" [INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print(" [INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = all_flux[reference]
        else:
            ref = 0 * np.median(all_flux, axis=0)

        spectrum = myc.tableXY(
            load["wave"], load["reference_spectrum"] * load["correction_factor"]
        )
        spectrum_norm = myc.tableXY(
            load["wave"], load["reference_spectrum"] * load["correction_factor"]
        )

        s1 = spectrum_norm.y
        ds1 = np.gradient(s1)

        if type(proxies_corr):
            proxy = np.array(self.table[proxies_corr])
        else:
            proxy = np.array(proxies_corr)
            proxies_corr = "Vec"

        self.yarara_time_variations(
            sub_dico=sub_dico,
            wave_min=np.min(load["wave"]),
            wave_max=np.max(load["wave"]),
            smooth_corr=smooth_corr,
            substract_map=substract_map,
            add_map=add_map,
            proxy_corr=proxy,
            Plot=False,
            reference=reference,
        )
        diff_backup, diff_backup_std, wave = self.yarara_map(
            sub_dico=sub_dico,
            wave_min=np.min(load["wave"]),
            wave_max=np.max(load["wave"]),
            substract_map=substract_map,
            add_map=add_map,
            Plot=False,
            reference=reference,
        )

        r_wb = self.r_corr.copy()
        r_wb[np.isnan(r_wb)] = 0
        r_wb[np.isinf(r_wb)] = 0
        s_wb = self.slope_corr.copy()
        s_wb[np.isnan(s_wb)] = 0
        s_wb[np.isinf(s_wb)] = 0
        slope_fitted = s_wb.copy()

        spectrum = myc.tableXY(ds1.copy(), s1.copy())

        # if sort_values:
        #     spectrum.order(order=sort_wb)
        #     s_wb = s_wb[sort_wb]
        #     r_wb = r_wb[sort_wb]

        mask_depth = (spectrum.y <= flux_max) & (spectrum.y >= 0.02)
        spectrum.masked(mask_depth)
        r_wb = r_wb[mask_depth]
        s_wb = s_wb[mask_depth]

        spectrum_backup = spectrum.copy()
        s_wb_backup = s_wb.copy()

        mask_r = abs(r_wb) > rmin
        spectrum.masked(mask_r)
        r_wb = r_wb[mask_r]
        s_wb = s_wb[mask_r]
        order = np.argsort(abs(r_wb))

        plt.figure(figsize=(20, 10))

        bbin = bins != 0
        spectrum_backup = spectrum.copy()
        s_wb_backup2 = s_wb.copy()

        val = -1
        for ordering in [order, np.arange(len(order))]:
            spectrum = spectrum_backup.copy()
            s_wb = s_wb_backup2.copy()
            val += 1
            plt.subplot(2, (3 + bbin), 1 + (3 + bbin) * val)
            plt.title("%s correlation (R>%.2f)" % (proxies_corr, rmin), fontsize=15)
            ax = plt.gca()
            ax.tick_params(labelsize=14)
            vmin = np.percentile(s_wb, 5)
            vmax = np.percentile(s_wb, 95)
            plt.scatter(
                spectrum.x[ordering],
                spectrum.y[ordering],
                c=s_wb[ordering],
                cmap="brg",
                vmin=vmin,
                vmax=vmax,
                s=3,
                alpha=alpha_p,
            )
            plt.axvline(color="k", ls=":", zorder=100)
            plt.xlabel(r"d$f$/d$\lambda$ [unit arb.]", fontsize=15)
            plt.ylabel(r"flux normalised", fontsize=15)
            ax2 = plt.colorbar()
            if bins != 0:
                x = np.linspace(np.min(spectrum.x), np.max(spectrum.x), bins)
                y = np.linspace(np.min(spectrum.y), np.max(spectrum.y), bins)

                x_bin = []
                y_bin = []
                z_bin = []

                for xi in range(len(x) - 1):
                    for yi in range(len(y) - 1):
                        mask = (
                            (spectrum.x < x[xi + 1])
                            & (spectrum.x > x[xi])
                            & (spectrum.y < y[yi + 1])
                            & (spectrum.y > y[yi])
                        )
                        if sum(mask):
                            x_bin.append(np.nanmean(spectrum.x[mask]))
                            y_bin.append(np.nanmean(spectrum.y[mask]))
                            z_bin.append(np.nanmean(s_wb[mask]))

                spectrum.x = np.array(x_bin)
                spectrum.y = np.array(y_bin)
                s_wb = np.array(z_bin)

            plt.subplot(2, (3 + bbin), 2 + (3 + bbin) * val, sharex=ax, sharey=ax)
            plt.title("Fitted model", fontsize=15)
            spectrum.fit_poly2d(
                s_wb,
                expo1=expo1,
                expo2=expo2,
                maximum=maximum,
                Draw=True,
                alpha_p=int(bins != 0),
                cmap="brg",
                vmin=vmin,
                vmax=vmax,
            )
            plt.axvline(color="k", ls=":", zorder=100)
            plt.xlabel(r"d$f$/d$\lambda$ [unit arb.]", fontsize=15)
            plt.ylabel(r"flux normalised", fontsize=15)

            if bbin:
                plt.subplot(2, (3 + bbin), 3 + (3 + bbin) * val, sharex=ax, sharey=ax)
                plt.title(
                    "Residues(%.2f/%.2f)" % (np.std(spectrum.z_res), np.std(s_wb)), fontsize=15
                )
                plt.scatter(
                    spectrum.x,
                    spectrum.y,
                    c=spectrum.z_res,
                    cmap="brg",
                    vmin=vmin,
                    vmax=vmax,
                    s=3,
                    alpha=alpha_p,
                )
                plt.axvline(color="k", ls=":", zorder=100)
                plt.xlabel(r"d$f$/d$\lambda$ [unit arb.]", fontsize=15)
                plt.ylabel(r"flux normalised", fontsize=15)
                ax2 = plt.colorbar()

            s_fitted = np.ravel(
                spectrum.poly_model(
                    spectrum_backup.x[:, np.newaxis],
                    spectrum_backup.y[:, np.newaxis],
                    spectrum.poly2_coefficient,
                )
            )

            plt.subplot(2, (3 + bbin), (3 + bbin) + (3 + bbin) * val, sharex=ax, sharey=ax)
            plt.title(
                "Residues (%.2f/%.2f)" % (np.std(s_wb_backup - s_fitted), np.std(s_wb_backup)),
                fontsize=15,
            )
            plt.scatter(
                spectrum_backup.x[ordering],
                spectrum_backup.y[ordering],
                c=(s_wb_backup - s_fitted)[ordering],
                cmap="brg",
                vmin=vmin,
                vmax=vmax,
                s=3,
                alpha=alpha_p,
            )
            plt.axvline(color="k", ls=":", zorder=100)
            plt.xlabel(r"d$f$/d$\lambda$ [unit arb.]", fontsize=15)
            plt.ylabel(r"flux normalised", fontsize=15)
            ax2 = plt.colorbar()
            ax2.ax.set_ylabel("Slope", fontsize=15)

        plt.subplots_adjust(left=0.05, right=0.95, wspace=0.3, bottom=0.10, top=0.95, hspace=0.35)

        plt.savefig(self.dir_root + "IMAGES/Model_profile_correction.png")

        coeff = np.ravel(
            spectrum.poly_model(ds1[:, np.newaxis], s1[:, np.newaxis], spectrum.poly2_coefficient)
        )
        mask_depth = (s1 <= 0.95) & (s1 >= 0.05)
        # coeff[~mask_depth] = 0

        slope_model = coeff

        correction_profile = coeff * (proxy - np.mean(proxy))[:, np.newaxis]
        correction_profile -= np.median(correction_profile, axis=0)

        diff2_backup = diff_backup - correction_profile

        idx_min = myf.find_nearest(wave, wave_min)[0]
        idx_max = myf.find_nearest(wave, wave_max)[0] + 1

        new_wave = wave[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))

        plt.axes([0.05, 0.66, 0.90, 0.25])
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * diff_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
        plt.colorbar(cax=cbaxes)

        plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * diff2_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
        plt.colorbar(cax=cbaxes2)

        plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * correction_profile[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
        plt.colorbar(cax=cbaxes3)

        plt.savefig(self.dir_root + "IMAGES/Correction_profile.png")

        to_be_saved = {"wave": wave, "correction_map": correction_profile}
        myf.pickle_dump(
            to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_profile.p", "wb")
        )

        print("\nComputation of the new continua, wait ... \n")
        time.sleep(0.5)

        # new_conti = conti*(diff_backup+ref)/(diff2_backup+ref+epsilon)
        # new_continuum = new_conti.copy()
        # new_continuum[all_flux==0] = conti[all_flux==0]
        # new_continuum[new_continuum!=new_continuum] = conti[new_continuum!=new_continuum] #to supress mystic nan appearing
        # new_continuum[np.isnan(new_continuum)] = conti[np.isnan(new_continuum)]
        # new_continuum[new_continuum==0] = conti[new_continuum==0]
        # new_continuum = self.uncorrect_hole(new_continuum,conti)

        # i = -1
        # for j in tqdm(files):
        #     i+=1
        #     file = pd.read_pickle(j)
        #     output = {'continuum_'+continuum:new_continuum[i]}
        #     file['matching_profile'] = output
        #     file['matching_profile']['parameters'] = {'reference_spectrum':reference, 'sub_dico_used':sub_dico, 'proxy':proxies_corr,
        #                                                 'minimum_r_corr_selection':rmin,'step':step+1}
        #     ras.save_pickle(j,file)

        self.dico_actif = "matching_profile"

    def yarara_kde_mask(self):
        myf.print_box("\n---- RECIPE : KDE SPECTRUM EXTRACTION ----\n")

        directory = self.directory
        self.import_table()
        self.import_material()
        load = self.material

        spectrum_norm = myc.tableXY(
            load["wave"], load["reference_spectrum"] * load["correction_factor"]
        )

        s1 = spectrum_norm.y
        ds1 = np.gradient(s1)

        spec_rep = myc.tableXY(ds1, s1)

        vertices_curve1 = list(pd.read_pickle(root + "/Python/Material/KDE1_CB_cenB.p").values())
        vertices_curve2 = list(pd.read_pickle(root + "/Python/Material/KDE2_CB_cenB.p").values())

        mask_ll = np.array(
            [spec_rep.inside_kde(vertices_curve1[j]) for j in range(len(vertices_curve1))]
        )
        mask_tr = np.array(
            [spec_rep.inside_kde(vertices_curve2[j]) for j in range(len(vertices_curve2))]
        )
        mask_lr = np.array(
            [
                spec_rep.inside_kde(vertices_curve1[j] * np.array([-1, 1]))
                for j in range(len(vertices_curve1))
            ]
        )  # sym
        mask_tl = np.array(
            [
                spec_rep.inside_kde(vertices_curve2[j] * np.array([-1, 1]))
                for j in range(len(vertices_curve2))
            ]
        )  # sym

        self.mask_ll = np.sum(mask_ll * np.arange(1, 1 + len(mask_ll))[:, np.newaxis], axis=0) - 1
        self.mask_tr = np.sum(mask_tr * np.arange(1, 1 + len(mask_tr))[:, np.newaxis], axis=0) - 1
        self.mask_lr = np.sum(mask_lr * np.arange(1, 1 + len(mask_lr))[:, np.newaxis], axis=0) - 1
        self.mask_tl = np.sum(mask_tl * np.arange(1, 1 + len(mask_tl))[:, np.newaxis], axis=0) - 1

    def yarara_cb(
        self,
        sub_dico="matching_mad",
        sub_dico_rv="matching_mad",
        vec_corr=None,
        continuum="linear",
        substract_map=[],
        add_map=[],
        sig1=1.5,
        sig2=2.5,
        calib_std=10.00e-4,
        method="mean",
    ):
        """
        Produce the measurement of theconvective blueshift (CB) proxy (based on alpha Cen B kernel KDE plot).
        Proxy is produce by taking symetric kernel if flux versus dflux/dlambda plot.

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        plot : True/False, Plot the proxies time-series
        calib_std : std error due to flat-field photon noise
        window_extraction : window of the large window extraction
        window_core : window in wavelength bin elements to extract the core (if None choosen to maximise the std )
        optimize : choose if optimization on H, K or both
        """

        myf.print_box("\n---- RECIPE : CB PROXY EXTRACTION ----\n")

        directory = self.directory
        rv_sys = self.rv_sys
        self.import_table()
        jdb = np.array(self.table["jdb"])
        self.import_material()
        self.import_proxies()

        load = self.material
        file_random = self.import_spectrum()
        wave = file_random["wave"]
        kw = "_planet" * self.planet

        spectrum_norm = myc.tableXY(
            load["wave"], load["reference_spectrum"] * load["correction_factor"]
        )
        s1 = spectrum_norm.y
        ds1 = np.gradient(s1)

        spec_rep = myc.tableXY(ds1, s1)

        sigmas_values = pd.read_pickle(root + "/Python/Material/KDE1_CB_cenB.p").keys()
        sigmas = [
            float(s.split("_")[1]) for s in sigmas_values
        ]  # [0.1,0.5,1,1.5,2,2.5,3][::-1] #to not modify

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        fluxes = []
        maps_std = []

        # self.import_lbl()
        # rv_table = myc.table(self.lbl[sub_dico_rv]['lbl'][0])
        # valid = (self.lbl[sub_dico_rv]['catalog']['valid']) #& (sts.lbl[sub_dico_rv]['catalog']['line_depth']>0.3)
        # vec_lbl = rv_table.rv_subselection(rv_std=self.lbl[sub_dico_rv]['lbl_std'][0], selection=np.array(valid))
        # vec_lbl.x = self.table.jdb
        # vec_lbl.yerr = np.sqrt(vec_lbl.yerr**2+0.7**2)

        if vec_corr is not None:
            vec_lbl = vec_corr.copy()
        else:
            try:
                vec_lbl = sts.import_ccf_timeseries(
                    "CCF_kitcat_mask_" + self.starname, sub_dico_rv, "rv"
                )
            except:
                vec_lbl = myc.tableXY(
                    self.table.jdb, self.table.ccf_rv * 1000, self.table.ccf_rv_std * 1000
                )

        if kw != "":
            print("\n---- PLANET ACTIVATED ----")
        time.sleep(1)

        for j in tqdm(files):
            file = pd.read_pickle(j)
            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]

            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            fluxes.append(f_norm)
            maps_std.append(f_norm_std + calib_std)

        fluxes = np.array(fluxes)
        maps_std = np.array(maps_std)
        fluxes *= np.array(load["correction_factor"])
        maps_std *= np.array(load["correction_factor"])

        for maps in substract_map:
            fluxes = self.yarara_substract_map(fluxes, maps, correction_factor=True)

        for maps in add_map:
            fluxes = self.yarara_add_map(fluxes, maps, correction_factor=True)

        med_flux = np.median(fluxes, axis=0)
        maps = fluxes - med_flux

        try:
            self.mask_ll
        except:
            print("[ERROR] You must first ran the recipe yarara_kde_mask")

        def kernel_val(Kr):
            if not np.sum(Kr):
                val1 = 0
            else:
                if method == "mean":
                    val1 = np.mean(maps[:, Kr], axis=1)

                elif method == "median":
                    val1 = np.median(maps[:, Kr], axis=1)

                elif method == "weighted":
                    val1 = np.sum(maps[:, Kr] / maps_std[:, Kr] ** 2, axis=1) / np.sum(
                        1 / maps_std[:, Kr] ** 2, axis=1
                    )

            return val1

        if sig1 is None:
            sig1, dust = np.meshgrid(sigmas[::-1], sigmas[::-1])
            sig1 = np.ravel(sig1)[1:]
        else:
            sig1 = [sig1]

        if sig2 is None:
            dust, sig2 = np.meshgrid(sigmas[::-1], sigmas[::-1])
            sig2 = np.ravel(sig2)[1:]
        else:
            sig2 = [sig2]

        if len(sig1) > len(sig2):
            sig2 = sig2 * len(sig1)
        if len(sig2) > len(sig1):
            sig1 = sig1 * len(sig2)

        def proxy_extraction(sig_1, sig_2):
            a_left = (self.mask_ll >= myf.find_nearest(sigmas, sig_1)[0]) & (ds1 < 0)
            a_right = (self.mask_lr >= myf.find_nearest(sigmas, sig_1)[0]) & (ds1 > 0)
            b_left = (self.mask_tl >= myf.find_nearest(sigmas, sig_2)[0]) & (ds1 < 0)
            b_right = (self.mask_tr >= myf.find_nearest(sigmas, sig_2)[0]) & (ds1 > 0)

            inter_left = a_left & b_left
            inter_right = a_right & b_right

            a_left = a_left & (~inter_left)
            a_right = a_right & (~inter_right)
            b_left = b_left & (~inter_left)
            b_right = b_right & (~inter_right)

            factor = 1 * len(a_left) / sum(s1 < 0.90) + 0

            a1 = 100 * sum(a_left) / len(a_left) * factor
            a2 = 100 * sum(a_right) / len(a_right) * factor
            b1 = 100 * sum(b_left) / len(b_left) * factor
            b2 = 100 * sum(b_right) / len(b_right) * factor

            print("\n----------")
            print("sigma kde 1 : %.1f, sigma kde 2 : %.1f \n" % (sig_1, sig_2))
            print("----------\n")
            print("A_left : %.0f (%.0f)" % (sum(a_left), a1))
            print("A_right : %.0f (%.0f)" % (sum(a_right), a2))
            print("B_left : %.0f (%.0f)" % (sum(b_left), b1))
            print("B_right : %.0f (%.0f)" % (sum(b_right), b2))

            a = kernel_val(a_left) + kernel_val(a_right)
            b = kernel_val(b_left) + kernel_val(b_right)
            wb_build = myc.tableXY(vec_lbl.x, a - b)
            correlation_rv = vec_lbl.corr(wb_build, color="r", Draw=False)
            correlation_ca2 = wb_build.corr(self.ca2, Draw=False)
            correlation_wb = wb_build.corr(self.wb, Draw=False)

            print("Rms : %.2f m/s" % (correlation_rv.fit_line_rms))
            print(
                "R_pearson (RV|CaII|WB) : %.2f | %.2f | %.2f"
                % (
                    correlation_rv.r_pearson_w,
                    correlation_ca2.r_pearson_w,
                    correlation_wb.r_pearson_w,
                )
            )

            return (
                a1,
                a2,
                b1,
                b2,
                a_left,
                a_right,
                b_left,
                b_right,
                wb_build,
                correlation_rv,
                correlation_rv.fit_line_rms,
            )

        if (len(sig1) == 1) & (len(sig2) == 1):
            sig1 = sig1[0]
            sig2 = sig2[0]
        else:
            rms = []
            for sig_1, sig_2 in zip(sig1, sig2):
                (
                    a1,
                    a2,
                    b1,
                    b2,
                    a_left,
                    a_right,
                    b_left,
                    b_right,
                    wb_build,
                    s,
                    rms_res,
                ) = proxy_extraction(sig_1, sig_2)
                rms.append(rms_res)
            rms = np.array(rms)
            sig1 = sig1[rms.argmin()]
            sig2 = sig2[rms.argmin()]

        a1, a2, b1, b2, a_left, a_right, b_left, b_right, wb_build, s, rms_res = proxy_extraction(
            sig1, sig2
        )

        plt.figure(figsize=(24, 5))
        plt.axes([0.05, 0.10, 0.2, 0.85])
        plt.xlim(-0.13, 0.13)
        plt.title(
            r"$A_{left} = %.0f$  / $A_{right} = %.0f$ / $B_{left} = %.0f$ / $B_{right} = %.0f $"
            % (a1, a2, b1, b2)
        )
        plt.scatter(spec_rep.x, spec_rep.y, color="k", s=1)

        plt.scatter(spec_rep.x[a_left], spec_rep.y[a_left], color="g", s=5, alpha=0.1)
        plt.scatter(spec_rep.x[a_right], spec_rep.y[a_right], color="purple", s=5, alpha=0.1)

        plt.scatter(spec_rep.x[b_left], spec_rep.y[b_left], color="b", s=5, alpha=0.1)
        plt.scatter(spec_rep.x[b_right], spec_rep.y[b_right], color="red", s=5, alpha=0.1)

        plt.ylabel("Flux normalised", fontsize=13)
        plt.xlabel(r"$df/d\lambda$", fontsize=13)

        a = kernel_val(a_left) + kernel_val(a_right)
        b = kernel_val(b_left) + kernel_val(b_right)
        wb_build = myc.tableXY(vec_lbl.x, a - b)

        wb_build.yerr *= 0
        wb_build.yerr += 2.82e-5
        wb_build.rms_w()

        vec_lbl.recenter(who="Y")
        wb_build.recenter(who="Y")

        plt.axes([0.3, 0.575, 0.4, 0.375])
        plt.title(method + "_sig" + str(sig1) + "_sig" + str(sig2), fontsize=14)
        vec_lbl.rms_w()
        wb_build.rms_w()
        fac = wb_build.rms / vec_lbl.rms
        wb_build.y /= fac
        wb_build.yerr /= fac
        vec_lbl.plot()
        wb_build.plot(color="r")
        plt.ylabel("RV [m/s]", fontsize=13)
        plt.xlabel("Time [days]", fontsize=13)

        ax = plt.gca()
        plt.axes([0.3, 0.10, 0.4, 0.375])
        wb_build.periodogram(nb_perm=1, color="r", Norm=True, level=0.99)
        vec_lbl.periodogram(nb_perm=1, color="k", Norm=True, level=0.99)
        plt.axes([0.75, 0.10, 0.2, 0.85])
        s = vec_lbl.corr(wb_build, color="r")
        plt.ylabel("RV [m/s]", fontsize=13)
        plt.xlabel("Proxy [u.a]", fontsize=13)

        plt.show(block=False)

        wb_build.y *= fac
        wb_build.yerr *= fac

        self.cb = wb_build

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file["parameters"]["CB"] = wb_build.y[i]
            file["parameters"]["CB_std"] = wb_build.yerr[i]
            myf.pickle_dump(file, open(j, "wb"))

        self.yarara_analyse_summary()

        plt.savefig(self.dir_root + "IMAGES/CB_extraction.png")

    def yarara_cb2(self, sub_dico="matching_mad"):
        """
        Produce the measurement of theconvective blueshift (CB) proxy (based on alpha Cen B kernel KDE plot).
        Proxy is produce by taking symetric kernel if flux versus dflux/dlambda plot.

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        plot : True/False, Plot the proxies time-series
        calib_std : std error due to flat-field photon noise
        window_extraction : window of the large window extraction
        window_core : window in wavelength bin elements to extract the core (if None choosen to maximise the std )
        optimize : choose if optimization on H, K or both
        """

        myf.print_box("\n---- RECIPE : CB PROXY EXTRACTION 2----\n")

        directory = self.directory
        rv_sys = self.rv_sys
        self.import_table()
        jdb = np.array(self.table["jdb"])
        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        ccf_output = self.yarara_ccf(
            mask="mask_cb", sub_dico="matching_mad", squared=False, plot=False, save=False
        )

        wb_build = ccf_output["contrast"]
        wb_build.y = 1 - wb_build.y
        wb_build.yerr *= 0
        wb_build.yerr += np.nanstd(wb_build.y)

        self.cb2 = wb_build

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file["parameters"]["CB2"] = wb_build.y[i]
            file["parameters"]["CB2_std"] = wb_build.yerr[i]
            myf.pickle_dump(file, open(j, "wb"))

        self.yarara_analyse_summary()

        self.import_proxies()
        table_proxy = self.table_proxy

        plt.figure(figsize=(15, 5))
        plt.subplot(3, 1, 1)
        ax = plt.gca()
        plt.scatter(jdb, table_proxy["WB"], label="WB")
        plt.legend()
        plt.subplot(3, 1, 2, sharex=ax)
        ax = plt.gca()
        plt.scatter(jdb, table_proxy["CB"], label="CB")
        plt.legend()
        plt.subplot(3, 1, 3, sharex=ax)
        ax = plt.gca()
        plt.scatter(jdb, table_proxy["CB2"], label="CB2")
        plt.legend()
        plt.subplots_adjust(hspace=0)
        plt.savefig(self.dir_root + "IMAGES/CB2_extraction.png")

    def yarara_wb_ca2(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        substract_proxy="CaII",
        substract_map=[],
        add_map=[],
        save=True,
        plot=True,
        calib_std=10.00e-4,
        window_extraction=0.35,
        window_core=None,
        optimize="both",
        p_noise=1 / np.inf,
    ):
        """
        Produce the measurement of the Wilson-Bappu proxy (better to use after removing CaIi fit).
        Proxy is produce by taking -/+/- sign of the integrated flux.

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        plot : True/False, Plot the proxies time-series
        calib_std : std error due to flat-field photon noise
        window_extraction : window of the large window extraction
        window_core : window in wavelength bin elements to extract the core (if None choosen to maximise the std )
        optimize : choose if optimization on H, K or both
        """

        myf.print_box("\n---- RECIPE : WB PROXY EXTRACTION ----\n")

        directory = self.directory
        rv_sys = self.rv_sys
        self.import_table()
        jdb = np.array(self.table["jdb"])
        self.import_material()
        load = self.material
        file_random = self.import_spectrum()
        wave = file_random["wave"]
        kw = "_planet" * self.planet

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        Ca2H = 3968.469
        Ca2K = 3933.663

        fluxes = []
        err_fluxes = []
        if optimize == "both":
            opt = 0
        elif optimize == "H":
            opt = 1
        else:
            opt = 2

        if kw != "":
            print("\n---- PLANET ACTIVATED ----")
        time.sleep(1)

        for j in tqdm(files):
            file = pd.read_pickle(j)
            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]

            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            fluxes.append(f_norm)
            err_fluxes.append(f_norm_std + calib_std)

        fluxes = np.array(fluxes)
        err_fluxes = np.array(err_fluxes)
        fluxes *= np.array(load["correction_factor"])
        err_fluxes *= np.array(load["correction_factor"])

        noise_matrix, noise_values = self.yarara_poissonian_noise(
            noise_wanted=p_noise, wave_ref=None, flat_snr=True
        )
        fluxes += noise_matrix
        err_fluxes = np.sqrt(err_fluxes**2 + noise_values**2)

        for maps in substract_map:
            fluxes = self.yarara_substract_map(fluxes, maps, correction_factor=True)

        for maps in add_map:
            fluxes = self.yarara_add_map(fluxes, maps, correction_factor=True)

        med_flux = np.median(fluxes, axis=0)
        fluxes = fluxes - med_flux

        idx1 = int(myf.find_nearest(wave, 3900)[0])
        idx2 = int(myf.find_nearest(wave, 4000)[0]) + 1

        if (idx1 + idx2 - 1) == 0:
            wb = myc.tableXY(jdb, np.zeros(len(jdb)))
            wb_k = myc.tableXY(jdb, np.zeros(len(jdb)))
            wb_h = myc.tableXY(jdb, np.zeros(len(jdb)))
            core = myc.tableXY(jdb, np.zeros(len(jdb)))
        else:
            wave = wave[idx1:idx2]
            fluxes = fluxes[:, idx1:idx2]
            err_fluxes = err_fluxes[:, idx1:idx2]

            proxy = np.array([np.ones(len(jdb)), np.array(self.table[substract_proxy])])
            f = myc.table(fluxes.T)
            f.fit_base(base_vec=proxy, weight=(1 / err_fluxes**2).T)
            fluxes = f.vec_residues.T

            if rv_sys is None:
                if file_random["parameters"]["RV_sys"] is not None:
                    rv_sys = 1000 * file_random["parameters"]["RV_sys"]
                else:
                    rv_sys = 0
            else:
                rv_sys *= 1000

            if window_extraction is None:
                window_extraction = list(np.arange(0.25, 0.41, 0.01)[::-1])
            else:
                window_extraction = [window_extraction]

            s = []
            loop = 1
            num = -1
            length = len(window_extraction) - 1
            while loop:
                num += 1
                win_extraction = window_extraction[num]
                left_CaIIK = int(
                    myf.find_nearest(wave, myf.doppler_r(Ca2K, rv_sys)[0] - win_extraction)[0]
                )
                middle_CaIIK = int(myf.find_nearest(wave, myf.doppler_r(Ca2K, rv_sys)[0])[0])
                right_CaIIK = int(
                    myf.find_nearest(wave, myf.doppler_r(Ca2K, rv_sys)[0] + win_extraction)[0]
                )

                left_CaIIH = int(
                    myf.find_nearest(wave, myf.doppler_r(Ca2H, rv_sys)[0] - win_extraction)[0]
                )
                middle_CaIIH = int(myf.find_nearest(wave, myf.doppler_r(Ca2H, rv_sys)[0])[0])
                right_CaIIH = int(
                    myf.find_nearest(wave, myf.doppler_r(Ca2H, rv_sys)[0] + win_extraction)[0]
                )

                CaIIK_pxl = np.min([middle_CaIIK - left_CaIIK, right_CaIIK - middle_CaIIK])
                CaIIH_pxl = np.min([middle_CaIIH - left_CaIIH, right_CaIIH - middle_CaIIH])

                pxl = np.min([CaIIK_pxl, CaIIH_pxl])
                if not pxl:
                    pxl = 2

                wb_ca2 = []
                wb_ca2_std = []
                wb_iq = []
                wb_ca2h = []
                wb_ca2h_std = []
                wbh_iq = []
                wb_ca2k = []
                wb_ca2k_std = []
                wbk_iq = []
                core = []
                core_std = []
                core_iq = []

                if window_core is None:
                    liste = np.arange(1, pxl)
                else:
                    liste = [window_core]

                kernel = []
                for win in liste:
                    ker = np.zeros(len(med_flux))
                    ker[idx1:idx2][middle_CaIIK - pxl : middle_CaIIK + pxl + 1] = -1
                    ker[idx1:idx2][middle_CaIIK - win : middle_CaIIK + win + 1] = 1

                    ker[idx1:idx2][middle_CaIIH - pxl : middle_CaIIH + pxl + 1] = -1
                    ker[idx1:idx2][middle_CaIIH - win : middle_CaIIH + win + 1] = 1

                    kernel.append(ker.copy())

                    CoreK = np.sum(fluxes[:, middle_CaIIK - win : middle_CaIIK + win + 1], axis=1)
                    CoreK_std = np.sqrt(
                        np.sum(
                            (err_fluxes[:, middle_CaIIK - win : middle_CaIIK + win]) ** 2, axis=1
                        )
                    )
                    CaIIK = np.sum(
                        fluxes[:, middle_CaIIK - pxl : middle_CaIIK + pxl + 1], axis=1
                    ) - 2 * np.sum(fluxes[:, middle_CaIIK - win : middle_CaIIK + win], axis=1)
                    CaIIK_std = np.sum(
                        (err_fluxes[:, middle_CaIIK - pxl : middle_CaIIK + pxl + 1]) ** 2, axis=1
                    )
                    CaIIK_std = np.sqrt(CaIIK_std)
                    CaIIK /= 2 * pxl
                    CaIIK_std /= 2 * pxl

                    CoreH = np.sum(fluxes[:, middle_CaIIH - win : middle_CaIIH + win + 1], axis=1)
                    CoreH_std = np.sqrt(
                        np.sum(
                            (err_fluxes[:, middle_CaIIH - win : middle_CaIIH + win + 1]) ** 2,
                            axis=1,
                        )
                    )
                    CaIIH = np.sum(
                        fluxes[:, middle_CaIIH - pxl : middle_CaIIH + pxl + 1], axis=1
                    ) - 2 * np.sum(fluxes[:, middle_CaIIH - win : middle_CaIIH + win + 1], axis=1)
                    CaIIH_std = np.sum(
                        (err_fluxes[:, middle_CaIIH - pxl : middle_CaIIH + pxl + 1]) ** 2, axis=1
                    )
                    CaIIH_std = np.sqrt(CaIIH_std)
                    CaIIH /= 2 * pxl
                    CaIIH_std /= 2 * pxl

                    wb_ca2.append(0.5 * (CaIIH + CaIIK))
                    wb_ca2_std.append(0.5 * (np.sqrt(CaIIH_std**2 + CaIIK_std**2)))

                    wb_ca2h.append(CaIIH)
                    wb_ca2h_std.append(CaIIH_std)

                    wb_ca2k.append(CaIIK)
                    wb_ca2k_std.append(CaIIK_std)

                    wb_iq.append(myf.IQ(wb_ca2[-1]))
                    wbh_iq.append(myf.IQ(wb_ca2h[-1]))
                    wbk_iq.append(myf.IQ(wb_ca2k[-1]))

                    core.append(CoreK + CoreH)
                    core_std.append(np.sqrt(CoreK_std**2 + CoreH_std**2))
                    core_iq.append(myf.IQ(core[-1]))

                wb_ca2 = np.array(wb_ca2)
                wb_ca2_std = np.array(wb_ca2_std)
                wb_ca2h = np.array(wb_ca2h)
                wb_ca2h_std = np.array(wb_ca2h_std)
                wb_ca2k = np.array(wb_ca2k)
                wb_ca2k_std = np.array(wb_ca2k_std)
                wb_iq = np.array(wb_iq)
                wbh_iq = np.array(wbh_iq)
                wbk_iq = np.array(wbk_iq)

                kernel = np.array(kernel)

                core = np.array(core)
                core_std = np.array(core_std)
                core_iq = np.array(core_iq)

                s.append(np.max([wb_iq, wbh_iq, wbk_iq][opt]))
                if num == length:
                    window_extraction = window_extraction + [
                        window_extraction[np.array(s).argmax()]
                    ]
                elif num == (length + 1):
                    break

            opti = [wb_iq.argmax(), wbh_iq.argmax(), wbk_iq.argmax()][opt]
            self.wb_kernel = kernel[opti]

            if (len(wb_iq) > 1) & (plot):
                plt.figure(figsize=(24, 9))
                plt.axes([0.05, 0.07, 0.27, 0.89])
                plt.title(
                    "Optimization %s :     %s     %.2f AA     %.0f pxl"
                    % (["H&K", "H", "K"][opt], sub_dico, win_extraction, liste[opti])
                )
                plt.plot(liste, wb_iq, color="k")
                plt.plot(liste, wbh_iq, color="r")
                plt.plot(liste, wbk_iq, color="b")
                plt.axvline(x=liste[opti], color="k", alpha=0.3)

            print("\nOptimization on CaII %s" % (["H&K", "H", "K"][opt]))
            print("\nBest Window for extraction : %.2f AA" % (win_extraction))
            print("\nBest window for the core : %.0f pixels" % (liste[opti]))

            self.wb_win_pos = liste[opti]
            self.wb_win_neg = win_extraction

            wb = myc.tableXY(jdb, -wb_ca2[opti], wb_ca2_std[opti])
            wb_k = myc.tableXY(jdb, -wb_ca2k[opti], wb_ca2k_std[opti])
            wb_h = myc.tableXY(jdb, -wb_ca2h[opti], wb_ca2h_std[opti])

            core = myc.tableXY(jdb, core[core_iq.argmax()], core_std[core_iq.argmax()])

            wb_k.rm_outliers(who="Yerr", replace=False)
            wb_h.rm_outliers(who="Yerr", replace=False)
            wb.rm_outliers(who="Yerr", replace=False)

            if plot:
                if len(wb_iq) > 1:
                    plt.axes([0.37, 0.76, 0.60, 0.2])
                else:
                    plt.figure()
                    plt.subplot(4, 1, 1)
                ax = plt.gca()
                ca2 = myc.tableXY(jdb, self.table["CaII"], self.table["CaII_std"])
                ca2.rm_outliers(who="Yerr")
                ca2.plot(label="CaII")
                plt.legend()

                ca2.substract_polyfit(3)

                if len(wb_iq) > 1:
                    plt.axes([0.37, 0.53, 0.60, 0.2])
                else:
                    plt.subplot(4, 1, 2)
                ax2 = plt.gca()
                ca2.detrend_poly.periodogram(nb_perm=1, color="k", Norm=True, level=0.99)

                if len(wb_iq) > 1:
                    plt.axes([0.37, 0.30, 0.60, 0.2], sharex=ax)
                else:
                    plt.subplot(4, 1, 3, sharex=ax)
                wb_k.plot(color="b", label="WB K_line", alpha=0.3, mask=wb_k.mask)
                wb_h.plot(color="r", label="WB H_line", alpha=0.3, mask=wb_h.mask)
                wb.plot(label="Wilson-Bappu", mask=wb.mask)

                plt.legend()
                if len(wb_iq) > 1:
                    plt.axes([0.37, 0.07, 0.60, 0.2])
                else:
                    plt.subplot(4, 1, 4, sharex=ax2)
                wb_h.periodogram(nb_perm=1, color="r", Norm=True, level=0.99)
                wb_k.periodogram(nb_perm=1, color="b", Norm=True, level=0.99)
                wb.periodogram(nb_perm=1, Norm=True, level=0.99)

                plt.show(block=False)

        self.w_bappu = wb
        self.w_bappu_k = wb_k
        self.w_bappu_h = wb_h
        self.wilson_bappu_core = core
        if save:
            for i, j in enumerate(files):
                file = pd.read_pickle(j)
                file["parameters"]["WB_K"] = wb_k.y[i]
                file["parameters"]["WB_H"] = wb_h.y[i]
                file["parameters"]["WB"] = wb.y[i]
                file["parameters"]["WB_K_std"] = wb_k.yerr[i]
                file["parameters"]["WB_H_std"] = wb_h.yerr[i]
                file["parameters"]["WB_std"] = wb.yerr[i]
                myf.pickle_dump(file, open(j, "wb"))

        self.yarara_analyse_summary()

        if plot:
            plt.savefig(self.dir_root + "IMAGES/Wilson-Bappu_extraction.pdf")

    def yarara_wb_h1(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        plot=True,
        calib_std=10.00e-4,
        window_extraction=[0.70, 0.25, 0.15],
        window_core=[None, None, None],
        substract_map=[],
        add_map=[],
    ):
        """
        Produce the measurement of the Wilson-Bappu proxy (better to use after removing CaIi fit).
        Proxy is produce by taking -/+/- sign of the integrated flux.

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        plot : True/False, Plot the proxies time-series
        calib_std : std error due to flat-field photon noise
        window_extraction : window of the large window extraction for Halpha, Hbeta, Hgamma
        window_core : window in wavelength bin elements to extract the core (if None choosen to maximise the std )
        """

        directory = self.directory
        rv_sys = self.rv_sys
        self.import_table()
        tab = self.table
        jdb = np.array(self.table["jdb"])
        self.import_material()
        load = self.material
        file_random = self.import_spectrum()
        wave = file_random["wave"]

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        if rv_sys is None:
            if file_random["parameters"]["RV_sys"] is not None:
                rv_sys = 1000 * file_random["parameters"]["RV_sys"]
            else:
                rv_sys = 0
        else:
            rv_sys *= 1000

        Ha = 6562.79
        Hb = 4861.35
        Hc = 4340.472  # Heps is in fact H zeta

        fluxes = []
        err_fluxes = []

        for j in tqdm(files):
            file = pd.read_pickle(j)
            f = file["flux"]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]

            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            fluxes.append(f_norm)
            err_fluxes.append(f_norm_std + calib_std)

        fluxes = np.array(fluxes)
        err_fluxes = np.array(err_fluxes)
        fluxes *= np.array(load["correction_factor"])
        err_fluxes *= np.array(load["correction_factor"])

        for maps in substract_map:
            fluxes = self.yarara_substract_map(fluxes, maps, correction_factor=True)

        for maps in add_map:
            fluxes = self.yarara_add_map(fluxes, maps, correction_factor=True)

        med_flux = np.median(fluxes, axis=0)
        fluxes = fluxes - med_flux

        w_bappu_h1 = []

        for name, line, win1, win2 in zip(
            ["Ha", "Hb", "Hc"], [Ha, Hb, Hc], window_extraction, window_core
        ):
            print("--- Line analysed %s ---" % (name))
            left = int(myf.find_nearest(wave, myf.doppler_r(line, rv_sys)[0] - win1)[0])
            middle = int(myf.find_nearest(wave, myf.doppler_r(line, rv_sys)[0])[0])
            right = int(myf.find_nearest(wave, myf.doppler_r(line, rv_sys)[0] + win1)[0])

            pxl = np.min([middle - left, right - middle])

            wb_ca2 = []
            wb_ca2_std = []
            wb_iq = []

            if win2 is None:
                liste = np.arange(0, pxl)
            else:
                liste = [win2]

            for win1 in liste:
                Hflux = np.sum(fluxes[:, middle - pxl : middle + pxl + 1], axis=1) - np.sum(
                    fluxes[:, middle - win1 : middle + win1 + 1], axis=1
                )
                Hflux_std = np.sum((err_fluxes[:, middle - pxl : middle + pxl + 1]) ** 2, axis=1)
                Hflux_std = np.sqrt(Hflux_std)
                Hflux /= 2 * pxl
                Hflux_std /= 2 * pxl

                wb_ca2.append(Hflux)
                wb_ca2_std.append(Hflux_std)
                wb_iq.append(myf.IQ(wb_ca2[-1]))

            wb_ca2 = np.array(wb_ca2)
            wb_ca2_std = np.array(wb_ca2_std)
            wb_iq = np.array(wb_iq)

            print("\nBest window for the core : %.0f pixels" % (liste[wb_iq.argmax()]))

            wb = myc.tableXY(jdb, -wb_ca2[wb_iq.argmax()], wb_ca2_std[wb_iq.argmax()])

            plt.figure()
            plt.subplot(3, 1, 1)
            ax = plt.gca()
            ca2 = myc.tableXY(jdb, tab["CaII"], tab["CaII_std"])
            ca2.plot()
            plt.title("Line %s" % (name))

            plt.subplot(3, 1, 2, sharex=ax)
            wb.plot(label="Wilson-Bappu")
            plt.legend()
            plt.subplot(3, 1, 3)
            wb.periodogram(nb_perm=1)
            plt.show(block=False)

            w_bappu_h1.append(wb)

        self.w_bappu_all_h1 = w_bappu_h1
        self.w_bappu_h1 = myc.tableXY(
            w_bappu_h1[0].x,
            w_bappu_h1[0].y + w_bappu_h1[1].y + w_bappu_h1[2].y,
            np.sqrt(w_bappu_h1[0].yerr ** 2 + w_bappu_h1[1].yerr ** 2 + w_bappu_h1[2].yerr ** 2),
        )

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file["parameters"]["WB_H1"] = self.w_bappu_h1.y[i]
            file["parameters"]["WB_H1_std"] = self.w_bappu_h1.yerr[i]
            myf.pickle_dump(file, open(j, "wb"))

        self.yarara_analyse_summary()

    def yarara_metallicity(
        self,
        ref=[root + "/Python/Material/kitcat_cleaned_mask_Sun.p", 5800],
        continuum_tresh=0.95,
        min_nb_lines=10,
    ):

        """
        The Sun HARPS spectra were taken with the new fibers
        """
        self.import_kitcat(clean=True)
        kitcat = self.kitcat

        rv_sys = kitcat["spectre"]["rv_sys"]
        star = myc.tableXY(
            myf.doppler_r(kitcat["spectre"]["wave"], rv_sys * 1000)[1],
            kitcat["spectre"]["flux"] * kitcat["spectre"]["correction_factor"],
            0 * kitcat["spectre"]["wave"],
        )
        kitcat_star = kitcat["catalogue"]

        kitcat = pd.read_pickle(ref[0])
        rv_sys_ref = kitcat["spectre"]["rv_sys"]
        ref = myc.tableXY(
            myf.doppler_r(kitcat["spectre"]["wave"], rv_sys_ref * 1000)[1],
            kitcat["spectre"]["flux"] * kitcat["spectre"]["correction_factor"],
            0 * kitcat["spectre"]["wave"],
        )
        kitcat_ref = kitcat["catalogue"]

        kitcat_ref["dist_continuum"] = np.min(kitcat_ref[["flux_left", "flux_right"]], axis=1)
        kitcat_star["dist_continuum"] = np.min(kitcat_star[["flux_left", "flux_right"]], axis=1)

        for j in range(4):
            kit_star = kitcat_star.loc[
                (kitcat_star["dist_continuum"] > (continuum_tresh - j * 0.05))
                & (kitcat_star["dist_continuum"] < 1.01)
                & (kitcat_star["blend_crit"] == 1)
            ]
            kitcat_ref = kitcat_ref.loc[
                (kitcat_ref["dist_continuum"] > continuum_tresh)
                & (kitcat_ref["dist_continuum"] < 1.01)
                & (kitcat_ref["blend_crit"] == 1)
            ]
            if len(kit_star) > 100:
                kitcat_star = kit_star

        kitcat_star = kitcat_star.dropna(subset=["element"])
        kitcat_ref = kitcat_ref.dropna(subset=["element"])

        kitcat_star = kitcat_star.reset_index(drop=True)
        kitcat_ref = kitcat_ref.reset_index(drop=True)

        match = myf.match_nearest(
            np.array(kitcat_star["freq_mask0"]), np.array(kitcat_ref["freq_mask0"])
        )
        match = match[abs(match[:, -1]) < 0.02]

        kitcat_star = kitcat_star.loc[match[:, 0].astype("int")]
        kitcat_ref = kitcat_ref.loc[match[:, 1].astype("int")]

        kitcat_star = kitcat_star.reset_index(drop=True)
        kitcat_ref = kitcat_ref.reset_index(drop=True)

        mask = kitcat_ref["element"] == kitcat_star["element"]
        kitcat_ref = kitcat_ref.loc[mask]
        kitcat_ref = kitcat_ref.reset_index(drop=True)
        kitcat_star = kitcat_star.loc[mask]
        kitcat_star = kitcat_star.reset_index(drop=True)

        counts = kitcat_star["element"].value_counts()
        counts = counts.loc[counts > min_nb_lines]
        element_kept = np.array(list(counts.keys()))

        mask = np.in1d(kitcat_ref["element"], element_kept)
        kitcat_ref = kitcat_ref.loc[mask]
        kitcat_ref = kitcat_ref.reset_index(drop=True)
        kitcat_star = kitcat_star.loc[mask]
        kitcat_star = kitcat_star.reset_index(drop=True)

        counts = kitcat_star["element"].value_counts()
        print("\n Number of lines availible for the mettalicity : %.0f" % (len(kitcat_ref)))
        print("\n Number or lines per element :\n")
        print(counts)
        print("\n")

        if len(kitcat_ref) < 10:
            self.yarara_star_info(FeH=["YARARA", np.nan])
        else:
            plt.figure(figsize=(20, 5))
            plt.plot(star.x, star.y, color="k")
            plt.plot(ref.x, ref.y, color="gray")
            plt.scatter(
                myf.doppler_r(np.array(kitcat_star["wave"]), rv_sys * 1000)[1],
                1 - np.array(kitcat_star["line_depth"]),
                color="b",
                zorder=10,
            )
            plt.scatter(
                myf.doppler_r(np.array(kitcat_ref["wave"]), rv_sys_ref * 1000)[1],
                1 - np.array(kitcat_ref["line_depth"]),
                color="r",
                zorder=11,
            )

            if np.product(
                np.std(np.log(kitcat_star["line_depth"] / kitcat_ref["line_depth"]), axis=0)
            ):
                test = myc.tableXY(
                    np.arange(len(kitcat_ref["E_low"])),
                    np.log(kitcat_star["line_depth"] / kitcat_ref["line_depth"]),
                )
                base_vec = np.array(
                    [
                        np.ones(len(kitcat_ref["E_low"])),
                        kitcat_ref["E_low"],
                        kitcat_ref["line_depth"],
                        kitcat_ref["E_low"] * kitcat_ref["line_depth"],
                    ]
                )
                test.fit_base(base_vec)

                plt.figure()
                test = myc.tableXY(kitcat_ref["E_low"], kitcat_ref["line_depth"])
                test.fit_poly2d(
                    np.log(kitcat_star["equivalent_width"] / kitcat_ref["equivalent_width"]),
                    expo1=1,
                    expo2=1,
                    Draw=True,
                )

                plt.figure(figsize=(10, 10))
                k = 0
                for elem in element_kept:
                    loca = kitcat_ref["element"] == elem
                    k += 1
                    plt.subplot(2, len(element_kept), k)
                    plt.title(
                        "Element : %s (%.2f)"
                        % (
                            elem,
                            np.median(
                                np.log(
                                    kitcat_star.loc[loca, "line_depth"]
                                    / kitcat_ref.loc[loca, "line_depth"]
                                )
                            ),
                        )
                    )
                    plt.scatter(
                        kitcat_ref.loc[loca, "E_low"],
                        np.log(
                            kitcat_star.loc[loca, "line_depth"]
                            / kitcat_ref.loc[loca, "line_depth"]
                        ),
                        c=kitcat_ref.loc[loca, "line_depth"],
                        cmap="brg",
                    )
                    plt.xlabel(r"$\chi$ [eV]", fontsize=13)
                    plt.ylabel(r"log($d/d_{ref}$)", fontsize=13)
                    plt.ylim(-3, 3)
                    plt.xlim(-0.2, None)
                    plt.axhline(y=0, color="k")
                    plt.axvline(x=0, ls=":", color="k")
                    plt.colorbar()

                    plt.subplot(2, len(element_kept), k + len(element_kept))
                    plt.title(
                        "Element : %s (%.2f)"
                        % (
                            elem,
                            np.median(
                                np.log(
                                    kitcat_star.loc[loca, "equivalent_width"]
                                    / kitcat_ref.loc[loca, "equivalent_width"]
                                )
                            ),
                        )
                    )
                    plt.scatter(
                        kitcat_ref.loc[loca, "E_low"],
                        np.log(
                            kitcat_star.loc[loca, "equivalent_width"]
                            / kitcat_ref.loc[loca, "equivalent_width"]
                        ),
                        c=kitcat_ref.loc[loca, "line_depth"],
                        cmap="brg",
                    )
                    plt.ylim(-3, 3)
                    plt.xlim(-0.2, None)
                    plt.xlabel(r"$\chi$ [eV]", fontsize=13)
                    plt.ylabel(r"log($w/w_{ref}$)", fontsize=13)
                    plt.axhline(y=0, color="k")
                    plt.axvline(x=0, ls=":", color="k")
                    plt.colorbar()

                    if elem == "Fe1":
                        val = np.median(
                            np.log(
                                kitcat_star.loc[loca, "equivalent_width"]
                                / kitcat_ref.loc[loca, "equivalent_width"]
                            )
                        )
                        print("[Fe/H] estimated around %.2f" % (val))
                        self.yarara_star_info(FeH=["YARARA", np.round(val, 2)])

                plt.subplots_adjust(
                    wspace=0.35, hspace=0.35, left=0.09, right=0.91, top=0.92, bottom=0.09
                )

    def yarara_stenflo(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        blended=False,
        morpho=False,
        telluric=False,
    ):
        """Based on Stenflo 1977 analysis"""
        plt.close("all")

        directory = self.directory
        self.import_table()
        self.import_material()
        self.import_kitcat()

        mat = self.material
        catalog = self.kitcat["catalogue"]

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        fluxes = []
        err_fluxes = []
        file_random = self.import_spectrum()
        grid = file_random["wave"]

        for j in tqdm(files):
            file = pd.read_pickle(j)
            f = file["flux"]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            fluxes.append(f_norm)
            err_fluxes.append(f_norm_std)

        fluxes = np.array(fluxes)
        err_fluxes = np.array(err_fluxes)
        fluxes *= np.array(mat["correction_factor"])
        err_fluxes *= np.array(mat["correction_factor"])

        # extract lines for the analysis

        catalog = catalog.loc[catalog["lande_mean"] < 10]
        counter = catalog["element"].value_counts()
        elements_kept = counter.loc[counter > 200].keys()
        colors = ["k", "r", "b", "g", "purple", "pink", "orange"]
        cmaps = ["Greys", "Reds", "Blues", "Greens", "Blues", "Reds", "Reds"]

        dico = {"full": 0}

        for l, elem in enumerate(elements_kept):
            print("---- %s lines ----" % (elem))
            sub_table = catalog[catalog["element"] == elem]
            print("Number of %s lines : %.0f" % (elem, len(sub_table)))
            if not blended:
                sub_table = sub_table[sub_table["blend_crit"] == 1]
                print("Number of %s lines unblended : %.0f" % (elem, len(sub_table)))
            if not morpho:
                sub_table = sub_table[sub_table["morpho_crit"] == 1]
                print("Number of %s lines with good morpho : %.0f" % (elem, len(sub_table)))
            if not telluric:
                sub_table = sub_table[sub_table["rel_contam"] < 0.01]
                print("Number of %s lines with without telluric : %.0f" % (elem, len(sub_table)))
            sub_table = sub_table[abs(sub_table["min_depth"] / sub_table["line_depth"]) > 0.75]
            sub_table = sub_table[sub_table["lande_mean"] < 10]
            sub_table = sub_table[sub_table["line_depth"] > 0.2]

            print("Number of %s lines with good pseudo continuum : %.0f" % (elem, len(sub_table)))
            final_table = sub_table[
                [
                    "wave",
                    "wave_left",
                    "wave_right",
                    "freq_mask0",
                    "line_depth",
                    "depth_rel",
                    "min_depth",
                    "lande_mean",
                    "zeeman",
                    "E_low",
                ]
            ]

            plt.figure(1, figsize=(18, 6))
            plt.plot(mat["wave"], mat["reference_spectrum"] * mat["correction_factor"], color="k")
            for k in final_table["wave"]:
                plt.axvline(x=k, color=colors[l])
            table = myc.tableXY(final_table["lande_mean"], final_table["E_low"])
            table.joint_plot(
                columns=[r"$g_{eff}$", r"$\chi_{low}$"],
                marginal_kws={"color": colors[l], "lw": 0.5},
                joint_kws={"colors": None, "cmap": cmaps[l]},
            )

            # extract fluxes

            idx_left = myf.find_nearest(grid, np.array(final_table["wave_left"]))[0]
            idx_right = myf.find_nearest(grid, np.array(final_table["wave_right"]))[0]

            borders = np.vstack([idx_left, idx_right]).T

            levels = np.arange(10, 71, 10)
            W = np.zeros((len(levels), len(fluxes), len(borders)))
            B = np.zeros((len(levels), len(fluxes), len(borders)))
            S = np.zeros((len(fluxes), len(borders)))

            for line in tqdm(range(len(borders))):
                for time_t in range(len(fluxes)):
                    profile = myc.tableXY(
                        grid[borders[line, 0] : borders[line, 1] + 1],
                        fluxes[time_t, borders[line, 0] : borders[line, 1] + 1],
                        err_fluxes[time_t, borders[line, 0] : borders[line, 1] + 1],
                    )
                    profile.interpolate(new_grid=30, interpolate_x=False)
                    mini_y = profile.y.min()
                    mini_x = profile.y.argmin()
                    profile.y = (profile.y - mini_y) / (1 - mini_y)
                    # profile.x = (profile.x - profile.x[mini_x])/profile.x[mini_x]*3e5

                    left = myc.tableXY(
                        profile.x[0 : mini_x + 1],
                        profile.y[0 : mini_x + 1],
                        profile.yerr[0 : mini_x + 1],
                    )
                    right = myc.tableXY(
                        profile.x[mini_x:], profile.y[mini_x:], profile.yerr[mini_x:]
                    )
                    left.switch()
                    right.switch()
                    left.interpolate(
                        new_grid=np.arange(0, left.x.max(), 0.01), interpolate_x=False
                    )
                    right.interpolate(
                        new_grid=np.arange(0, right.x.max(), 0.01), interpolate_x=False
                    )

                    k = (mini_y + 1) * 0.5

                    area = np.sum(abs(np.gradient(left.y))[0:50] * left.x[0:50]) + np.sum(
                        abs(np.gradient(right.y))[0:50] * right.x[0:50]
                    )

                    area_conv = ((right.y[50] - left.y[50]) * 0.5 - area) * (k - mini_y)

                    S[time_t, line] = (
                        area_conv * 1e6 / np.array(final_table["freq_mask0"])[line]
                    )  # fraunhofer units
                    w = (
                        (right.y[levels] - left.y[levels])
                        / np.array(final_table["freq_mask0"])[line]
                        * 3e5
                    )
                    bis = (
                        (
                            0.5 * (right.y[levels] + left.y[levels])
                            - np.array(final_table["wave"])[line]
                        )
                        / np.array(final_table["wave"])[line]
                        * 3e8
                    )
                    sigma = np.sqrt(-(w**2) / (8 * np.log(1 - levels / 100)))

                    W[:, time_t, line] = sigma
                    B[:, time_t, line] = bis

            dico[elem] = {"table": final_table, "width": W, "bis": B, "S": S}

        del dico["full"]

        ref = np.array(self.table["snr"]).argmax()

        all_S = []
        all_W = []

        for elem in dico.keys():
            all_S.append(dico[elem]["S"][ref, :])
            all_W.append(dico[elem]["width"][len(levels) - 1, ref, :])

        all_S = np.hstack(all_S)
        all_W = np.hstack(all_W)

        t = myc.tableXY(all_S, all_W)
        t.binned_scatter(6, Plot=False)
        sig2 = t.biny + 2 * t.binsup[0]

        for elem in dico.keys():
            S = dico[elem]["S"][ref, :]
            W = dico[elem]["width"][len(levels) - 1, ref, :]
            t2 = myc.tableXY(S, W)
            t2.binned_scatter(t.bins, extend=False, Plot=False)

            kept = t2.y < sig2[t2.binidx]

            final_table = dico[elem]["table"][kept]
            W = dico[elem]["width"][:, :, kept]
            B = dico[elem]["bis"][:, :, kept]
            S = dico[elem]["S"][:, kept]

            dico[elem] = {"table": final_table, "width": W, "bis": B, "S": S}

            plt.figure()
            for j in range(6):
                plt.subplot(2, 3, j + 1)
                a = myc.tableXY(S[ref, :], W[j + 1, ref, :])
                a.myscatter()
                plt.scatter(S[ref, :], W[j + 1, ref, :], c=np.array(final_table["E_low"]))

        tab = self.table

        tab_low = tab.loc[tab["CaII"] < np.percentile(tab["CaII"], 33)]
        tab_low = tab_low.sort_values(by="snr")
        ref = tab_low.index[-1]
        info_day = tab_low.loc[ref]
        print(
            "\nInfo of the selected day to define B=0 : \njdb = %.2f \nsnr = %.0f \nCaII = %.2f"
            % (info_day["jdb"], info_day["snr"], info_day["CaII"])
        )

        for elem in dico.keys():
            S = dico[elem]["S"]
            W = dico[elem]["width"]
            table = dico[elem]["table"]
            geff = np.array(table["lande_mean"])
            Elow = np.array(table["E_low"])
            wave = np.array(table["freq_mask0"])

            v_ref = myc.tableXY(S[ref], W[4, ref, :])
            v_ref.fit_poly(Draw=False, d=2)
            for c in range(len(levels)):
                for time_t in range(len(S)):
                    s = S[time_t]
                    w = W[c, time_t]
                    v0 = np.polyval(v_ref.poly_coefficient, s)
                    geff = np.array(table["lande_mean"])
                    Elow = np.array(table["E_low"])

    # =============================================================================
    # COMPUTE THE TELLURIC CCF
    # =============================================================================

    def yarara_telluric(
        self,
        sub_dico="matching_anchors",
        continuum="linear",
        suppress_broad=True,
        delta_window=5,
        mask=None,
        weighted=False,
        reference=True,
        display_ccf=False,
        ratio=False,
        normalisation="slope",
        ccf_oversampling=3,
        wave_max=None,
        wave_min=None,
    ):

        """
        Plot all the RASSINE spectra in the same plot

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        mask : The telluric mask used to cross correlate with the spectrum (mask should be located in MASK_CCF)
        reference : True/False or 'norm', True use the matching anchors of reference, False use the continuum of each spectrum, norm use the continuum normalised spectrum (not )
        display_ccf : display all the ccf
        normalisation : 'left' or 'slope'. if left normalise the CCF by the most left value, otherwise fit a line between the two highest point
        planet : True/False to use the flux containing the injected planet or not

        """

        kw = "_planet" * self.planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        myf.print_box("\n---- RECIPE : COMPUTE TELLURIC CCF MOMENT ----\n")

        self.import_table()

        if np.nanmax(self.table.berv) - np.nanmin(self.table.berv) < 5:
            reference = False
            ratio = False
            myf.make_sound("warning")
            print(
                Fore.YELLOW
                + "\n [WARNING] BERV SPAN too low to consider ratio spectra as reliable. Diff spectra will be used.\n"
                + Fore.RESET
            )

        berv_max = np.max(abs(self.table["berv" + kw]))
        directory = self.directory
        planet = self.planet

        rv_shift = np.array(self.table["rv_shift"]) * 1000

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("---- DICO %s used ----" % (sub_dico))

        one_file = pd.read_pickle(files[0])
        grid = one_file["wave"]
        flux = one_file["flux" + kw] / one_file[sub_dico]["continuum_linear"]
        dg = grid[1] - grid[0]
        ccf_sigma = int(one_file["parameters"]["fwhm_ccf"] * 10 / 3e5 * 6000 / dg)
        test = myc.tableXY(grid, flux)

        telluric_tag = "telluric"
        if mask is None:
            mask = "telluric"

        if type(mask) == str:
            if mask == "h2o":
                telluric_tag = "h2o"
            elif mask == "o2":
                telluric_tag = "o2"
            mask = np.genfromtxt(root + "/Python/MASK_CCF/mask_telluric_" + mask + ".txt")
            mask = mask[mask[:, 0].argsort()]
            # mask = mask[(mask[:,0]>6200)&(mask[:,0]<6400)]

        wave_tel = 0.5 * (mask[:, 0] + mask[:, 1])
        mask = mask[(wave_tel < np.max(grid)) & (wave_tel > np.min(grid))]
        wave_tel = wave_tel[(wave_tel < np.max(grid)) & (wave_tel > np.min(grid))]

        test.clip(min=[mask[0, 0], None], max=[mask[-1, 0], None])
        test.rolling(window=ccf_sigma, quantile=1)  # to supress telluric in broad asorption line
        tt, matrix = myf.clustering((test.roll < 0.97).astype("int"), tresh=0.5, num=0.5)
        t = np.array([k[0] for k in tt]) == 1
        matrix = matrix[t, :]

        keep_telluric = np.ones(len(wave_tel)).astype("bool")
        for j in range(len(matrix)):
            left = test.x[matrix[j, 0]]
            right = test.x[matrix[j, 1]]

            c1 = np.sign(myf.doppler_r(wave_tel, 30000)[0] - left)
            c2 = np.sign(myf.doppler_r(wave_tel, 30000)[1] - left)
            c3 = np.sign(myf.doppler_r(wave_tel, 30000)[0] - right)
            c4 = np.sign(myf.doppler_r(wave_tel, 30000)[1] - right)
            keep_telluric = keep_telluric & ((c1 == c2) * (c1 == c3) * (c1 == c4))

        if (sum(keep_telluric) > 25) & (
            suppress_broad
        ):  # to avoid rejecting all tellurics for cool stars
            mask = mask[keep_telluric]
        print("\n [INFO] %.0f lines available in the telluric mask" % (len(mask)))
        plt.figure()
        plt.plot(grid, flux)
        for j in 0.5 * (mask[:, 0] + mask[:, 1]):
            plt.axvline(x=j, color="k")

        self.yarara_ccf(
            sub_dico=sub_dico,
            continuum=continuum,
            mask=mask,
            weighted=weighted,
            delta_window=delta_window,
            reference=reference,
            plot=True,
            save=False,
            ccf_oversampling=ccf_oversampling,
            display_ccf=display_ccf,
            normalisation=normalisation,
            ratio=ratio,
            rv_borders=10,
            rv_range=int(berv_max + 7),
            rv_sys=0,
            rv_shift=rv_shift,
            wave_max=wave_max,
            wave_min=wave_min,
        )

        plt.figure(figsize=(6, 6))
        plt.axes([0.15, 0.3, 0.8, 0.6])
        self.ccf_rv.yerr *= 0
        self.ccf_rv.yerr += 50
        self.ccf_rv.plot(modulo=365.25, label="%s ccf rv" % (telluric_tag))
        plt.scatter(self.table.jdb % 365.25, self.table.berv * 1000, color="b", label="berv")
        plt.legend()
        plt.ylabel("RV [m/s]")
        plt.xlabel("Time %365.25 [days]")
        plt.axes([0.15, 0.08, 0.8, 0.2])
        plt.axhline(y=0, color="k", alpha=0.5)
        plt.errorbar(
            self.table.jdb % 365.25,
            self.table.berv * 1000 - self.ccf_rv.y,
            self.ccf_rv.yerr,
            fmt="ko",
        )
        self.berv_offset = np.nanmedian(self.table.berv * 1000 - self.ccf_rv.y) / 1000
        print("\n [INFO] Difference with the BERV : %.0f m/s" % (self.berv_offset * 1000))
        plt.ylabel(r"$\Delta$RV [m/s]")
        plt.savefig(self.dir_root + "IMAGES/telluric_control_check_%s.pdf" % (telluric_tag))

        output = self.ccf_timeseries

        mask_fail = (
            abs(self.table.berv * 1000 - self.ccf_rv.y) > 1000
        )  # rv derive to different fromn the berv -> CCF not ttrustworthly
        mask_fail = mask_fail | np.isnan(self.ccf_rv.y)
        if sum(mask_fail):
            myf.make_sound("warning")
            print(
                Fore.YELLOW
                + " \n[WARNING] There are %.0f datapoints incompatible with the BERV values"
                % (sum(mask_fail))
                + Fore.RESET
            )
            for k in range(len(output)):
                output[k][mask_fail] = np.nanmedian(output[k][~mask_fail])

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file["parameters"][telluric_tag + "_ew"] = output[0][i]
            file["parameters"][telluric_tag + "_contrast"] = output[1][i]
            file["parameters"][telluric_tag + "_rv"] = output[2][i]
            file["parameters"][telluric_tag + "_fwhm"] = output[5][i]
            file["parameters"][telluric_tag + "_center"] = output[6][i]
            file["parameters"][telluric_tag + "_depth"] = output[7][i]
            myf.pickle_dump(file, open(j, "wb"))

        self.yarara_analyse_summary()

    # =============================================================================
    # COMPUTE THE CCF OF THE RASSINE SPECTRUM
    # =============================================================================

    def yarara_ccf(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        mask=None,
        mask_name=None,
        ccf_name=None,
        mask_col="weight_rv",
        treshold_telluric=1,
        ratio=False,
        element=None,
        reference=True,
        weighted=True,
        plot=False,
        display_ccf=False,
        save=True,
        save_ccf_profile=False,
        normalisation="left",
        del_outside_max=False,
        bis_analysis=False,
        ccf_oversampling=1,
        rv_range=None,
        rv_borders=None,
        delta_window=5,
        debug=False,
        rv_sys=None,
        rv_shift=None,
        speed_up=True,
        force_brute=False,
        wave_min=None,
        wave_max=None,
        squared=True,
        p_noise=1 / np.inf,
        substract_map=[],
        add_map=[],
    ):
        """
        Compute the CCF of a spectrum, reference to use always the same continuum (matching_anchors highest SNR).
        Display_ccf to plot all the individual CCF. Plot to plot the FWHM, contrast and RV.

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        mask : The line mask used to cross correlate with the spectrum (mask should be located in MASK_CCF otherwise KITCAT dico)
        mask_col : Column of the KitCat column to use for the weight
        threshold_telluric : Maximum telluric contamination to keep a stellar line in the mask
        reference : True/False or 'norm', True use the matching anchors of reference, False use the continuum of each spectrum, norm use the continuum normalised spectrum (not )
        plot : True/False to plot the RV time-series
        display_ccf : display all the ccf subproduct
        save : True/False to save the informations iun summary table
        normalisation : 'left' or 'slope'. if left normalise the CCF by the most left value, otherwise fit a line between the two highest point
        del_outside maximam : True/False to delete the CCF outside the two bump in personal mask
        speed_up : remove region from the CCF not crossed by a line in the mask to speed up the code
        force_brute : force to remove the region excluded by the brute mask

        """

        directory = self.directory
        planet = self.planet

        def replace_none(y, yerr):
            if yerr is None:
                return np.nan, 1e6
            else:
                return y, yerr

        if rv_range is None:
            rv_range = int(3 * self.fwhm)
            print("\n [INFO] RV range updated to : %.1f kms" % (rv_range))

        if rv_borders is None:
            rv_borders = int(2 * self.fwhm)
            print("\n [INFO] RV borders updated to : %.1f kms" % (rv_borders))

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        self.import_table()

        files = glob.glob(directory + "RASSI*.p")
        files = np.array(self.table["filename"])  # updated 29.10.21 to allow ins_merged
        files = np.sort(files)

        flux = []
        snr = []
        conti = []
        flux_err = []
        jdb = []
        berv = []

        epsilon = 1e-12

        file_random = self.import_spectrum()
        self.import_table()
        self.import_material()
        load = self.material

        mask_loc = mask_name
        if mask_name is None:
            mask_name = "No name"
            mask_loc = "No name"

        if mask is None:
            mask = self.mask_harps
            mask_name = self.mask_harps

        if type(mask) == str:
            if mask.split(".")[-1] == "p":
                loc_mask = self.dir_root + "KITCAT/"
                mask_name = mask
                mask_loc = loc_mask + mask
                dico = pd.read_pickle(mask_loc)["catalogue"]
                dico = dico.loc[dico["rel_contam"] < treshold_telluric]
                if "valid" in dico.keys():
                    dico = dico.loc[dico["valid"]]
                if element is not None:
                    dico = dico.loc[dico["element"] == element]
                mask = np.array([np.array(dico["freq_mask0"]), np.array(dico[mask_col])]).T
                mask = mask[mask[:, 1] != 0]
                print("\n [INFO] Nb lines in the CCF mask : %.0f" % (len(dico)))

            else:
                mask_name = mask
                mask_loc = root + "/Python/MASK_CCF/" + mask + ".txt"
                mask = np.genfromtxt(mask_loc)
                mask = np.array([0.5 * (mask[:, 0] + mask[:, 1]), mask[:, 2]]).T

        if type(mask) == pd.core.frame.DataFrame:
            dico = mask
            mask = np.array(
                [
                    np.array(mask["freq_mask0"]).astype("float"),
                    np.array(mask[mask_col]).astype("float"),
                ]
            ).T

        print("\n [INFO] CCF mask selected : %s \n" % (mask_loc))

        if rv_sys is None:
            if file_random["parameters"]["RV_sys"] is not None:
                rv_sys = 1000 * file_random["parameters"]["RV_sys"]
            else:
                rv_sys = 0
        else:
            rv_sys *= 1000

        if rv_shift is None:
            rv_shift = np.zeros(len(files))

        print("\n [INFO] RV sys : %.2f [km/s] \n" % (rv_sys / 1000))

        mask[:, 0] = myf.doppler_r(mask[:, 0], rv_sys)[0]

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
            snr.append(file["parameters"]["SNR_5500"])
            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            try:
                berv.append(file["parameters"]["berv"])
            except:
                berv.append(0)

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum] + epsilon
            c_std = file["continuum_err"]

            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)

            flux.append(f_norm)
            flux_err.append(f_norm_std)
            conti.append(c - epsilon)

        wave = np.array(wave)
        flux = np.array(flux)
        flux_err = np.array(flux_err)
        snr = np.array(snr)
        conti = np.array(conti)
        jdb = np.array(jdb)
        berv = np.array(berv)
        grid = wave

        flux, flux_err, wave = self.yarara_map(
            sub_dico=sub_dico,
            planet=self.planet,
            wave_min=None,
            wave_max=None,
            Plot=False,
            reference=False,
            substract_map=substract_map,
            add_map=add_map,
            correction_factor=False,
        )  # 04.08.21 in order to include add and substract map

        flux *= np.array(load["correction_factor"])
        flux_err *= np.array(load["correction_factor"])

        noise_matrix, noise_values = self.yarara_poissonian_noise(
            noise_wanted=p_noise, wave_ref=None, flat_snr=True
        )
        flux += noise_matrix
        flux_err = np.sqrt(flux_err**2 + noise_values**2)

        if reference == True:
            norm_factor = np.array(load["color_template"])
            flux *= norm_factor
            flux_err *= norm_factor
        elif reference == "norm":
            if ratio:
                norm_factor = np.array(load["reference_spectrum"]) * np.array(
                    load["correction_factor"]
                )
                norm_factor[norm_factor == 0] = 1
                flux /= norm_factor
                flux_err /= norm_factor
            else:
                pass
        elif reference == "master_snr":
            norm_factor = np.array(load["master_snr_curve"]) ** 2
            norm_factor[np.isnan(norm_factor)] = 1
            norm_factor *= np.nanmean(np.array(load["color_template"])) / np.nanmean(norm_factor)
            flux *= norm_factor
            flux_err *= norm_factor
        else:
            flux *= conti
            flux_err *= conti

        if sub_dico == "matching_brute":
            force_brute = True

        mask_shifted = myf.doppler_r(mask[:, 0], (rv_range + 5) * 1000)

        if force_brute:
            brute_mask = np.array(load["mask_brute"])
            used_region = ((grid) >= mask_shifted[1][:, np.newaxis]) & (
                (grid) <= mask_shifted[0][:, np.newaxis]
            )
            line_killed = np.sum(brute_mask * used_region, axis=1) == 0
            mask = mask[line_killed]
            mask_shifted = myf.doppler_r(mask[:, 0], (rv_range + 5) * 1000)

        mask = mask[
            (myf.doppler_r(mask[:, 0], 30000)[0] < grid.max())
            & (myf.doppler_r(mask[:, 0], 30000)[1] > grid.min()),
            :,
        ]  # supres line farther than 30kms
        if wave_min is not None:
            mask = mask[mask[:, 0] > wave_min, :]
        if wave_max is not None:
            mask = mask[mask[:, 0] < wave_max, :]

        print("\n [INFO] Nb lines in the mask : %.0f \n" % (len(mask)))

        mask_min = np.min(mask[:, 0])
        mask_max = np.max(mask[:, 0])

        # supress useless part of the spectra to speed up the CCF
        grid_min = int(myf.find_nearest(grid, myf.doppler_r(mask_min, -100000)[0])[0])
        grid_max = int(myf.find_nearest(grid, myf.doppler_r(mask_max, 100000)[0])[0])
        grid = grid[grid_min:grid_max]

        log_grid = np.linspace(np.log10(grid).min(), np.log10(grid).max(), len(grid))
        dgrid = log_grid[1] - log_grid[0]
        # dv = (10**(dgrid)-1)*299.792e6

        # computation of region free of spectral line to increase code speed

        if speed_up:
            used_region = ((10**log_grid) >= mask_shifted[1][:, np.newaxis]) & (
                (10**log_grid) <= mask_shifted[0][:, np.newaxis]
            )
            used_region = (np.sum(used_region, axis=0) != 0).astype("bool")
            print(
                "\n [INFO] Percentage of the spectrum used : %.1f [%%] (%.0f) \n"
                % (100 * sum(used_region) / len(grid), len(grid))
            )
            time.sleep(1)
        else:
            used_region = np.ones(len(grid)).astype("bool")

        if (
            not os.path.exists(self.dir_root + "CCF_MASK/CCF_" + mask_name.split(".")[0] + ".fits")
        ) | (force_brute):
            print(
                "\n [INFO] CCF mask reduced for the first time, wait for the static mask producing...\n"
            )
            time.sleep(1)
            mask_wave = np.log10(mask[:, 0])
            mask_contrast = mask[:, 1] * weighted + (1 - weighted)

            mask_hole = (
                mask[:, 0] > myf.doppler_r(file_random["parameters"]["hole_left"], -30000)[0]
            ) & (mask[:, 0] < myf.doppler_r(file_random["parameters"]["hole_right"], 30000)[0])
            mask_contrast[mask_hole] = 0

            log_grid_mask = np.arange(
                log_grid.min() - 10 * dgrid, log_grid.max() + 10 * dgrid + dgrid / 10, dgrid / 11
            )
            log_mask = np.zeros(len(log_grid_mask))

            # mask_contrast /= np.sqrt(np.nansum(mask_contrast**2)) #UPDATE 04.05.21 (DOES NOT WORK)

            match = myf.identify_nearest(mask_wave, log_grid_mask)
            for j in np.arange(-delta_window, delta_window + 1, 1):
                log_mask[match + j] = (mask_contrast) ** (1 + int(squared))

            plt.figure()
            plt.plot(log_grid_mask, log_mask)

            if (not force_brute) & (mask_name != "No name"):
                hdu = fits.PrimaryHDU(np.array([log_grid_mask, log_mask]).T)
                hdul = fits.HDUList([hdu])
                hdul.writeto(self.dir_root + "CCF_MASK/CCF_" + mask_name.split(".")[0] + ".fits")
                print(
                    "\n [INFO] CCF mask saved under : %s"
                    % (self.dir_root + "CCF_MASK/CCF_" + mask_name.split(".")[0] + ".fits")
                )
        else:
            print(
                "\n [INFO] CCF mask found : %s"
                % (self.dir_root + "CCF_MASK/CCF_" + mask_name.split(".")[0] + ".fits")
            )
            log_grid_mask, log_mask = fits.open(
                self.dir_root + "CCF_MASK/CCF_" + mask_name.split(".")[0] + ".fits"
            )[0].data.T

        log_template = interp1d(
            log_grid_mask, log_mask, kind="linear", bounds_error=False, fill_value="extrapolate"
        )(log_grid)

        flux = flux[:, grid_min:grid_max]
        flux_err = flux_err[:, grid_min:grid_max]

        amplitudes = []
        amplitudes_std = []
        rvs = []
        rvs_std = []
        fwhms = []
        fwhms_std = []
        ew = []
        ew_std = []
        centers = []
        centers_std = []
        depths = []
        depths_std = []
        bisspan = []
        bisspan_std = []
        b0s = []
        b1s = []
        b2s = []
        b3s = []
        b4s = []

        if display_ccf:
            plt.figure()

        now = datetime.datetime.now()
        print(
            "\n Computing CCF (Current time %.0fh%.0fm%.0fs) \n"
            % (now.hour, now.minute, now.second)
        )

        all_flux = []
        for j, i in enumerate(files):
            all_flux.append(
                interp1d(
                    np.log10(myf.doppler_r(grid, rv_shift[j])[0]),
                    flux[j],
                    kind="cubic",
                    bounds_error=False,
                    fill_value="extrapolate",
                )(log_grid)
            )
        all_flux = np.array(all_flux)

        all_flux_err = []
        for j, i in enumerate(files):
            all_flux_err.append(
                interp1d(
                    np.log10(myf.doppler_r(grid, rv_shift[j])[0]),
                    flux_err[j],
                    kind="linear",
                    bounds_error=False,
                    fill_value="extrapolate",
                )(log_grid)
            )
        all_flux_err = np.array(all_flux_err)

        vrad, ccf_power, ccf_power_std = myf.ccf(
            log_grid[used_region],
            all_flux[:, used_region],
            log_template[used_region],
            rv_range=rv_range,
            oversampling=ccf_oversampling,
            spec1_std=all_flux_err[:, used_region],
        )  # to compute on all the ccf simultaneously

        now = datetime.datetime.now()
        print("")
        print(
            "\n CCF computed (Current time %.0fh%.0fm%.0fs) \n"
            % (now.hour, now.minute, now.second)
        )

        try:
            self.all_ccf_saved[sub_dico] = (vrad, ccf_power, ccf_power_std)
        except AttributeError:
            self.all_ccf_saved = {sub_dico: (vrad, ccf_power, ccf_power_std)}

        ccf_ref = np.median(ccf_power, axis=1)
        continuum_ccf = np.argmax(ccf_ref)
        top_ccf = np.argsort(ccf_ref)[
            -int(len(ccf_ref) / 2) :
        ]  # roughly half of a CCF is made of the continuum

        ccf_snr = 1 / (
            np.std((ccf_power - ccf_ref[:, np.newaxis])[top_ccf], axis=0)
            / np.mean(ccf_power[continuum_ccf])
        )
        print(" [INFO] SNR CCF continuum median : %.0f\n" % (np.median(ccf_snr)))

        # noise_ccf = ccf_power_std
        # w = noise_ccf/(np.gradient(ccf_ref)/np.gradient(vrad)+epsilon)[:,np.newaxis]
        # w[w==0] = np.min(w[w!=0])/10
        # svrad_phot = 1/np.sqrt(np.sum(1/w**2,axis=0))
        # scaling = 820/np.mean(np.gradient(vrad)) #to penalize oversampling in vrad
        # svrad_phot*=scaling
        # self.svrad_phot = svrad_phot

        noise_ccf = [
            (np.sqrt(ccf_ref / np.max(ccf_ref)) * ccf_ref[continuum_ccf])[:, np.newaxis] / ccf_snr,
            ccf_power_std,
        ][
            int(ratio)
        ]  # assume that the noise in the continuum is white (okay for matching_mad but wrong when tellurics are still there)
        sigma_rv = noise_ccf / (abs(np.gradient(ccf_ref)) / np.gradient(vrad))[:, np.newaxis]
        w_rv = (1 / sigma_rv) ** 2
        svrad_phot = 1 / np.sqrt(np.sum(w_rv, axis=0))
        scaling = np.sqrt(820 / np.mean(np.gradient(vrad)))  # to penalize oversampling in vrad
        svrad_phot *= scaling

        svrad_phot[svrad_phot == 0] = 2 * np.max(svrad_phot)  # in case of null values

        print(" [INFO] Photon noise RV median : %.2f m/s\n " % (np.median(svrad_phot)))

        svrad_phot2 = {}
        svrad_phot2["rv"] = 10 ** (
            0.98 * np.log10(svrad_phot) - 3.08
        )  # from photon noise simulations Photon_noise_CCF.py
        svrad_phot2["contrast"] = 10 ** (
            0.98 * np.log10(svrad_phot) - 3.58
        )  # from photon noise simulations Photon_noise_CCF.py
        svrad_phot2["fwhm"] = 10 ** (
            0.98 * np.log10(svrad_phot) - 2.94
        )  # from photon noise simulations Photon_noise_CCF.py
        svrad_phot2["center"] = 10 ** (
            0.98 * np.log10(svrad_phot) - 2.83
        )  # from photon noise simulations Photon_noise_CCF.py
        svrad_phot2["depth"] = 10 ** (
            0.97 * np.log10(svrad_phot) - 3.62
        )  # from photon noise simulations Photon_noise_CCF.py
        svrad_phot2["ew"] = 10 ** (
            0.97 * np.log10(svrad_phot) - 3.47
        )  # from photon noise simulations Photon_noise_CCF.py
        svrad_phot2["vspan"] = 10 ** (
            0.98 * np.log10(svrad_phot) - 2.95
        )  # from photon noise simulations Photon_noise_CCF.py

        self.svrad_phot = svrad_phot2["rv"]

        print(
            " [INFO] Photon noise RV from calibration : %.2f m/s\n "
            % (np.median(svrad_phot2["rv"]) * 1000)
        )

        for j, i in enumerate(files):
            if bis_analysis:
                print("File (%.0f/%.0f) %s SNR %.0f reduced" % (j + 1, len(files), i, snr[j]))
            file = pd.read_pickle(i)
            # log_spectrum = interp1d(np.log10(grid), flux[j], kind='cubic', bounds_error=False, fill_value='extrapolate')(log_grid)
            # vrad, ccf_power_old = myf.ccf2(log_grid, log_spectrum,  log_grid_mask, log_mask)
            # vrad, ccf_power_old = myf.ccf(log_grid, log_spectrum, log_template, rv_range=45, oversampling=ccf_oversampling)
            ccf_power_old = ccf_power[:, j]
            ccf_power_old_std = ccf_power_std[:, j]
            ccf = myc.tableXY(vrad / 1000, ccf_power_old, ccf_power_old_std)
            ccf.yerr = np.sqrt(abs(ccf.y))

            ccf.y *= -1
            ccf.find_max(vicinity=5)

            ccf.diff(replace=False)
            ccf.deri.y = np.abs(ccf.deri.y)
            for jj in range(3):
                ccf.deri.find_max(vicinity=4 - jj)
                if len(ccf.deri.x_max) > 1:
                    break

            first_max = ccf.deri.x_max[np.argsort(ccf.deri.y_max)[-1]]
            second_max = ccf.deri.x_max[np.argsort(ccf.deri.y_max)[-2]]

            ccf.y *= -1
            if (np.min(abs(ccf.x_max - 0.5 * (first_max + second_max))) < 5) & (self.fwhm < 15):
                center = ccf.x_max[np.argmin(abs(ccf.x_max - 0.5 * (first_max + second_max)))]
            else:
                center = ccf.x[ccf.y.argmin()]
            ccf.x -= center

            if not del_outside_max:
                mask = (ccf.x > -rv_borders) & (ccf.x < rv_borders)
                ccf.supress_mask(mask)
            else:
                ccf.find_max(vicinity=10)
                ccf.index_max = np.sort(ccf.index_max)
                mask = np.zeros(len(ccf.x)).astype("bool")
                mask[ccf.index_max[0] : ccf.index_max[1] + 1] = True
                ccf.supress_mask(mask)

            if normalisation == "left":
                norm = ccf.y[0]
            else:
                max1 = np.argmax(ccf.y[0 : int(len(ccf.y) / 2)])
                max2 = np.argmax(ccf.y[int(len(ccf.y) / 2) :]) + int(len(ccf.y) / 2)
                fmax1 = ccf.y[max1]
                fmax2 = ccf.y[max2]
                norm = (fmax2 - fmax1) / (max2 - max1) * (np.arange(len(ccf.y)) - max2) + fmax2
            ccf.yerr /= norm
            ccf.y /= norm

            if ratio:
                ccf.yerr *= 0
                ccf.yerr += 0.01

            if display_ccf:
                ccf.plot(color=None)

            # bis #interpolated on 10 m/s step
            moments = np.zeros(5)
            b0 = []
            b1 = []
            b2 = []
            b3 = []
            b4 = []

            if bis_analysis:
                ccf.x *= 1000
                drv = 10
                border = np.min([abs(ccf.x.min()), abs(ccf.x.max())])
                border = (border // drv) * drv
                grid_vrad = np.arange(-border, border + drv, drv)
                ccf.interpolate(new_grid=grid_vrad, replace=False)
                ccf.interpolated.yerr *= 0
                ccf.interpolated.y = (ccf.interpolated.y - ccf.interpolated.y.min()) / (
                    ccf.interpolated.y.max() - ccf.interpolated.y.min()
                )
                ccf.interpolated.my_bisector(oversampling=10, between_max=True)
                ccf.interpolated.bis.clip(min=[0.01, None], max=[0.7, None])
                bis = ccf.interpolated.bis
                bis.interpolate(new_grid=np.linspace(0.01, 0.7, 20))
                bis.y -= bis.y[0]
                if save:
                    save_bis = {"bis_flux": bis.x, "bis_rv": bis.y, "bis_rv_std": bis.yerr}
                    file["ccf_bis"] = save_bis
                bis.y = np.gradient(bis.y)
                for p in range(5):
                    moments[p] = np.sum(bis.x**p * bis.y)
                b0.append(moments[0])
                b1.append(moments[1])
                b2.append(moments[2])
                b3.append(moments[3])
                b4.append(moments[4])
                ccf.x /= 1000
            else:
                b0.append(0)
                b1.append(0)
                b2.append(0)
                b3.append(0)
                b4.append(0)

            ccf.clip(min=[-0.5, None], max=[0.5, None], replace=False)
            if len(ccf.clipped.x) < 7:
                ccf.clip(min=[-2, None], max=[2, None], replace=False)
            ccf.clipped.fit_poly()
            a, b, c = ccf.clipped.poly_coefficient
            para_center = -b / (2 * a) + center
            para_depth = a * (-b / (2 * a)) ** 2 + b * (-b / (2 * a)) + c
            centers.append(para_center)
            depths.append(1 - para_depth)

            EW = np.sum(ccf.y - 1) / len(ccf.y)
            ew.append(EW)
            save_ccf = {
                "ccf_flux": ccf.y,
                "ccf_flux_std": ccf.yerr,
                "ccf_rv": ccf.x + center,
                "reference": reference,
                "ew": EW,
            }

            para_ccf = {"para_rv": para_center, "para_depth": para_depth}

            ccf.fit_gaussian(Plot=False)  # ,guess=[-self.contrast,0,self.fwhm/2.355,1])

            rv_ccf = ccf.params["cen"].value + center
            rv_ccf_std = ccf.params["cen"].stderr
            rv_ccf, rv_ccf_std = replace_none(rv_ccf, rv_ccf_std)
            rv_ccf_std = svrad_phot2["rv"][j]
            factor = rv_ccf_std / abs(rv_ccf)
            scaling_noise = {
                "amp": 0.32,
                "wid": 1.33,
                "depth": 0.29,
                "center": 1.79,
                "bisspan": 1.37,
                "ew": 0.42,
            }

            contrast_ccf = -ccf.params["amp"].value
            contrast_ccf_std = ccf.params["amp"].stderr
            contrast_ccf, contrast_ccf_std = replace_none(contrast_ccf, contrast_ccf_std)
            contrast_ccf_std = svrad_phot2["contrast"][
                j
            ]  # abs(contrast_ccf)*factor*scaling_noise['amp']

            wid_ccf = ccf.params["wid"].value
            wid_ccf_std = ccf.params["wid"].stderr
            wid_ccf, wid_ccf_std = replace_none(wid_ccf, wid_ccf_std)
            wid_ccf_std = svrad_phot2["fwhm"][j]  # abs(wid_ccf)*factor*scaling_noise['wid']

            offset_ccf = ccf.params["offset"].value
            offset_ccf_std = ccf.params["offset"].stderr
            offset_ccf, offset_ccf_std = replace_none(offset_ccf, offset_ccf_std)

            amplitudes.append(contrast_ccf)
            amplitudes_std.append(contrast_ccf_std)
            rvs.append(rv_ccf)
            rvs_std.append(rv_ccf_std)
            fwhms.append(wid_ccf)
            fwhms_std.append(wid_ccf_std)
            bisspan.append(rv_ccf - para_center)
            bisspan_ccf_std = svrad_phot2["vspan"][
                j
            ]  # abs(rv_ccf - para_center)*factor*scaling_noise['bisspan']
            bisspan_std.append(bisspan_ccf_std)

            ew_std.append(svrad_phot2["ew"][j])  # abs(EW)*factor*scaling_noise['ew'])
            centers_std.append(
                svrad_phot2["center"][j]
            )  # abs(para_center)*factor*scaling_noise['center'])
            depths_std.append(
                svrad_phot2["depth"][j]
            )  # abs(1-para_depth)*factor*scaling_noise['depth'])

            save_ccf["ew_std"] = ew_std
            para_ccf["para_rv_std"] = centers_std
            para_ccf["para_depth_std"] = depths_std

            file["ccf"] = save_ccf
            file["ccf_parabola"] = para_ccf

            b0s.append(moments[0])
            b1s.append(moments[1])
            b2s.append(moments[2])
            b3s.append(moments[3])
            b4s.append(moments[4])
            if save:
                save_gauss = {
                    "contrast": contrast_ccf,
                    "contrast_std": contrast_ccf_std,
                    "rv": rv_ccf,
                    "rv_std": rv_ccf_std,
                    "rv_std_phot": svrad_phot2["rv"][j],
                    "fwhm": wid_ccf,
                    "fwhm_std": wid_ccf_std,
                    "offset": offset_ccf,
                    "offset_std": offset_ccf_std,
                    "vspan": rv_ccf - para_center,
                    "vspan_std": bisspan_std,
                    "b0": moments[0],
                    "b1": moments[1],
                    "b2": moments[2],
                    "b3": moments[3],
                    "b4": moments[4],
                }

                file["ccf_gaussian"] = save_gauss
                #
                #                    ccf.my_bisector(between_max=True,oversampling=50)
                #                    bis = myc.tableXY(ccf.bisector[5::50,1],ccf.bisector[5::50,0]+center,ccf.bisector[5::50,2])
                #
                #                    save_bis = {'bis_flux':bis.x,'bis_rv':bis.y,'bis_rv_std':bis.yerr}
                #                    file['ccf_bis'] = save_bis

                myf.pickle_dump(file, open(i, "wb"))

        # try:
        #     rvs_std = np.array(self.table['rv_dace_std'])/1000
        # except:
        #     pass

        rvs_std_backup = np.array(self.table["rv_dace_std"]) / 1000
        rvs_std = svrad_phot2["rv"]
        rvs_std[rvs_std == 0] = rvs_std_backup[rvs_std == 0]

        fwhms = np.array(fwhms).astype("float") * 2.355
        fwhms_std = np.array(fwhms_std).astype("float") * 2.355

        self.warning_rv_borders = False
        if np.median(fwhms) > (rv_borders / 1.5):
            print("[WARNING] The CCF is larger than the RV borders for the fit")
            self.warning_rv_borders = True

        self.ccf_rv = myc.tableXY(jdb, np.array(rvs) * 1000, np.array(rvs_std) * 1000)
        self.ccf_centers = myc.tableXY(jdb, np.array(centers) * 1000, np.array(centers_std) * 1000)
        self.ccf_contrast = myc.tableXY(jdb, amplitudes, amplitudes_std)
        self.ccf_depth = myc.tableXY(jdb, depths, depths_std)
        self.ccf_fwhm = myc.tableXY(jdb, fwhms, fwhms_std)
        self.ccf_vspan = myc.tableXY(jdb, np.array(bisspan) * 1000, np.array(bisspan_std) * 1000)
        self.ccf_ew = myc.tableXY(jdb, np.array(ew), np.array(ew_std))
        self.ccf_bis0 = myc.tableXY(jdb, b0s, np.sqrt(2) * np.array(rvs_std) * 1000)
        self.ccf_timeseries = np.array(
            [
                ew,
                ew_std,
                amplitudes,
                amplitudes_std,
                rvs,
                rvs_std,
                svrad_phot2["rv"],
                fwhms,
                fwhms_std,
                centers,
                centers_std,
                depths,
                depths_std,
                b0s,
                bisspan,
                bisspan_std,
            ]
        )
        self.ccf_rv.rms_w()
        self.ccf_centers.rms_w()
        self.ccf_rv_shift = center

        ccf_infos = pd.DataFrame(
            self.ccf_timeseries.T,
            columns=[
                "ew",
                "ew_std",
                "contrast",
                "contrast_std",
                "rv",
                "rv_std",
                "rv_std_phot",
                "fwhm",
                "fwhm_std",
                "center",
                "center_std",
                "depth",
                "depth_std",
                "b0",
                "bisspan",
                "bisspan_std",
            ],
        )
        ccf_infos["jdb"] = jdb
        ccf_infos = {"table": ccf_infos, "creation_date": datetime.datetime.now().isoformat()}

        if not os.path.exists(self.directory + "Analyse_ccf.p"):
            ccf_summary = {"star_info": {"name": self.starname}}
            myf.pickle_dump(ccf_summary, open(self.directory + "/Analyse_ccf.p", "wb"))

        if ccf_name is None:
            ccf_name = sub_dico

        if save:
            if mask_name != "No name":
                file_summary_ccf = pd.read_pickle(self.directory + "Analyse_ccf.p")
                try:
                    file_summary_ccf["CCF_" + mask_name.split(".")[0]][ccf_name] = ccf_infos
                except KeyError:
                    file_summary_ccf["CCF_" + mask_name.split(".")[0]] = {ccf_name: ccf_infos}

                myf.pickle_dump(file_summary_ccf, open(self.directory + "/Analyse_ccf.p", "wb"))

        self.infos["latest_dico_ccf"] = ccf_name

        self.yarara_analyse_summary()

        if save_ccf_profile:
            self.yarara_ccf_save(mask_name.split(".")[0], sub_dico)

        if plot:
            plt.figure(figsize=(12, 10))
            plt.subplot(4, 2, 1)
            self.ccf_rv.plot(
                label=r"rms : %.2f | $\sigma_{\gamma}$ : %.2f"
                % (self.ccf_rv.rms, np.median(self.svrad_phot) * 1000)
            )
            plt.legend()
            plt.title("RV", fontsize=14)
            ax = plt.gca()

            plt.subplot(4, 2, 3, sharex=ax)  # .scatter(jdb,ew,color='k')
            self.ccf_ew.plot()
            plt.title("EW", fontsize=14)
            plt.ylim(
                np.nanpercentile(ew, 25) - 1.5 * myf.IQ(ew),
                np.nanpercentile(ew, 75) + 1.5 * myf.IQ(ew),
            )

            plt.subplot(4, 2, 5, sharex=ax)  # .scatter(jdb,amplitudes,color='k')
            self.ccf_contrast.plot()
            plt.title("Contrast", fontsize=14)
            plt.ylim(
                np.nanpercentile(amplitudes, 25) - 1.5 * myf.IQ(amplitudes),
                np.nanpercentile(amplitudes, 75) + 1.5 * myf.IQ(amplitudes),
            )

            plt.subplot(4, 2, 4, sharex=ax)  # .scatter(jdb,fwhms,color='k')
            self.ccf_fwhm.plot()
            plt.title("FWHM", fontsize=14)
            plt.ylim(
                np.nanpercentile(fwhms, 25) - 1.5 * myf.IQ(fwhms),
                np.nanpercentile(fwhms, 75) + 1.5 * myf.IQ(fwhms),
            )

            plt.subplot(4, 2, 6, sharex=ax)  # .scatter(jdb,depths,color='k')
            self.ccf_depth.plot()
            plt.title("Depth", fontsize=14)
            plt.ylim(
                np.nanpercentile(depths, 25) - 1.5 * myf.IQ(depths),
                np.nanpercentile(depths, 75) + 1.5 * myf.IQ(depths),
            )

            plt.subplot(4, 2, 2, sharex=ax, sharey=ax)
            self.ccf_centers.plot(label="rms : %.2f" % (self.ccf_centers.rms))
            plt.legend()
            plt.title("Center", fontsize=14)

            plt.subplot(4, 2, 7, sharex=ax).scatter(jdb, b0s, color="k")
            plt.title("BIS", fontsize=14)

            plt.subplot(4, 2, 8, sharex=ax)  # .scatter(jdb,bisspan,color='k')
            self.ccf_vspan.plot()
            plt.title(r"RV $-$ Center (VSPAN)", fontsize=14)
            plt.ylim(
                np.nanpercentile(self.ccf_vspan.y, 25) - 1.5 * myf.IQ(self.ccf_vspan.y),
                np.nanpercentile(self.ccf_vspan.y, 75) + 1.5 * myf.IQ(self.ccf_vspan.y),
            )
            plt.subplots_adjust(
                left=0.07, right=0.93, top=0.95, bottom=0.08, wspace=0.3, hspace=0.3
            )

            if bis_analysis:
                plt.figure(figsize=(12, 10))
                plt.subplot(3, 2, 1).scatter(jdb, b0s, color="k")
                ax = plt.gca()
                plt.title("B0", fontsize=14)
                plt.subplot(3, 2, 2, sharex=ax).scatter(
                    jdb, self.ccf_rv.y - self.ccf_centers.y, color="k"
                )
                plt.title("RV-Center", fontsize=14)
                plt.subplot(3, 2, 3, sharex=ax).scatter(jdb, b1s, color="k")
                plt.title("B1", fontsize=14)
                plt.subplot(3, 2, 4, sharex=ax).scatter(jdb, b2s, color="k")
                plt.title("B2", fontsize=14)
                plt.subplot(3, 2, 5, sharex=ax).scatter(jdb, b3s, color="k")
                plt.title("B3", fontsize=14)
                plt.subplot(3, 2, 6, sharex=ax).scatter(jdb, b4s, color="k")
                plt.title("B4", fontsize=14)

        return {
            "rv": self.ccf_rv,
            "cen": self.ccf_centers,
            "contrast": self.ccf_contrast,
            "fwhm": self.ccf_fwhm,
            "vspan": self.ccf_vspan,
        }

    def yarara_ccf_save(self, mask, sub_dico):
        self.import_ccf()
        table = self.table_ccf["CCF_" + mask]

        all_ccf_saved = self.all_ccf_saved[sub_dico]
        vrad = all_ccf_saved[0]
        ccfs = all_ccf_saved[1]
        ccfs_std = all_ccf_saved[2]
        creation_date = table[sub_dico]["creation_date"]
        table_ccf_moments = table[sub_dico]["table"][
            [
                "jdb",
                "rv",
                "rv_std",
                "contrast",
                "contrast_std",
                "fwhm",
                "fwhm_std",
                "bisspan",
                "bisspan_std",
                "ew",
                "ew_std",
            ]
        ]

        if not os.path.exists(self.directory + "Analyse_ccf_saved.p"):
            file_to_save = {}
        else:
            file_to_save = pd.read_pickle(self.directory + "/Analyse_ccf_saved.p")

        ccf_infos = {
            "ccf_vrad": vrad,
            "ccf_flux": ccfs,
            "ccf_flux_std": ccfs_std,
            "table": table_ccf_moments,
            "creation_date": creation_date,
        }

        try:
            file_to_save["CCF_" + mask][sub_dico] = ccf_infos
        except KeyError:
            file_to_save["CCF_" + mask] = {sub_dico: ccf_infos}

        myf.pickle_dump(file_to_save, open(self.directory + "/Analyse_ccf_saved.p", "wb"))

    def yarara_ccf_replace_nan(self, last_dico="matching_mad"):
        self.import_ccf()
        self.import_table()
        self.import_dico_tree()

        if last_dico is not None:
            self.dico_tree = self.dico_tree.loc[
                0 : np.where(self.dico_tree["dico"] == last_dico)[0][0]
            ]

        modified = False
        for mask in self.table_ccf.keys():
            dico_ccf = pd.DataFrame(np.array(list(self.table_ccf[mask].keys())), columns=["dico"])
            dico_tree = pd.merge(self.dico_tree, dico_ccf, on="dico")
            dico_tree = dico_tree.sort_values(by="step")
            liste_dico = np.array(dico_tree["dico"])

            for dico1, dico2 in zip(liste_dico[1:], liste_dico[:-1]):
                table1 = self.table_ccf[mask][dico1]["table"]
                table2 = self.table_ccf[mask][dico2]["table"]
                mask_nan = np.isnan(table1)
                table1[mask_nan] = table2[mask_nan]
                if np.sum(np.array(mask_nan)):
                    modified = True
                    print(
                        " [INFO] NaN found in the CCF table : %s with sub_dico %s. Replaced by sub_dico %s"
                        % (mask, dico1, dico2)
                    )

        if modified:
            myf.pickle_dump(self.table_ccf, open(self.directory + "/Analyse_ccf.p", "wb"))

    def yarara_ccf_delta_window(self, ccf_mask, sub_dico="matching_mad", Plot=False):

        self.import_star_info()

        windows = [3, 5, 10, 15, 20, 40, 60, 80, 100]
        rms = []
        win = []
        all_rv = []

        rv_range = [10, self.star_info["FWHM"]["fixed"]][int(self.star_info["FWHM"]["fixed"] > 10)]

        for i, w in enumerate(windows):
            print("\n [INFO] Analysing time-series with windows : %.0f" % (w))
            os.system("rm " + self.dir_root + "CCF_MASK/*" + ccf_mask.split(".")[0] + "*")
            output = self.yarara_ccf(
                mask=ccf_mask,
                plot=False,
                sub_dico=sub_dico,
                ccf_oversampling=1,
                rv_range=rv_range * 1.5,
                delta_window=w,
                display_ccf=False,
                save=False,
            )
            output["rv"].recenter(who="Y")
            output["rv"].rms_w()
            rms_computed = output["rv"].rms
            print("\n [INFO] RV rms : %.2f m/s" % (rms_computed))
            rms.append(rms_computed)
            win.append(w)
            all_rv.append(output["rv"])

            if i:
                if rms[-1] > rms[-2]:
                    break

        if Plot:
            plt.figure()

            offset = np.mean(rms) * 5
            for i, rv in enumerate(all_rv):
                rv.y += offset * i
                rv.plot(
                    color=None,
                    label=r"w : %.0f | rms : %.2f ($\sigma=$%.2f)"
                    % (win[i], rv.rms, np.median(rv.yerr)),
                )
                plt.axhline(y=offset * i, color="k")
            plt.legend()

            plt.figure()
            plt.scatter(win, rms)
            plt.ylabel("RV rms", fontsize=14)
            plt.xlabel("CCF delta windows", fontsize=14)

        opt_window = int(win[np.argmin(rms)])
        self.ccf_delta_window = opt_window

        os.system("rm " + self.dir_root + "CCF_MASK/*" + ccf_mask.split(".")[0] + "*")
        output = self.yarara_ccf(
            mask=ccf_mask,
            plot=False,
            sub_dico=sub_dico,
            ccf_oversampling=1,
            rv_range=15,
            delta_window=opt_window,
            display_ccf=False,
            save=False,
        )
        self.yarara_star_info(CCF_delta=["YARARA", opt_window])

    def yarara_ccf_kitcat(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        mask=None,
        mask_old=None,
        treshold_telluric=1,
        reference=True,
        display_ccf=False,
        save=True,
        normalisation="left",
        bis_analysis=True,
        rv_range=20,
        ccf_oversampling=3,
        cols_catalogue=[
            "mask_deep",
            "mask_medium",
            "mask_shallow",
            "mask_blue",
            "mask_green",
            "mask_red",
            "depth_rel",
        ],
    ):

        if mask is None:
            loc_mask = glob.glob(self.dir_root + "KITCAT/kitcat_mask*.p")[0]
            mask = loc_mask.split("/")[-1]

        self.kitcat_ccf = {}

        self.import_table()
        try:
            dace = myc.tableXY(self.table["jdb"], self.table["rv_dace"], self.table["rv_dace_std"])
            dace.rms_w()
            dace.recenter(who="Y")
            self.kitcat_ccf["dace"] = dace
        except KeyError:
            pass

        if mask_old is not None:
            self.yarara_ccf(
                sub_dico=sub_dico,
                continuum=continuum,
                mask=mask_old,
                reference=True,
                plot=False,
                display_ccf=False,
                save=True,
                ccf_oversampling=ccf_oversampling,
                normalisation=normalisation,
                rv_range=rv_range,
            )
            self.ccf_rv.rms_w()
            self.ccf_rv.recenter(who="Y")
            self.kitcat_ccf[mask_old] = self.ccf_rv

        for mask_col in cols_catalogue:
            self.yarara_ccf(
                sub_dico=sub_dico,
                continuum=continuum,
                mask=mask,
                mask_col=mask_col,
                treshold_telluric=treshold_telluric,
                reference=True,
                plot=False,
                ccf_oversampling=ccf_oversampling,
                display_ccf=False,
                save=True,
                normalisation=normalisation,
                rv_range=rv_range,
            )
            self.ccf_rv.rms_w()
            self.ccf_rv.recenter(who="Y")
            self.kitcat_ccf[mask_col] = self.ccf_rv

        vec = self.kitcat_ccf

        p1 = myc.tableXY(
            vec["mask_deep"].x,
            vec["mask_deep"].y - vec["mask_medium"].y,
            np.sqrt(2 * 3) * vec["mask_deep"].yerr,
        )
        p2 = myc.tableXY(
            vec["mask_medium"].x,
            vec["mask_medium"].y - vec["mask_shallow"].y,
            np.sqrt(2 * 3) * vec["mask_medium"].yerr,
        )
        p3 = myc.tableXY(
            vec["mask_shallow"].x,
            vec["mask_deep"].y - vec["mask_shallow"].y,
            np.sqrt(2 * 3) * vec["mask_deep"].yerr,
        )

        p4 = myc.tableXY(
            vec["mask_blue"].x,
            vec["mask_blue"].y - vec["mask_green"].y,
            np.sqrt(2 * 3) * vec["mask_blue"].yerr,
        )
        p5 = myc.tableXY(
            vec["mask_green"].x,
            vec["mask_green"].y - vec["mask_red"].y,
            np.sqrt(2 * 3) * vec["mask_green"].yerr,
        )
        p6 = myc.tableXY(
            vec["mask_red"].x,
            vec["mask_blue"].y - vec["mask_red"].y,
            np.sqrt(2 * 3) * vec["mask_red"].yerr,
        )

        plt.figure(figsize=(12, 16))
        plt.subplot(4, 2, 1)
        vec["depth_rel"].plot()
        ax1 = plt.gca()
        plt.subplot(4, 2, 2)
        vec["depth_rel"].periodogram(Norm=True, nb_perm=1, ofac=3)
        ax2 = plt.gca()
        plt.subplot(4, 2, 3, sharex=ax1)
        p1.plot()
        plt.subplot(4, 2, 4, sharex=ax2)
        p1.periodogram(nb_perm=1, Norm=True, ofac=3)
        plt.subplot(4, 2, 5, sharex=ax1)
        p2.plot()
        plt.subplot(4, 2, 6, sharex=ax2)
        p2.periodogram(nb_perm=1, Norm=True, ofac=3)
        plt.subplot(4, 2, 7, sharex=ax1)
        p3.plot()
        plt.subplot(4, 2, 8, sharex=ax2)
        p3.periodogram(nb_perm=1, Norm=True, ofac=3)

        plt.figure(figsize=(12, 16))
        plt.subplot(4, 2, 1)
        vec["depth_rel"].plot()
        ax1 = plt.gca()
        plt.subplot(4, 2, 2)
        vec["depth_rel"].periodogram(Norm=True, nb_perm=1, ofac=3)
        ax2 = plt.gca()
        plt.subplot(4, 2, 3, sharex=ax1)
        p4.plot()
        plt.subplot(4, 2, 4, sharex=ax2)
        p4.periodogram(nb_perm=1, Norm=True, ofac=3)
        plt.subplot(4, 2, 5, sharex=ax1)
        p5.plot()
        plt.subplot(4, 2, 6, sharex=ax2)
        p5.periodogram(nb_perm=1, Norm=True, ofac=3)
        plt.subplot(4, 2, 7, sharex=ax1)
        p6.plot()
        plt.subplot(4, 2, 8, sharex=ax2)
        p6.periodogram(nb_perm=1, Norm=True, ofac=3)

        new_proxies = [p1, p2, p3, p4, p5, p6]

        self.kitcat_proxies = new_proxies

        num_perio = np.zeros(6)
        num_power = np.zeros(6)

        for j in range(6):
            num_perio[j] = len(new_proxies[j].period_significant)
            num_power[j] = np.sum(new_proxies[j].power_significant)

        # best_depth = num_power[0:3].argmax()
        # best_color = num_power[3:6].argmax()

    #        proxy_kept = []
    #        if num_power[0:3][best_depth]!=0:
    #            proxy_kept.append(new_proxies[best_depth].y)
    #        if num_power[3:6][best_color]!=0:
    #            proxy_kept.append(new_proxies[best_depth].y)
    #
    #        proxy_kept.append(np.ones(len(p1.x)))
    #        proxy_kept = np.array(proxy_kept)
    #
    #        vec['line_depth'].fit_base(proxy_kept)
    #        vec['line_depth'].vec_residues.rms_w()
    #        vec['line_depth'].rms_w()
    #        plt.figure()
    #        plt.subplot(2,1,1)
    #        vec['line_depth'].plot(label='rms : %.2f'%(vec['line_depth'].rms))
    #        plt.legend()
    #        plt.subplot(2,1,2)
    #        vec['line_depth'].vec_residues.plot(label='rms : %.2f'%(vec['line_depth'].vec_residues.rms))
    #        plt.legend()

    # =============================================================================
    # Plot all CCF (long analysis)
    # =============================================================================

    def yarara_ccf_all(
        self,
        continuum="linear",
        mask=None,
        mask_col="weight_rv",
        treshold_telluric=1,
        reference=True,
        display_ccf=False,
        save=True,
        save_ccf_profile=True,
        normalisation="left",
        rv_range=20,
        ccf_oversampling=3,
        dicos="all",
    ):

        test_file = self.import_spectrum()

        if (type(dicos) == str) & (dicos == "all"):
            all_dico = self.all_dicos
        elif (type(dicos) == str) & (dicos == "light"):
            all_dico = self.light_dicos
        else:
            all_dico = dicos

        all_dico = np.array(all_dico)[np.in1d(all_dico, list(test_file.keys()))]

        self.all_ccf = []

        for last in all_dico[::-1]:
            print("\n ------ \n Dico %s analysing \n ------ \n" % (last))
            self.yarara_ccf(
                sub_dico=last,
                continuum=continuum,
                mask=mask,
                mask_col=mask_col,
                treshold_telluric=treshold_telluric,
                reference=True,
                plot=False,
                display_ccf=display_ccf,
                save=save,
                save_ccf_profile=save_ccf_profile,
                normalisation=normalisation,
                rv_range=rv_range,
                ccf_oversampling=ccf_oversampling,
            )
            self.all_ccf.append(self.ccf_timeseries)
            plt.close("all")

        print("\n ------ \n Dico without color correction analysing \n ------ \n")

        self.yarara_ccf(
            sub_dico="matching_anchors",
            continuum=continuum,
            mask=mask,
            reference=False,
            plot=False,
            display_ccf=display_ccf,
            save=save,
            save_ccf_profile=save_ccf_profile,
            normalisation=normalisation,
            rv_range=rv_range,
            ccf_oversampling=ccf_oversampling,
        )

    def yarara_ccf_fit(
        self,
        dico_name,
        sub_dico="matching_diff",
        proxies=["CaII", "WB"],
        time_detrending=2,
        substract_rv=None,
        rm_keplerian=True,
    ):

        self.import_table()
        self.import_ccf()
        dace = self.import_dace_sts(substract_model=False)

        file_test = self.import_spectrum()
        step = file_test[sub_dico]["parameters"]["step"]

        tab = self.table
        tab_ccf = self.table_ccf

        jdb = tab_ccf[list(tab_ccf.keys())[-1]][sub_dico]["table"]["jdb"]

        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        files = np.array(self.table["filename"])  # updated 29.10.21 to allow ins_merged
        files = np.sort(files)

        masks = list(tab_ccf.keys())[1:]

        jdb_m = np.array(jdb - np.mean(jdb))
        jdb_m = jdb_m / np.std(jdb_m)

        base_vec = np.array([jdb_m**p for p in np.arange(time_detrending + 1)])

        if substract_rv is None:
            substract_rv = np.zeros(len(jdb))

        for p in proxies:
            if type(p) == str:
                base_vec = np.vstack([base_vec, np.array(tab[p])])
            else:
                base_vec = np.vstack([base_vec, p])

        keplerians_founded = False
        if rm_keplerian:
            files_keplerians = glob.glob(self.dir_root + "KEPLERIAN/Vectors_keplerian_fitted*.p")
            if len(files_keplerians):
                print("\n [INFO] File keplerians selected : %s" % (np.sort(files_keplerians)[-1]))
                tab_keplerian = pd.read_pickle(np.sort(files_keplerians)[-1])
                nb_v = np.sum([i[0] == "V" for i in tab_keplerian.keys()])
                nb_p = np.sum([i[0] == "P" for i in tab_keplerian.keys()])
                nb_vec_kep = nb_v + 2 * nb_p
                keplerians_founded = True
                print("\n [INFO] Keplerians model founded : ", list(tab_keplerian.keys()))

                for i in tab_keplerian.keys():
                    if i[0] == "V":
                        base_vec = np.vstack([base_vec, tab_keplerian[i]])

                for i in tab_keplerian.keys():
                    if i[0] == "P":
                        base_vec = np.vstack([base_vec, tab_keplerian[i]])

        print("\n-------- DICO %s --------" % (sub_dico))

        masks = []
        for m in tab_ccf.keys():
            if sub_dico in tab_ccf[m].keys():
                if m[0] == "C":
                    masks.append(m.split("CCF_")[1])

        self.model_keplerian_fitted = {}
        for mask in masks:
            canevas = np.array(tab_ccf["CCF_" + mask][sub_dico]["table"]).copy()
            new_canevas = pd.DataFrame(
                canevas, columns=tab_ccf["CCF_" + mask][sub_dico]["table"].keys()
            )

            rv = myc.tableXY(
                jdb, tab_ccf["CCF_" + mask][sub_dico]["table"]["rv"] * 1000, tab["rv_dace_std"]
            )
            rv.y -= substract_rv
            rv.fit_base(base_vec)

            model_keplerian = 0
            if keplerians_founded:
                model_keplerian = np.sum(rv.all_vec_fitted[:, -2 * nb_p - nb_v :], axis=1)

            self.model_keplerian_fitted[mask] = model_keplerian  # only save for the gaussian
            rv.vec_residues.y += model_keplerian

            rv.y += substract_rv
            rv.rms_w()
            rv.vec_residues.rms_w()
            new_canevas["rv"] = rv.vec_residues.y / 1000
            print("\n---- Mask %s (gaussian) ----" % ("CCF_" + mask))
            print("old rms : %.2f\nnew rms : %.2f" % (rv.rms, rv.vec_residues.rms))

            rv = myc.tableXY(
                jdb, tab_ccf["CCF_" + mask][sub_dico]["table"]["center"] * 1000, tab["rv_dace_std"]
            )
            rv.y -= substract_rv
            rv.fit_base(base_vec)

            model_keplerian = 0
            if keplerians_founded:
                model_keplerian = np.sum(rv.all_vec_fitted[:, -2 * nb_p - nb_v :], axis=1)
            rv.vec_residues.y += model_keplerian

            rv.y += substract_rv
            rv.rms_w()
            rv.vec_residues.rms_w()
            new_canevas["center"] = rv.vec_residues.y / 1000
            print("\n---- Mask %s (parabola) ----" % ("CCF_" + mask))
            print("old rms : %.2f\nnew rms : %.2f" % (rv.rms, rv.vec_residues.rms))

            tab_ccf["CCF_" + mask][dico_name] = {
                "table": new_canevas,
                "creation_date": datetime.datetime.now().isoformat(),
            }

        myf.pickle_dump(tab_ccf, open(self.directory + "/Analyse_ccf.p", "wb"))

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file["ccf_rv"] = (rv.vec_residues.y / 1000)[i]
            file["ccf_gaussian"]["rv"] = (rv.vec_residues.y / 1000)[i]
            file["ccf_gaussian"]["rv_std"] = (dace.yerr / 1000)[i]
            file[dico_name] = {
                "parameters": {
                    "sub_dico_used": sub_dico,
                    "proxies_fitted": proxies,
                    "time_detrending": time_detrending,
                    "step": step + 1,
                }
            }
            myf.pickle_dump(file, open(j, "wb"))

        self.yarara_analyse_summary()
        self.dico_actif = dico_name

    def yarara_plot_rcorr_dace(self, bin_length=1, detrend=2, vmin=0.3):
        self.import_dace_summary(bin_length=bin_length)
        table_dace = self.table_dace
        table_dace["rjd"] = table_dace["jdb"]
        cols = np.in1d(
            np.array(
                [
                    "vrad",
                    "fwhm",
                    "contrast",
                    "v_span",
                    "bis_span",
                    "ca",
                    "ha",
                    "na",
                    "skew",
                    "rhk",
                    "s_mw",
                ]
            ),
            np.array(table_dace.keys()),
        )
        cols = np.array(
            [
                "vrad",
                "fwhm",
                "contrast",
                "v_span",
                "bis_span",
                "ca",
                "ha",
                "na",
                "skew",
                "rhk",
                "s_mw",
            ]
        )[cols]

        v = []
        v_col = []
        for l in cols:
            vec = myc.tableXY(table_dace["rjd"].astype("float"), table_dace[l].astype("float"))
            if np.nansum(abs(vec.y)) > 0:
                vec.replace_nan()
                v_col.append(l)
                vec.substract_polyfit(detrend, replace=True)
                v.append(vec.y)

        v = np.array(v)
        m = myc.table(v)
        plt.figure(figsize=(12, 12))
        m.r_matrix(name=v_col, absolute=True, vmin=vmin)
        plt.savefig(
            self.dir_root
            + "IMAGES/Proxy_R_matrix_dace_bin%.0f_d%.0f.pdf" % (int(bin_length), int(detrend))
        )

    def yarara_plot_rcorr(self, vec=None, vec_name=None, bin_length=1, detrend=2, vmin=0.3):
        self.import_table()
        table = self.table
        table["rjd"] = table["jdb"]
        cols = np.in1d(
            np.array(
                [
                    "ccf_rv",
                    "ccf_fwhm",
                    "ccf_contrast",
                    "CaII",
                    "Ha",
                    "Hb",
                    "Hc",
                    "CaI",
                    "NaD",
                    "WB",
                    "CB",
                    "shell_fitted",
                    "BIS",
                    "BIS2",
                ]
            ),
            np.array(table.keys()),
        )
        cols = np.array(
            [
                "ccf_rv",
                "ccf_fwhm",
                "ccf_contrast",
                "CaII",
                "Ha",
                "Hb",
                "Hc",
                "CaI",
                "NaD",
                "WB",
                "CB",
                "shell_fitted",
                "BIS",
                "BIS2",
            ]
        )[cols]

        v = []
        v_col = []

        if vec is not None:
            if vec_name is None:
                v_col.append("manual_vec")
            else:
                v_col.append(vec_name)

            if np.nansum(abs(vec.y)) > 0:
                vec.replace_nan()
                if bin_length:
                    vec.night_stack(bin_length=bin_length, replace=True)
                vec.substract_polyfit(detrend, replace=True)
                v.append(vec.y)

        for l in cols:
            vec = myc.tableXY(table["jdb"].astype("float"), table[l].astype("float"))
            if np.nansum(abs(vec.y)) > 0:
                vec.replace_nan()
                v_col.append(l)
                if bin_length:
                    vec.night_stack(bin_length=bin_length, replace=True)
                vec.substract_polyfit(detrend, replace=True)
                v.append(vec.y)

        v = np.array(v)
        m = myc.table(v)
        plt.figure(figsize=(12, 12))
        m.r_matrix(name=v_col, absolute=True, vmin=vmin)
        plt.savefig(
            self.dir_root
            + "IMAGES/Proxy_R_matrix_bin%.0f_d%.0f.pdf" % (int(bin_length), int(detrend))
        )

    def yarara_plot_comp_dace(
        self,
        input_class,
        photon_noise=0.5,
        detrend=2,
        zorder1=1,
        zorder2=1,
        p_min=0,
        substract_model=False,
    ):

        me = input_class

        mean_time = np.mean(self.table["jdb"])

        dace = myc.tableXY(self.table["jdb"], self.table["rv_dace"], self.table["rv_dace_std"])
        dace.yerr = np.sqrt(dace.yerr**2 + photon_noise**2)
        dace.x -= mean_time
        dace.substract_polyfit(detrend, replace=True)
        dace.x += mean_time

        model = np.array(self.table["rv_shift"]) * 1000
        if substract_model:
            dace.y -= model

        me.yerr = dace.yerr
        me.x -= mean_time
        me.substract_polyfit(detrend, replace=True)
        me.x += mean_time

        plt.figure(figsize=(18, 6))
        plt.subplot(2, 1, 1)
        plt.title("%s" % (self.starname), fontsize=14)
        dace.rms_w()
        dace.plot(color="k", label="DRS (rms : %.2f m/s)" % (dace.rms), capsize=0, zorder=2)
        me.rms_w()
        me.plot(
            color="b",
            label="YARARA processed (rms : %.2f m/s)" % (me.rms),
            capsize=0,
            zorder=zorder1,
        )
        plt.xlabel("Jdb - 2,400,000 [days]", fontsize=14)
        plt.ylabel("RV [m/s]", fontsize=14)
        plt.legend()

        plt.subplot(2, 1, 2)
        dace.periodogram(nb_perm=1, Norm=True, zorder=2, p_min=p_min)
        me.periodogram(nb_perm=1, Norm=True, color="b", zorder=zorder2, p_min=p_min)
        plt.axhline(y=1, color="k", ls="-.", label="FAP 1%")
        plt.xlabel("Period [days]", fontsize=14)
        plt.ylabel("Power normalised", fontsize=14)
        plt.legend()

        plt.subplots_adjust(top=0.95, left=0.08, right=0.96, bottom=0.09, hspace=0.35)

    def yarara_plot_comp_dace_v2(
        self,
        mask,
        sub_dico,
        photon_noise=0.5,
        zorder1=1,
        zorder2=1,
        p_min=0,
        m_out=0,
        deg=2,
        substract_model=False,
    ):

        self.import_table()

        old = self.import_ccf_timeseries(mask, "matching_diff", "rv")
        new = self.import_ccf_timeseries(mask, sub_dico, "rv")
        dace = self.import_dace_sts(substract_model=substract_model)

        old.species_recenter(species=self.table["ins"], ref=0)
        new.species_recenter(species=self.table["ins"], ref=0)
        dace.species_recenter(species=self.table["ins"], ref=0)

        baseline = np.max(old.x) - np.min(old.x)

        old.y -= np.nanmedian(old.y)
        new.y -= np.nanmedian(new.y)
        dace.y -= np.nanmedian(dace.y)

        dace.yerr = np.sqrt(dace.yerr**2 + photon_noise**2)
        old.yerr = np.sqrt(old.yerr**2 + photon_noise**2)
        new.yerr = np.sqrt(new.yerr**2 + photon_noise**2)

        old.substract_polyfit(deg, replace=True)
        new.substract_polyfit(deg, replace=True)
        dace.substract_polyfit(deg, replace=True)

        old.rms_w()
        new.rms_w()
        dace.rms_w()

        plt.figure(figsize=(15, 10))
        plt.subplot(4, 2, 1)
        old.plot(label="YARARA input (rms : %.2f m/s)" % (old.rms))
        dace.plot(label="DRS (rms : %.2f m/s)" % (dace.rms), color="b")
        plt.legend()
        plt.subplot(4, 2, 2)
        diff = myc.tableXY(old.x, old.y - dace.y, old.yerr)
        diff.substract_polyfit(2)
        diff.rms_w()
        diff.detrend_poly.rms_w()
        diff.plot(
            color="darkblue",
            label="rms : %.2f m/s \nrms detrended : %.2f m/s" % (diff.rms, diff.detrend_poly.rms),
            capsize=0,
        )
        plt.legend()
        plt.subplot(4, 2, 3)
        ax = plt.gca()
        old.periodogram(Norm=True, nb_perm=1, ofac=5, p_min=p_min, p_max=baseline)
        dace.periodogram(Norm=True, nb_perm=1, color="b", ofac=5, p_min=p_min, p_max=baseline)
        old.plot_lowest_perio(dace, color1="k", color2="b")

        plt.subplot(4, 2, 4, sharex=ax, sharey=ax)
        diff.periodogram(Norm=True, nb_perm=1, color="gray", ofac=5, p_min=p_min, p_max=baseline)
        diff.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="darkblue", ofac=5, p_min=p_min, p_max=baseline
        )

        plt.subplot(4, 2, 5)
        old.plot(label="YARARA input (rms : %.2f m/s)" % (old.rms))
        new.plot(label="YARARA output (rms : %.2f m/s)" % (new.rms), color="r")
        plt.legend()
        plt.subplot(4, 2, 6)
        diff = myc.tableXY(old.x, old.y - new.y, old.yerr)
        diff.substract_polyfit(2)
        diff.rms_w()
        diff.detrend_poly.rms_w()
        diff.plot(
            color="purple",
            label="rms : %.2f m/s \nrms detrended : %.2f m/s" % (diff.rms, diff.detrend_poly.rms),
            capsize=0,
        )
        plt.legend(prop={"size": 14})
        plt.subplot(4, 2, 7, sharex=ax, sharey=ax)
        old.periodogram(Norm=True, nb_perm=1, ofac=5, p_min=p_min, p_max=baseline)
        new.periodogram(Norm=True, nb_perm=1, color="r", ofac=5, p_min=p_min, p_max=baseline)
        old.plot_lowest_perio(new, color1="k", color2="r")

        plt.subplot(4, 2, 8, sharex=ax, sharey=ax)
        diff.periodogram(Norm=True, nb_perm=1, color="gray", ofac=5, p_min=p_min, p_max=baseline)
        diff.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="purple", ofac=5, p_min=p_min, p_max=baseline
        )

        plt.savefig(self.dir_root + "IMAGES/YARARA_vs_DACE_%s.pdf" % (mask))

        dace.species_recenter(species=self.table["ins"], ref=0)
        new.species_recenter(species=self.table["ins"], ref=0)
        old.species_recenter(species=self.table["ins"], ref=0)

        if m_out:
            mask_out = (myf.rm_outliers(dace.y, kind="inter", m=m_out)[0]) & (
                myf.rm_outliers(new.y, kind="inter", m=m_out)[0]
            )
            new.masked(mask_out)
            dace.masked(mask_out)

        new.rms_w()
        dace.rms_w()

        plt.figure(figsize=(20, 14))
        plt.subplot(4, 1, 1)
        plt.title(
            "DRS (rms : %.2f m/s)                     YARARA output (rms : %.2f m/s) : %s"
            % (dace.rms, new.rms, sub_dico),
            fontsize=16,
        )
        dace.plot(label="DRS", color="b", zorder=2, capsize=0)
        new.plot(label="YARARA", color="g", zorder=zorder1, capsize=0)
        plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=16)
        plt.ylabel("RV [m/s]", fontsize=16)
        plt.legend(prop={"size": 14})
        ax = plt.gca()
        plt.subplot(4, 1, 3, sharex=ax)
        plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=16)
        plt.ylabel(r"$\Delta$RV [m/s]", fontsize=16)
        diff = myc.tableXY(dace.x, dace.y - new.y, dace.yerr)
        diff.substract_polyfit(2)
        diff.rms_w()
        diff.detrend_poly.rms_w()
        diff.plot(
            color="k",
            label="rms : %.2f m/s \nrms : %.2f m/s" % (diff.rms, diff.detrend_poly.rms),
            capsize=0,
        )
        plt.subplot(4, 1, 2)
        dace.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="b", ofac=10, zorder=2, p_min=p_min, p_max=baseline
        )
        new.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="g", ofac=10, zorder=zorder2, p_min=p_min, p_max=baseline
        )
        dace.detrend_poly.plot_lowest_perio(new.detrend_poly)
        # plt.ylim(0,None)

        plt.xlabel("Period [days]", fontsize=16)
        plt.ylabel("Power", fontsize=16)
        ax = plt.gca()
        plt.subplot(4, 1, 4, sharex=ax)
        # diff.periodogram(Norm=True, nb_perm=1, color='gray', ofac=10)
        diff.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="k", ofac=10, p_min=p_min, p_max=baseline
        )
        plt.ylim(0, None)
        plt.xlabel("Period [days]", fontsize=16)
        plt.ylabel("Power", fontsize=16)
        plt.subplots_adjust(top=0.94, left=0.08, right=0.95, bottom=0.10, hspace=0.35)
        plt.savefig(self.dir_root + "IMAGES/RV_after_YARARA_v1_%s.pdf" % (mask))

        plt.show(block=False)

    def yarara_plot_comp_dace_v3(
        self,
        mask,
        sub_dico1="matching_morpho",
        sub_dico2="matching_empca",
        photon_noise=0.5,
        zorder1=1,
        zorder2=1,
        p_min=0,
        m_out=0,
        deg=2,
        substract_model=False,
        reference="DRS",
    ):

        self.import_table()

        old = self.import_ccf_timeseries(mask, "matching_diff", "rv")
        new = self.import_ccf_timeseries(mask, sub_dico1, "rv")
        last = self.import_ccf_timeseries(mask, sub_dico2, "rv")
        dace = self.import_dace_sts(substract_model=substract_model)

        old.species_recenter(species=self.table["ins"], ref=0)
        new.species_recenter(species=self.table["ins"], ref=0)
        dace.species_recenter(species=self.table["ins"], ref=0)
        last.species_recenter(species=self.table["ins"], ref=0)

        baseline = np.max(old.x) - np.min(old.x)

        old.y -= np.nanmedian(old.y)
        new.y -= np.nanmedian(new.y)
        last.y -= np.nanmedian(last.y)
        dace.y -= np.nanmedian(dace.y)

        # dace.yerr = np.sqrt(dace.yerr**2+photon_noise**2) #calib noise already injected in the dace rvs 02.08.21
        old.yerr = np.sqrt(old.yerr**2 + photon_noise**2)
        new.yerr = np.sqrt(new.yerr**2 + photon_noise**2)
        last.yerr = np.sqrt(last.yerr**2 + photon_noise**2)
        dace.yerr = np.sqrt(dace.yerr**2 + photon_noise**2)

        old.substract_polyfit(deg, replace=True)
        new.substract_polyfit(deg, replace=True)
        dace.substract_polyfit(deg, replace=True)
        last.substract_polyfit(deg, replace=True)

        old.rms_w()
        new.rms_w()
        dace.rms_w()
        last.rms_w()

        # lg1 = 'YARARA RAW'#' (rms : %.2f m/s)'%(dace.rms)
        lg2 = "YARARA V1"  #' (rms : %.2f m/s)'%(new.rms)
        lg3 = "YARARA V2"  #' (rms : %.2f m/s)'%(last.rms)

        pmax = 0
        plt.figure(figsize=(15, 7))
        plt.subplot(3, 1, 1)
        ax = plt.gca()
        if self.planet:
            self.yarara_indicate_planet(color="g")
        if reference == "DRS":
            dace.periodogram(
                Norm=True,
                nb_perm=1,
                color="k",
                ofac=10,
                p_min=p_min,
                p_max=baseline,
                legend=reference,
            )
            plt.legend(loc=1)
            pmax = np.max([pmax, dace.power_max])
        else:
            old.periodogram(
                Norm=True,
                nb_perm=1,
                color="k",
                ofac=10,
                p_min=p_min,
                p_max=baseline,
                legend=reference,
            )
            plt.legend(loc=1)
            pmax = np.max([pmax, old.power_max])
        plt.tick_params(top=True, direction="inout", which="both")
        plt.subplot(3, 1, 2, sharex=ax, sharey=ax)
        if self.planet:
            self.yarara_indicate_planet(color="g")
        new.periodogram(
            Norm=True, nb_perm=1, color="k", ofac=10, p_min=p_min, p_max=baseline, legend=lg2
        )
        plt.legend(loc=1)
        pmax = np.max([pmax, new.power_max])
        plt.tick_params(top=True, direction="inout", which="both")
        plt.subplot(3, 1, 3, sharex=ax, sharey=ax)
        if self.planet:
            self.yarara_indicate_planet(color="g")
        last.periodogram(
            Norm=True, nb_perm=1, color="k", ofac=10, p_min=p_min, p_max=baseline, legend=lg3
        )
        plt.ylim(0, None)
        plt.legend(loc=1)
        pmax = np.max([pmax, last.power_max])
        plt.ylim(0, pmax + 0.02)
        plt.subplots_adjust(hspace=0, left=0.08, right=0.97, top=0.95, bottom=0.09)
        plt.tick_params(top=True, direction="inout", which="both")
        plt.savefig(self.dir_root + "IMAGES/Global_periodogram_reduction_%s.pdf" % (mask))

        plt.figure(figsize=(15, 10))
        plt.subplot(6, 2, 1)
        dace.plot(label="DRS (rms : %.2f m/s)" % (dace.rms), color="b", capsize=0)
        old.plot(label="YARARA input (rms : %.2f m/s)" % (old.rms), capsize=0)
        plt.legend()
        plt.subplot(6, 2, 2)
        diff = myc.tableXY(old.x, old.y - dace.y, old.yerr)
        diff.substract_polyfit(2)
        diff.rms_w()
        diff.detrend_poly.rms_w()
        diff.plot(
            color="darkblue",
            label="rms : %.2f m/s \nrms detrended : %.2f m/s" % (diff.rms, diff.detrend_poly.rms),
            capsize=0,
        )
        plt.legend()
        plt.subplot(6, 2, 3)
        ax = plt.gca()
        old.periodogram(Norm=True, nb_perm=1, ofac=5, p_min=p_min, p_max=baseline)
        dace.periodogram(Norm=True, nb_perm=1, color="b", ofac=5, p_min=p_min, p_max=baseline)
        old.plot_lowest_perio(dace, color1="k", color2="b")

        plt.subplot(6, 2, 4, sharex=ax)
        diff.periodogram(Norm=True, nb_perm=1, color="gray", ofac=5, p_min=p_min, p_max=baseline)
        diff.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="darkblue", ofac=5, p_min=p_min, p_max=baseline
        )

        plt.subplot(6, 2, 5)
        new.plot(label="YARARA output v1.0 (rms : %.2f m/s)" % (new.rms), color="r", capsize=0)
        old.plot(label="YARARA input (rms : %.2f m/s)" % (old.rms), capsize=0)
        plt.legend()
        plt.subplot(6, 2, 6)
        diff = myc.tableXY(old.x, old.y - new.y, old.yerr)
        diff.substract_polyfit(2)
        diff.rms_w()
        diff.detrend_poly.rms_w()
        diff.plot(
            color="purple",
            label="rms : %.2f m/s \nrms detrended : %.2f m/s" % (diff.rms, diff.detrend_poly.rms),
            capsize=0,
        )
        plt.legend()
        plt.subplot(6, 2, 7, sharex=ax, sharey=ax)
        old.periodogram(Norm=True, nb_perm=1, ofac=5, p_min=p_min, p_max=baseline)
        new.periodogram(Norm=True, nb_perm=1, color="r", ofac=5, p_min=p_min, p_max=baseline)
        old.plot_lowest_perio(new, color1="k", color2="r")

        plt.subplot(6, 2, 8, sharex=ax)
        diff.periodogram(Norm=True, nb_perm=1, color="gray", ofac=5, p_min=p_min, p_max=baseline)
        diff.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="purple", ofac=5, p_min=p_min, p_max=baseline
        )

        plt.subplot(6, 2, 9)
        new.plot(label="YARARA output v1.0 (rms : %.2f m/s)" % (new.rms), color="r", capsize=0)
        last.plot(label="YARARA output v2.0 (rms : %.2f m/s)" % (last.rms), color="g", capsize=0)
        plt.legend()
        plt.subplot(6, 2, 10)
        diff = myc.tableXY(new.x, new.y - last.y, old.yerr)
        diff.substract_polyfit(2)
        diff.rms_w()
        diff.detrend_poly.rms_w()
        diff.plot(
            color="orange",
            label="rms : %.2f m/s \nrms detrended : %.2f m/s" % (diff.rms, diff.detrend_poly.rms),
            capsize=0,
        )
        plt.legend()
        plt.subplot(6, 2, 11, sharex=ax, sharey=ax)
        new.periodogram(Norm=True, nb_perm=1, color="r", ofac=5, p_min=p_min, p_max=baseline)
        last.periodogram(Norm=True, nb_perm=1, ofac=5, color="g", p_min=p_min, p_max=baseline)
        new.plot_lowest_perio(last, color1="r", color2="g")

        plt.subplot(6, 2, 12, sharex=ax)
        diff.periodogram(Norm=True, nb_perm=1, color="gray", ofac=5, p_min=p_min, p_max=baseline)
        diff.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="orange", ofac=5, p_min=p_min, p_max=baseline
        )
        plt.subplots_adjust(top=0.94, left=0.08, right=0.95, bottom=0.10, hspace=0.35)

        plt.savefig(self.dir_root + "IMAGES/YARARA_vs_DACE_%s.pdf" % (mask))

        if m_out:
            mask_out = (myf.rm_outliers(dace.y, kind="inter", m=m_out)[0]) & (
                myf.rm_outliers(last.y, kind="inter", m=m_out)[0]
            )
            last.masked(mask_out)
            dace.masked(mask_out)

        last.rms_w()
        dace.rms_w()

        plt.figure(figsize=(20, 14))
        plt.subplot(4, 1, 1)
        plt.title(
            "DRS (rms : %.2f m/s)                     YARARA output (rms : %.2f m/s) : %s"
            % (dace.rms, last.rms, sub_dico2),
            fontsize=16,
        )
        dace.plot(label="DRS", color="b", zorder=2, capsize=0)
        last.plot(label="YARARA", color="g", zorder=zorder1, capsize=0)
        plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=16)
        plt.ylabel("RV [m/s]", fontsize=16)
        plt.legend(prop={"size": 14})
        plt.subplot(4, 1, 3)
        plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=16)
        plt.ylabel(r"$\Delta$RV [m/s]", fontsize=16)
        diff = myc.tableXY(dace.x, dace.y - last.y, dace.yerr)
        diff.substract_polyfit(2)
        diff.rms_w()
        diff.detrend_poly.rms_w()
        diff.plot(
            color="k",
            label="rms : %.2f m/s \nrms : %.2f m/s" % (diff.rms, diff.detrend_poly.rms),
            capsize=0,
        )
        plt.subplot(4, 1, 2)
        dace.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="b", ofac=10, zorder=2, p_min=p_min, p_max=baseline
        )
        last.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="g", ofac=10, zorder=zorder2, p_min=p_min, p_max=baseline
        )
        dace.detrend_poly.plot_lowest_perio(last.detrend_poly, color1="b", color2="g")
        # plt.ylim(0,None)
        plt.xlabel("Period [days]", fontsize=16)
        plt.ylabel("Power", fontsize=16)

        ax = plt.gca()
        plt.subplot(4, 1, 4, sharex=ax)
        # diff.periodogram(Norm=True, nb_perm=1, color='gray', ofac=10)
        diff.detrend_poly.periodogram(
            Norm=True, nb_perm=1, color="k", ofac=10, p_min=p_min, p_max=baseline
        )
        plt.ylim(0, None)
        plt.xlabel("Period [days]", fontsize=16)
        plt.ylabel("Power", fontsize=16)
        plt.subplots_adjust(top=0.94, left=0.08, right=0.95, bottom=0.10, hspace=0.35)
        plt.savefig(self.dir_root + "IMAGES/RV_after_YARARA_v2_%s.pdf" % (mask))

        plt.show(block=False)

    def yarara_curve_rv(self, last_dico=None, poly_deg=0, metric="gaussian", noise=0, maskes=None):

        self.import_ccf()
        self.import_table()
        self.import_dico_tree()

        if last_dico is not None:
            self.dico_tree = self.dico_tree.loc[
                0 : np.where(self.dico_tree["dico"] == last_dico)[0][0]
            ]

        file_test = self.import_spectrum()

        if maskes is None:
            maskes = list(self.table_ccf.keys())
            maskes.pop(0)

        if type(maskes) is not list:
            maskes = [maskes]

        maskes = np.sort(maskes)

        if metric == "both":
            metric = ["gaussian", "parabola"]
        elif metric == "gaussian":
            metric = ["gaussian"]
        else:
            metric = ["parabola"]

        planet = np.zeros(len(self.table["jdb"]))
        if self.planet:
            self.import_planet()
            planet = self.rv_planet.y

        dace = self.import_dace_sts(substract_model=False)

        # dace = myc.tableXY(self.table['jdb'],self.table['rv_dace']+planet,self.table['rv_dace_std'])
        dace.yerr = np.sqrt(dace.yerr**2 + noise**2)
        dace.substract_polyfit(poly_deg, replace=True)
        dace.rms_w()

        plt.figure(figsize=(15, 7))
        plt.subplot(1, len(metric), 1)
        ax = plt.gca()
        for i, m in enumerate(metric):
            plt.subplot(1, len(metric), i + 1, sharex=ax, sharey=ax)
            plt.title("Metric : %s" % (m), fontsize=13)

            for mask in maskes:
                print("---- MASK : %s ----" % (mask))
                table_ccf = self.table_ccf[mask]
                dico_reduced = table_ccf.keys()
                dico_tree = self.dico_tree

                dico_tree = dico_tree.loc[dico_tree["dico"].isin(dico_reduced)].copy()
                column = ["center", "rv"][m == "gaussian"]
                dico_tree["rms"] = np.nan

                for j in np.array(dico_tree["dico"]):
                    new = myc.tableXY(dace.x, table_ccf[j]["table"][column] * 1000, dace.yerr)
                    self.debug = new, j, mask
                    if len(new.x) == len(new.y):
                        new.replace_nan()
                        new.substract_polyfit(poly_deg, replace=True)
                        new.rms_w()
                        dico_tree.loc[dico_tree["dico"] == j, "rms"] = new.rms

                plt.plot(
                    np.array(dico_tree["step"]),
                    np.array(dico_tree["rms"]),
                    marker=["x", "o"][mask[0] == "C"],
                    label="%s (rms : %.2f m/s)" % (mask, np.min(dico_tree["rms"])),
                )

            plt.ylabel("RV rms [m/s]", fontsize=16)
            plt.xticks(
                np.array(dico_tree["step"]),
                np.array(dico_tree["dico"]),
                rotation=45,
                fontsize=13,
                ha="right",
            )
            plt.axhline(y=dace.rms, color="k", ls=":", label="DACE (rms : %.2f m/s)" % (dace.rms))
            ax = plt.gca()
            if ax.get_ylim()[1] > (dace.rms * 1.3):
                plt.ylim(None, dace.rms * 1.3)
            if ax.get_ylim()[0] < 0:
                plt.ylim(0, None)

            plt.legend(prop={"size": 14})
            plt.subplots_adjust(bottom=0.27, left=0.06, right=0.96, top=0.94, wspace=0.35)
        plt.savefig(self.dir_root + "IMAGES/Correction_curve.pdf")

        plt.show(block=False)

    #        plt.figure(9,figsize=(10,10))
    #        plt.plot(save_rms,'ko')
    #        plt.ylabel('RV rms [m/s]',fontsize=13)
    #        plt.xticks(np.arange(len(save_name)), save_name, rotation=90)
    #        plt.axhline(y=0,color='k',ls=':')
    #        plt.subplots_adjust(bottom=0.25,left=0.08,right=0.93,top=0.94,wspace=0.35)
    #        plt.savefig(self.dir_root+'IMAGES/Step_diff_curve.pdf')

    def yarara_get_best_mask(
        self, sub_dico=["matching_mad"], poly_deg=0, metric="gaussian", noise=0
    ):

        self.import_ccf()
        self.import_table()
        self.import_dico_tree()

        file_test = self.import_spectrum()

        maskes = list(self.table_ccf.keys())
        maskes.pop(0)

        dace = myc.tableXY(self.table["jdb"], self.table["rv_dace"], self.table["rv_dace_std"])
        dace.yerr = np.sqrt(dace.yerr**2 + noise**2)
        dace.substract_polyfit(poly_deg, replace=True)
        dace.rms_w()

        if type(sub_dico) is not list:
            sub_dico = [sub_dico]

        if type(maskes) is not list:
            maskes = [maskes]

        rms = []
        masks = []
        sdico = []
        for j in sub_dico:
            for mask in maskes:
                if (mask[0] == "L") & (j == "matching_mad"):
                    j = "matching_morpho"
                table_ccf = self.table_ccf[mask]
                dico_reduced = table_ccf.keys()
                dico_tree = self.dico_tree
                dico_tree = dico_tree.loc[dico_tree["dico"].isin(dico_reduced)].copy()
                column = ["center", "rv"][metric == "gaussian"]

                try:
                    new = myc.tableXY(
                        table_ccf[j]["table"]["jdb"],
                        table_ccf[j]["table"][column] * 1000,
                        dace.yerr,
                    )
                    new.species_recenter(species=self.table["ins"], ref=0)
                    new.substract_polyfit(poly_deg, replace=True)
                    new.rms_w()
                    rms.append(new.rms)
                    masks.append(mask)
                    sdico.append(j)
                except:
                    pass
        masks = np.array(masks)
        rms = np.array(rms)
        sdico = np.array(sdico)
        # print(masks,rms,sdico)
        print(
            "\n [INFO] Best mask for sub dico %s : %s \n"
            % (sdico[np.argmin(rms)], masks[np.argmin(rms)])
        )
        return masks[np.argmin(rms)]

    def yarara_periodogram_map(
        self,
        mask=None,
        p_min=0.7,
        ofac=10,
        alias=365.26,
        period_split=5,
        fap=1,
        photon_noise=0,
        vmax=1,
        plot_1day=False,
        planet=None,
    ):

        if mask is None:
            mask = "LBL_ITER_kitcat_mask_" + self.starname

        self.import_ccf()
        self.import_table()
        self.import_dico_tree()

        # file_test = self.import_spectrum()
        # table_ccf = self.table_ccf[mask]

        dico_reduced = np.array(self.dico_tree["dico"])
        dace = self.import_dace_sts(substract_model=False)

        dace.yerr = np.sqrt(dace.yerr**2 + photon_noise**2)
        dace.periodogram(Plot=False, p_min=p_min, ofac=ofac, level=1 - fap / 100)

        all_power = [dace.power / dace.fap]
        name = ["drs"]

        for j in tqdm(dico_reduced):
            try:
                vec_lbl = self.import_ccf_timeseries(mask, j, "rv")
                vec_lbl.yerr = dace.yerr.copy()
                vec_lbl.periodogram(Plot=False, p_min=p_min, ofac=ofac, level=1 - fap / 100)
                all_power.append(vec_lbl.power / vec_lbl.fap)
                name.append(j.split("matching_")[1])
            except:
                pass

        all_power = np.array(all_power)

        plt.figure(figsize=(16, 10))

        plt.axes([0.1, 0.04, 0.85, 0.29 + 0.4 * (1 - int(plot_1day))])
        myf.my_colormesh(
            dace.freq,
            np.arange(len(name)),
            all_power,
            cmap="Blues",
            vmax=vmax,
            vmin=0.5,
            shading=None,
        )
        ax = plt.gca()
        ax.set_yticks(np.arange(len(name)) + 0.5)
        ax.set_yticklabels(name)

        if planet is not None:
            for j in planet:
                plt.axvline(x=1 / j, color="r", lw=1, ls=":", alpha=1)

        for j in range(len(name))[1::2]:
            plt.axhline(y=j, color="k", lw=1, alpha=0.25)

        for j in range(len(name))[::2]:
            plt.axhline(y=j, color="k", lw=1, alpha=0.5)

        if alias is not None:
            plt.axvline(x=1 / alias, color="g", lw=1)
            plt.tick_params(direction="in", top=True)
            for j in 1 / myf.calculate_alias(alias)[0]:
                plt.axvline(x=j, color="g", lw=1, ls=":")

        plt.xlim(np.min(dace.freq), 1 / period_split)
        ax.invert_xaxis()

        plt.axes([0.1, 0.33 + 0.4 * (1 - int(plot_1day)), 0.85, 0.15], sharex=ax)
        dace.periodogram(Plot=False, p_min=p_min, ofac=ofac, Norm=True, level=1 - fap / 100)
        vec_lbl.periodogram(
            Plot=False, p_min=p_min, ofac=ofac, Norm=True, color="b", level=1 - fap / 100
        )
        plt.ylim(0, np.max(vec_lbl.power) + 0.05)
        plt.axhline(dace.fap, color="k", alpha=0.3, ls="-.")
        plt.plot(dace.freq, dace.power, label="drs", color="k")
        plt.plot(vec_lbl.freq, vec_lbl.power, label=name[-1], color="b")
        plt.legend()
        plt.tick_params(direction="in", top=True, labelbottom=False)

        if planet is not None:
            for j in planet:
                plt.axvline(x=1 / j, color="r", lw=1, ls=":")

        ax1 = plt.gca()
        x = ax1.get_xticks()[1:-1]
        ax2 = ax1.twiny()

        ticks = np.ravel(np.arange(1, 10) * (10 ** np.arange(3)[:, np.newaxis]))
        ax2.set_xticks(1 / ticks)
        ax2.set_xlabel("Period [days]", fontsize=14)

        new_labels = ticks
        new_labels = ["%.0f" % (i) * ((1 - np.log10(i) % 1) == 1) for i in new_labels]
        ax2.set_xticklabels(new_labels)
        ax2.set_xlim(ax1.get_xlim())

        if alias is not None:
            plt.axvline(x=1 / alias, color="g", lw=1)
            for j in 1 / myf.calculate_alias(alias)[0]:
                plt.axvline(x=j, color="g", lw=1, ls=":")

        if plot_1day:
            plt.axes([0.1, 0.53, 0.85, 0.29])
            myf.my_colormesh(
                dace.freq,
                np.arange(len(name)),
                all_power,
                cmap="Blues",
                vmax=1,
                vmin=0.5,
                shading=None,
            )
            ax = plt.gca()
            ax.set_yticks(np.arange(len(name)) + 0.5)
            ax.set_yticklabels(name)
            for j in range(len(name))[1::2]:
                plt.axhline(y=j, color="k", lw=1, alpha=0.25)

            for j in range(len(name))[::2]:
                plt.axhline(y=j, color="k", lw=1, alpha=0.5)

            plt.axvline(x=1 / alias, color="g", lw=1)
            plt.tick_params(direction="in", top=True, labelbottom=False)
            for j in 1 / myf.calculate_alias(alias)[0]:
                plt.axvline(x=j, color="g", lw=1, ls=":")

            plt.xlim(1 - 1 / period_split, 1 + 1 / period_split)
            ax.invert_xaxis()

            plt.axes([0.1, 0.82, 0.85, 0.15], sharex=ax)
            dace.periodogram(Plot=False, p_min=p_min, ofac=ofac, Norm=True, level=1 - fap / 100)
            vec_lbl.periodogram(
                Plot=False, p_min=p_min, ofac=ofac, Norm=True, color="b", level=1 - fap / 100
            )
            plt.plot(dace.freq, dace.power, label="drs", color="k")
            plt.plot(vec_lbl.freq, vec_lbl.power, label=name[-1], color="b")
            plt.axhline(dace.fap, color="k", alpha=0.3, ls="-.")
            plt.legend()
            plt.tick_params(direction="in", top=True, labelbottom=False)

            plt.axvline(x=1 / alias, color="g", lw=1)
            for j in 1 / myf.calculate_alias(alias)[0]:
                plt.axvline(x=j, color="g", lw=1, ls=":")

            ax1 = plt.gca()
            x = ax1.get_xticks()[1:-1]
            ax2 = ax1.twiny()

            ax2.set_xlim(ax1.get_xlim())
            ax2.set_xticks(x)
            new_labels = 1 / x
            new_labels = ["%.3f" % (i) for i in new_labels]
            ax2.set_xticklabels(new_labels)

        plt.subplots_adjust(top=0.95, left=0.07, right=0.93, bottom=0.06, hspace=0.45, wspace=0.4)
        plt.savefig(self.dir_root + "IMAGES/Map_periodogram_%s.png" % (mask))

    def yarara_add_keplerians(self, vec=[], periods=[], from_data=True, p_tol=10):
        def return_intersection(v1, v2):
            match = myf.match_nearest(np.array(v1), np.array(v2))
            mean_p = 0.5 * (match[:, -2] + match[:, -3])
            diff = 100 * abs(match[:, -1]) / (mean_p)
            diff = diff.astype("int")
            periods = list(mean_p[diff <= p_tol])
            return periods

        self.import_table()
        t0 = 0
        time = np.array(self.table.jdb)
        ext = "0"

        if from_data:
            periods = []
            table = pd.read_pickle(self.dir_root + "KEPLERIAN/Planets_fitted_table_iter.p")
            v1 = np.array(table["yarara_v1_lbl"]["p"])
            v2 = np.array(table["yarara_v2_lbl"]["p"])
            converged = 0
            if (len(v1) != 0) & (len(v2) != 0):
                periods = return_intersection(
                    v1, v2
                )  # take as reference the intersection of v1 and v2
                print(
                    "\n [INFO] %.0f Keplerians detected in V1 and V2 at : " % (len(periods)),
                    ["%.2f" % (i) for i in periods],
                    " days",
                )
                print(
                    "\n [INFO] Planets added in the model and will be used if rerunning the pipeline"
                )

                if len(periods):
                    if os.path.exists(self.dir_root + "KEPLERIAN/Vectors_keplerian_fitted_1.p"):
                        old_table = pd.read_pickle(
                            self.dir_root + "KEPLERIAN/Vectors_keplerian_fitted_1.p"
                        )
                        old_periods = []
                        for k in list(old_table.keys()):
                            if k[0] == "P":
                                old_periods.append(float(k[1:]))
                        old_periods = np.array(old_periods)
                        new_periods = return_intersection(old_periods, periods)
                        if len(new_periods) == len(old_periods):
                            converged = 1
                            periods = v2  # if converged take v2 as the new reference
                            print(
                                "\n [INFO] Keplerians model already converged, changing from V1&V2 to V2 model"
                            )
                            print(
                                "\n [INFO] %.0f Keplerians detected in V2 at : " % (len(periods)),
                                ["%.2f" % (i) for i in periods],
                                " days",
                            )

                        if os.path.exists(
                            self.dir_root + "KEPLERIAN/Vectors_keplerian_fitted_2.p"
                        ):
                            converged = 1
                            periods = v2  # if converged take v2 as the new reference
                            print(
                                "\n [INFO] %.0f Keplerians detected in V2 at : " % (len(periods)),
                                ["%.2f" % (i) for i in periods],
                                " days",
                            )

            ext = ["1", "2"][converged]
        base = {}

        for n, v in enumerate(vec):
            base["V%.0f" % (n + 1)] = v

        for p in periods:
            base["P%.2f" % (p)] = np.array(
                [np.sin(2 * np.pi * (time - t0) / p), np.cos(2 * np.pi * (time - t0) / p)]
            )

        if len(base):
            myf.pickle_dump(
                base, open(self.dir_root + "KEPLERIAN/Vectors_keplerian_fitted_%s.p" % (ext), "wb")
            )

    def yarara_keplerian_fit(self, vec, periods=[365.25], time_detrending=0, t0=None):

        if t0 is None:
            t0 = np.min(vec.x)

        base = []
        for p in periods:
            base.append(np.sin(2 * np.pi * (vec.x - t0) / p))
            base.append(np.cos(2 * np.pi * (vec.x - t0) / p))

        for t in np.arange(time_detrending + 1):
            base.append((vec.x - t0) ** t)

        base = np.array(base)
        vec.fit_base(base, num_sim=1000)

        csin = vec.coeff_fitted[: -(1 + time_detrending) : 2]
        ccos = vec.coeff_fitted[1 : -(1 + time_detrending) : 2]

        csin_std = vec.coeff_fitted_std[: -(1 + time_detrending) : 2]
        ccos_std = vec.coeff_fitted_std[1 : -(1 + time_detrending) : 2]

        amp = np.sqrt(csin**2 + ccos**2)
        phase = np.arctan2(csin, ccos) - np.pi / 2  # because fit sinus in inject_planet

        amp_err = 2 * np.sqrt(csin**2 * csin_std**2 + ccos**2 * ccos_std**2) / amp

        return {
            "p": np.array(periods),
            "k": amp,
            "phi": phase,
            "k_std": amp_err,
            "phi_std": 0 * amp_err,
        }

    def yarara_keplerian_chain(
        self,
        mask,
        sub_dico="matching_empca",
        periods=None,
        time_detrending=0,
        p_max=3000,
        substract_model=False,
    ):
        self.import_dico_tree()
        self.import_ccf()
        self.import_table()
        ccfs = self.table_ccf[mask]
        dico_reduced = ccfs.keys()
        dico_tree = self.dico_tree
        if sub_dico in list(dico_tree["dico"]):
            val = dico_tree.loc[dico_tree["dico"].isin([sub_dico])].copy()["step"].values[0]
            dico_tree = dico_tree.loc[dico_tree["step"] <= val]
        dico_tree = list(dico_tree.loc[dico_tree["dico"].isin(dico_reduced)].copy()["dico"])

        parameter = []

        if periods is None:
            file_test = self.import_spectrum()
            periods = file_test["parameters"]["planet_injected"]["period"]
            amplitudes = file_test["parameters"]["planet_injected"]["amp"]
            amplitudes_std = file_test["parameters"]["planet_injected"]["amp_std"]
            phases = file_test["parameters"]["planet_injected"]["phase"]
            alpha = 0.6
        else:
            amplitudes = np.zeros(len(periods))
            amplitudes_std = np.zeros(len(periods))
            phases = np.zeros(len(periods))
            alpha = 0

        drs = self.import_dace_sts(substract_model=substract_model)
        if self.starname != "Sun":
            drs.y -= np.array(self.table["rv_shift"]) * 1000

        output = self.yarara_keplerian_fit(drs, periods=periods, time_detrending=time_detrending)
        parameter.append(
            [output["p"], output["k"], output["phi"], output["k_std"], output["phi_std"]]
        )

        n_col = 4
        fig = plt.figure(figsize=(17, 12))
        plt.subplot(
            (len(dico_tree) + 1) // n_col + int((len(dico_tree) + 1) % n_col != 0), n_col, 1
        )
        drs.substract_polyfit(time_detrending, replace=True)
        drs.periodogram(p_max=p_max, Norm=True)
        plt.title("DRS")
        plt.xlabel("")
        for j in periods:
            plt.axvline(x=j, color="r", alpha=0.2)

        c = 1
        for d in dico_tree:
            c += 1
            vec = myc.tableXY(
                ccfs[d]["table"]["jdb"],
                ccfs[d]["table"]["rv"] * 1000,
                ccfs[d]["table"]["rv_std"] * 1000,
            )
            output = self.yarara_keplerian_fit(
                vec, periods=periods, time_detrending=time_detrending
            )
            parameter.append(
                [output["p"], output["k"], output["phi"], output["k_std"], output["phi_std"]]
            )
            plt.subplot(
                (len(dico_tree) + 1) // n_col + int((len(dico_tree) + 1) % n_col != 0), n_col, c
            )
            vec.substract_polyfit(time_detrending, replace=True)
            vec.periodogram(p_max=p_max, Norm=True)
            plt.title(d)
            plt.xlabel("")
            for j in periods:
                plt.axvline(x=j, color="r", alpha=0.2)
        plt.subplots_adjust(hspace=0.75, wspace=0.4, top=0.95, left=0.07, right=0.95, bottom=0.05)
        dico_tree = np.hstack([np.array(["drs"]), dico_tree])

        v1_loc = np.where(dico_tree == "matching_morpho")[0] + 0.5

        mat = np.array(parameter)
        mat[:, 2, :] *= 180 / np.pi
        mat[:, 4, :] *= 180 / np.pi

        colors = ["blue", "orange", "green", "red", "purple", "magenta", "cyan", "brown", "black"]

        fig = plt.figure(figsize=(17, 12))
        plt.subplot(4, 1, 3)
        plt.axvline(x=v1_loc, ls=":", color="k")
        for j in range(len(periods)):
            plt.plot(
                np.arange(len(mat)),
                mat[:, 1, j],
                marker="o",
                label=("P=%.2f" % (periods[j])),
                color=colors[j],
            )
            plt.fill_between(
                np.arange(len(mat)),
                mat[:, 1, j] - np.ones(len(mat)) * amplitudes_std[j],
                mat[:, 1, j] + np.ones(len(mat)) * amplitudes_std[j],
                color=colors[j],
                alpha=0.15,
            )
        for j in range(len(amplitudes)):
            plt.axhline(y=amplitudes[j], color=colors[j], ls=":")
        plt.ylabel("K amplitude [m/s]", fontsize=16)
        plt.xticks(
            np.arange(len(dico_tree)), len(dico_tree) * [""], rotation=45, ha="right", fontsize=16
        )
        plt.tick_params(direction="in", top=True)

        mat[:, 1, :] = (mat[:, 1, :] - amplitudes) * 100 / amplitudes
        amplitudes_std = np.array(amplitudes_std) / np.array(amplitudes) * 100

        plt.subplot(4, 1, 4)
        plt.axvline(x=v1_loc, ls=":", color="k")
        for j in range(len(periods)):
            plt.plot(
                np.arange(len(mat)),
                mat[:, 1, j],
                marker="o",
                label=("P=%.2f" % (periods[j])),
                color=colors[j],
            )
            plt.fill_between(
                np.arange(len(mat)),
                mat[:, 1, j] - np.ones(len(mat)) * amplitudes_std[j],
                mat[:, 1, j] + np.ones(len(mat)) * amplitudes_std[j],
                color=colors[j],
                alpha=0.15,
            )
        plt.axhline(y=0, color="k")
        plt.legend(fontsize=16)
        plt.ylabel(r"$\Delta$ K [%]", fontsize=16)
        plt.xticks(np.arange(len(dico_tree)), dico_tree, rotation=45, ha="right", fontsize=16)
        plt.tick_params(direction="in", top=True)
        # plt.ylim(-29,29)
        mat[:, 1, :] = mat[:, 1, :] * amplitudes / 100 + amplitudes
        amplitudes_std = amplitudes_std * amplitudes / 100

        for j in range(len(periods)):
            ax = fig.add_subplot(201 + j + 10 * len(periods), projection="polar")
            ax.set_rlabel_position(90 + phases[j])
            plt.plot(
                np.linspace(0, 2 * np.pi, 100),
                np.ones(100) * amplitudes[j],
                color="k",
                ls="-",
                lw=2.5,
                alpha=alpha,
            )
            plt.fill_between(
                np.linspace(0, 2 * np.pi, 100),
                np.ones(100) * amplitudes[j] - amplitudes_std[j],
                np.ones(100) * amplitudes[j] + amplitudes_std[j],
                color="k",
                alpha=0.1,
                lw=0,
            )
            # plt.plot(np.linspace(0,2*np.pi,100),,color='k',ls='-.',lw=2.5,alpha=0.1)

            plt.axvline(x=phases[j], color="k", lw=2.5, ls="-", alpha=alpha)
            plt.polar(mat[:, 2, j] * np.pi / 180, mat[:, 1, j], color=colors[j], marker="o")
            plt.title("P=%.2f days" % (periods[j]), fontsize=16)

        plt.subplots_adjust(left=0.05, right=0.95, top=0.92, bottom=0.15, hspace=0.15)

        plt.savefig(self.dir_root + "KEPLERIAN/Planet_injected_d%.0f.png" % (time_detrending))

        new_mat = np.ones((np.shape(mat)[0], np.shape(mat)[1] + 5, np.shape(mat)[2]))

        new_mat[:, :-5, :] = mat
        new_mat[:, -5, :] = phases * np.ones(len(mat))[:, np.newaxis]
        new_mat[:, -4, :] = amplitudes * np.ones(len(mat))[:, np.newaxis]
        new_mat[:, -3, :] = amplitudes_std * np.ones(len(mat))[:, np.newaxis]
        new_mat[:, -2, :] = (mat[:, 1, :] - amplitudes) * 100 / amplitudes
        new_mat[:, -1, :] = (
            np.array(amplitudes_std)
            / np.array(amplitudes)
            * 100
            * np.ones(len(mat))[:, np.newaxis]
        )

        save = {
            "nb_planet": len(periods),
            "var": [
                "period",
                "k",
                "phi",
                "k_std",
                "phi_std",
                "phase",
                "amp",
                "amp_std",
                "rel_diff",
                "rel_diff_std",
            ],
        }
        for i, j in enumerate(dico_tree):
            save[j] = new_mat[i]

        self.keplerian_chain = save

    def yarara_plot_all_rv(
        self,
        mask,
        last_dico,
        ofac=10,
        p_min=0,
        nb_perm=1,
        photon_noise=0.5,
        deg_detrending=2,
        method="iterative",
    ):

        """mask : Dico entries to analyse (e.g CCF_G2)"""

        self.import_dico_chain(last_dico, method=method)

        self.import_ccf()
        self.import_table()
        self.import_dico_tree()

        dico_tree = self.dico_tree
        file_test = self.import_spectrum()

        table_ccf = self.table_ccf[mask]

        dico_reduced = table_ccf.keys()
        dico_chain = self.dico_chain
        dico_chain = np.array(dico_chain)[np.in1d(dico_chain, list(dico_reduced))]

        nb_plot = len(dico_chain)
        all_dico_name = []
        self.all_ccf = []
        i = 0
        for last in dico_chain:
            i += 1
            all_dico_name.append(last)
            print(
                " ------ \n Dico %s imported (last modif : %s) \n ------ \n"
                % (last, table_ccf[last]["creation_date"])
            )
            self.all_ccf.append(np.array(table_ccf[last]["table"]).T)

        columns_table = np.array(list(table_ccf[last]["table"].keys()))
        column_rv = np.where(columns_table == "rv")[0][0]

        try:
            self.all_ccf.append(np.array(table_ccf["matching_anchors"]["table"]).T)
            all_dico_name.append("no_color_correction")
        except:
            nb_plot -= 1
        concatenation = np.array(self.all_ccf)

        matrix = concatenation[::-1]
        harm = 1 / np.arange(1, 6)
        harm = harm[harm <= 1]

        self.all_rms = []

        planet = np.zeros(len(self.table["jdb"]))
        if self.planet:
            self.import_planet()
            planet = self.rv_planet.y

        dace = self.import_dace_sts(substract_model=False)

        # dace = myc.tableXY(self.table['jdb'], self.table['rv_dace']+planet, self.table['rv_dace_std'])
        # model_prefitted = np.array(self.table['rv_shift'])*1000
        # if self.starname!='Sun':
        #     dace.y -= model_prefitted
        dace.recenter(who="Y")

        time.sleep(1)
        rows = (len(matrix) + 2) // 3

        plt.figure("all-rv", figsize=(20, 3.2 * rows))
        plt.figure("all-perio", figsize=(20, 3.2 * rows))

        for j in tqdm(range(len(matrix) - 1)):
            plt.figure("all-rv")
            new = myc.tableXY(dace.x, matrix[j + 1][column_rv] * 1000, dace.yerr)
            old = myc.tableXY(dace.x, matrix[j][column_rv] * 1000, dace.yerr)

            new.rms_w()
            old.rms_w()

            self.all_rms.append([new.rms, old.rms])

            diff = myc.tableXY(old.x, new.y - old.y, old.yerr * 0 + 0.10)
            # if not j:
            #     plt.subplot((len(matrix)+1)//3,3,1)
            #     ax2=plt.gca()
            # else:
            plt.subplot((len(matrix) + 1) // 3, 3, j + 1)  # ,sharex=ax2,sharey=ax2)
            diff.substract_polyfit(2)
            diff.detrend_poly.rms_w()
            diff.recenter(who="Y")
            p2p = diff.detrend_poly.y.max() - diff.detrend_poly.y.min()
            plt.title(
                "%s - %s" % (all_dico_name[nb_plot - j], all_dico_name[nb_plot - j - 1]),
                fontsize=14,
            )
            diff.plot(
                label="rms : %.2f m/s \np2p : %.2f m/s" % (diff.detrend_poly.rms, p2p), capsize=0
            )
            plt.ylabel("RV [m/s]", fontsize=13)
            plt.legend(prop={"size": 14})

            plt.figure("all-perio")
            # if not j:
            #     plt.subplot((len(matrix)+1)//3,3,1)
            #     ax1=plt.gca()
            # else:
            plt.subplot((len(matrix) + 1) // 3, 3, j + 1)
            plt.title(
                "%s - %s" % (all_dico_name[nb_plot - j], all_dico_name[nb_plot - j - 1]),
                fontsize=14,
            )
            diff.periodogram(Norm=True, nb_perm=nb_perm, p_min=p_min, color="k")
            for k in harm:
                plt.axvline(x=365.25 * k, color="k", ls=":")
        plt.figure("all-rv")
        plt.subplots_adjust(
            top=0.95, left=0.07, right=0.93, bottom=0.06, hspace=[0.45, 0.6][rows > 4], wspace=0.4
        )
        plt.savefig(self.dir_root + "IMAGES/Each_correction_%s_rv.pdf" % (mask))
        plt.figure("all-perio")
        plt.subplots_adjust(
            top=0.95, left=0.07, right=0.93, bottom=0.06, hspace=[0.45, 0.6][rows > 4], wspace=0.4
        )
        plt.savefig(self.dir_root + "IMAGES/Each_correction_%s_periodogram.pdf" % (mask))

        rows = len(matrix) - 1
        plt.figure(9, figsize=(20, 2 * rows))
        for j in tqdm(range(len(matrix) - 1)):
            new = myc.tableXY(dace.x, matrix[j + 1][column_rv] * 1000, dace.yerr)
            old = myc.tableXY(dace.x, matrix[j][column_rv] * 1000, dace.yerr)

            new.rms_w()
            old.rms_w()

            self.all_rms.append([new.rms, old.rms])

            diff = myc.tableXY(old.x, new.y - old.y, old.yerr * 0 + 0.10)
            if not j:
                plt.subplot(rows, 2, 1)
                ax2 = plt.gca()
            else:
                plt.subplot(rows, 2, 2 * j + 1, sharex=ax2, sharey=ax2)
            diff.substract_polyfit(2)
            diff.detrend_poly.rms_w()
            diff.recenter(who="Y")
            p2p = diff.detrend_poly.y.max() - diff.detrend_poly.y.min()
            plt.title(
                "%s - %s" % (all_dico_name[nb_plot - j], all_dico_name[nb_plot - j - 1]),
                fontsize=14,
            )
            diff.plot(
                label="rms : %.2f m/s  |  p2p : %.2f m/s" % (diff.detrend_poly.rms, p2p), capsize=0
            )
            plt.ylabel("RV [m/s]", fontsize=13)
            plt.legend(prop={"size": 14})

            if not j:
                plt.subplot(rows, 2, 2)
                ax1 = plt.gca()
            else:
                plt.subplot(rows, 2, 2 * j + 2, sharex=ax1)
            plt.title(
                "%s - %s" % (all_dico_name[nb_plot - j], all_dico_name[nb_plot - j - 1]),
                fontsize=14,
            )
            diff.periodogram(Norm=True, nb_perm=nb_perm, p_min=p_min, color="k")
            if (j + 1) != rows:
                plt.xlabel("")
            for k in harm:
                plt.axvline(x=365.25 * k, color="k", ls=":")

        plt.subplots_adjust(top=0.97, left=0.07, right=0.97, bottom=0.03, hspace=0.45, wspace=0.25)
        plt.savefig(self.dir_root + "IMAGES/Each_correction_%s.pdf" % (mask))

        dace.yerr = np.sqrt(
            photon_noise**2 + dace.yerr**2
        )  # add arbitrary 10 cm/s or error bar

        new = myc.tableXY(dace.x, matrix[-1][column_rv] * 1000, dace.yerr)
        old = myc.tableXY(dace.x, matrix[1][column_rv] * 1000, dace.yerr)
        old.rms_w()
        new.rms_w()
        dace.rms_w()

        color_cycle = ["k", "b", "r", "g"]

        plt.figure(4, figsize=(20, 10))
        nb_graphic = (len(matrix) + 2) // 3
        plt.subplot(nb_graphic, 3, 1)
        dace.periodogram(
            Norm=True, p_min=p_min, nb_perm=nb_perm, ofac=ofac, color="k", legend="DACE"
        )
        old_graph = dace
        for j in tqdm(range(len(matrix))):
            new = myc.tableXY(dace.x, matrix[j][column_rv] * 1000, dace.yerr)
            new.substract_line(replace=True)
            new.periodogram(
                Norm=True,
                nb_perm=nb_perm,
                ofac=ofac,
                color=color_cycle[int((j + 1) % 4)],
                p_min=p_min,
                legend=all_dico_name[nb_plot - j],
            )
            plt.legend()
            new.plot_lowest_perio(
                old_graph, color1=color_cycle[int((j + 1) % 4)], color2=color_cycle[int((j) % 4)]
            )

            for h in harm:
                plt.axvline(x=365.25 * h, color="k", ls=":")
            if j + 1 != len(matrix):
                plt.subplot(nb_graphic, 3, j + 2)
                new.periodogram(
                    Norm=True,
                    nb_perm=nb_perm,
                    ofac=ofac,
                    color=color_cycle[(j + 1) % 4],
                    p_min=p_min,
                    legend=all_dico_name[nb_plot - j],
                )
                old_graph = new

        plt.subplots_adjust(top=0.97, left=0.07, right=0.97, bottom=0.03, hspace=0.45, wspace=0.25)

        self.debug = matrix

        new = myc.tableXY(dace.x, matrix[-1][column_rv] * 1000, dace.yerr)
        old = myc.tableXY(dace.x, matrix[1][column_rv] * 1000, dace.yerr)

        new.y -= np.nanmedian(new.y)
        old.y -= np.nanmedian(old.y)

        new.substract_polyfit(deg_detrending, replace=True)
        dace.substract_polyfit(deg_detrending, replace=True)

        old.rms_w()
        new.rms_w()
        dace.rms_w()

        plt.savefig(self.dir_root + "IMAGES/All_periodogram_%s.pdf" % (mask))

        plt.figure(5, figsize=(15, 10))
        plt.subplot(4, 2, 1)
        dace.plot(label="DRS (rms : %.2f m/s)" % (dace.rms), color="b")
        old.plot(label="YARARA input (rms : %.2f m/s)" % (old.rms))
        plt.legend()
        plt.subplot(4, 2, 2)
        diff = myc.tableXY(old.x, old.y - dace.y, old.yerr)
        diff.substract_polyfit(2)
        diff.rms_w()
        diff.detrend_poly.rms_w()
        diff.plot(
            color="darkblue",
            label="rms : %.2f m/s \nrms : %.2f m/s" % (diff.rms, diff.detrend_poly.rms),
        )
        plt.legend()
        plt.subplot(4, 2, 3)
        old.periodogram(Norm=True, nb_perm=1, ofac=5)
        dace.periodogram(Norm=True, nb_perm=1, color="b", ofac=5)
        old.plot_lowest_perio(dace, color1="k", color2="b")

        ax = plt.gca()
        plt.subplot(4, 2, 4)
        diff.periodogram(Norm=True, nb_perm=1, color="gray", ofac=5)
        diff.detrend_poly.periodogram(Norm=True, nb_perm=1, color="darkblue", ofac=5)
        diff.plot_lowest_perio(diff.detrend_poly, color1="gray", color2="darkblue")

        plt.subplot(4, 2, 5)
        old.plot(label="YARARA input (rms : %.2f m/s)" % (old.rms))
        new.plot(label="YARARA output (rms : %.2f m/s)" % (new.rms), color="r")
        plt.legend()
        plt.subplot(4, 2, 6)
        diff = myc.tableXY(old.x, old.y - new.y, old.yerr)
        diff.substract_polyfit(2)
        diff.rms_w()
        diff.detrend_poly.rms_w()
        diff.plot(
            color="purple",
            label="rms : %.2f m/s \nrms : %.2f m/s" % (diff.rms, diff.detrend_poly.rms),
        )
        plt.legend()
        plt.subplot(4, 2, 7, sharex=ax, sharey=ax)
        old.periodogram(Norm=True, nb_perm=1, ofac=5)
        new.periodogram(Norm=True, nb_perm=1, color="r", ofac=5)
        old.plot_lowest_perio(new, color1="k", color2="r")

        plt.subplot(4, 2, 8)
        diff.periodogram(Norm=True, nb_perm=1, color="gray", ofac=5)
        diff.detrend_poly.periodogram(Norm=True, nb_perm=1, color="purple", ofac=5)
        diff.plot_lowest_perio(diff.detrend_poly, color1="gray", color2="purple")

        plt.savefig(self.dir_root + "IMAGES/YARARA_vs_DACE_%s.pdf" % (mask))

        plt.figure(42, figsize=(20, 14))
        plt.subplot(4, 1, 1)
        plt.title(
            "DRS (rms : %.2f m/s)                     YARARA output (rms : %.2f m/s) : %s"
            % (dace.rms, new.rms, last_dico),
            fontsize=16,
        )
        dace.plot(label="DRS", color="b", capsize=0)
        new.plot(label="YARARA", color="g", capsize=0)
        plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=16)
        plt.ylabel("RV [m/s]", fontsize=16)
        plt.legend(prop={"size": 14})
        plt.subplot(4, 1, 3)
        plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=16)
        plt.ylabel(r"$\Delta$RV [m/s]", fontsize=16)
        diff = myc.tableXY(dace.x, dace.y - new.y, dace.yerr)
        diff.substract_polyfit(2)
        diff.rms_w()
        diff.detrend_poly.rms_w()
        diff.plot(
            color="k",
            label="rms : %.2f m/s \nrms : %.2f m/s" % (diff.rms, diff.detrend_poly.rms),
            capsize=0,
        )
        plt.subplot(4, 1, 2)
        new.detrend_poly.periodogram(Norm=True, nb_perm=1, color="g", ofac=10)
        dace.detrend_poly.periodogram(Norm=True, nb_perm=1, color="b", ofac=10)
        dace.detrend_poly.plot_lowest_perio(new.detrend_poly, color1="b", color2="g")
        # plt.ylim(0,None)

        ax = plt.gca()
        plt.subplot(4, 1, 4, sharex=ax)
        # diff.periodogram(Norm=True, nb_perm=1, color='gray', ofac=10)
        diff.detrend_poly.periodogram(Norm=True, nb_perm=1, color="k", ofac=10)
        plt.ylim(0, None)
        plt.subplots_adjust(top=0.94, left=0.08, right=0.95, bottom=0.10, hspace=0.35)
        plt.savefig(self.dir_root + "IMAGES/RV_after_YARARA_%s.pdf" % (mask))

        plt.show(block=False)

    # =============================================================================
    #     COMPUTE BIS
    # =============================================================================

    def yarara_ccf_bis(self, ccf_output, sub_dico="matching_diff", reduction="empca"):
        self.import_table()
        nb_comp = 3
        rv = ccf_output["rv"].y

        all_ccf = self.all_ccf_saved[sub_dico]
        vrad = all_ccf[0]
        new_vrad = myc.tableXY(np.arange(len(vrad)), abs(vrad))

        center = np.where(vrad == 0)[0][0]
        new_vrad = np.linspace(0, np.max(vrad), len(vrad) - center)
        new_vrad = np.hstack([-new_vrad[1::][::-1], new_vrad])

        ccf_power = all_ccf[1]
        ccf_power_std = all_ccf[2]

        new_ccf = []
        new_ccf_std = []
        for j in range(len(ccf_power.T)):
            ccf = myc.tableXY(vrad - rv[j], ccf_power[:, j], ccf_power_std[:, j])
            ccf.interpolate(new_grid=new_vrad, method="cubic", replace=False)
            new_ccf.append(ccf.interpolated.y)
            new_ccf_std.append(ccf.interpolated.yerr)

        new_ccf = np.array(new_ccf)
        new_ccf_std = np.array(new_ccf_std)
        ccf_med = np.median(new_ccf, axis=0)
        ccf_res = new_ccf - ccf_med

        center = np.where(new_vrad == 0)[0][0]

        if len(rv) == center:  # to avoid crash in fit_base
            center -= 1
            ccf_res = ccf_res[:, 1:-1]
            ccf_med = ccf_med[1:-1]
            new_ccf = new_ccf[:, 1:-1]
            new_ccf_std = new_ccf_std[:, 1:-1]

        ccf_res -= np.median(ccf_res, axis=1)[:, np.newaxis]
        if False:
            kernel = myc.tableXY(np.arange(len(ccf_med)), abs(np.gradient(ccf_med)))
        else:
            kernel = myc.tableXY(np.arange(len(ccf_med)), abs(np.gradient(ccf_med)) ** 2 / ccf_med)
        kernel.center_symmetrise(center, Plot=False)

        ccf_res_weighted = ccf_res * kernel.y_sym
        ccf_res_std = new_ccf_std * kernel.y_sym

        ccf_res_sym = ccf_res_weighted.copy()
        ccf_res_sym[:, 0:center] *= -1

        ccf_res_stacked = ccf_res_sym.copy()
        ccf_res_stacked[:, 0:center] = 0.5 * (
            ccf_res_stacked[:, 0:center] + ccf_res_stacked[:, center + 1 :][:, ::-1]
        )
        ccf_res_stacked[:, center + 1 :] = (
            0.5
            * (ccf_res_stacked[:, 0:center] + ccf_res_stacked[:, center + 1 :][:, ::-1])[:, ::-1]
        )
        ccf_res_std = new_ccf_std / np.sqrt(2)

        plt.figure(figsize=(18, 14))
        plt.subplot(2, 4, 1)
        plt.imshow(ccf_res, aspect="auto")
        plt.axvline(x=center, color="k")
        plt.title("residuals CCF")
        ax = plt.gca()
        plt.subplot(2, 4, 2, sharex=ax, sharey=ax)
        plt.imshow(ccf_res_weighted, aspect="auto")
        plt.axvline(x=center, color="k")
        plt.title("weighted")
        plt.subplot(2, 4, 3, sharex=ax, sharey=ax)
        plt.imshow(ccf_res_sym, aspect="auto")
        plt.axvline(x=center, color="k")
        plt.title("symetrize")
        plt.subplot(2, 4, 4, sharex=ax, sharey=ax)
        plt.imshow(ccf_res_stacked, aspect="auto")
        plt.axvline(x=center, color="k")
        plt.title("mirror averaged")
        plt.subplots_adjust(left=0.06, right=0.97, top=0.95, bottom=0.10)

        matrix = myc.table(ccf_res_stacked[:, 0:center])
        w = ccf_res_std[:, 0:center]

        zscore, phi, base_vec = matrix.dim_reduction(reduction, nb_comp, w)

        self.debug = (matrix, base_vec, nb_comp)

        matrix.fit_base(base_vec[:, 0:nb_comp].T)

        plt.subplot(2, 4, 5, sharex=ax, sharey=ax)
        m1 = (matrix.table[:, 0] * base_vec[:, 0][:, np.newaxis]).T
        plt.imshow(
            np.hstack([m1, np.zeros(len(ccf_res))[:, np.newaxis], m1[:, ::-1]]), aspect="auto"
        )
        plt.axvline(x=center, color="k")
        plt.title(reduction.upper() + "1")

        plt.subplot(2, 4, 6, sharex=ax, sharey=ax)
        m2 = (matrix.table[:, 1] * base_vec[:, 1][:, np.newaxis]).T
        plt.imshow(
            np.hstack([m2, np.zeros(len(ccf_res))[:, np.newaxis], m2[:, ::-1]]), aspect="auto"
        )
        plt.title(reduction.upper() + "2")
        plt.axvline(x=center, color="k")

        plt.subplot(2, 4, 7, sharex=ax, sharey=ax)
        m3 = (matrix.table[:, 2] * base_vec[:, 2][:, np.newaxis]).T
        plt.imshow(
            np.hstack([m3, np.zeros(len(ccf_res))[:, np.newaxis], m3[:, ::-1]]), aspect="auto"
        )
        plt.title(reduction.upper() + "3")
        plt.axvline(x=center, color="k")

        maps = np.dot(matrix.coeff_fitted, base_vec.T)

        plt.subplot(2, 4, 8, sharex=ax, sharey=ax)
        plt.imshow(
            np.hstack([maps, np.zeros(len(ccf_res))[:, np.newaxis], maps[:, ::-1]]), aspect="auto"
        )
        plt.title(reduction.upper() + "_sum")
        plt.axvline(x=center, color="k")

        plt.savefig(self.dir_root + "IMAGES/BIS_CCF_maps.pdf")

        vec1 = np.hstack([base_vec[:, 0], np.zeros(1), base_vec[:, 0][::-1]])
        vec1 = myc.tableXY(np.arange(len(vec1)), vec1)
        vec1.y -= np.min(vec1.y)
        vec1.y /= np.max(vec1.y)
        vec1.y *= -len(ccf_res) / 4

        vec2 = np.hstack([base_vec[:, 1], np.zeros(1), base_vec[:, 1][::-1]])
        vec2 = myc.tableXY(np.arange(len(vec2)), vec2)
        vec2.y -= np.min(vec2.y)
        vec2.y /= np.max(vec2.y)
        vec2.y *= -len(ccf_res) / 4

        vec3 = np.hstack([base_vec[:, 2], np.zeros(1), base_vec[:, 2][::-1]])
        vec3 = myc.tableXY(np.arange(len(vec3)), vec3)
        vec3.y -= np.min(vec3.y)
        vec3.y /= np.max(vec3.y)
        vec3.y *= -len(ccf_res) / 4

        vectors = []

        b1 = myc.tableXY(self.table.jdb, matrix.coeff_fitted[:, 0])
        b1.znorm()
        proj = b1.corr(ccf_output["vspan"], Draw=False)
        b1.y /= proj.lin_slope_w
        b1.yerr = ccf_output["vspan"].yerr
        r1 = proj.r_pearson_w

        b2 = myc.tableXY(self.table.jdb, matrix.coeff_fitted[:, 1])
        b2.znorm()
        proj = b2.corr(ccf_output["vspan"], Draw=False)
        b2.y /= proj.lin_slope_w
        b2.yerr = ccf_output["vspan"].yerr
        r2 = proj.r_pearson_w

        b3 = myc.tableXY(self.table.jdb, matrix.coeff_fitted[:, 2])
        b3.znorm()
        proj = b3.corr(ccf_output["vspan"], Draw=False)
        b3.y /= proj.lin_slope_w
        b3.yerr = ccf_output["vspan"].yerr
        r3 = proj.r_pearson_w

        plt.figure(figsize=(18, 12))
        plt.axes([0.04, 0.70, 0.2, 0.25])
        plt.imshow(
            np.hstack([m1, np.zeros(len(ccf_res))[:, np.newaxis], m1[:, ::-1]]), aspect="auto"
        )
        vec1.plot(ls="-")
        plt.axvline(x=center, color="k", ls=":")
        plt.axhline(y=vec1.y[center + 1], color="k", ls=":")
        plt.title(reduction.upper() + "1 (%.2f)" % (abs(r1)))

        plt.axes([0.32, 0.85, 0.65, 0.10])
        b1.plot()
        vectors.append(b1)
        plt.xlabel("Time")
        plt.ylabel("Coeff1")

        plt.axes([0.32, 0.70, 0.65, 0.10])
        b1.periodogram(Norm=True, p_min=2)
        plt.xlabel("Period [days]", fontsize=11)
        plt.ylabel("Power", fontsize=11)

        plt.axes([0.04, 0.40, 0.2, 0.25])
        plt.imshow(
            np.hstack([m2, np.zeros(len(ccf_res))[:, np.newaxis], m2[:, ::-1]]), aspect="auto"
        )
        vec2.plot(ls="-")
        plt.axvline(x=center, color="k", ls=":")
        plt.axhline(y=vec2.y[center + 1], color="k", ls=":")
        plt.title(reduction.upper() + "2 (%.2f)" % (abs(r2)))

        plt.axes([0.32, 0.55, 0.65, 0.10])
        b2.plot()
        vectors.append(b2)
        plt.xlabel("Time")
        plt.ylabel("Coeff2")

        plt.axes([0.32, 0.40, 0.65, 0.10])
        b2.periodogram(Norm=True, p_min=2)
        plt.xlabel("Period [days]", fontsize=11)
        plt.ylabel("Power", fontsize=11)

        plt.axes([0.04, 0.10, 0.2, 0.25])
        plt.imshow(
            np.hstack([m3, np.zeros(len(ccf_res))[:, np.newaxis], m3[:, ::-1]]), aspect="auto"
        )
        vec3.plot(ls="-")
        plt.axvline(x=center, color="k", ls=":")
        plt.axhline(y=vec3.y[center + 1], color="k", ls=":")
        plt.title(reduction.upper() + "3 (%.2f)" % (abs(r3)))

        plt.axes([0.32, 0.25, 0.65, 0.10])
        b3.plot()
        vectors.append(b3)
        plt.xlabel("Time")
        plt.ylabel("Coeff3")

        plt.axes([0.32, 0.10, 0.65, 0.10])
        b3.periodogram(Norm=True, p_min=2)
        plt.xlabel("Period [days]", fontsize=11)
        plt.ylabel("Power", fontsize=11)

        plt.savefig(self.dir_root + "IMAGES/BIS_CCF_PCA.pdf")

        return vectors

    def yarara_ccf_bis_backup(
        self,
        sub_dico="matching_diff",
        dv=0.1,
        df=0.05,
        fiber_changed=[0, 54900, 57161.5, 100000],
        correct_psf=True,
    ):

        self.import_table()
        tab = self.table
        snr = np.array(tab["snr"])
        jdb = np.array(tab["jdb"])
        rv = np.array(tab["ccf_rv"])
        ca2 = np.array(tab["CaII"])
        order = snr.argsort()[::-1]

        limit_sup = np.where(np.array(fiber_changed) > jdb.max())[0][0]
        limit_inf = np.where(np.array(fiber_changed) < jdb.min())[0][-1]
        fiber_changed = list(np.array(fiber_changed)[limit_inf : limit_sup + 1])

        if not correct_psf:
            fiber_changed = [0, 100000]

        classes = np.ones(len(tab["jdb"])) * (len(fiber_changed) - 1)

        #        references = []
        for j in range(len(fiber_changed) - 1):
            classes[tab["jdb"] < fiber_changed[j + 1]] -= 1
        #            wh = np.where((np.array(tab['jdb'])[order]<fiber_changed[j+1])&(np.array(tab['jdb'])[order]>fiber_changed[j]))[0][0]
        #            references.append(order[wh])
        #        references = np.array(references)

        if len(fiber_changed) == 2:
            references = np.array([order[0]])  # highest snr by default
        else:
            t = myc.tableXY(jdb, snr)
            plt.figure(figsize=(20, 10))
            plt.subplot(3, 1, 1)
            for j in fiber_changed[1:-1]:
                plt.axvline(x=j, color="r")
            t.myscatter()
            plt.xlabel("Time", fontsize=14)
            plt.ylabel("SNR", fontsize=14)
            ax = plt.gca()

            plt.subplot(3, 1, 2, sharex=ax)
            t = myc.tableXY(jdb, rv)
            t.myscatter()
            for j in fiber_changed[1:-1]:
                plt.axvline(x=j, color="r")
            plt.xlabel("Time", fontsize=14)
            plt.ylabel("RV", fontsize=14)

            plt.subplot(3, 1, 3, sharex=ax)
            t = myc.tableXY(jdb, ca2)
            t.myscatter()
            for j in fiber_changed[1:-1]:
                plt.axvline(x=j, color="r")
            plt.xlabel("Time", fontsize=14)
            plt.ylabel("CaH&K", fontsize=14)

            plt.show(block=False)
            answer = "none"
            while len(answer) != (len(fiber_changed) - 1):
                answer = myf.sphinx(
                    "Choose %.0f days of reference (close in time and good snr)"
                    % (len(fiber_changed) - 1)
                )
                answer = np.array(answer.split(",")).astype("int")
            references = answer
            plt.close()

            print("\n")
            print("Days of references : ", jdb[references])
            print("\n")

        vrad = self.all_ccf_saved[sub_dico][0]
        ccfs = self.all_ccf_saved[sub_dico][1]
        ccfs /= np.max(ccfs[:, references[0]])
        mini = ccfs[:, references[0]].min()

        self.ccf_of_reference = (vrad, ccfs[:, references[0]])

        liste = -np.arange(-0.90, -mini, df)[::-1]
        self.bis_flux_liste = liste

        grid_rv = np.linspace(-20000, 20000, len(np.arange(-20000.0, 20000.0 + dv, dv)))

        all_bis = []

        for j in tqdm(range(len(ccfs.T))):
            test = myc.tableXY(vrad, ccfs[:, j])
            test.null()
            test.clip(min=[-21000, None], max=[21000, None])
            test.interpolate(new_grid=grid_rv, interpolate_x=False)
            mini = test.y.argmin()

            loc = np.argsort(abs(test.y - liste[:, np.newaxis]), axis=1)[:, 0:7]
            while 1 - np.product((loc < mini)[:, 0] != (loc < mini)[:, 1]):
                issue = np.where((loc < mini)[:, 0] == (loc < mini)[:, 1])[0]
                for k in issue:
                    loc[k, 1:-1] = loc[k, 2:]

            bis = 0.5 * (test.x[loc[:, 0]] + test.x[loc[:, 1]])

            all_bis.append(bis)

        all_bis = np.array(all_bis)
        all_bis_recenter = all_bis - all_bis[references[classes.astype("int")], :]

        plt.figure(figsize=(20, 10))

        plt.subplot(2, 2, 1)
        for j in range(len(references)):
            plt.plot(all_bis[references[j]], liste, lw=2)
        plt.xlabel("RV [m/s]", fontsize=14)
        plt.ylabel("Normalised flux", fontsize=14)

        plt.subplot(2, 2, 2)
        plt.plot(all_bis[references[0]] * 0, liste, lw=2)
        for j in range(1, len(references)):
            plt.plot(all_bis[references[j]] - all_bis[references[0]], liste, lw=2)
        plt.axvline(x=0, color="k", ls=":")
        plt.xlabel("Shift [m/s]", fontsize=14)
        plt.ylabel("Normalised flux", fontsize=14)

        plt.subplot(2, 2, 3)
        for j in range(len(np.unique(classes))):
            plt.plot(np.median(all_bis[np.where(classes == j)[0]], axis=0), liste, lw=2)
        plt.xlabel("RV [m/s]", fontsize=14)
        plt.ylabel("Normalised flux", fontsize=14)

        plt.subplot(2, 2, 4)
        for j in range(len(np.unique(classes))):
            plt.plot(
                np.median(all_bis[np.where(classes == j)[0]] - all_bis[references[0]], axis=0),
                liste,
                lw=2,
            )
        plt.axvline(x=0, color="k", ls=":")
        plt.xlabel("Shift [m/s]", fontsize=14)
        plt.ylabel("Normalised flux", fontsize=14)

        self.all_bis = all_bis_recenter

        plt.figure(figsize=(20, 12))
        for j in range(len(liste)):
            if j == 0:
                plt.subplot(int(len(liste) / 2) + 1, 2, j + 1)
                ax = plt.gca()
            else:
                plt.subplot(int(len(liste) / 2) + 1, 2, j + 1, sharex=ax, sharey=ax)
            plt.scatter(
                tab["jdb"],
                all_bis_recenter[:, j],
                c=tab["ccf_rv"] * 1000,
                label="%.2f" % (liste[j]),
            )
            plt.legend(loc=2)
            plt.xlabel("Time", fontsize=14)
            plt.ylabel(r"$\Delta$ BIS [m/s]", fontsize=14)
            plt.colorbar()
        plt.subplots_adjust(hspace=0.40, wspace=0.15, top=0.95, bottom=0.05, right=0.95, left=0.05)

        # gradient bisectors
        bis_gradient = np.gradient(all_bis_recenter, axis=1)
        self.all_bis_gradient = bis_gradient

    def yarara_bis_pairplot(self, kde_plot=False, large=True):

        self.import_table()
        tab = self.table

        bis_gradient = self.all_bis_gradient
        all_bis_recenter = self.all_bis
        liste = self.bis_flux_liste

        # pairplot1
        dic = {}
        for j in range(len(liste)):
            dic["%.2f" % (liste[j])] = all_bis_recenter[:, j]

        dic = pd.DataFrame(dic)
        dic["rv"] = tab["ccf_rv"]

        if large:
            for j in range(len(liste)):
                dic["G%.2f" % (liste[j])] = bis_gradient[:, j]

        dic["CaII"] = tab["CaII"]
        dic["Ha"] = tab["Ha"]
        dic["Hb"] = tab["Hb"]
        dic["Hc"] = tab["Hc"]
        dic["Hd"] = tab["Hd"]
        dic["classes"] = "none"
        table = myc.table(dic)
        plt.figure(figsize=(20, 20))
        table.pairplot(col_species="classes", kde_plot=kde_plot)

        if not large:
            # pairplot2
            dic_diff = {}
            for j in range(len(liste)):
                dic_diff["%.2f" % (liste[j])] = bis_gradient[:, j]
            dic_diff = pd.DataFrame(dic_diff)

            dic_diff["classes"] = "none"
            dic_diff["rv"] = tab["ccf_rv"]
            dic_diff["CaII"] = tab["CaII"]
            dic_diff["Ha"] = tab["Ha"]
            dic_diff["Hb"] = tab["Hb"]
            dic_diff["Hc"] = tab["Hc"]
            dic_diff["Hd"] = tab["Hd"]
            table = myc.table(dic_diff)
            plt.figure(figsize=(20, 20))
            table.pairplot(col_species="classes")

    def yarara_correct_bis_multilinear(
        self,
        weight_power=2,
        expo_time=3,
        expo_proxy=3,
        crit="variance",
        pca_comp=0,
        correct_offset=True,
        planet=[0, 94.0409, 0],
        nb_perm=1,
    ):

        self.import_table()
        tab = self.table

        rv_planet = planet[0] * np.sin(2 * np.pi / planet[1] * tab["jdb"] + planet[2])
        self.injected_planet = np.array(rv_planet)

        bis_gradient = self.all_bis_gradient
        all_bis_recenter = self.all_bis + np.array(rv_planet)[:, np.newaxis]

        liste = self.bis_flux_liste

        # vrad = self.ccf_of_reference[0]
        ccf_reference = self.ccf_of_reference[1]

        ccf_profile_weight = myc.tableXY(ccf_reference, abs(np.gradient(ccf_reference)))
        ccf_profile_weight.order()
        ccf_profile_weight.smooth()
        ccf_profile_weight.interpolate(new_grid=liste, method="linear", interpolate_x=False)
        weights = ccf_profile_weight.y**weight_power
        weights /= np.sum(weights)

        def from_bis_to_rv(array):  # convert the bisector matrix into rv timeseries
            return np.sum(array * weights, axis=1)

        proxies_gradient = [
            "CaII",
            "CaIIK",
            "CaIIH",
            "Ha",
            "Hb",
            "Hc",
            "Hd",
            "HeID3",
            "MgIa",
            "MgIb",
            "MgIc",
            "NaD1",
            "NaD2",
        ]
        proxies_offset = ["G%.2f" % (j) for j in liste] + [
            "CaII",
            "CaIIK",
            "CaIIH",
            "Ha",
            "Hb",
            "Hc",
            "Hd",
            "HeID3",
            "MgIa",
            "MgIb",
            "MgIc",
            "NaD1",
            "NaD2",
        ]

        for j in range(len(liste)):
            tab["G%.2f" % (liste[j])] = bis_gradient[:, j]

        # polyfit offset
        print("FIT OF THE OFFSET BIS")
        variable = np.arange(len(liste))
        all_rms = []
        all_init = []
        for num in variable:
            z = all_bis_recenter[:, num]
            y = np.array(tab["jdb"] - np.median(tab["jdb"]))
            save_rms = []

            all_init.append(np.std(z))
            for proxy in proxies_offset:
                x = np.array(tab[proxy])
                test = myc.tableXY(x, y)
                test.fit_poly2d(
                    z,
                    expo1=expo_proxy,
                    expo2=expo_time,
                    maximum=True,
                    Draw=False,
                    cmap="seismic",
                    vmin=np.percentile(z, 2.5),
                    vmax=np.percentile(z, 97.5),
                )
                save_rms.append(np.std(test.z_res))

            all_rms.append(save_rms)
            proxy = proxies_offset[np.array(save_rms).argmin()]
            print(
                "Selection of the best proxy for variable depth %.2f: %s with rms : %.2f (%.2f) m/s"
                % (liste[num], proxy, np.array(save_rms).min(), np.std(z))
            )
            x = np.array(tab[proxy])

        all_rms = np.array(all_rms)
        all_init = np.array(all_init)
        all_variance = 100 * abs(all_init - np.min(all_rms, axis=1)) / all_init

        plt.figure(figsize=(10, 10))
        plt.subplot(2, 1, 1)
        plt.plot(liste, np.min(all_rms, axis=1), label="best fit (smallest residues)")
        plt.plot(liste, all_init, label="init")
        plt.xlabel("Flux normalised", fontsize=14)
        plt.ylabel("Std BIS", fontsize=14)
        plt.legend()
        ax = plt.gca()
        plt.subplot(2, 1, 2, sharex=ax)
        plt.plot(liste, all_variance, label="init")
        plt.xlabel("Flux normalised", fontsize=14)
        plt.ylabel("Variance explained [%]", fontsize=14)

        button = True

        if crit == "variance":
            depth = all_variance.argmax()
        elif crit == "std":
            depth = np.min(all_rms, axis=1).argmin()
        else:
            p = np.sum(all_rms * weights[:, np.newaxis], axis=0).argmin()
            best_proxy = proxies_offset[p]
            button = False

        def bis_metric(array):
            if button:
                return array[:, depth]
            else:
                return from_bis_to_rv(array)

        if button:
            print("\nBets variable for offset : Depth %.2f\n " % (liste[depth]))
            z = all_bis_recenter[:, depth]
            y = np.array(tab["jdb"] - np.median(tab["jdb"]))
            save_rms_offset = []

            print("initial rms bis : %.2f m/s" % (np.std(z)))
            for proxy in proxies_offset:
                x = np.array(tab[proxy])
                test = myc.tableXY(x, y)
                test.fit_poly2d(
                    z,
                    expo1=expo_proxy,
                    expo2=expo_time,
                    maximum=True,
                    Draw=False,
                    cmap="seismic",
                    vmin=np.percentile(z, 2.5),
                    vmax=np.percentile(z, 97.5),
                )
                print("proxies : %s with rms : %.2f m/s" % (proxy, np.std(test.z_res)))
                save_rms_offset.append(np.std(test.z_res))

            best_proxy = proxies_offset[np.array(save_rms_offset).argmin()]
            print(
                "\nSelection of the best proxy : %s with rms : %.2f m/s\n"
                % (best_proxy, np.array(save_rms).min())
            )
            x = np.array(tab[best_proxy])

            plt.figure()
            fit_offset = myc.tableXY(x, y)
            fit_offset.fit_poly2d(
                z,
                expo1=3,
                expo2=3,
                maximum=True,
                Draw=True,
                cmap="seismic",
                vmin=np.percentile(z, 2.5),
                vmax=np.percentile(z, 97.5),
            )
            plt.xlabel("Activity proxy " + best_proxy, fontsize=14)
            plt.ylabel(r"Time - $T_0$ [days]", fontsize=14)
            plt.title(
                "rms : %.2f   versus   rms res : %.2f"
                % (np.std(fit_offset.z), np.std(fit_offset.z_res))
            )
        else:
            y = np.array(tab["jdb"] - np.median(tab["jdb"]))
            x = np.array(tab[best_proxy])
            test = myc.tableXY(x, y)
            all_offset_fitted = []
            for num in np.arange(len(liste)):
                z = all_bis_recenter[:, num]
                test.fit_poly2d(
                    z,
                    expo1=expo_proxy,
                    expo2=expo_time,
                    maximum=True,
                    Draw=False,
                    cmap="seismic",
                    vmin=np.percentile(z, 2.5),
                    vmax=np.percentile(z, 97.5),
                )
                print(
                    "For depth : %.2f and proxies : %s with rms : %.2f m/s"
                    % (liste[num], proxy, np.std(test.z_res))
                )
                all_offset_fitted.append(test.z_fitted)
            all_offset_fitted = np.array(all_offset_fitted)

            plt.figure()
            for num in np.arange(len(liste)):
                plt.scatter(tab["jdb"], all_offset_fitted[num, :], label="%.2f" % (liste[num]))
            plt.title("Offset fitted with proxy : %s" % (best_proxy))
            plt.scatter(
                tab["jdb"],
                np.sum(all_offset_fitted * weights[:, np.newaxis], axis=0),
                color="k",
                label="weighted average",
            )
            plt.legend()
            fit_offset = myc.tableXY(
                tab["jdb"], np.sum(all_offset_fitted * weights[:, np.newaxis], axis=0)
            )
            fit_offset.z_fitted = fit_offset.y
            self.offset_fitted = fit_offset.y

        if pca_comp:
            median_bis_gradient = np.median(bis_gradient, axis=0)
            test = myc.table(bis_gradient - median_bis_gradient)
            test.PCA(comp_max=pca_comp)
            plt.figure()
            for j in range(len(test.vec.T)):
                plt.subplot(len(test.vec.T), 1, j + 1)
                plt.plot(liste, test.vec[:, j], color="k")
            test.fit_base(np.vstack([test.vec.T, np.ones(len(liste))]))

            fit_gradient = myc.tableXY(0 * np.ravel(bis_gradient), 0 * np.ravel(bis_gradient))
            fit_gradient.z = np.ravel(test.table + median_bis_gradient)
            fit_gradient.z_fitted = np.ravel(test.vec_fitted + median_bis_gradient)
            fit_gradient.z_res = np.ravel(test.vec_residues)

            proxy = "CaII"
            jet = plt.get_cmap("brg")
            cNorm = mplcolors.Normalize(vmin=np.min(tab[proxy]), vmax=np.max(tab[proxy]))
            scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)

            x2 = np.ravel(
                (np.array(tab[proxy]) * np.ones(np.shape(bis_gradient)[1])[:, np.newaxis]).T
            )

        else:
            # polyfit
            z2 = np.ravel(bis_gradient)
            y2 = np.ravel([liste] * np.shape(bis_gradient)[0])
            save_rms = []
            print("initial rms gradient bis : %.2f m/s" % (np.std(z)))
            for proxy in proxies_gradient:
                x2 = np.ravel(
                    (np.array(tab[proxy]) * np.ones(np.shape(bis_gradient)[1])[:, np.newaxis]).T
                )
                test = myc.tableXY(x2, y2)
                test.fit_poly2d(
                    z2,
                    expo1=3,
                    expo2=3,
                    maximum=True,
                    Draw=False,
                    cmap="seismic",
                    vmin=np.percentile(z2, 2.5),
                    vmax=np.percentile(z2, 97.5),
                )
                print("proxies : %s with rms : %.2f m/s" % (proxy, np.std(test.z_res)))
                save_rms.append(np.std(test.z_res))

            proxy = proxies_gradient[np.array(save_rms).argmin()]
            print("\nSelection of the best proxy : %s\n" % (proxy))

            jet = plt.get_cmap("brg")
            cNorm = mplcolors.Normalize(vmin=np.min(tab[proxy]), vmax=np.max(tab[proxy]))
            scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)

            x2 = np.ravel(
                (np.array(tab[proxy]) * np.ones(np.shape(bis_gradient)[1])[:, np.newaxis]).T
            )

            plt.figure()
            fit_gradient = myc.tableXY(x2, y2)
            fit_gradient.fit_poly2d(
                z2,
                expo1=3,
                expo2=3,
                maximum=True,
                Draw=True,
                cmap="seismic",
                vmin=np.percentile(z2, 2.5),
                vmax=np.percentile(z2, 97.5),
            )
            plt.xlabel("Activity proxy " + proxy, fontsize=14)
            plt.ylabel("Flux normalised", fontsize=14)
            plt.title(
                "rms : %.2f   versus   rms res : %.2f"
                % (np.std(fit_gradient.z), np.std(fit_gradient.z_res))
            )

        def plot_color(array):
            for j in range(len(array)):
                colorVal = scalarMap.to_rgba(np.array(tab[proxy])[j])
                plt.plot(array[j, :], liste, color=colorVal, alpha=0.2)
                plt.plot(np.median(array, axis=0), liste, color="k", lw=2.5)
                plt.axvline(x=0, color="gray")
                plt.ylabel("Normalised flux", fontsize=14)
                plt.ylim(None, 1)

        bis_gradient = np.reshape(fit_gradient.z, np.shape(bis_gradient))
        bis_gradient_res = np.reshape(fit_gradient.z_res, (np.shape(bis_gradient)))
        bis_gradient_fitted = np.reshape(fit_gradient.z_fitted, (np.shape(bis_gradient)))
        bis_fitted = np.cumsum(bis_gradient_fitted, axis=1)

        offset_fitted = fit_offset.z_fitted
        if correct_offset:
            bis_fitted = (
                bis_fitted - bis_metric(bis_fitted)[:, np.newaxis] + offset_fitted[:, np.newaxis]
            )
        else:
            bis_fitted -= np.median(bis_fitted, axis=1)[:, np.newaxis]

        all_bis_recenter_res = all_bis_recenter - bis_fitted

        plt.figure(figsize=(12, 12))

        plt.subplot(3, 2, 1)
        plot_color(bis_gradient)
        plt.xlabel("GRADIENT BIS [m/s]", fontsize=14)
        ax2 = plt.gca()

        plt.subplot(3, 2, 2)
        plot_color(all_bis_recenter)
        plt.xlabel("BIS [m/s]", fontsize=14)
        ax1 = plt.gca()

        plt.subplot(3, 2, 3, sharex=ax2, sharey=ax2)
        plot_color(bis_gradient_fitted)
        plt.xlabel("GRADIENT BIS [m/s]", fontsize=14)
        ax2 = plt.gca()

        plt.subplot(3, 2, 4, sharex=ax1, sharey=ax1)
        plot_color(bis_fitted)
        plt.xlabel("BIS [m/s]", fontsize=14)
        ax1 = plt.gca()

        plt.subplot(3, 2, 5, sharex=ax2, sharey=ax2)
        plot_color(bis_gradient_res)
        plt.xlabel("GRADIENT BIS [m/s]", fontsize=14)
        ax2 = plt.gca()

        plt.subplot(3, 2, 6, sharex=ax1, sharey=ax1)
        plot_color(all_bis_recenter_res)
        plt.xlabel("BIS [m/s]", fontsize=14)
        ax1 = plt.gca()

        plt.figure(figsize=(12, 12))
        plt.subplot(2, 2, 1)
        uncorrected = myc.tableXY(tab["jdb"], from_bis_to_rv(all_bis_recenter))
        corrected = myc.tableXY(tab["jdb"], from_bis_to_rv(all_bis_recenter_res))

        corrected.yerr *= 0
        corrected.yerr += 0.5

        uncorrected.yerr *= 0
        uncorrected.yerr += 0.5

        corrected.rms_w()
        uncorrected.rms_w()

        uncorrected.plot(color="k", label="uncorrected (rms : %.2f m/s)" % (uncorrected.rms))
        corrected.plot(color="r", label="corrected (rms : %.2f m/s)" % (corrected.rms))
        diff = myc.tableXY(uncorrected.x, uncorrected.y - corrected.y, corrected.yerr)
        plt.legend()
        ax = plt.gca()
        plt.subplot(2, 2, 2, sharex=ax)
        diff.plot()

        plt.subplot(2, 2, 3)
        uncorrected.periodogram(Norm=True, nb_perm=nb_perm, color="k")
        corrected.periodogram(Norm=True, nb_perm=nb_perm, color="r")

        plt.subplot(2, 2, 4)
        diff.periodogram(Norm=True, nb_perm=nb_perm, color="k")

        self.rv_corrected = corrected
        self.rv_uncorrected = uncorrected

    def yarara_correct_bis_pca(self, nb_comp=5, weight_power=2):

        try:
            self.all_bis
            button = True
        except AttributeError:
            print("Launch first the recipes ---> yarara_ccf_bis")
            button = False

        if button:
            self.import_table()
            tab = self.table

            liste = self.bis_flux_liste
            all_bis_recenter = self.all_ccf_bis
            # vrad = self.ccf_of_reference[0]
            ccf_reference = self.ccf_of_reference[1]

            ccf_profile_weight = myc.tableXY(ccf_reference, abs(np.gradient(ccf_reference)))
            ccf_profile_weight.order()
            ccf_profile_weight.smooth()
            ccf_profile_weight.interpolate(new_grid=liste)
            weights = ccf_profile_weight.y**weight_power
            weights /= np.sum(weights)

            bis_gradient = np.gradient(all_bis_recenter, axis=1)
            median_bis_gradient = np.median(bis_gradient, axis=0)

            test = myc.table(bis_gradient - median_bis_gradient)
            test.PCA(comp_max=nb_comp, num_sim=500, standardize=True)
            for j in range(len(test.vec.T)):
                plt.subplot(len(test.vec.T), 1, j + 1)
                plt.plot(liste, test.vec[:, j], color="k")

            test.fit_base(np.vstack([test.vec.T, np.ones(len(liste))]))
            corr_bis = np.cumsum(test.vec_fitted, axis=1)
            corr_bis -= np.median(corr_bis, axis=0)
            all_bis_corrected = all_bis_recenter - corr_bis

            plt.figure(figsize=(15, 15))
            plt.subplot(2, 2, 1)
            plt.title("Initials bisectors gradient")
            plt.xlabel("CCF flux normalised", fontsize=14)
            plt.ylabel("RV [m/s]", fontsize=14)
            for j in range(len(test.table)):
                plt.plot(liste, test.table[j, :], color="k", alpha=0.05)
            plt.fill_between(
                liste,
                np.std(test.table, axis=0),
                -np.std(test.table, axis=0),
                alpha=0.7,
                color="r",
                label="std",
            )
            plt.legend()
            plt.ylim(-4, 4)
            ax = plt.gca()

            plt.subplot(2, 2, 2, sharex=ax, sharey=ax)
            plt.title("PCA (n=%.0f) residuals bisectors gradient" % (nb_comp))
            plt.xlabel("CCF flux normalised", fontsize=14)
            plt.ylabel("RV [m/s]", fontsize=14)
            for j in range(len(test.table)):
                plt.plot(liste, test.vec_residues[j, :], color="k", alpha=0.05)
            plt.fill_between(
                liste,
                np.std(test.vec_residues, axis=0),
                -np.std(test.vec_residues, axis=0),
                alpha=0.7,
                color="r",
            )

            plt.subplot(2, 2, 3, sharex=ax)
            plt.title("Initials bisectors")
            plt.xlabel("CCF flux normalised", fontsize=14)
            plt.ylabel("RV [m/s]", fontsize=14)
            for j in range(len(test.table)):
                plt.plot(liste, all_bis_recenter[j, :], color="k", alpha=0.05)
            plt.fill_between(
                liste,
                np.std(all_bis_recenter, axis=0),
                -np.std(all_bis_recenter, axis=0),
                alpha=0.7,
                color="r",
                label="std",
            )
            ax2 = plt.gca()

            plt.subplot(2, 2, 4, sharex=ax2, sharey=ax2)
            plt.title("PCA (n=%.0f) corrected bisectors" % (nb_comp))
            plt.xlabel("CCF flux normalised", fontsize=14)
            plt.ylabel("RV [m/s]", fontsize=14)
            for j in range(len(test.table)):
                plt.plot(liste, all_bis_corrected[j, :], color="k", alpha=0.05)
            plt.fill_between(
                liste,
                np.std(all_bis_corrected, axis=0),
                -np.std(all_bis_corrected, axis=0),
                alpha=0.7,
                color="r",
                label="std",
            )

            plt.subplots_adjust(top=0.95, bottom=0.07, left=0.07, right=0.93, hspace=0.35)

            plt.figure(figsize=(24, 12))
            plt.subplot(2, 2, 1)
            plt.scatter(
                tab["jdb"],
                (tab["ccf_rv"] - np.median(tab["ccf_rv"])) * 1000,
                color="k",
                label="RV from Gaussian fit",
            )
            plt.scatter(
                tab["jdb"],
                np.sum(all_bis_recenter * weights, axis=1),
                color="r",
                label="RV reconstructed from BIS",
            )
            plt.legend()
            plt.xlabel("Time", fontsize=14)
            plt.ylabel("RV [m/s]", fontsize=14)
            ax = plt.gca()

            plt.subplot(2, 2, 2, sharex=ax, sharey=ax)
            plt.scatter(
                tab["jdb"],
                (tab["ccf_rv"] - np.median(tab["ccf_rv"])) * 1000
                - np.sum(all_bis_recenter * weights, axis=1),
                color="purple",
            )
            plt.xlabel("Time", fontsize=14)
            plt.ylabel(r"$\Delta RV$ [m/s]", fontsize=14)

            plt.subplot(2, 2, 3, sharex=ax, sharey=ax)
            plt.scatter(
                tab["jdb"],
                np.sum(all_bis_recenter * weights, axis=1),
                color="r",
                label="RV reconstructed from BIS",
            )
            plt.scatter(
                tab["jdb"],
                np.sum(all_bis_corrected * weights, axis=1),
                color="b",
                label="RV reconstructed from corrected BIS",
            )
            plt.legend()
            plt.xlabel("Time", fontsize=14)
            plt.ylabel("RV [m/s]", fontsize=14)

            plt.subplot(2, 2, 4, sharex=ax, sharey=ax)
            plt.scatter(
                tab["jdb"],
                np.sum(all_bis_recenter * weights, axis=1)
                - np.sum(all_bis_corrected * weights, axis=1),
                color="purple",
            )
            plt.xlabel("Time", fontsize=14)
            plt.ylabel(r"$\Delta RV$ [m/s]", fontsize=14)

            uncorrected = myc.tableXY(tab["jdb"], np.sum(all_bis_recenter * weights, axis=1))
            corrected = myc.tableXY(tab["jdb"], np.sum(all_bis_corrected * weights, axis=1))

            plt.figure(figsize=(16, 8))
            plt.subplot(2, 1, 1)
            uncorrected.periodogram(nb_perm=1)
            plt.subplot(2, 1, 2)
            corrected.periodogram(nb_perm=1)

        # myf.my_colormesh(liste, np.arange(len(bis_gradient)), bis_gradient - median_bis_gradient, vmin=-1, vmax=1, cmap=cmap)

    # =============================================================================
    # VISUALISE THE RASSINE TIMESERIES AND ITS CORRELATION WITH A PROXY
    # =============================================================================

    def yarara_map(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        planet=False,
        modulo=None,
        unit=1.0,
        wave_min=4000,
        wave_max=4300,
        time_min=None,
        time_max=None,
        index="index",
        ratio=False,
        reference="median",
        berv_shift=False,
        rv_shift=False,
        new=True,
        Plot=True,
        substract_map=[],
        add_map=[],
        correction_factor=True,
        p_noise=1 / np.inf,
    ):

        """
        Display the time-series spectra with proxies and its correlation

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : Minimum x axis limit
        wave_max : Maximum x axis limit
        time_min : Minimum y axis limit
        time_max : Maximum y axis limit
        index : 'index' or 'time' if 'time', display the time-series with blank color if no spectra at specific time  (need roughly equidistant time-series spectra)
        zoom : int-type, to improve the resolution of the 2D plot
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum usedin the difference
        berv_shift : keyword column to use to move in terrestrial rest-frame, km/s speed
        rv_shift : keyword column to use to shift spectra, m/s speed
        cmap : cmap of the 2D plot
        low_cmap : vmin cmap colorbar
        high_cmap : vmax cmap colorbar

        """

        directory = self.directory
        self.import_material()
        load = self.material

        self.import_table()

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        zoom = self.zoom
        smooth_map = self.smooth_map
        cmap = self.cmap
        low_cmap = self.low_cmap
        high_cmap = self.high_cmap

        files = glob.glob(directory + "RASSI*.p")
        files = np.array(self.table["filename"])  # updated 29.10.21 to allow ins_merged

        files = np.sort(files)

        flux = []
        err_flux = []
        snr = []
        jdb = []
        berv = []
        rv = []

        epsilon = 1e-12

        for i, j in enumerate(files):
            self.current_file = j
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum] + epsilon
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)

            snr.append(file["parameters"]["SNR_5500"])
            flux.append(f_norm)
            err_flux.append(f_norm_std)

            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            if type(berv_shift) != np.ndarray:
                try:
                    berv.append(file["parameters"][berv_shift])
                except:
                    berv.append(0)
            else:
                berv = berv_shift
            if type(rv_shift) != np.ndarray:
                try:
                    rv.append(file["parameters"][rv_shift])
                except:
                    rv.append(0)
            else:
                rv = rv_shift

        del self.current_file

        wave = np.array(wave)
        flux = np.array(flux)
        err_flux = np.array(err_flux)
        snr = np.array(snr)
        jdb = np.array(jdb)
        berv = np.array(berv)
        rv = np.array(rv)
        rv = rv - np.median(rv)

        self.debug = flux

        if correction_factor:
            flux *= np.array(load["correction_factor"])
            err_flux *= np.array(load["correction_factor"])

        for maps in substract_map:
            flux = self.yarara_substract_map(flux, maps, correction_factor=correction_factor)

        for maps in add_map:
            flux = self.yarara_add_map(flux, maps, correction_factor=correction_factor)

        idx_min = 0
        idx_max = len(wave)
        idx2_min = 0
        idx2_max = len(flux)

        if wave_min is not None:
            idx_min, val, dist = myf.find_nearest(wave, wave_min)
            if val < wave_min:
                idx_min += 1
        if wave_max is not None:
            idx_max, val, dist = myf.find_nearest(wave, wave_max)
            idx_max += 1
            if val > wave_max:
                idx_max -= 1

        if time_min is not None:
            idx2_min = time_min
        if time_max is not None:
            idx2_max = time_max + 1

        if (idx_min == 0) & (idx_max == 0):
            idx_max = myf.find_nearest(wave, np.min(wave) + (wave_max - wave_min))[0]

        noise_matrix, noise_values = self.yarara_poissonian_noise(
            noise_wanted=p_noise, wave_ref=None, flat_snr=True
        )
        flux += noise_matrix
        err_flux = np.sqrt(err_flux**2 + noise_values**2)

        old_length = len(wave)
        wave = wave[int(idx_min) : int(idx_max)]
        flux = flux[int(idx2_min) : int(idx2_max), int(idx_min) : int(idx_max)]
        err_flux = err_flux[int(idx2_min) : int(idx2_max), int(idx_min) : int(idx_max)]

        snr = snr[int(idx2_min) : int(idx2_max)]
        jdb = jdb[int(idx2_min) : int(idx2_max)]
        berv = berv[int(idx2_min) : int(idx2_max)]

        if np.sum(abs(rv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, flux[j], 0 * wave)
                test.x = myf.doppler_r(test.x, rv[j])[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True)
                flux[j] = test.y

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            ref = np.median(flux, axis=0)
        elif reference == "master":
            ref = (np.array(load["reference_spectrum"]) * np.array(load["correction_factor"]))[
                int(idx_min) : int(idx_max)
            ]
        elif type(reference) == int:
            ref = flux[reference]
        elif type(reference) == np.ndarray:
            ref = reference.copy()
            if len(ref) == old_length:
                ref = ref[int(idx_min) : int(idx_max)]
        else:
            ref = 0 * np.median(flux, axis=0)
            low_cmap = 0
            high_cmap = 1

        if low_cmap is None:
            low_cmap = np.percentile(flux - ref, 2.5)
        if high_cmap is None:
            high_cmap = np.percentile(flux - ref, 97.5)

        if ratio:
            diff = myf.smooth2d(flux / (ref + epsilon), smooth_map)
            low_cmap = 1 - 0.005
            high_cmap = 1 + 0.005
        else:
            diff = myf.smooth2d(flux - ref, smooth_map)

        if modulo is not None:
            diff = self.yarara_map_folded(diff, modulo=modulo, jdb=jdb)[0]

        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, diff[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True)
                diff[j] = test.y

        self.map = (wave, diff)

        if index != "index":
            dtime = np.median(np.diff(jdb))
            liste_time = np.arange(jdb.min(), jdb.max() + dtime, dtime)
            match_time = myf.match_nearest(liste_time, jdb)

            snr2 = np.nan * np.ones(len(liste_time))
            jdb2 = np.nan * np.ones(len(liste_time))
            diff2 = np.median(diff) * np.ones((len(liste_time), len(wave)))

            snr2[match_time[:, 0].astype("int")] = snr[match_time[:, 1].astype("int")]
            jdb2[match_time[:, 0].astype("int")] = jdb[match_time[:, 1].astype("int")]
            diff2[match_time[:, 0].astype("int"), :] = diff[match_time[:, 1].astype("int"), :]

            snr = snr2
            jdb = jdb2
            diff = diff2
        if Plot:
            if new:
                fig = plt.figure(figsize=(24, 6))
                plt.axes([0.1, 0.1, 0.8, 0.8])
            myf.my_colormesh(
                wave,
                np.arange(len(diff)),
                diff * unit,
                zoom=zoom,
                vmin=low_cmap * unit,
                vmax=high_cmap * unit,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
            plt.ylim(0, None)
            if new:
                cbaxes = fig.add_axes([0.86 + 0.04, 0.1, 0.01, 0.8])
                ax = plt.colorbar(cax=cbaxes)
                ax.ax.set_ylabel(r"$\Delta$ flux normalised")
        return diff, err_flux, wave

    def yarara_map_database(
        self, sub_dico, instrument, wave_min=None, wave_max=None, modulo=None, var_sorting="time"
    ):
        root = self.dir_root.split("Yarara")[0]
        files = glob.glob(
            root + "Yarara/*/data/s1d/" + instrument + "/CORRECTION_MAP/*" + sub_dico + "*"
        )
        stars = [f.split("Yarara/")[1].split("/")[0] for f in files]
        stars = np.array(stars)

        all_wave_min = []
        all_wave_max = []

        for f in files:
            wave = pd.read_pickle(f)["wave"]
            all_wave_min.append(np.min(wave))
            all_wave_max.append(np.max(wave))

        max_wave = np.min(all_wave_max)
        min_wave = np.max(all_wave_min)

        if wave_min is None:
            wave_min = min_wave
        if wave_max is None:
            wave_max = max_wave

        if wave_min < min_wave:
            wave_min = min_wave
        if wave_max > max_wave:
            wave_max = max_wave

        all_time = []
        all_maps = []
        all_stars = []
        all_berv = []
        c = -1
        for f in tqdm(files):
            c += 1
            maps = pd.read_pickle(f)
            wave = maps["wave"]
            i1 = 0
            i2 = len(wave)
            if np.min(wave) != wave_min:
                i1 = myf.find_nearest(wave, wave_min)[0][0]
            if np.max(wave) != wave_max:
                i2 = myf.find_nearest(wave, wave_max)[0][0]

            # i1=100000 ; i2 = 10000+i1
            maps = maps["correction_map"]
            summary = pd.read_pickle(
                f.split("/CORRECTION_MAP")[0] + "/WORKSPACE/Analyse_summary.p"
            )
            time = summary["jdb"]
            berv = summary["berv"]

            if len(maps) == len(time):
                if var_sorting == "berv":
                    sorting = np.argsort(berv)
                else:
                    sorting = np.arange(len(berv))
                all_maps.append(maps[:, i1 : i2 + 1][sorting])
                all_time.append(time[sorting])
                all_berv.append(berv[sorting])
                all_stars.append(np.ones(len(time)) * c)
            else:
                print(
                    "\n [WARNING] Length of the time and of the correction map are not identical, star %s not considered"
                    % (stars[c])
                )

        all_maps = np.vstack(all_maps)
        all_time = np.hstack(all_time)
        all_berv = np.hstack(all_berv)
        all_stars = np.hstack(all_stars)

        plt.figure(figsize=(22, 15))
        plt.axes([0.06, 0.52, 0.57, 0.45])
        plt.imshow(all_maps, vmin=-0.005, vmax=0.005, cmap="plasma", aspect="auto")
        plt.ylabel("Spectra index (star-%s ordering)" % (var_sorting), fontsize=14)
        plt.tick_params(direction="in", labelbottom=False, top=True, right=True)
        ax = plt.gca()
        plt.axes([0.63, 0.52, 0.12, 0.45], sharey=ax)
        for j in np.unique(all_stars):
            plt.scatter(
                j * np.ones(len(np.where(all_stars == j)[0])),
                np.where(all_stars == j)[0],
                label="%s" % (stars[int(j)]),
            )
        plt.legend()
        plt.tick_params(direction="in", labelleft=False, labelbottom=False, top=True, right=True)

        plt.axes([0.75, 0.52, 0.12, 0.45], sharey=ax)
        for j in np.unique(all_stars):
            plt.scatter(
                all_time[np.where(all_stars == j)[0]],
                np.where(all_stars == j)[0],
                label="%s" % (stars[int(j)]),
            )
        plt.tick_params(direction="in", labelleft=False, labelbottom=False, top=True, right=True)

        plt.axes([0.87, 0.52, 0.12, 0.45], sharey=ax)
        for j in np.unique(all_stars):
            plt.scatter(
                all_berv[np.where(all_stars == j)[0]],
                np.where(all_stars == j)[0],
                label="%s" % (stars[int(j)]),
            )
        plt.tick_params(direction="in", labelleft=False, labelbottom=False, top=True, right=True)

        if modulo is not None:
            if var_sorting == "time":
                all_time = all_time % modulo
            elif var_sorting == "berv":
                all_berv = all_berv % modulo

        if var_sorting == "time":
            sorting = np.argsort(all_time)
        elif var_sorting == "berv":
            sorting = np.argsort(all_berv)

        all_time = all_time[sorting]
        all_berv = all_berv[sorting]
        all_stars = all_stars[sorting]
        all_maps = all_maps[sorting]
        wave = np.array(wave)

        plt.axes([0.06, 0.07, 0.57, 0.45], sharex=ax)
        plt.imshow(all_maps, vmin=-0.005, vmax=0.005, cmap="plasma", aspect="auto")
        plt.ylabel("Spectra index (%s-star ordering)" % (var_sorting), fontsize=14)
        plt.xlabel("Wavelength index", fontsize=14)
        plt.tick_params(direction="in", top=True, right=True)
        ax = plt.gca()
        plt.axes([0.63, 0.07, 0.12, 0.45], sharey=ax)
        for j in np.unique(all_stars):
            plt.scatter(
                j * np.ones(len(np.where(all_stars == j)[0])),
                np.where(all_stars == j)[0],
                label="%s" % (stars[int(j)]),
            )
        plt.tick_params(direction="in", labelleft=False, top=True, right=True)
        plt.xlabel("Star index", fontsize=14)

        plt.axes([0.75, 0.07, 0.12, 0.45], sharey=ax)
        for j in np.unique(all_stars):
            plt.scatter(
                all_time[np.where(all_stars == j)[0]],
                np.where(all_stars == j)[0],
                label="%s" % (stars[int(j)]),
            )
        plt.tick_params(direction="in", labelleft=False, top=True, right=True)
        plt.xlabel("Time jdb", fontsize=14)

        plt.axes([0.87, 0.07, 0.12, 0.45], sharey=ax)
        for j in np.unique(all_stars):
            plt.scatter(
                all_berv[np.where(all_stars == j)[0]],
                np.where(all_stars == j)[0],
                label="%s" % (stars[int(j)]),
            )
        plt.tick_params(direction="in", labelleft=False, labelbottom=False, top=True, right=True)
        plt.xlabel("Berv [km/s]", fontsize=14)
        plt.subplots_adjust(left=0.07, right=0.95, top=0.95, bottom=0.09)

    def yarara_substract_map(self, flux_init, map_name, correction_factor=False):
        self.import_material()
        load = self.material
        corr = np.array(load["correction_factor"]) * int(correction_factor) + (
            1 - int(correction_factor)
        )
        correction_map = pd.read_pickle(
            self.dir_root + "CORRECTION_MAP/map_matching_" + map_name + ".p"
        )["correction_map"]
        flux_modified = flux_init - correction_map * corr
        return flux_modified

    def yarara_add_map(self, flux_init, map_name, correction_factor=False):
        self.import_material()
        load = self.material
        corr = np.array(load["correction_factor"]) * int(correction_factor) + (
            1 - int(correction_factor)
        )
        correction_map = pd.read_pickle(
            self.dir_root + "CORRECTION_MAP/map_matching_" + map_name + ".p"
        )["correction_map"]
        flux_modified = flux_init + correction_map * corr
        return flux_modified

    def yarara_map_folded(self, map_to_fold, std_map=None, modulo=365.25, jdb=None, nb_bins=None):
        self.import_table()
        tab = self.table
        if jdb is None:
            jdb = np.array(tab["jdb"])
        jdb_mod = np.sort(jdb % modulo)
        dphi_loc = np.argsort(np.gradient(jdb_mod))[-2:]
        dphi = 0
        if np.max(np.diff(jdb_mod)) > (modulo / 5):
            dphi = np.mean(jdb_mod[dphi_loc])

        jdb_mod = (jdb - dphi) % modulo  # substract dphi by convention
        new_index = jdb_mod.argsort()

        if std_map is None:
            std_map = np.ones(np.shape(map_to_fold))

        map_folded = map_to_fold[new_index]
        std_map_folded = std_map[new_index]
        weights_map_folded = 1 / std_map_folded**2
        jdb_mod = jdb_mod[new_index]

        jdb_binned = jdb_mod.copy()
        if nb_bins is not None:
            if nb_bins > 30:
                nb_bins = 30
            map_binned = []
            std_map_binned = []
            limits = np.linspace(jdb_mod.min(), jdb_mod.max(), nb_bins + 1)
            idx = myf.find_nearest(jdb_mod, limits)[0]

            jdb_binned = np.array(
                [np.mean(jdb_mod[idx[i] : idx[i + 1] + 1]) for i in range(nb_bins)]
            )

            for j in range(nb_bins):
                binned_row = np.sum(
                    map_folded[idx[j] : idx[j + 1] + 1]
                    * weights_map_folded[idx[j] : idx[j + 1] + 1],
                    axis=0,
                ) / np.sum(weights_map_folded[idx[j] : idx[j + 1] + 1], axis=0)
                std_binned_row = np.sqrt(
                    1 / np.sum(weights_map_folded[idx[j] : idx[j + 1] + 1], axis=0)
                )

                map_binned.append(binned_row)
                std_map_binned.append(std_binned_row)

            map_folded = np.array(map_binned)
            std_map_folded = np.array(std_map_binned)

        return map_folded, std_map_folded, new_index, jdb_mod, jdb_binned, dphi

    def yarara_map_rv(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        planet=False,
        wave_min=4000,
        wave_max=4300,
        reference="median",
        berv_shift=False,
        new=True,
        sigma_detector=11,
        sigma_flat=1e-3,
        time_ref=0,
    ):

        """
        Display the time-series spectra with proxies and its correlation

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : Minimum x axis limit
        wave_max : Maximum x axis limit
        time_min : Minimum y axis limit
        time_max : Maximum y axis limit
        index : 'index' or 'time' if 'time', display the time-series with blank color if no spectra at specific time  (need roughly equidistant time-series spectra)
        zoom : int-type, to improve the resolution of the 2D plot
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum usedin the difference
        berv_shift : keyword column to use to move in terrestrial rest-frame
        cmap : cmap of the 2D plot
        low_cmap : vmin cmap colorbar
        high_cmap : vmax cmap colorbar

        """

        directory = self.directory

        kw = "_planet" * planet

        zoom = self.zoom
        smooth_map = self.smooth_map
        cmap = self.cmap
        low_cmap = self.low_cmap
        high_cmap = self.high_cmap
        self.import_material()
        load = self.material

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        flux = []
        snr = []
        conti = []
        jdb = []
        berv = []

        epsilon = 1e-12

        for i, j in enumerate(files):
            self.current_file = j
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
            snr.append(file["parameters"]["SNR_5500"])
            flux.append(file["flux" + kw] / (file[sub_dico]["continuum_" + continuum] + epsilon))
            conti.append(file[sub_dico]["continuum_" + continuum] + epsilon)

            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            if type(berv_shift) != np.ndarray:
                try:
                    berv.append(file["parameters"][berv_shift])
                except:
                    berv.append(0)
            else:
                berv = berv_shift

        del self.current_file

        wave = np.array(wave)
        flux = np.array(flux)
        conti = np.array(conti)
        snr = np.array(snr)
        jdb = np.array(jdb)
        berv = np.array(berv)
        berv = berv - np.mean(berv)

        idx_min = 0
        idx_max = len(wave)
        idx2_min = 0
        idx2_max = len(flux)

        if wave_min is not None:
            idx_min = myf.find_nearest(wave, wave_min)[0]
        if wave_max is not None:
            idx_max = myf.find_nearest(wave, wave_max)[0] + 1

        wave = wave[int(idx_min) : int(idx_max)]
        flux = flux[int(idx2_min) : int(idx2_max), int(idx_min) : int(idx_max)]
        conti = conti[int(idx2_min) : int(idx2_max), int(idx_min) : int(idx_max)]
        # std_conti = np.sqrt(sigma_detector**2+(sigma_flat*conti)**2+conti)
        std_flux = np.sqrt(sigma_detector**2 + (sigma_flat * conti) ** 2 + flux * conti)
        std_flux_norm = std_flux / conti

        snr = snr[int(idx2_min) : int(idx2_max)]
        jdb = jdb[int(idx2_min) : int(idx2_max)]
        berv = berv[int(idx2_min) : int(idx2_max)]

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            ref = np.median(flux, axis=0)
        elif reference == "master":
            ref = np.array(load["reference_spectrum"])
            ref = ref[int(idx_min) : int(idx_max)]
        elif type(reference) == int:
            ref = flux[reference]
        else:
            ref = 0 * np.median(flux, axis=0)

        if low_cmap is None:
            low_cmap = np.percentile(flux - ref, 2.5)
        if high_cmap is None:
            high_cmap = np.percentile(flux - ref, 97.5)

        diff = myf.smooth2d(flux - ref, smooth_map)

        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, diff[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                diff[j] = test.y

        dflux = np.gradient(ref) / file["parameters"]["dwave"]

        coeff = 3e8 / wave.astype("float64")
        diff_rv = coeff * diff / dflux
        map_weight = (wave) ** 2 * np.abs(dflux) ** 2 * np.ones(len(berv))[:, np.newaxis]
        map_weight /= np.percentile(map_weight, 95)
        diff_rv[:, ref > 0.95] = 0  # remove all weight smaller than 10%
        diff_rv[map_weight < 0.1] = 0  # remove all weight smaller than 10%
        diff_rv[abs(diff_rv) > 1000] = 0
        diff_rv[diff_rv == 0] = np.nan

        map_std_rv = coeff * std_flux_norm / np.abs(dflux)
        map_std_rv[diff_rv == 0] = 0
        map_weight2 = 1 / map_std_rv**2

        plt.figure(figsize=(12, 16))
        plt.subplot(3, 1, 2).plot(wave, ref, color="k")
        ax = plt.gca()
        plt.xlim(wave_min, wave_max)
        plt.subplot(3, 1, 1, sharex=ax)
        myf.my_colormesh(
            wave, np.arange(len(diff)), diff, zoom=zoom, vmin=low_cmap, vmax=high_cmap, cmap=cmap
        )
        plt.axhline(y=time_ref, color="k")
        plt.tick_params(direction="in", top=True, right=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)

        plt.subplot(3, 1, 3, sharex=ax)
        plt.errorbar(wave, diff_rv[time_ref], yerr=map_std_rv[time_ref], fmt="ko")
        plt.axhline(y=0, color="r")

        self.map_rv = (wave, diff_rv, map_weight)

        plt.figure()
        plt.subplot(4, 1, 1)
        plt.imshow(diff_rv, aspect="auto", vmin=-30, vmax=30, cmap="seismic")
        ax = plt.gca()
        plt.subplot(4, 1, 2, sharex=ax, sharey=ax)
        plt.imshow(map_weight, aspect="auto")
        plt.subplot(4, 1, 3, sharex=ax, sharey=ax)
        plt.imshow(map_weight2, aspect="auto")
        plt.subplot(4, 1, 4, sharex=ax)
        plt.plot(ref)
        plt.figure()
        plt.subplot(2, 2, 1).scatter(ref, np.gradient(ref), c=wave, cmap="jet")
        plt.subplot(2, 2, 2).scatter(
            ref, np.gradient(ref), c=np.std(diff_rv, axis=0), cmap="plasma", vmin=0, vmax=10
        )

        mask_nan = np.isnan(np.nanstd(diff_rv, axis=0))
        test = myc.tableXY(ref[~mask_nan], np.gradient(ref)[~mask_nan])
        test.fit_poly2d(np.nanstd(diff_rv, axis=0)[~mask_nan], Draw=True)
        plt.subplot(2, 2, 3).scatter(
            ref[~mask_nan],
            np.gradient(ref)[~mask_nan],
            c=diff_rv[193][~mask_nan],
            cmap="plasma",
            vmin=-30,
            vmax=30,
        )

        self.map = (wave, diff)

        return diff, wave, diff_rv

    def yarara_map_all(
        self,
        continuum="linear",
        wave_min=None,
        wave_max=None,
        time_min=None,
        time_max=None,
        index="index",
        reference="median",
        berv_shift=False,
        new=True,
    ):

        planet = self.planet
        self.import_dico_tree()

        dico_tree = self.dico_tree
        all_dico = self.all_dicos

        directory_images = "/".join(self.directory.split("/")[0:-2]) + "/IMAGES/"

        if not os.path.exists(directory_images):
            os.system("mkdir " + directory_images)

        if not os.path.exists(directory_images + "%s_%s" % (wave_min, wave_max)):
            os.system("mkdir " + directory_images + "%s_%s" % (wave_min, wave_max))

        test_file = self.import_spectrum()

        crossed_dico = np.array(all_dico)[np.in1d(all_dico, list(test_file.keys()))]

        dico_found = np.array(dico_tree["dico"])[
            np.in1d(np.array(dico_tree["dico"]), crossed_dico)
        ]

        dico_chain = np.hstack(["matching_anchors", dico_found])

        # last = np.array(crossed_dico)[~np.in1d(crossed_dico,dico_chain)][0]

        # nb_plot = len(crossed_dico)+1

        # counter=nb_plot
        for counter, last in enumerate(dico_chain):
            if "continuum_" + continuum in test_file[last].keys():
                print("sub dico %s being processed..." % (last))
                plt.figure(figsize=(15, 5))
                self.yarara_map(
                    sub_dico=last,
                    continuum=continuum,
                    planet=planet,
                    wave_min=wave_min,
                    wave_max=wave_max,
                    time_min=time_min,
                    time_max=time_max,
                    index=index,
                    reference=reference,
                    berv_shift=berv_shift,
                    new=True,
                )
                plt.savefig(
                    directory_images
                    + "%s_%s/" % (wave_min, wave_max)
                    + "Map_%s_%s_" % (wave_min, wave_max)
                    + str(counter + 1).zfill(2)
                    + "_%s.png" % (last)
                )
                # counter-=1
                plt.close("all")
                # if counter!=0:
                #    last = np.array(dico_chain)[np.where(np.array(crossed_dico)==last)[0]][0]

        plt.figure(figsize=(15, 10))
        plt.subplot(2, 1, 1)
        plt.title("Before YARARA processing", fontsize=14)
        self.yarara_map(
            sub_dico="matching_diff",
            continuum=continuum,
            wave_min=wave_min,
            wave_max=wave_max,
            time_min=time_min,
            time_max=time_max,
            index=index,
            reference=reference,
            berv_shift=berv_shift,
            new=False,
        )
        ax = plt.gca()
        plt.subplot(2, 1, 2, sharex=ax, sharey=ax)
        plt.title("After YARARA processing", fontsize=14)
        self.yarara_map(
            sub_dico="matching_mad",
            continuum=continuum,
            wave_min=wave_min,
            wave_max=wave_max,
            time_min=time_min,
            time_max=time_max,
            index=index,
            reference=reference,
            berv_shift=berv_shift,
            new=False,
        )
        plt.subplots_adjust(top=0.97, right=0.97, left=0.05, bottom=0.05)
        plt.savefig(directory_images + "%s_%s/" % (wave_min, wave_max) + "Before_after.png")

    def yarara_film(
        self,
        wave_min=6200,
        wave_max=6200,
        sub_dico1="matching_diff",
        sub_dico2="matching_pca",
        reference="master",
        continuum="linear",
        nb_bins=48,
        folding="berv",
        magnificience=10,
        season=0,
        origin=0,
        color="k",
    ):
        """"""
        print("\n[INFO] TO BE LAUNCHED IN A TERMINAL AND NOT IN SPYDER\n")

        self.import_material()
        load = self.material
        self.import_table()
        self.import_dico_tree()
        snr = np.array(self.table["snr"])

        grid = load["wave"]

        sb_rv = sub_dico2

        rv_best = "matching_diff"
        if "matching_pca" in np.array(self.dico_tree["dico"]):
            rv_best = "matching_pca"
        if "matching_mad" in np.array(self.dico_tree["dico"]):
            rv_best = "matching_mad"
        if "matching_empca" in np.array(self.dico_tree["dico"]):
            rv_best = "matching_empca"

        line_plot = False
        if wave_max == wave_min:
            line_plot = True
            if sub_dico1 == sub_dico2:
                sb_rv = np.array(
                    self.dico_tree.loc[self.dico_tree["dico"] == sub_dico1, "dico_used"]
                )[0]
            rv_line1 = self.lbl_plot(wave_srf=wave_min, plot=False, sub_dico=sub_dico1)
            rv_line2 = self.lbl_plot(wave_srf=wave_min, plot=False, sub_dico=sb_rv)
            rv_line1.minus(rv_line2)
            rv_best = self.lbl_plot(wave_srf=wave_min, plot=False, sub_dico=rv_best)
            wave_min = self.wave_ref_srf
            wave_max = self.wave_ref_srf

            wave_max += 0.5
            wave_min -= 0.5
        else:
            rv_line1 = myc.tableXY(self.table.jdb, np.zeros(len(self.table.jdb)))
            rv_best = myc.tableXY(self.table.jdb, np.zeros(len(self.table.jdb)))

        idx_min = myf.find_nearest(grid, wave_min)[0][0]
        idx_max = myf.find_nearest(grid, wave_max)[0][0]

        if sub_dico1 == sub_dico2:
            flux, flux_std, wave = self.yarara_map(
                wave_min=wave_min,
                wave_max=wave_max,
                sub_dico=sub_dico1,
                reference=False,
                Plot=False,
            )
            contam_matrix = pd.read_pickle(
                self.dir_root + "/CORRECTION_MAP/map_" + sub_dico1 + ".p"
            )
            contam_wave = contam_matrix["wave"]
            contam = contam_matrix["correction_map"]
            idx_min1 = myf.find_nearest(contam_wave, wave_min)[0][0]
            idx_max1 = myf.find_nearest(contam_wave, wave_max)[0][0]
            contam = contam[:, idx_min1:idx_max1]
            contam_std = flux_std
            reference = "master"
        else:
            flux_contam, flux_contam_std, wave = self.yarara_map(
                wave_min=wave_min,
                wave_max=wave_max,
                sub_dico=sub_dico1,
                reference=False,
                Plot=False,
            )
            flux, flux_std, wave = self.yarara_map(
                wave_min=wave_min,
                wave_max=wave_max,
                sub_dico=sub_dico2,
                reference=False,
                Plot=False,
            )
            contam = flux_contam - flux
            contam_std = flux_std

        ref = None
        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            ref = np.median(flux, axis=0)
        elif reference == "master":
            ref = np.array(load["reference_spectrum"])[int(idx_min) : int(idx_max) + 1]
        elif type(reference) == int:
            ref = flux[reference]
        elif type(reference) == np.ndarray:
            ref = reference
            if len(reference) > len(wave):
                ref = ref[int(idx_min) : int(idx_max) + 1]

        if ref is None:
            flux = np.ones(np.shape(flux)) * ref

        jdb = np.array(self.table["jdb"])

        if season:
            seasons = myf.detect_obs_season(jdb, min_gap=40)
            season -= 1
            t_min = seasons[season, 0]
            t_max = seasons[season, 1]
        else:
            t_min = 0
            t_max = len(jdb)

        if folding is None:
            folding = "jdb"

        folding_std = np.ones(len(self.table.jdb))

        if type(folding) == str:
            folding = np.array(self.table[folding])
            try:
                folding_std = np.array(self.table[folding + "_std"])
            except:
                pass

        elif type(folding) == float:
            folding = jdb % folding

        folding_vec = myc.tableXY(jdb, folding, folding_std)

        table = self.table.copy()
        table = table.loc[t_min : t_max + 1]

        folding = folding_vec.y[t_min : t_max + 1]
        folding_std = folding_vec.yerr[t_min : t_max + 1]

        rv = rv_line1.y[t_min : t_max + 1]
        rv_std = rv_line1.yerr[t_min : t_max + 1]

        flux = flux[t_min : t_max + 1]
        flux_std = flux_std[t_min : t_max + 1]

        contam = contam[t_min : t_max + 1]
        contam_std = contam_std[t_min : t_max + 1]

        jdb = jdb[t_min : t_max + 1]
        jdb_std = folding_std

        save = myc.tableXY(jdb, folding, folding_std)
        save2 = myc.tableXY(folding, rv + rv_best.y[t_min : t_max + 1], rv_std)

        matrix = np.hstack([contam, rv[:, np.newaxis], folding[:, np.newaxis], jdb[:, np.newaxis]])
        matrix_std = np.hstack(
            [contam_std, rv_std[:, np.newaxis], folding_std[:, np.newaxis], jdb_std[:, np.newaxis]]
        )

        index_time = np.argsort(folding)

        matrix = matrix[index_time]
        matrix_std = matrix_std[index_time]

        flux = flux[index_time]
        flux_std = flux_std[index_time]

        if nb_bins is not None:
            vec = folding[index_time]
            vec = myf.transform_min_max(vec)
            vec = vec // (1 / nb_bins)

            mask_bin = (vec == np.unique(vec)[:, np.newaxis]).astype("int")

            matrix_binned = np.dot(matrix.T / matrix_std.T**2, mask_bin.T) / np.dot(
                1 / matrix_std.T**2, mask_bin.T
            )
            matrix_binned_std = np.sqrt(1 / np.dot(1 / matrix_std.T**2, mask_bin.T))

            matrix = matrix_binned.T
            matrix_std = matrix_binned_std.T

            flux_binned = np.dot(flux.T / flux_std.T**2, mask_bin.T) / np.dot(
                1 / flux_std.T**2, mask_bin.T
            )
            flux_binned_std = np.sqrt(1 / np.dot(1 / flux_std.T**2, mask_bin.T))

            flux = flux_binned.T
            flux_std = flux_binned_std.T

        jdb = matrix[:, -1]
        jdb_std = matrix_std[:, -1]

        folding = matrix[:, -2]
        folding_std = matrix_std[:, -2]

        rv = matrix[:, -3]
        rv_std = matrix_std[:, -3]

        contam = matrix[:, :-3]
        contam_std = matrix_std[:, :-3]

        ylim_min = np.min(flux + contam)
        ylim_max = np.max(flux + contam)

        ylim2_min = np.min(contam)
        ylim2_max = np.max(contam)

        ylim_min = [0, ylim_min - 0.05][int(ylim_min > 0)]
        ylim_max = [0, ylim_max + 0.05][int(ylim_max < 1.01)]
        ylim2_min = [0, ylim2_min - 0.001][int(1 > 0)]
        ylim2_max = [0, ylim2_max + 0.001][int(0 < 1)]

        jet = plt.get_cmap("jet")
        vmin = np.min(folding)
        vmax = np.max(folding)

        fold = myc.tableXY(jdb, folding, folding_std)

        cNorm = mplcolors.Normalize(vmin=vmin, vmax=vmax)
        scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)

        def get_color(val):
            return [scalarMap.to_rgba(val), color][int(color is not None)]

        os.system("mkdir " + self.dir_root + "FILM/temp")

        for j in range(len(folding)):
            colorVal = get_color(folding[j])

            plt.close()
            plt.figure(figsize=(20 - 10 * int(line_plot), 10))
            plt.axes([0.12, 0.6 - 0.15 * int(origin != 0), 0.8, 0.35 + 0.15 * int(origin != 0)])
            plt.plot(wave, ref + contam[j] * magnificience, color=colorVal)
            plt.plot(wave, ref, color="k")
            if origin:
                plt.plot(wave, contam[j] * magnificience + 1, color=colorVal, alpha=0.7)
                plt.axhline(y=1, color="k", ls=":", alpha=0.7)
            plt.xlim(wave_min, wave_max)
            plt.ylim(ylim_min, ylim_max)
            plt.ylabel("Flux normalised")
            ax = plt.gca()

            plt.tick_params(top=True, direction="in", labelbottom=False)
            if not origin:
                plt.axes([0.12, 0.45, 0.8, 0.15], sharex=ax)
                plt.plot(wave, contam[j] * 100, color=colorVal)
                plt.axhline(y=0, color="r")
                plt.ylabel("Residuals [%]")
                plt.xlabel(r"Wavelength [$\AA$]")
                plt.ylim(ylim2_min * 100, ylim2_max * 100)
                plt.tick_params(top=True, direction="in")

            plt.axes([0.12, 0.05 + 0.2 * int(line_plot), 0.8, 0.3 - 0.2 * int(line_plot)])
            save.plot(capsize=0, alpha=0.4)
            plt.scatter(
                save.x,
                save.y,
                c=save.y,
                cmap="jet",
                vmin=vmin,
                vmax=vmax,
                zorder=10,
                edgecolor="k",
            )

            if line_plot:
                plt.axes([0.12, 0.1, 0.8, 0.1])
                save2.plot(capsize=0, alpha=0.4)
                plt.scatter(
                    fold.y,
                    rv,
                    c=fold.y,
                    cmap="jet",
                    vmin=vmin,
                    vmax=vmax,
                    zorder=10,
                    edgecolor="k",
                )

            plt.savefig(self.dir_root + "FILM/temp/images_" + str(j).zfill(4))

        new_dir = self.dir_root + "FILM/film_%s_wave_%.2f_%.2f" % (sub_dico2, wave_min, wave_max)
        os.system("mkdir " + new_dir)

        os.system(
            "convert -delay 5 -loop 1 "
            + self.dir_root
            + "FILM/temp/images*.png "
            + new_dir
            + "/YARARA_film_%s_%.2f_%.2f_%s.gif" % (sub_dico2, wave_min, wave_max, self.starname)
        )
        os.system(
            "convert -delay 5 -quality 100 "
            + self.dir_root
            + "FILM/temp/images*.png "
            + new_dir
            + "/YARARA_film_%s_%.2f_%.2f_%s.mpeg" % (sub_dico2, wave_min, wave_max, self.starname)
        )
        os.system("rm -rf " + self.dir_root + "FILM/temp")

    def yarara_map_film(self, region="red"):

        regions = [[3920, 3980], [5730, 5800]][int((region == "red"))]
        wave_min = regions[0]
        wave_max = regions[1]

        dir_output = self.dir_root + "IMAGES/%s_%s/" % (wave_min, wave_max)
        os.system(
            "convert -delay 50 -quality 100 "
            + dir_output
            + "Map*.png "
            + dir_output
            + "YARARA_film_%s_%s_%s.mpeg" % (wave_min, wave_max, self.starname)
        )

    def yarara_time_variations(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        reference="median",
        berv_shift=False,
        wave_min=None,
        wave_max=None,
        time_min=None,
        time_max=None,
        index="index",
        ratio=False,
        gui=False,
        smooth_corr=5,
        proxy_corr="CaII",
        proxy_detrending=0,
        modulo=None,
        stitching=False,
        substract_map=[],
        add_map=[],
        Plot=True,
        p_noise=1 / np.inf,
        anchor_points=False,
    ):

        """
        Display the time-series spectra with proxies and its correlation

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : Minimum x axis limit
        wave_max : Maximum x axis limit
        time_min : Minimum y axis limit
        time_max : Maximum y axis limit
        index : 'index' or 'time' if 'time', display the time-series with blank color if no spectra at specific time  (need roughly equidistant time-series spectra)
        zoom : int-type, to improve the resolution of the 2D plot
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution
        smooth_corr = smooth thecoefficient  ofcorrelation curve
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum usedin the difference
        berv_shift : True/False to move in terrestrial rest-frame
        proxy_corr : keyword  of the proxies from RASSINE dictionnary to use in the correlation
        proxy_detrending : Degree of the polynomial fit to detrend the proxy
        cmap : cmap of the 2D plot
        low_cmap : vmin cmap colorbar
        high_cmap : vmax cmap colorbar

        """

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        cmap = self.cmap
        low_cmap = self.low_cmap
        high_cmap = self.high_cmap
        planet = self.planet
        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)
        self.import_table()
        self.import_material()
        load = self.material
        factor_corr = load["correction_factor"]

        flux = []
        err_flux = []
        snr = []
        prox = []
        prox_std = []
        jdb = []
        berv = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
                self.wave = wave
            snr.append(file["parameters"]["SNR_5500"])
            if type(proxy_corr) == str:
                try:
                    prox.append(file["parameters"][proxy_corr])
                except KeyError:
                    prox.append(np.array(self.table[proxy_corr])[i])
                try:
                    prox_std.append(file["parameters"][proxy_corr + "_std"])
                except KeyError:
                    prox_std.append(1)

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            flux.append(f_norm)
            err_flux.append(f_norm_std)

            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            if type(berv_shift) != np.ndarray:
                try:
                    berv.append(file["parameters"][berv_shift + kw])
                except:
                    berv.append(0)
            else:
                berv = berv_shift
        wave = np.array(wave)
        flux = np.array(flux)
        err_flux = np.array(err_flux)
        snr = np.array(snr)
        proxy = np.array(prox)
        proxy_std = np.array(prox_std)

        if not len(proxy):
            proxy = proxy_corr
            proxy_std = np.std(proxy_corr) * np.ones(len(proxy_corr))

        for maps in substract_map:
            flux = self.yarara_substract_map(flux, maps, correction_factor=False)

        for maps in add_map:
            flux = self.yarara_add_map(flux, maps, correction_factor=False)

        jdb = np.array(jdb)
        berv = np.array(berv)
        # berv = berv-np.mean(berv)

        # proxy = proxy.detrend_line.y

        idx_min = 0
        idx_max = len(wave)
        idx2_min = 0
        idx2_max = len(flux)

        if wave_min is not None:
            idx_min = myf.find_nearest(wave, wave_min)[0]
        if wave_max is not None:
            idx_max = myf.find_nearest(wave, wave_max)[0] + 1
        if time_min is not None:
            idx2_min = myf.find_nearest(jdb, time_min)[0]
        if time_max is not None:
            idx2_max = myf.find_nearest(jdb, time_max)[0] + 1

        if stitching:
            stitch = np.array(load["stitching_delta"])
            stitch = np.array(load["wave"])[stitch != 0]

            stitch_wave = myf.doppler_r(stitch, (np.array(self.table["berv" + kw]) - berv) * 1000)[
                0
            ]
            stitch_time = (
                np.arange(len(stitch_wave)) * np.ones(len(stitch_wave[0]))[:, np.newaxis]
            ).T

            stitch_time = stitch_time[int(idx2_min) : int(idx2_max)]
            stitch_wave = stitch_wave[int(idx2_min) : int(idx2_max)]
        else:
            stitch_time = np.array([])
            stitch_wave = np.array([])

        noise_matrix, noise_values = self.yarara_poissonian_noise(
            noise_wanted=p_noise, wave_ref=None, flat_snr=True
        )
        flux += noise_matrix
        err_flux = np.sqrt(err_flux**2 + noise_values**2)

        wave = wave[int(idx_min) : int(idx_max)]
        flux = flux[int(idx2_min) : int(idx2_max), int(idx_min) : int(idx_max)]
        err_flux = err_flux[int(idx2_min) : int(idx2_max), int(idx_min) : int(idx_max)]

        factor_corr = factor_corr[int(idx_min) : int(idx_max)]

        proxy = proxy[int(idx2_min) : int(idx2_max)]
        proxy_std = proxy_std[int(idx2_min) : int(idx2_max)]

        snr = snr[int(idx2_min) : int(idx2_max)]
        jdb = jdb[int(idx2_min) : int(idx2_max)]
        berv = berv[int(idx2_min) : int(idx2_max)]

        proxy = myc.tableXY(jdb, proxy, proxy_std)
        proxy.substract_polyfit(proxy_detrending)
        proxy.detrend_poly.rms_w()
        proxy_rms = proxy.detrend_poly.rms
        proxy = proxy.detrend_poly.y

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            ref = np.median(flux, axis=0)
        elif reference == "master":
            ref = np.array(load["reference_spectrum"])[int(idx_min) : int(idx_max)]
        elif type(reference) == int:
            ref = flux[reference]
        elif type(reference) == np.ndarray:
            ref = reference
            if len(reference) > len(wave):
                ref = ref[int(idx_min) : int(idx_max)]
        else:
            ref = 0 * np.median(flux, axis=0)
            low_cmap = 0
            high_cmap = 1

        if low_cmap is None:
            low_cmap = np.percentile(flux - ref, 2.5)
        if high_cmap is None:
            high_cmap = np.percentile(flux - ref, 97.5)

        if ratio:
            diff = myf.smooth2d(flux / (ref + epsilon), smooth_map)
            low_cmap = 1 - 0.005
            high_cmap = 1 + 0.005
        else:
            diff = myf.smooth2d(flux - ref, smooth_map)

        berv_trigger = 0
        if np.sum(abs(berv)) != 0:
            berv_trigger = 1
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, diff[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                diff[j] = test.y

        if modulo is not None:
            diff, diff_std, new_index, jdb, jdb1, jdb_offset = self.yarara_map_folded(
                diff, modulo=modulo, jdb=jdb
            )
            proxy = proxy[new_index]
            snr = snr[new_index]
            if stitching:
                stitch_wave = stitch_wave[new_index]

        if stitching:
            self.stitching_plot = (stitch_wave, stitch_time)

        rslope = np.median(
            (diff - np.mean(diff, axis=0)) / ((proxy - np.mean(proxy))[:, np.newaxis]), axis=0
        )
        t = myc.table(diff)
        t.rms_w(1 / err_flux**2, axis=0)
        # rcorr = rslope*np.std(proxy)/np.std(diff,axis=0) #old version unweighted
        rcorr = (
            rslope * proxy_rms / (t.rms + epsilon)
        )  # need good weighting of the proxy and the flux

        if berv_trigger:
            rcorr[0:100] = 0
            rcorr[-100:] = 0
            rslope[0:100] = 0
            rslope[-100:] = 0

        r_corr = myc.tableXY(wave, rcorr)
        r_slope = myc.tableXY(wave, rslope)

        r_corr.smooth(box_pts=smooth_corr, shape="savgol", replace=False)
        r_slope.smooth(box_pts=smooth_corr, shape="savgol", replace=False)

        r_slope.y_smoothed = self.uncorrect_hole(
            (r_slope.y_smoothed)[:, np.newaxis].T,
            np.zeros(len(r_slope.y_smoothed))[:, np.newaxis].T,
        )[0]

        self.slope_corr = r_slope.y_smoothed
        self.r_corr = r_corr.y_smoothed
        self.std_norm_corr = proxy_rms

        dtime = np.median(np.diff(jdb))
        liste_time = np.arange(jdb.min(), jdb.max() + dtime, dtime)
        match_time = myf.match_nearest(liste_time, jdb)

        if index != "index":
            snr2 = np.nan * np.ones(len(liste_time))
            proxy2 = np.nan * np.ones(len(liste_time))
            jdb2 = np.nan * np.ones(len(liste_time))
            diff2 = np.median(diff) * np.ones((len(liste_time), len(wave)))

            snr2[match_time[:, 0].astype("int")] = snr[match_time[:, 1].astype("int")]
            proxy2[match_time[:, 0].astype("int")] = proxy[match_time[:, 1].astype("int")]
            jdb2[match_time[:, 0].astype("int")] = jdb[match_time[:, 1].astype("int")]
            diff2[match_time[:, 0].astype("int"), :] = diff[match_time[:, 1].astype("int"), :]

            snr = snr2
            proxy = proxy2
            jdb = jdb2
            diff = diff2

        if Plot:
            fig = plt.figure(figsize=(21, 9))

            plt.axes([0.06, 0.28, 0.7, 0.35])
            plt.scatter(stitch_wave, stitch_time, zorder=10, color="k", marker=".")
            myf.my_colormesh(
                wave,
                np.arange(len(diff)),
                diff * 100,
                zoom=zoom,
                vmin=low_cmap * 100,
                vmax=high_cmap * 100,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            if not gui:
                cbaxes = fig.add_axes([0.7 + 0.06, 0.28, 0.01, 0.35])
                plt.colorbar(cax=cbaxes)

            plt.axes([0.06, 0.805, 0.7, 0.175], sharex=ax)
            plt.plot(wave, rslope, color="gray")
            plt.plot(wave, r_slope.smoothed.y, color="k")
            plt.axhline(y=0, color="r")
            plt.xlim(wave_min, wave_max)
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel(r"$S$", fontsize=14)

            plt.axes([0.06, 0.63, 0.7, 0.175], sharex=ax)
            plt.plot(wave, rcorr, color="gray")
            plt.plot(wave, r_corr.smoothed.y, color="k")
            plt.axhline(y=-0.55, color="r", alpha=0.4)
            plt.axhline(y=0.55, color="r", alpha=0.4)
            plt.axhline(y=0, color="r")
            plt.ylim(-1.15, 1.15)
            plt.axhline(y=1, alpha=0.1)
            plt.axhline(y=-1, alpha=0.1)
            plt.xlim(wave_min, wave_max)
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel(r"$\mathcal{R}$", fontsize=14)

            gui = int(gui)
            plt.axes([0.82 - 0.06 * gui, 0.28, 0.05 + 0.01 * gui, 0.35], sharey=ax)
            plt.plot(proxy, np.arange(len(snr)), "k-")
            plt.tick_params(direction="in", top=True, right=True, labelleft=False)
            plt.xlabel("proxy", fontsize=14)

            plt.axes([0.88 - 0.06 * gui, 0.28, 0.05 + 0.01 * gui, 0.35], sharey=ax)
            plt.plot(snr, np.arange(len(snr)), "k-")
            plt.tick_params(direction="in", top=True, right=True, labelleft=False)
            plt.xlabel("SNR", fontsize=14)

            plt.axes([0.94 - 0.06 * gui, 0.28, 0.05 + 0.01 * gui, 0.35], sharey=ax)
            if modulo is not None:
                plt.plot((jdb - 53000) % modulo, np.arange(len(snr)), "k-")
            else:
                plt.plot((jdb - 53000), np.arange(len(snr)), "k-")
            plt.tick_params(direction="in", top=True, right=True, labelleft=False)
            plt.xlabel("jdb - 53k", fontsize=14)

            if berv[snr.argmax()]:
                waves = myf.doppler_r(wave, berv[snr.argmax()] * 1000)[1]
            else:
                waves = wave

            plt.axes([0.06, 0.08, 0.7, 0.2], sharex=ax)
            plt.plot(waves, flux[snr.argmax()] * factor_corr, color="k")
            plt.xlabel(r"Wavelength $\AA$", fontsize=14)
            plt.ylabel("Flux normalised", fontsize=14)
            plt.tick_params(direction="in", top=True, right=True)
            plt.ylim(-0.15, 1.15)
            plt.axhline(y=1, alpha=0.2, ls=":")
            plt.axhline(y=0, alpha=0.2, ls=":")

            lvline = plt.axvline(x=wave[0], color="r")

            if anchor_points:
                anchor_wave = self.import_spectrum()["matching_anchors"]["anchor_wave"]
                vec_anchor = myc.tableXY(anchor_wave, np.ones(len(anchor_wave)))
                vec_anchor.clip(min=[np.nanmin(wave), None], max=[np.nanmax(wave), None])
                for anchor in vec_anchor.x:
                    plt.axvline(x=anchor, color="k", ls="--")

            if gui:
                plt.axes([0.94, 0.28, 0.05 + 0.0085 * gui, 0.35], sharey=ax)
                (l1,) = plt.plot(diff[:, 0], np.arange(len(snr)), "k-")
                plt.tick_params(direction="in", top=True, right=True, labelleft=False)
                plt.xlabel("w.c", fontsize=14)
                ax2 = plt.gca()

                class Index(object):
                    t.x = wave[0]
                    t.y = 0

                    def update(self, newx, newy):
                        lvline.set_xdata(x=newx)
                        new_idx = int(myf.find_nearest(wave, newx)[0])
                        l1.set_xdata(diff[:, new_idx])
                        min_max = np.max(diff[:, new_idx]) - np.min(diff[:, new_idx])
                        ax2.set_xlim(
                            np.min(diff[:, new_idx]) - min_max * 0.1,
                            np.max(diff[:, new_idx]) + min_max * 0.1,
                        )
                        plt.draw()
                        fig.canvas.draw_idle()

                t = Index()

                def onclick(event):
                    if event.dblclick:
                        t.update(event.xdata, event.ydata)

                fig.canvas.mpl_connect("button_press_event", onclick)

            plt.figure()
            plt.subplot(1, 2, 1)
            plt.scatter(
                flux[snr.argmax()],
                r_corr.smoothed.y,
                color="k",
                alpha=0.1,
                cmap="jet",
                vmin=4000,
                vmax=6000,
            )
            plt.axhline(y=0, color="r")
            plt.xlabel("Flux/Continuum", fontsize=14)
            plt.ylabel("R", fontsize=14)
            ax = plt.gca()
            plt.subplot(1, 2, 2, sharex=ax)
            plt.scatter(
                flux[snr.argmax()],
                r_slope.smoothed.y,
                color="k",
                alpha=0.1,
                cmap="jet",
                vmin=4000,
                vmax=6000,
            )
            plt.axhline(y=0, color="r")
            plt.xlabel("Flux/Continuum", fontsize=14)
            plt.ylabel("S", fontsize=14)
            plt.show()

        self.depth_correlation = np.array(
            [flux[snr.argmax()], wave, r_corr.smoothed.y, r_slope.smoothed.y]
        )

    def kitcat_rm_bands(self, mask):
        """Mask must be either a boolean array of wavelenbgth size or a string of material"""
        self.import_material()
        wave = np.array(self.material["wave"])

        if type(mask) == str:
            mask = np.array(self.material)[mask]

        for i in ["", "cleaned_"]:
            self.import_kitcat(clean=bool(i))
            kitcat_wave = np.array(self.kitcat["catalogue"]["wave"])
            match = myf.find_nearest(wave, kitcat_wave)[0]

            kept = np.array(1 - mask[match]).astype("bool")

            self.kitcat["catalogue"] = self.kitcat["catalogue"].loc[np.arange(len(kept))[kept]]
            self.kitcat["catalogue"] = self.kitcat["catalogue"].reset_index(drop=True)

            myf.pickle_dump(
                self.kitcat,
                open(self.dir_root + "KITCAT/kitcat_" + i + "mask_" + self.starname + ".p"),
            )

    def kitcat_match_spectrum(self, windows="large", clean=False):
        self.import_kitcat(clean=clean)
        self.import_material()
        pixels = self.yarara_get_pixels()
        orders = self.yarara_get_orders()

        load = self.material

        kitcat = self.kitcat["catalogue"]
        wave = load["wave"]

        centers = np.array(kitcat["wave"])

        if windows == "large":
            borders = np.array(kitcat[["wave_left", "wave_right"]])
        else:
            borders = np.array(
                [kitcat["wave"] - kitcat["win_mic"], kitcat["wave"] + kitcat["win_mic"]]
            )

        left = myf.find_nearest(np.array(wave), borders[:, 0])[0]
        right = myf.find_nearest(np.array(wave), borders[:, 1])[0]
        center = myf.find_nearest(np.array(wave), centers)[0]

        vec = np.zeros(len(wave)) * np.nan
        for j in range(len(left)):
            r = right[j] + 1
            l = left[j]
            vec[l:r] = j

        load["kitcat_vec"] = vec

        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

        # pixels_line = []
        # orders_line =[]
        # for l in np.arange(0,np.nanmax(vec)+1):
        #     pixels_line.append(np.mean(pixels[np.array(vec)==l],axis=0))
        #     orders_line.append(np.mean(orders[np.array(vec)==l],axis=0))
        # pixels_line = np.array(pixels_line).astype('int')
        # orders_line=np.round(np.array(orders_line),0)

        pixels_line = np.array(pixels[center]).astype("int")
        orders_line = np.array(orders[center]).astype("int")

        nb_dim = np.shape(pixels_line)[1]

        pix = np.zeros(np.shape(pixels_line))
        ords = np.zeros(np.shape(orders_line))
        for l in np.arange(0, len(pixels_line)):
            ordering = np.random.choice(np.arange(0, nb_dim), nb_dim, replace=False)
            pix[l] = pixels_line[l][ordering]
            ords[l] = orders_line[l][ordering]

        pix = pix.astype("int")
        ords = ords.astype("int")

        for n in range(np.shape(pixels_line)[1]):
            kitcat["pixels_l" + str(n + 1)] = pix[:, n]
            kitcat["orders_l" + str(n + 1)] = ords[:, n]

        kitcat["qc_telluric"] = (kitcat["rel_contam"] == 0).astype("int")
        for contam in ["stitching", "ghost_a", "ghost_b", "thar", "borders_pxl", "merged"]:
            try:
                mask = np.array(load[contam]).astype("bool")
                index = np.unique(np.array(load.loc[mask, "kitcat_vec"].dropna()).astype("int"))
                kitcat["qc_" + contam] = 1
                kitcat.loc[index, "qc_" + contam] = 0
            except:
                print("Contamination unfounded : ", contam)
        try:
            for n in np.unique(load["detector"]):
                mask = np.array(load["detector"]) == n
                index = np.unique(np.array(load.loc[mask, "kitcat_vec"].dropna()).astype("int"))
                kitcat.loc[index, "qc_detector"] = n + 1
        except:
            pass

        kitcat["valid"] = kitcat["qc_borders_pxl"] == 1  # reject lines on the borders

        self.kitcat["catalogue"] = kitcat

        if clean:
            myf.pickle_dump(
                self.kitcat,
                open(self.dir_root + "KITCAT/kitcat_cleaned_mask_" + self.starname + ".p", "wb"),
            )
        else:
            myf.pickle_dump(
                self.kitcat,
                open(self.dir_root + "KITCAT/kitcat_mask_" + self.starname + ".p", "wb"),
            )

    def kitcat_match_property(self, kw, clean=False):
        self.import_material()
        self.import_kitcat(clean=clean)
        kitcat_vec = self.material["kitcat_vec"]
        table = self.kitcat["catalogue"].copy()
        if kw in list(table.keys()):
            vec = table.reindex(kitcat_vec)[kw]
            return np.array(vec)
        else:
            print(
                "[ERROR] %s is not a column of your table, choose among : \n\n" % (kw),
                list(table.keys()),
            )

    def kitcat_statistic_telluric(
        self, clean=False, telluric_ext="", method="linear", mask_col="weight_rv"
    ):

        plot_model = 0
        if root == "/Users/cretignier/Documents":
            plot_model = 1

        self.import_kitcat(clean=clean)
        if plot_model:
            self.import_telluric(ext=telluric_ext, method=method)
            self.import_telluric_water(ext=telluric_ext, method=method)
            self.import_telluric_oxygen(method=method)

            water_model = self.water_model
            oxy_model = self.oxy_model

            water_model.y = 1 - water_model.y
            oxy_model.y = 1 - oxy_model.y

        spectrum = myc.tableXY(self.kitcat["spectre"]["wave"], self.kitcat["spectre"]["flux"])
        kitcat = self.kitcat["catalogue"]

        kitcat[mask_col] = kitcat[mask_col] ** 2

        max_contam = np.max(kitcat["rel_contam"])
        max_contam = 0.15
        contam = np.linspace(0, max_contam, 50)
        mask_contam = (np.array(kitcat["rel_contam"]) - contam[:, np.newaxis]) <= 0
        curve = np.sum(mask_contam, axis=1)

        curve2 = np.array([sum(np.array(kitcat[mask_col])[j]) for j in mask_contam])
        curve2 /= np.sum(np.array(kitcat[mask_col]))

        coeff = np.median(kitcat[mask_col]) / np.median(kitcat["depth_rel"])

        plt.figure(figsize=(18, 9))
        plt.subplot(2 + plot_model, 1, 1)
        plt.title("%s" % (self.starname), fontsize=14)
        spectrum.plot(color="k", ls="-", alpha=0.2, label="spectrum")
        plt.scatter(
            kitcat["wave"],
            kitcat[mask_col] / coeff,
            c=kitcat["rel_contam"],
            vmax=0.1,
            cmap="jet",
            zorder=100,
            label="weights",
        )
        plt.legend(loc=2)
        plt.xlim(np.min(kitcat["wave"]), np.max(kitcat["wave"]))
        plt.ylim(-0.1, 1.2)
        ax = plt.gca()
        plt.xlabel(r"Wavelengh [$\AA$]", fontsize=14)
        plt.ylabel(r"Normalised flux", fontsize=14)
        if plot_model:
            plt.subplot(2 + plot_model, 1, 2, sharex=ax)
            water_model.plot(color="b", ls="-")
            oxy_model.plot(color="r", ls="-")
            plt.xlabel(r"Wavelengh [$\AA$]", fontsize=14)
            plt.ylabel(r"Normalised flux", fontsize=14)
        plt.subplot(2 + plot_model, 3, 3 * (1 + plot_model) + 3)
        for j in range(len(contam) // 5):
            cval = contam[::5][j]
            tab = myf.bands_binning(
                np.arange(3900, 10000, 400),
                np.array(kitcat["wave"])[mask_contam[5 * j]],
                np.ones(sum(mask_contam[5 * j])),
                binning="median",
            )
            plt.plot(tab["x"], tab["nb"], label=cval, marker=".")
        plt.xlabel(r"Bands wavelengh [$\AA$]", fontsize=14)
        plt.ylabel(r"Nb lines in 400 $\AA$ bands", fontsize=14)
        plt.yscale("log")
        plt.subplot(2 + plot_model, 3, 3 * (1 + plot_model) + 2)
        plt.plot(contam, curve, color="k")
        for j in range(len(contam) // 5):
            plt.scatter(contam[::5][j], curve[::5][j], zorder=10)
        plt.grid()
        plt.xlabel("Telluric rel contam", fontsize=14)
        plt.ylabel("Nb lines (cumulative)\n(tot=%.0f)" % (len(kitcat)), fontsize=14)
        plt.subplot(2 + plot_model, 3, 3 * (1 + plot_model) + 1)
        plt.plot(contam, curve2, color="k")
        for j in range(len(contam) // 5):
            plt.scatter(contam[::5][j], curve2[::5][j], zorder=10)
        plt.grid()
        plt.xlabel("Telluric rel contam", fontsize=14)
        plt.ylabel(
            "Weight normed (cumulative)\n(weight=1 -> %.0f)"
            % (np.sum(np.array(kitcat[mask_col]))),
            fontsize=14,
        )
        plt.subplots_adjust(left=0.10, right=0.95, top=0.95, bottom=0.10, hspace=0.3, wspace=0.3)

        plt.savefig(
            self.dir_root
            + "KITCAT/kitcat"
            + ["", "_cleaned"][clean]
            + "_telluric_investigation.png"
        )

    def kitcat_statistic_lbl(
        self,
        clean=False,
        sub_dico="matching_cosmics",
        method="linear",
        mask_col="weight_rv",
        telluric_tresh=1,
        produce_mask=None,
        element=None,
    ):

        self.import_kitcat(clean=clean)
        self.import_lbl_iter()
        self.import_star_info()
        self.import_material()

        spectrum = myc.tableXY(self.kitcat["spectre"]["wave"], self.kitcat["spectre"]["flux"])
        kitcat = self.kitcat["catalogue"]
        lbl = self.lbl_iter[sub_dico]["lbl"][0]
        lbl_cat = self.lbl_iter[sub_dico]["catalog"]

        tel_contam = [np.array(kitcat["wave"]), np.array(kitcat["rel_contam"])]

        kitcat = kitcat.loc[kitcat["rel_contam"] <= telluric_tresh].reset_index(drop=True)

        if element is not None:
            kitcat = kitcat.dropna(subset=["element"]).reset_index(drop=True)
            kitcat = kitcat.loc[kitcat["element"].str[0 : len(element)] == element].reset_index(
                drop=True
            )
            element = "_" + element
        else:
            element = ""

        kitcat[mask_col] = kitcat[mask_col] ** 2

        if len(lbl) != len(kitcat):
            match = myf.match_nearest(np.array(kitcat["wave"]), np.array(lbl_cat["wave"]))[
                :, 0:2
            ].astype("int")
            lbl_cat = lbl_cat.loc[match[:, 1]]

        lbl_iq = np.log10(np.array(lbl_cat["rms"]))

        max_contam = np.nanpercentile(lbl_iq, 75) + 1.5 * myf.IQ(lbl_iq)
        min_contam = np.nanpercentile(lbl_iq, 25) - 1.5 * myf.IQ(lbl_iq)
        contam = np.linspace(min_contam, max_contam, 50)
        mask_contam = (lbl_iq - contam[:, np.newaxis]) <= 0
        curve = np.sum(mask_contam, axis=1)

        curve2 = np.array([sum(np.array(kitcat[mask_col])[j]) for j in mask_contam])
        curve2 /= np.sum(np.array(kitcat[mask_col]))

        coeff = np.median(kitcat[mask_col]) / np.median(kitcat["depth_rel"])

        plt.figure(figsize=(18, 9))
        plt.subplot(2, 1, 1)
        plt.title("%s" % (self.starname), fontsize=14)
        spectrum.plot(color="k", ls="-", alpha=0.2, label="spectrum")
        plt.scatter(
            kitcat["wave"],
            kitcat[mask_col] / coeff,
            c=kitcat["rel_contam"],
            vmax=0.1,
            cmap="jet",
            zorder=100,
            label="weights",
        )
        plt.legend(loc=2)
        plt.xlim(np.min(kitcat["wave"]), np.max(kitcat["wave"]))
        plt.ylim(-0.1, 1.2)
        ax = plt.gca()
        plt.xlabel(r"Wavelengh [$\AA$]", fontsize=14)
        plt.ylabel(r"Normalised flux", fontsize=14)
        plt.subplot(2, 3, 6)
        for j in range(len(contam) // 5):
            cval = contam[::5][j]
            tab = myf.bands_binning(
                np.arange(3900, 10000, 400),
                np.array(kitcat["wave"])[mask_contam[5 * j]],
                np.ones(sum(mask_contam[5 * j])),
                binning="median",
            )
            plt.plot(tab["x"], tab["nb"], label=cval, marker=".")
        plt.xlabel(r"Bands wavelengh [$\AA$]", fontsize=14)
        plt.ylabel(r"Nb lines in 400 $\AA$ bands", fontsize=14)
        plt.yscale("log")
        plt.subplot(2, 3, 5)
        plt.plot(10 ** (contam), curve, color="k")
        for j in range(len(contam) // 5):
            plt.scatter(10 ** (contam[::5][j]), curve[::5][j], zorder=10)
        plt.grid()
        plt.xlabel("RV rms", fontsize=14)
        plt.ylabel("Nb lines (cumulative)\n(tot=%.0f)" % (len(kitcat)), fontsize=14)
        plt.xscale("log")
        plt.subplot(2, 3, 4)
        plt.title("Rel contam < %.0f %%" % (telluric_tresh * 100))
        plt.plot(10 ** (contam), curve2, color="k")
        for j in range(len(contam) // 5):
            plt.scatter(10 ** (contam[::5][j]), curve2[::5][j], zorder=10)
        plt.grid()
        plt.xscale("log")
        plt.xlabel("RV rms", fontsize=14)
        plt.ylabel(
            "Weight normed (cumulative)\n(weight=1 -> %.0f)"
            % (np.sum(np.array(kitcat[mask_col]))),
            fontsize=14,
        )
        plt.subplots_adjust(left=0.10, right=0.95, top=0.95, bottom=0.10, hspace=0.3, wspace=0.3)

        plt.savefig(
            self.dir_root + "KITCAT/kitcat" + ["", "_cleaned"][clean] + "_rv_rms_investigation.png"
        )

        if produce_mask is not None:

            load = self.material
            wave_raw_kitcat = np.array(
                pd.read_pickle(glob.glob(self.dir_root + "KITCAT/intermediate*")[0])["catalogue"][
                    "wave"
                ]
            )

            rv_treshold = 0
            if produce_mask:

                if produce_mask >= 1:
                    rv_treshold = produce_mask
                    xhline1 = curve2[myf.find_nearest(10 ** (contam), rv_treshold)[0]]
                else:  # reject X % of higher rms lines
                    rv_treshold = int(
                        10 ** (contam)[myf.find_nearest(curve2, 1 - produce_mask)[0]]
                    )
                    xhline1 = 1 - produce_mask

                print(" [INFO] Lines with RV rms higher than %.0f m/s rejected" % (rv_treshold))
                lbl_cat = lbl_cat.loc[lbl_cat["rms"] < rv_treshold]

                plt.axvline(x=rv_treshold, color="b", alpha=0.5)
                plt.axhline(y=xhline1, color="b", alpha=0.5)

                plt.savefig(
                    self.dir_root
                    + "KITCAT/kitcat"
                    + ["", "_cleaned"][clean]
                    + "_rv_rms_investigation.png"
                )

            rv_sys = self.star_info["Rv_sys"]["fixed"]

            lbl_cat["weight_rv"] = lbl_cat[mask_col] ** 2  # ESPRESSO DRS convention

            wave = myf.doppler_r(np.array(lbl_cat["wave"]), rv_sys * 1000)[1]
            wave_srf = myf.doppler_r(np.array(lbl_cat["wave"]), rv_sys * 0)[1]

            weight = np.array(lbl_cat[mask_col]) ** (0.5)
            mask = np.array([wave, wave, weight]).T
            mask_srf = np.array([wave_srf, wave_srf, weight]).T

            plt.figure(figsize=(18, 5))
            plt.plot(load["wave"], load["reference_spectrum"], color="k")
            plt.plot(
                self.kitcat["spectre"]["wave"], self.kitcat["spectre"]["flux_telluric"], color="r"
            )

            for l, t in zip(tel_contam[0], tel_contam[1]):
                plt.text(l, -0.1, "%.0f" % (t * 100), ha="center")

            for j in wave_raw_kitcat:
                plt.plot([j, j], [0, 1], color="k", alpha=0.4, ls=":")

            for j in lbl_cat["wave"]:
                plt.plot([j, j], [0, 1.2], color="b", alpha=0.4)
            plt.plot([j, j], [0, 1.2], color="b", alpha=0.4, label="KitCat")

            self.import_mask_harps(stellar_frame=True)

            for j in self.ccf_mask_harps[:, 0]:
                plt.plot([j, j], [-0.2, 1], color="g", alpha=0.4)
            plt.plot([j, j], [-0.2, 1], color="g", alpha=0.4, label=self.mask_harps)
            plt.legend()
            plt.axhline(y=0, color="r", alpha=0.5)

            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
            plt.ylabel(r"Flux normalised", fontsize=14)

            plt.subplots_adjust(left=0.06, right=0.97, top=0.97, bottom=0.17)

            np.savetxt(
                self.dir_root
                + "KITCAT/Mask_KitCat_%s_%s_t%.0f_rv%.0f_%s%s.txt"
                % (
                    self.starname,
                    self.instrument,
                    telluric_tresh * 100,
                    int(rv_treshold),
                    mask_col,
                    element,
                ),
                mask,
                fmt="%.6f",
            )
            np.savetxt(
                self.dir_root
                + "KITCAT/Mask_KitCat_%s_%s_t%.0f_rv%.0f_%s%s_SRF.txt"
                % (
                    self.starname,
                    self.instrument,
                    telluric_tresh * 100,
                    int(rv_treshold),
                    mask_col,
                    element,
                ),
                mask_srf,
                fmt="%.6f",
            )

            print("\n [INFO] Nb lines in the mask : %.0f" % (len(mask)))
            print("\n [INFO] Weight of the mask : %.0f" % (sum(mask[:, 2] ** 2)))

    def kitcat_produce_generic_mask(
        self, telluric_tresh=0.03, weight_col="weight_rv", clean=False, rv_sys=None
    ):
        self.import_kitcat(clean=clean)
        self.import_star_info()
        self.import_table()

        kitcat = self.kitcat["catalogue"]
        stellar_type = self.star_info["Sp_type"]["fixed"]
        teff = int(self.star_info["Teff"]["Gray"])
        if rv_sys is None:
            rv_sys1 = self.star_info["Rv_sys"]["fixed"]
            rv_sys2 = np.round(np.median(self.table.rv_dace) / 1000, 2)
            if abs(rv_sys2 - rv_sys1) > 1.5:
                print(
                    Fore.YELLOW
                    + " [WARNING] There is a conflict between the DACE RV_sys and the star_info RV_sys"
                    + Fore.RESET
                )
                print(" [INFO] RV_dace = %.2f / RV_found = %.2f" % (rv_sys2, rv_sys1))
            else:
                rv_sys = rv_sys1
                print(" [INFO] RV_sys detected as : %.2f kms" % (rv_sys))

        if rv_sys is not None:
            kitcat = kitcat.loc[kitcat["rel_contam"] < telluric_tresh]
            print(" [INFO] Sp type detected as : %s" % (stellar_type))
            print(" [INFO] Teff computed as : %s K" % (teff))
            print(" [INFO] NB lines in the formed mask : %.0f" % (len(kitcat)))
            wave = np.array(kitcat["wave"])
            weight = np.array(kitcat[weight_col])
            wave = myf.doppler_r(wave, rv_sys * 1000)[1]
            self.debug = wave
            mask = np.array([wave, wave, weight]).T
            np.savetxt(
                self.dir_root
                + "KITCAT/kitcat"
                + ["", "_cleaned"][clean]
                + "_"
                + self.starname
                + "_"
                + stellar_type
                + ".txt",
                mask,
                fmt=["%.6f", "%.6f", "%6f"],
            )
            print(
                " [INFO] Generic mask saved under : ",
                self.dir_root
                + "KITCAT/Kitcat"
                + ["", "_cleaned"][clean]
                + "_"
                + self.starname
                + "_"
                + stellar_type
                + ".txt",
            )

    def yarara_kitcat_plot(
        self, sub_dico="matching_diff", wave_min=4435, wave_max=4445, nb_bins=30
    ):
        self.import_kitcat()
        self.import_ccf()
        self.import_star_info()
        rv_sys = self.star_info["Rv_sys"]["fixed"]

        kitcat_spectrum = self.kitcat["spectre"]
        kitcat = self.kitcat["catalogue"]
        mask = np.genfromtxt(root + "/Python/MASK_CCF/" + self.mask_harps + ".txt")

        mask = mask[mask[:, 0] > np.min(kitcat_spectrum["wave"])]
        mask = mask[mask[:, 0] < np.max(kitcat_spectrum["wave"])]

        nb_lines = len(mask)
        mask[:, 0] = myf.doppler_r(mask[:, 0], rv_sys * 1000)[0]
        mask[:, 1] = myf.doppler_r(mask[:, 1], rv_sys * 1000)[0]
        mask_wave = 0.5 * (mask[:, 0] + mask[:, 1])

        min_wave = np.min(np.array(kitcat_spectrum["wave"]))
        max_wave = np.max(np.array(kitcat_spectrum["wave"]))

        sub_kitcat = kitcat.loc[
            (kitcat["wave"] > wave_min - 10) & (kitcat["wave"] < wave_max + 10)
        ]
        mask = mask[(mask[:, 0] > wave_min - 10) & (mask[:, 0] < wave_max + 10)]

        wave_harps = 0.5 * (mask[:, 0] + mask[:, 1])

        phot_harps = 1000 * np.array(
            self.table_ccf["CCF_" + self.mask_harps][sub_dico]["table"]["rv_std_phot"]
        )
        phot_kitcat = 1000 * np.array(
            self.table_ccf["CCF_kitcat_mask_" + self.starname][sub_dico]["table"]["rv_std_phot"]
        )

        fac = np.median(phot_harps / phot_kitcat)

        plt.figure(figsize=(20, 6))
        plt.axes([0.05, 0.52, 0.65, 0.4])
        plt.title(
            "Comparison of CCF masks (HARPS versus Kit-Cat) : %.0f versus %.0f"
            % (nb_lines, len(kitcat["wave"])),
            fontsize=14,
        )
        plt.axhline(y=1, color="k", alpha=0.3, ls=":")
        ax = plt.gca()
        plt.plot(
            np.array(kitcat_spectrum["wave"]),
            np.array(kitcat_spectrum["flux"] * (1 - 0 * kitcat_spectrum["correction_factor"])),
            color="k",
        )
        plt.tick_params(top=True, bottom=True, direction="in")
        plt.ylabel("Flux normalised", fontsize=14)
        for j in range(len(wave_harps)):
            plt.axvline(
                x=wave_harps[j], color="b", alpha=0.8, label="mask HARPS (%s)" % (self.mask_harps)
            )
            if not j:
                plt.legend(loc=2)

        plt.axes([0.05, 0.12, 0.65, 0.4], sharex=ax, sharey=ax)
        plt.axhline(y=1, color="k", alpha=0.3, ls=":")
        plt.plot(
            np.array(kitcat_spectrum["wave"]),
            np.array(kitcat_spectrum["flux"] * (1 - 0 * kitcat_spectrum["correction_factor"])),
            color="k",
        )
        plt.tick_params(top=True, bottom=True, direction="in")
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylabel("Flux normalised", fontsize=14)
        for i, j in enumerate(sub_kitcat.index):
            plt.axvline(x=sub_kitcat.loc[j, "wave"], color="r", alpha=0.8, label="mask Kit-Cat")
            plt.scatter(
                sub_kitcat.loc[j, "wave_left"], sub_kitcat.loc[j, "flux_left"], color="g", s=25
            )
            plt.scatter(
                sub_kitcat.loc[j, "wave_right"], sub_kitcat.loc[j, "flux_right"], color="g", s=25
            )
            plt.scatter(sub_kitcat.loc[j, "wave"], sub_kitcat.loc[j, "flux"], color="r", s=25)
            if not i:
                plt.legend(loc=2)

        plt.xlim(wave_min - 0.25, wave_max + 0.25)
        plt.ylim(-0.05, 1.19)

        plt.axes([0.75, 0.12, 0.23, 0.8])
        plt.title(
            "$\sigma^{\gamma}_{HARPS}$ / $\sigma^{\gamma}_{KitCat}$ = %.1f %% (x %.2f)"
            % (fac * 100, 1 / fac),
            fontsize=14,
        )
        plt.axvline(x=0.5 * (wave_min + wave_max), color="k", ls=":")
        plt.hist(mask_wave, np.linspace(min_wave, max_wave, nb_bins), color="b", alpha=0.5)
        plt.hist(kitcat["wave"], np.linspace(min_wave, max_wave, nb_bins), color="r", alpha=0.5)
        plt.hist(
            mask_wave,
            np.linspace(min_wave, max_wave, nb_bins),
            color="k",
            alpha=0.5,
            histtype="step",
        )
        plt.hist(
            kitcat["wave"],
            np.linspace(min_wave, max_wave, nb_bins),
            color="k",
            alpha=0.5,
            histtype="step",
        )
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylabel("Nb lines", fontsize=14)

        plt.savefig(self.dir_root + "IMAGES/KitCat_CCF_mask.pdf")

    def yarara_dbd(
        self,
        kitcat=None,
        continuum="linear",
        calib_std=1e-3,
        width_rv=2,
        power=2,
        debug=True,
        substract_map=[],
        all_dico=["matching_mad", "matching_activity", "matching_pca", "matching_diff"],
    ):
        """
        col = 0  -> DBD depth
        col = 1  -> DBD center
        col = 2  -> DBD center - CCF (planetary free)
        """

        planet = self.planet

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        self.import_table()
        self.import_ccf()
        self.import_material()
        self.import_dico_chain(last_dico="matching_mad")
        self.import_lbl()
        if all_dico is None:
            all_dico = ["matching_mad", "matching_activity", "matching_pca", "matching_diff"]
        all_dico = np.array(all_dico)[np.in1d(np.array(all_dico), self.dico_chain)]

        if not os.path.exists(self.directory + "Analyse_depth_by_depth.p"):
            self.dbd = {}
        else:
            self.import_dbd()

        c_lum = 299.792e6

        if kitcat is None:
            kitcat = glob.glob(self.dir_root + "KITCAT/kitcat_mask*.p")[0]
        kitcat_file = pd.read_pickle(kitcat)
        kitcat_dico = kitcat_file["catalogue"]
        kitcat_name = kitcat.split("/")[-1].split(".")[0]

        load = self.material
        jdb = self.table["jdb"]
        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        files = np.array(self.table["filename"])  # updated 29.10.21 to allow ins_merged
        files = np.sort(files)

        file_test = self.import_spectrum()
        grid = file_test["wave"]
        idx_center = myf.find_nearest(grid, np.array(kitcat_dico["wave_fitted"]))[0]

        for sub_dico in all_dico:

            kitcat_dico["valid"] = True
            kitcat_dico["valid"] = self.lbl[sub_dico]["catalog"]["valid"]
            kitcat_dico["weight"] = self.lbl[sub_dico]["catalog"]["weight"]
            ccf_rv = np.array(self.table_ccf["LBL_" + kitcat_name][sub_dico]["table"]["rv"] * 1000)
            ccf_rv_std = np.array(
                self.table_ccf["LBL_" + kitcat_name][sub_dico]["table"]["rv_std"] * 1000
            )

            print("\n ------ \n Dico %s analysing \n ------ \n" % (sub_dico))

            fluxes = []
            fluxes_err = []
            conti = []
            conti_err = []

            for i, j in enumerate(files):
                file = pd.read_pickle(j)
                fluxes.append(file["flux" + kw])
                fluxes_err.append(file["flux_err"])
                conti.append(file[sub_dico]["continuum_" + continuum])
                conti_err.append(file["continuum_err"])

            coordinates = np.zeros((len(fluxes), len(idx_center), 6))

            if power < 2:
                power = 2

            conti = np.array(conti)
            flux = np.array(fluxes)
            conti_err = np.array(conti_err)
            flux_err = np.array(fluxes_err)

            fluxes_norm, fluxes_norm_std = myf.flux_norm_std(flux, flux_err, conti, conti_err)
            fluxes_norm = np.array(fluxes_norm)
            fluxes_norm_std = np.sqrt(np.array(fluxes_norm_std) ** 2 + calib_std**2)

            fluxes_norm *= np.array(load["correction_factor"])
            fluxes_norm_std *= np.array(load["correction_factor"])

            for maps in substract_map:
                fluxes_norm = self.yarara_substract_map(fluxes_norm, maps, correction_factor=True)

            widths = (width_rv / (np.gradient(grid) / grid * c_lum / 1000)).astype("int")

            loop = 0
            counter = 0
            for j in tqdm(idx_center):
                wave_fitted = np.array(kitcat_dico["wave_fitted"])[loop]
                windows = widths[j]
                if windows < 2:
                    windows = 2
                spectrum_line = fluxes_norm[:, j - windows : j + windows + 1]
                spectrum_line_std = fluxes_norm_std[:, j - windows : j + windows + 1]

                if np.min(spectrum_line) > 0:
                    grid_line = grid[j - windows : j + windows + 1]
                    shift = np.mean(grid_line)
                    grid_line = grid_line - shift

                    line = myc.table(spectrum_line)
                    base_vec = np.vstack([grid_line**p for p in np.arange(power + 1)])
                    self.debug = base_vec, spectrum_line_std, line
                    line.fit_base(base_vec, weight=1 / spectrum_line_std**2)

                    std_depth = np.min(spectrum_line_std, axis=1)

                    if power == 2:
                        center = -0.5 * line.coeff_fitted[:, 1] / line.coeff_fitted[:, 2]
                        depth = np.polyval(line.coeff_fitted.T[::-1, :], center)
                        center = center + shift

                    else:
                        grid_line = np.linspace(grid_line.min(), grid_line.max(), 1000)
                        base_vec_overfitted = np.vstack(
                            [grid_line**p for p in np.arange(power + 1)]
                        )
                        matrix = np.dot(line.coeff_fitted, base_vec_overfitted)
                        depth = np.min(matrix, axis=1)
                        center = grid_line[np.argmin(matrix, axis=1)] + shift
                    rv = (center - wave_fitted) * 3e8 / wave_fitted
                    rv_std = (
                        np.ones(len(jdb)) * (np.percentile(rv, 84) - np.percentile(rv, 16)) / 8
                    )
                else:
                    rv = np.zeros(len(jdb))
                    depth = np.zeros(len(jdb))
                    std_depth = np.ones(len(jdb))
                    rv_std = np.ones(len(jdb))
                    counter += 1

                coordinates[:, loop, 0] = rv
                coordinates[:, loop, 1] = rv_std
                coordinates[:, loop, 2] = 1 - depth
                coordinates[:, loop, 3] = std_depth
                coordinates[:, loop, 4] = rv - ccf_rv
                coordinates[:, loop, 5] = np.sqrt(rv_std**2 + ccf_rv_std**2)
                loop += 1
            print("Nb lines rejected : %.0f" % (counter))

            self.dbd[sub_dico] = {
                "jdb": jdb,
                "dbd": np.array(
                    [coordinates[:, :, 2].T, coordinates[:, :, 0].T, coordinates[:, :, 4].T]
                ),
                "dbd_std": np.array(
                    [coordinates[:, :, 3].T, coordinates[:, :, 1].T, coordinates[:, :, 5].T]
                ),
                "catalog": kitcat_dico.copy(),
                "mask": kitcat_name,
            }
        myf.pickle_dump(self.dbd, open(self.directory + "Analyse_depth_by_depth.p", "wb"))

    def yarara_wbw(
        self,
        kitcat=None,
        continuum="linear",
        calib_std=1e-3,
        treshold_sym=0.40,
        treshold_asym=0.40,
        substract_map=[],
        add_map=[],
        all_dico=["matching_mad", "matching_activity", "matching_pca", "matching_diff"],
    ):
        """
        col = 0  -> WBW width
        col = 1  -> WBW depth
        col = 2  -> WBW center
        col = 3  -> WBW center - CCF (planetary free)

        """

        self.import_table()
        self.import_ccf()
        self.import_material()
        self.import_dico_chain(last_dico="matching_mad")
        self.import_lbl()
        all_dico = np.array(all_dico)[np.in1d(np.array(all_dico), self.dico_chain)]
        self.wbw = {}

        kw = "_planet" * self.planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        c_lum = 299.792e6

        if kitcat is None:
            kitcat = glob.glob(self.dir_root + "KITCAT/kitcat_mask*.p")[0]
        kitcat_file = pd.read_pickle(kitcat)
        kitcat_dico = kitcat_file["catalogue"]
        kitcat_name = kitcat.split("/")[-1].split(".")[0]

        load = self.material
        jdb = self.table["jdb"]
        directory = self.directory

        files = glob.glob(directory + "RASSI*.p")
        files = np.array(self.table["filename"])  # updated 29.10.21 to allow ins_merged
        files = np.sort(files)

        file_test = self.import_spectrum()
        grid = file_test["wave"]

        idx_left = myf.find_nearest(grid, np.array(kitcat_dico["wave_left"]))[0]
        idx_right = myf.find_nearest(grid, np.array(kitcat_dico["wave_right"]))[0]
        master = np.array(load["reference_spectrum"] * load["correction_factor"])

        print("Determination of optimal windows and offset for gaussian fit")
        time.sleep(1)
        offset = []
        left = []
        right = []
        flux_left = []
        flux_right = []
        sym_depth = []
        idx_left2 = []
        idx_right2 = []
        idx_center2 = []

        fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True, figsize=(16, 12))
        ax1.plot(grid, master, color="gray")
        for j in tqdm(range(len(idx_left))):
            wave_fitted = np.array(kitcat_dico["wave_fitted"])[j]
            grid_line = grid[idx_left[j] : idx_right[j] + 1]
            center = myf.find_nearest(grid_line, wave_fitted)[0]
            idx_center2.append(myf.find_nearest(grid, wave_fitted)[0])

            spectrum_line = master[idx_left[j] : idx_right[j] + 1].copy()
            off = np.min(1 - spectrum_line) - 0.001
            spectrum_line += off
            spectrum_line = 1 - spectrum_line
            spectrum_line /= np.max(spectrum_line)
            spectrum_line_log = np.log(abs(spectrum_line))
            ddf = myc.tableXY(grid_line, np.gradient(np.gradient(spectrum_line_log)))

            val = ddf.y[center]
            ddf.find_max(vicinity=3)
            ddf.index_max = ddf.index_max[
                abs(ddf.y_max - val) > 0.03 / 2
            ]  # to not select local max small close to line center
            ddf.index_max = np.unique(np.hstack([0, ddf.index_max, len(ddf.y) - 1]))
            ddf.index_max = np.setdiff1d(ddf.index_max, center)
            closest = np.argmin(abs(ddf.index_max - center))
            if ddf.index_max[closest] < center:
                l = ddf.index_max[closest]
                r = ddf.index_max[closest + 1]
            else:
                r = ddf.index_max[closest]
                l = ddf.index_max[closest - 1]

            mask = abs(ddf.y - val) < 0.03
            mask[0:l] = False
            mask[r + 1 :] = False

            val, borders = myf.clustering(mask, 0.5, 1)
            borders_shifted = abs(borders[:, :-1] - center)
            loc = np.where(borders_shifted == np.min(borders_shifted))[0][0]
            mask *= False
            mask[borders[loc, 0] : borders[loc, 1] + 2] = True

            sgrid = grid_line[mask]
            ax1.plot(grid_line, master[idx_left[j] : idx_right[j] + 1], color="k")
            ax1.plot(sgrid, master[idx_left[j] : idx_right[j] + 1][mask], color="r")
            ax2.plot(sgrid, spectrum_line[mask], color="k")
            ax2.axhline(y=treshold_sym, color="r", ls=":")
            ax3.plot(sgrid, spectrum_line_log[mask], color="k")
            ax4.plot(grid_line, np.gradient(np.gradient(spectrum_line_log)), color="k")
            ax4.plot(sgrid, np.gradient(np.gradient(spectrum_line_log))[mask], color="r")
            left.append(np.min(sgrid))
            right.append(np.max(sgrid))
            sym_depth.append(np.max([spectrum_line[mask][0], spectrum_line[mask][-1]]))
            idx_left2.append(myf.find_nearest(grid, np.min(sgrid))[0])
            idx_right2.append(myf.find_nearest(grid, np.max(sgrid))[0])
            flux_left.append(master[idx_left[j] : idx_right[j] + 1][mask][0])
            flux_right.append(master[idx_left[j] : idx_right[j] + 1][mask][-1])
            offset.append(off)

        offset = np.array(offset)
        sym_depth = np.array(sym_depth)
        left = np.array(left)
        right = np.array(right)
        flux_left = np.array(flux_left)
        flux_right = np.array(flux_right)
        idx_left2 = np.ravel(idx_left2)
        idx_right2 = np.ravel(idx_right2)
        idx_center2 = np.ravel(idx_center2)
        len_win_left = idx_center2 - idx_left2
        len_win_right = idx_right2 - idx_center2
        r = np.random.randn(len(idx_left2)) * 0.1
        r2 = np.random.randn(len(idx_left2)) * 0.01
        asym = (len_win_left - len_win_right) / (len_win_left + len_win_right)

        plt.figure(figsize=(10, 10))
        plt.scatter(len_win_left + r, r2 + asym, label="Nb lines : %.0f" % (len(r)), color="k")
        valid_for_fit = (
            (len_win_left >= 3)
            & (len_win_right >= 3)
            & (abs(asym) <= treshold_asym)
            & (sym_depth < treshold_sym)
            & ((len_win_left + len_win_right + 1) != len(jdb))
        )
        print("Number of lines valid for the fit : %.0f" % (sum(valid_for_fit)))
        plt.scatter(
            len_win_left[valid_for_fit] + r[valid_for_fit],
            r2[valid_for_fit] + asym[valid_for_fit],
            label="Nb lines valid : %.0f" % (sum(valid_for_fit)),
            color="r",
        )
        plt.legend()
        plt.xlabel("Len(LW)", fontsize=13)
        plt.ylabel("(Len(LW) - Len(RW))/(Len(LW) + Len(RW))", fontsize=13)

        for k in np.arange(len(valid_for_fit))[valid_for_fit]:
            i1 = int(idx_left2[k])
            i2 = int(idx_right2[k])
            ax1.plot(grid[i1:i2], master[i1:i2], color="b")

        for sub_dico in all_dico:

            kitcat_dico["valid"] = True
            kitcat_dico["valid"] = self.lbl[sub_dico]["catalog"]["valid"]
            inter = (
                (kitcat_dico["valid"]) & (valid_for_fit) & (kitcat_dico["morpho_crit"] == 1)
            )  # &(kitcat_dico['blend_crit']==1))
            if not np.sum(inter):
                inter = (kitcat_dico["valid"]) & (
                    valid_for_fit
                )  # &(kitcat_dico['blend_crit']==1))
            kitcat_dico["valid"] = inter.copy()

            rejection_criterion = np.array(["invalid_forfit"] * len(kitcat_dico))
            rejection_criterion[np.array(kitcat_dico["valid"])] = "valid"

            kitcat_dico["weight"] = self.lbl[sub_dico]["catalog"]["weight"]

            ccf_rv = np.array(self.table_ccf["LBL_" + kitcat_name][sub_dico]["table"]["rv"] * 1000)
            ccf_rv_std = np.array(
                self.table_ccf["LBL_" + kitcat_name][sub_dico]["table"]["rv_std"] * 1000
            )

            print("\n ------ \n Dico %s analysing \n ------ \n" % (sub_dico))

            fluxes = []
            fluxes_err = []
            conti = []
            conti_err = []

            for i, j in enumerate(files):
                file = pd.read_pickle(j)
                fluxes.append(file["flux" + kw])
                fluxes_err.append(file["flux_err"])
                conti.append(file[sub_dico]["continuum_" + continuum])
                conti_err.append(file["continuum_err"])

            coordinates = np.zeros((len(fluxes), len(idx_left2), 8))

            conti = np.array(conti)
            flux = np.array(fluxes)
            conti_err = np.array(conti_err)
            flux_err = np.array(fluxes_err)

            fluxes_norm, fluxes_norm_std = myf.flux_norm_std(flux, flux_err, conti, conti_err)
            fluxes_norm = np.array(fluxes_norm)
            fluxes_norm_std = np.sqrt(np.array(fluxes_norm_std) ** 2 + calib_std**2)

            fluxes_norm *= np.array(load["correction_factor"])
            fluxes_norm_std *= np.array(load["correction_factor"])

            for maps in substract_map:
                fluxes_norm = self.yarara_substract_map(fluxes_norm, maps, correction_factor=True)

            for maps in add_map:
                fluxes_norm = self.yarara_add_map(fluxes_norm, maps, correction_factor=True)

            chi2_log = []
            chi2 = []
            loop = 0
            for j in tqdm(range(len(idx_left2))):
                if np.array(kitcat_dico["valid"])[j] == True:
                    wave_fitted = np.array(kitcat_dico["wave_fitted"])[j]
                    line_profile = fluxes_norm[:, idx_left2[j] : idx_right2[j] + 1]
                    line_profile_std = fluxes_norm_std[:, idx_left2[j] : idx_right2[j] + 1]
                    spectrum_line = np.log(abs(1 - line_profile + offset[j]))
                    spectrum_line_std = abs(
                        fluxes_norm_std[:, idx_left2[j] : idx_right2[j] + 1] / (1 - line_profile)
                    )
                    grid_line = grid[idx_left2[j] : idx_right2[j] + 1]
                    shift = np.mean(grid_line)

                    grid_line = grid_line - shift

                    line = myc.table(spectrum_line)
                    base_vec = np.vstack([grid_line**p for p in np.arange(3)])
                    line.fit_base(base_vec, weight=1 / spectrum_line_std**2)
                    line_profile_fitted = 1 + offset[j] - np.exp(line.vec_fitted)
                    chi2_log.append(line.chi2)
                    chi2.append(
                        np.sum(
                            (line_profile - line_profile_fitted) ** 2 / line_profile_std, axis=1
                        )
                    )
                    std_depth = np.min(spectrum_line_std, axis=1)

                    mu = -0.5 * line.coeff_fitted[:, 1] / line.coeff_fitted[:, 2]
                    summit = np.polyval(line.coeff_fitted.T[::-1, :], mu)
                    center = mu + shift
                    width = np.sqrt(-0.5 / line.coeff_fitted[:, 2])
                    # mu = line.coeff_fitted[:,1] * width**2 #equivalent to center unshifted
                    # amp = np.exp(line.coeff_fitted[:,0] + 0.5*mu**2/width**2)-offset[j]

                    depth = 1 + offset[j] - np.exp(summit)
                    width = width * c_lum / wave_fitted / 1000
                    center = (center - wave_fitted) / wave_fitted * c_lum

                    coordinates[:, loop, 0] = depth
                    coordinates[:, loop, 1] = std_depth
                    coordinates[:, loop, 2] = width
                    coordinates[:, loop, 3] = std_depth
                    coordinates[:, loop, 4] = center
                    coordinates[:, loop, 5] = np.sqrt(ccf_rv_std**2)
                    coordinates[:, loop, 6] = center - ccf_rv
                    coordinates[:, loop, 7] = np.sqrt(ccf_rv_std**2)
                loop += 1

            chi2 = np.array(chi2)
            chi2_log = np.array(chi2_log)

            self.debug = chi2, chi2_log

            valid_chi2 = myf.rm_outliers(np.median(chi2, axis=1), kind="inter", m=2)[0]
            valid_chi2_log = myf.rm_outliers(np.median(chi2_log, axis=1), kind="inter", m=2)[0]
            valid = (valid_chi2) & (valid_chi2_log)
            rejection_criterion[np.array(kitcat_dico["valid"] == True)][~valid] = "invalid_chi2"

            kitcat_dico["rejection_criterion"] = rejection_criterion
            kitcat_dico.loc[kitcat_dico["valid"] == True, "valid"] = valid
            kitcat_dico["valid"] = kitcat_dico["valid"] & (
                np.median(coordinates, axis=0)[:, 2] < 50
            )  # reject width too large
            kitcat_dico.loc[
                np.median(coordinates, axis=0)[:, 2] > 50, "rejection_criterion"
            ] = "invalid_width"
            kitcat_dico["valid"] = kitcat_dico["valid"] & (
                abs(np.median(coordinates, axis=0)[:, 4]) < 100
            )  # reject center too far away
            kitcat_dico.loc[
                abs(np.median(coordinates, axis=0)[:, 4]) > 100, "rejection_criterion"
            ] = "invalid_center"

            v = np.product(np.std(coordinates, axis=0), axis=1) != 0
            kitcat_dico["valid"] = v.copy()
            print("\nNb valid lines : %.0f" % (sum(v)))

            valid_lines = np.array(kitcat_dico["valid"])
            for k in np.arange(len(valid_lines))[valid_lines]:
                i1 = int(idx_left2[k])
                i2 = int(idx_right2[k])
                ax1.plot(grid[i1:i2], master[i1:i2], color="yellow")

            self.wbw[sub_dico] = {
                "jdb": jdb,
                "wbw": np.array(
                    [
                        coordinates[:, :, 2].T,
                        coordinates[:, :, 0].T,
                        coordinates[:, :, 4].T,
                        coordinates[:, :, 6].T,
                    ]
                ),
                "wbw_std": np.array(
                    [
                        coordinates[:, :, 3].T,
                        coordinates[:, :, 1].T,
                        coordinates[:, :, 5].T,
                        coordinates[:, :, 7].T,
                    ]
                ),
                "catalog": kitcat_dico.copy(),
                "mask": kitcat_name,
            }
        myf.pickle_dump(self.wbw, open(self.directory + "Analyse_width_by_width.p", "wb"))

    def yarara_bbb(
        self,
        continuum="linear",
        kitcat=None,
        tresh_continuum=0.9,
        levels=np.arange(10, 71, 10),
        all_dico=["matching_diff", "matching_pca", "matching_activity"],
    ):
        """
        Compute the width according to Stenflo 1987, at several levels in the lines.
        The bissector is shifter from the CCF to concel RV signals.
        Last level will be use to detect blends

        col = 0  -> weighted average
        col = i  -> width at the level j

        """

        directory = self.directory
        self.import_table()
        self.import_material()
        self.import_ccf()
        self.import_lbl()

        self.import_dico_chain(last_dico="matching_mad")
        all_dico = np.array(all_dico)[np.in1d(np.array(all_dico), self.dico_chain)]

        self.wbw = {}
        self.bbb = {}

        if kitcat is None:
            kitcat = glob.glob(self.dir_root + "KITCAT/kitcat_mask*.p")[0]
        kitcat_name = kitcat.split("/")[-1].split(".")[0]
        kitcat_dico = pd.read_pickle(kitcat)["catalogue"]

        mat = self.material
        grid = mat["wave"]

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        catalog = kitcat_dico.copy()
        catalog["dist_continuum"] = np.min(catalog[["flux_left", "flux_right"]], axis=1)

        sub_table = catalog.copy()
        print("\n Number of lines : %.0f" % (len(sub_table)))
        sub_table = sub_table.loc[sub_table["blend_crit"] == 1]
        print(" Number of lines unblended : %.0f" % (len(sub_table)))
        sub_table = sub_table.loc[sub_table["morpho_crit"] == 1]
        print(" Number of lines with good morpho : %.0f" % (len(sub_table)))
        sub_table = sub_table.loc[sub_table["rel_contam"] < 0.01]
        print(" Number of lines with without telluric : %.0f" % (len(sub_table)))
        sub_table = sub_table.loc[sub_table["dist_continuum"] > tresh_continuum]
        percent = (100 - levels[-1] - 15) / 100
        percent = 0.05 * int(percent < 0.05) + percent * int(percent >= 0.05)
        sub_table = sub_table.loc[
            (1 - sub_table["dist_continuum"]) < (percent * sub_table["line_depth"])
        ]
        print(" Number of lines with good pseudo continuum : %.0f" % (len(sub_table)))
        final_table = sub_table[
            [
                "wave",
                "wave_left",
                "wave_right",
                "freq_mask0",
                "line_depth",
                "depth_rel",
                "min_depth",
                "dist_continuum",
                "flux",
                "flux_left",
                "flux_right",
                "equivalent_width",
            ]
        ]

        idx_left = myf.find_nearest(grid, np.array(final_table["wave_left"]))[0]
        idx_right = myf.find_nearest(grid, np.array(final_table["wave_right"]))[0]

        for sub_dico in all_dico:

            rv = np.array(self.table_ccf["LBL_" + kitcat_name][sub_dico]["table"]["rv"] * 1000)

            print("\n ------ \n Dico %s analysing \n ------ \n" % (sub_dico))

            valid_lines = self.lbl[sub_dico]["catalog"]["valid"]
            valid_lines = np.array(valid_lines.loc[final_table.index])

            inter_table = final_table.copy()
            inter_table = inter_table[valid_lines]
            print("Number of valid lines : %.0f" % (len(inter_table)))
            time.sleep(1)

            borders = np.vstack([idx_left[valid_lines], idx_right[valid_lines]]).T

            fluxes = []
            err_fluxes = []

            for j in tqdm(files):
                file = pd.read_pickle(j)
                f = file["flux"]
                f_std = file["flux_err"]
                c = file[sub_dico]["continuum_" + continuum]
                c_std = file["continuum_err"]
                f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
                fluxes.append(f_norm)
                err_fluxes.append(f_norm_std)

            fluxes = np.array(fluxes)
            err_fluxes = np.array(err_fluxes)
            fluxes *= np.array(mat["correction_factor"])
            err_fluxes *= np.array(mat["correction_factor"])

            # extract lines for the analysis

            W = np.zeros((len(levels), len(borders), len(fluxes)))
            B = np.zeros((len(levels), len(borders), len(fluxes)))

            W_std = np.zeros((len(levels), len(borders), len(fluxes)))
            B_std = np.zeros((len(levels), len(borders), len(fluxes)))

            S = np.zeros((len(borders), len(fluxes)))

            for line in tqdm(range(len(borders))):
                for time_t in range(len(fluxes)):

                    profile = myc.tableXY(
                        grid[borders[line, 0] : borders[line, 1] + 1],
                        fluxes[time_t, borders[line, 0] : borders[line, 1] + 1],
                        err_fluxes[time_t, borders[line, 0] : borders[line, 1] + 1],
                    )

                    profile.xerr = (
                        abs(np.gradient(profile.x) / (np.gradient(profile.y) + 1e-6))
                        * profile.yerr
                    )
                    profile.interpolate(new_grid=30, interpolate_x=False)

                    mini_y = profile.y.min()
                    mini_x = profile.y.argmin()
                    profile.y = (profile.y - mini_y) / (1 - mini_y)
                    # profile.x = (profile.x - profile.x[mini_x])/profile.x[mini_x]*3e5

                    left = myc.tableXY(
                        profile.x[0 : mini_x + 1],
                        profile.y[0 : mini_x + 1],
                        profile.xerr[0 : mini_x + 1],
                        profile.yerr[0 : mini_x + 1],
                    )
                    right = myc.tableXY(
                        profile.x[mini_x:],
                        profile.y[mini_x:],
                        profile.xerr[mini_x:],
                        profile.yerr[mini_x:],
                    )
                    left.switch()
                    right.switch()
                    left.interpolate(
                        new_grid=np.arange(0, left.x.max(), 0.01), interpolate_x=False
                    )
                    right.interpolate(
                        new_grid=np.arange(0, right.x.max(), 0.01), interpolate_x=False
                    )

                    k = (mini_y + 1) * 0.5

                    area = np.sum(abs(np.gradient(left.y))[0:50] * left.x[0:50]) + np.sum(
                        abs(np.gradient(right.y))[0:50] * right.x[0:50]
                    )

                    area_conv = ((right.y[50] - left.y[50]) * 0.5 - area) * (k - mini_y)

                    w = (
                        (right.y[levels] - left.y[levels])
                        / np.array(inter_table["freq_mask0"])[line]
                        * 3e5
                    )
                    bis = (
                        (
                            0.5 * (right.y[levels] + left.y[levels])
                            - np.array(inter_table["wave"])[line]
                        )
                        / np.array(inter_table["wave"])[line]
                        * 3e8
                    )
                    sigma = np.sqrt(
                        -(w**2) / (8 * np.log(1 - levels / 100))
                    )  # convert in FWHM for a gaussian profile
                    s = (
                        area_conv * 1e6 / np.array(inter_table["freq_mask0"])[line]
                    )  # fraunhofer units

                    w_std = (
                        np.sqrt(right.yerr[levels] ** 2 + left.yerr[levels] ** 2)
                        / np.array(inter_table["wave"])[line]
                        * 3e5
                    )
                    sigma_std = w_std / np.sqrt(-8 * np.log(1 - levels / 100))
                    bis_std = (
                        0.5
                        * np.sqrt(right.yerr[levels] ** 2 + left.yerr[levels] ** 2)
                        / np.array(inter_table["wave"])[line]
                        * 3e8
                    )

                    S[line, time_t] = s
                    W[:, line, time_t] = sigma
                    B[:, line, time_t] = bis

                    W_std[:, line, time_t] = sigma_std
                    B_std[:, line, time_t] = bis_std

            all_S = np.percentile(S, 5, axis=1)
            all_W = np.percentile(W[len(levels) - 1, :, :], 99, axis=1)
            t = myc.tableXY(all_S, all_W)
            t.binned_scatter(6, Plot=False)
            sig2 = t.biny + 2 * t.binsup[0]
            kept = t.y < sig2[t.binidx]

            B = B - rv  # cancel rv to supress planetary signals

            S *= kept[:, np.newaxis]
            W *= kept[:, np.newaxis]
            B *= kept[:, np.newaxis]

            print("Final number of valid lines : %.0f" % (sum(kept)))

            W_weights = W_std.copy()
            W_weights[W_weights == 0] = np.inf
            W_weights = 1 / W_weights**2

            B_weights = B_std.copy()
            B_weights[B_weights == 0] = np.inf
            B_weights = 1 / B_weights**2

            mean_W = np.sum(W * W_weights, axis=0) / np.sum(W_weights, axis=0)
            mean_W_std = 1 / np.sqrt(np.sum(W_weights, axis=0))

            mean_B = np.sum(B * B_weights, axis=0) / np.sum(B_weights, axis=0)
            mean_B_std = 1 / np.sqrt(np.sum(B_weights, axis=0))

            lines = np.array(inter_table.index)

            Widths = np.zeros((len(levels) + 1, len(kitcat_dico), len(fluxes)))
            Biss = np.zeros((len(levels) + 1, len(kitcat_dico), len(fluxes)))
            Strengths = np.zeros((len(kitcat_dico), len(fluxes)))

            Widths_std = np.zeros((len(levels) + 1, len(kitcat_dico), len(fluxes)))
            Biss_std = np.zeros((len(levels) + 1, len(kitcat_dico), len(fluxes)))

            Widths[0, lines, :] = mean_W
            Widths[1:, lines, :] = W

            Biss[0, lines, :] = mean_B
            Biss[1:, lines, :] = B
            Strengths[lines, :] = S

            Widths_std[0, lines, :] = mean_W_std
            Widths_std[1:, lines, :] = W_std
            Biss_std[0, lines, :] = mean_B_std
            Biss_std[1:, lines, :] = B_std

            valid = np.sum(Widths[0], axis=1) != 0
            kitcat_dico["valid"] = valid

            dico_wbw = {
                "jdb": self.table["jdb"],
                "wbw": Widths,
                "wbw_std": Widths_std,
                "catalog": kitcat_dico,
                "mask": kitcat_name,
                "S": Strengths,
            }
            dico_bbb = {
                "jdb": self.table["jdb"],
                "bbb": Biss,
                "bbb_std": Biss_std,
                "catalog": kitcat_dico,
                "mask": kitcat_name,
                "S": Strengths,
            }

            self.wbw[sub_dico] = dico_wbw
            self.bbb[sub_dico] = dico_bbb

        myf.pickle_dump(self.wbw, open(self.directory + "Analyse_width_by_width.p", "wb"))
        myf.pickle_dump(self.bbb, open(self.directory + "Analyse_bis_by_bis.p", "wb"))

    def yarara_aba(
        self,
        kitcat=None,
        continuum="linear",
        kw_dico="lbl",
        windows="large",
        substract_map=[],
        add_map=[],
        all_dico=None,
        calib_std=1e-3,
        rv_range=100,
    ):

        """
        col = 0  -> L
        col = 1  -> R
        col = 2  -> L - R
        col = 3  -> L + R
        col = 4  -> L - CCF
        col = 5  -> R - CCF
        col = 6  -> L - R - CCF
        col = 7  -> L + R - CCF

        """
        self.import_table()
        self.import_ccf()
        self.import_material()
        self.import_lbl()

        if not os.path.exists(self.directory + "Analyse_asym_by_asym.p"):
            self.aba = {}
        else:
            self.import_aba()

        planet = self.planet

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if type(all_dico) == str:
            all_dico = [all_dico]
        if all_dico is None:
            all_dico = self.lbl.keys()
            self.import_dico_chain("matching_mad")
            all_dico = all_dico[np.in1d(all_dico, self.dico_chain)]

        load = self.material
        jdb = self.table["jdb"]
        directory = self.directory
        file_test = self.import_spectrum()

        if kitcat is None:
            kitcat = glob.glob(self.dir_root + "KITCAT/kitcat_mask*.p")[0]
        kitcat_name = kitcat.split("/")[-1].split(".")[0]
        kitcat_file = pd.read_pickle(kitcat)
        kitcat_dico = kitcat_file["catalogue"]

        rv_max = np.max(
            abs(
                np.array(
                    self.table_ccf["LBL_" + kitcat_name]["matching_diff"]["table"]["rv"] * 1000
                )
            )
        )
        if rv_max > rv_range:
            if False:
                print(
                    Fore.YELLOW
                    + " [WARNING] The RV range of matching_diff is about %.1f m/s which is higher than the calibration curve between +/- %.0f m/s"
                    % (rv_max, rv_range)
                    + Fore.RESET
                )
                myf.make_sound("Warning")
                rv_range = rv_max + 5

        grid = kitcat_file["spectre"]["wave"]

        master_flux = kitcat_file["spectre"]["flux"] * np.array(load["correction_factor"])
        print("Production of the grid for asymmetry criterion")

        dgrid = np.round(file_test["parameters"]["dwave"], 8)

        wave_center = np.array(kitcat_dico["wave_fitted"])
        if windows == "large":
            wave_left = np.array(kitcat_dico["wave_left"])
            wave_right = np.array(kitcat_dico["wave_right"])
        elif windows == "medium":
            wave_left = np.array(kitcat_dico["wave_fitted"]) - np.array(kitcat_dico["win_mic"])
            wave_right = np.array(kitcat_dico["wave_fitted"]) + np.array(kitcat_dico["win_mic"])

        nb_left = ((wave_center - wave_left) / dgrid).astype("int")
        nb_right = ((wave_right - wave_center) / dgrid).astype("int")

        new_grid = []
        for k in np.arange(len(nb_left)):
            new_grid.append(wave_center[k] + dgrid * np.arange(-nb_left[k], nb_right[k] + 1))

        idx_lines = []
        count = 0
        for k in np.arange(len(new_grid)):
            idx_lines.append(np.arange(len(new_grid[k])) + count)
            count += len(new_grid[k])

        idx_lines_left = []
        idx_lines_right = []
        for k in np.arange(len(nb_left)):
            idx_lines_left.append(idx_lines[k][0 : nb_left[k]])
            idx_lines_right.append(idx_lines[k][nb_left[k] + 1 :])

        new_grid = np.hstack(new_grid)

        print("Computation of the Goodness of the lines")

        print(
            "Production of the master library between -%.0f and +%.0f m/s" % (rv_range, rv_range)
        )
        time.sleep(0.5)

        broad_rv_grid = np.arange(-rv_range, rv_range + 0.5, 0.5)
        left_area = []
        right_area = []
        for rv in tqdm(broad_rv_grid):
            master = myc.tableXY(myf.doppler_r(grid, rv)[0], master_flux, 0 * grid)
            master.interpolate(new_grid=new_grid, method="linear", interpolate_x=False)
            left_area.append([np.sum(1 - master.y[j]) for j in idx_lines_left])
            right_area.append([np.sum(1 - master.y[j]) for j in idx_lines_right])
        left_area = np.array(left_area)
        right_area = np.array(right_area)

        fine_rv_grid = np.arange(-rv_range, rv_range + 0.01, 0.01)
        left_area2 = []
        right_area2 = []
        for k in tqdm(range(len(left_area[0]))):
            vec = myc.tableXY(broad_rv_grid, left_area[:, k], 0 * broad_rv_grid)
            vec.interpolate(new_grid=fine_rv_grid, method="cubic", interpolate_x=False)
            left_area2.append(vec.y)
            vec = myc.tableXY(broad_rv_grid, right_area[:, k], 0 * broad_rv_grid)
            vec.interpolate(new_grid=fine_rv_grid, method="cubic", interpolate_x=False)
            right_area2.append(vec.y)
        left_area = np.array(left_area2).T
        right_area = np.array(right_area2).T

        calib_lines = np.array(
            [left_area, right_area, right_area - left_area, right_area + left_area]
        ).T
        # calib = {'calib':calib_lines}
        # myf.pickle_dump(calib,open(self.directory+'Analyse_calib_lines_area.p','wb'))

        plt.figure()
        plt.scatter(
            np.mean(calib_lines, axis=1)[:, 1] - np.mean(calib_lines, axis=1)[:, 0],
            np.mean(calib_lines, axis=1)[:, 1],
        )
        plt.axvline(x=0, label="symetric_lines", color="k")
        plt.xlabel("Right - Left", fontsize=14)
        plt.ylabel("Right", fontsize=14)
        plt.legend()

        files = glob.glob(directory + "RASSI*.p")
        files = np.array(self.table["filename"])  # updated 29.10.21 to allow ins_merged
        files = np.sort(files)

        for sub_dico in all_dico:
            kitcat_dico["valid"] = True
            kitcat_dico["valid"] = self.lbl[sub_dico]["catalog"]["valid"]
            kitcat_dico["weight"] = self.lbl[sub_dico]["catalog"]["weight"]

            ccf_rv = np.array(
                self.table_ccf[kw_dico.upper() + "_" + kitcat_name][sub_dico]["table"]["rv"] * 1000
            )

            if np.sum(abs(ccf_rv) > rv_range):
                print(
                    Fore.YELLOW
                    + "\n[WARNING] The RV is out of the calibration curve range"
                    + Fore.RESET
                )
                myf.make_sound("Warning")

            index_calib = np.round(ccf_rv * 100, 0).astype("int") + int(
                rv_range / 0.01
            )  # only work for the fine rv grid = np.arange(-100,100.01,0.01)
            mask_wrong = abs(ccf_rv) > rv_range
            index_calib[mask_wrong] = 0

            print("\n ------ \n Dico %s analysing \n ------ \n" % (sub_dico))

            flux = []
            conti = []
            flux_err = []
            conti_err = []
            for i, j in enumerate(files):
                file = pd.read_pickle(j)
                flux.append(file["flux" + kw])
                flux_err.append(file["flux_err"])
                conti.append(file[sub_dico]["continuum_" + continuum])
                conti_err.append(file["continuum_err"])

            conti = np.array(conti)
            flux = np.array(flux)
            conti_err = np.array(conti_err)
            flux_err = np.array(flux_err)

            flux_norm, flux_norm_std = myf.flux_norm_std(flux, flux_err, conti, conti_err)
            flux_norm = np.array(flux_norm)
            flux_norm_std = np.sqrt(np.array(flux_norm_std) ** 2 + calib_std**2)

            flux_norm *= np.array(load["correction_factor"])
            flux_norm_std *= np.array(load["correction_factor"])

            for maps in substract_map:
                flux_norm = self.yarara_substract_map(flux_norm, maps, correction_factor=True)

            for maps in add_map:
                flux_norm = self.yarara_add_map(flux_norm, maps, correction_factor=True)

            left_area = []
            right_area = []
            left_area_std = []
            right_area_std = []

            for k in tqdm(np.arange(len(conti))):
                master = myc.tableXY(grid, flux_norm[k], flux_norm_std[k])
                master.interpolate(new_grid=new_grid, method="linear", interpolate_x=False)
                left_area.append([np.sum(1 - master.y[j]) for j in idx_lines_left])
                right_area.append([np.sum(1 - master.y[j]) for j in idx_lines_right])
                left_area_std.append([np.sum(master.yerr[j] ** 2) for j in idx_lines_left])
                right_area_std.append([np.sum(master.yerr[j] ** 2) for j in idx_lines_right])

            left_area = np.array(left_area)
            right_area = np.array(right_area)
            left_area_std = np.sqrt(np.array(left_area_std))
            right_area_std = np.sqrt(np.array(right_area_std))

            aba = np.array(
                [left_area, right_area, right_area - left_area, right_area + left_area]
            ).T
            aba_std = np.array(
                [
                    left_area_std,
                    right_area_std,
                    np.sqrt(right_area_std**2 + left_area_std**2),
                    np.sqrt(right_area_std**2 + left_area_std**2),
                ]
            ).T

            self.debug = (aba, calib_lines)

            aba = np.array(
                [
                    aba[:, :, 0],
                    aba[:, :, 1],
                    aba[:, :, 2],
                    aba[:, :, 3],
                    (aba[:, :, 0] - calib_lines[:, index_calib, 0])
                    * (1 - mask_wrong.astype("int")),
                    (aba[:, :, 1] - calib_lines[:, index_calib, 1])
                    * (1 - mask_wrong.astype("int")),
                    (aba[:, :, 2] - calib_lines[:, index_calib, 2])
                    * (1 - mask_wrong.astype("int")),
                    (aba[:, :, 3] - calib_lines[:, index_calib, 3])
                    * (1 - mask_wrong.astype("int")),
                ]
            )

            aba_std = np.array(
                [
                    aba_std[:, :, 0],
                    aba_std[:, :, 1],
                    aba_std[:, :, 2],
                    aba_std[:, :, 3],
                    aba_std[:, :, 0],
                    aba_std[:, :, 1],
                    aba_std[:, :, 2],
                    aba_std[:, :, 3],
                ]
            )

            # self.aba[sub_dico] = {'jdb':jdb,'aba_ref':calib_lines,'aba':aba,'aba_std':aba_std,'catalog':kitcat_dico}
            self.aba[sub_dico] = {
                "jdb": jdb,
                "aba": aba,
                "aba_std": aba_std,
                "catalog": kitcat_dico.copy(),
                "mask": kitcat_name,
            }

        myf.pickle_dump(self.aba, open(self.directory + "Analyse_asym_by_asym.p", "wb"))

    def yarara_bt(
        self,
        kitcat=None,
        continuum="linear",
        windows="large",
        substract_map=[],
        add_map=[],
        all_dico=None,
        calib_std=1e-3,
        rv_range=100,
    ):

        """
        col = 0  -> T
        col = 1  -> B
        col = 2  -> T - B
        col = 3  -> T + B
        col = 4  -> T - CCF
        col = 5  -> B - CCF
        col = 6  -> T - B - CCF
        col = 7  -> T + B - CCF

        """
        self.import_table()
        self.import_ccf()
        self.import_material()
        self.import_lbl()
        self.bt = {}

        planet = self.planet

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if type(all_dico) == str:
            all_dico = [all_dico]
        if all_dico is None:
            all_dico = self.lbl.keys()
            self.import_dico_chain("matching_mad")
            all_dico = all_dico[np.in1d(all_dico, self.dico_chain)]

        load = self.material
        jdb = self.table["jdb"]
        directory = self.directory

        if kitcat is None:
            kitcat = glob.glob(self.dir_root + "KITCAT/kitcat_mask*.p")[0]
        kitcat_name = kitcat.split("/")[-1].split(".")[0]
        kitcat_file = pd.read_pickle(kitcat)
        kitcat_dico = kitcat_file["catalogue"]

        rv_max = np.max(
            abs(
                np.array(
                    self.table_ccf["LBL_ITER_" + kitcat_name]["matching_diff"]["table"]["rv"]
                    * 1000
                )
            )
        )

        if rv_max > rv_range:
            if False:
                print(
                    Fore.YELLOW
                    + " [WARNING] The RV range of matching_diff is about %.1f m/s which is higher than the calibration curve between +/- %.0f m/s"
                    % (rv_max, rv_range)
                    + Fore.RESET
                )
                myf.make_sound("Warning")
                rv_range = rv_max + 5

        grid = kitcat_file["spectre"]["wave"]

        master_flux = kitcat_file["spectre"]["flux"] * np.array(load["correction_factor"])
        print("Production of the grid for bt criterion")

        half_depth = np.array(kitcat_dico["line_depth"]) / 2

        if windows == "large":
            wave_left = np.array(kitcat_dico["wave_left"])
            wave_right = np.array(kitcat_dico["wave_right"])
        elif windows == "medium":
            wave_left = np.array(kitcat_dico["wave_fitted"]) - np.array(kitcat_dico["win_mic"])
            wave_right = np.array(kitcat_dico["wave_fitted"]) + np.array(kitcat_dico["win_mic"])

        idx_left = myf.find_nearest(grid, wave_left)[0]
        idx_right = myf.find_nearest(grid, wave_right)[0]

        idx_lines = []
        for k in np.arange(len(idx_left)):
            idx_lines.append(np.arange(idx_left[k], idx_right[k] + 1))

        idx_lines_top = []
        idx_lines_bottom = []
        for k in np.arange(len(idx_left)):
            loc_line = idx_lines[k]
            idx_lines_top.append(loc_line[master_flux[loc_line] >= (1 - half_depth[k])])
            idx_lines_bottom.append(loc_line[master_flux[loc_line] < (1 - half_depth[k])])

        print("Computation of the Goodness of the lines")

        print(
            "Production of the master library between -%.0f and +%.0f m/s" % (rv_range, rv_range)
        )
        time.sleep(0.5)

        broad_rv_grid = np.arange(-rv_range, rv_range + 0.5, 0.5)
        top_area = []
        bottom_area = []
        for rv in tqdm(broad_rv_grid):
            master = myc.tableXY(myf.doppler_r(grid, rv)[0], master_flux)
            master.interpolate(new_grid=grid, method="linear", interpolate_x=False)
            top_area.append([np.sum(1 - master.y[j]) for j in idx_lines_top])
            bottom_area.append([np.sum(1 - master.y[j]) for j in idx_lines_bottom])
        top_area = np.array(top_area)
        bottom_area = np.array(bottom_area)

        fine_rv_grid = np.arange(-rv_range, rv_range + 0.01, 0.01)
        top_area2 = []
        bottom_area2 = []
        for k in tqdm(range(len(top_area[0]))):
            vec = myc.tableXY(broad_rv_grid, top_area[:, k], 0 * broad_rv_grid)
            vec.interpolate(new_grid=fine_rv_grid, method="cubic", interpolate_x=False)
            top_area2.append(vec.y)
            vec = myc.tableXY(broad_rv_grid, bottom_area[:, k], 0 * broad_rv_grid)
            vec.interpolate(new_grid=fine_rv_grid, method="cubic", interpolate_x=False)
            bottom_area2.append(vec.y)
        top_area = np.array(top_area2).T
        bottom_area = np.array(bottom_area2).T

        calib_lines = np.array(
            [top_area, bottom_area, top_area - bottom_area, top_area + bottom_area]
        ).T
        # calib = {'calib':calib_lines}
        # myf.pickle_dump(calib,open(self.directory+'Analyse_calib_lines_area.p','wb'))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        for sub_dico in all_dico:
            kitcat_dico["valid"] = True
            kitcat_dico["valid"] = self.lbl[sub_dico]["catalog"]["valid"]
            kitcat_dico["weight"] = self.lbl[sub_dico]["catalog"]["weight"]

            ccf_rv = np.array(self.table_ccf["LBL_" + kitcat_name][sub_dico]["table"]["rv"] * 1000)

            if np.sum(abs(ccf_rv) > rv_range):
                print(
                    Fore.YELLOW
                    + " [WARNING] The RV is out of the calibration curve range"
                    + Fore.RESET
                )
                myf.make_sound("Warning")

            index_calib = np.round(ccf_rv * 100, 0).astype("int") + int(
                rv_range / 0.01
            )  # only work for the fine rv grid = np.arange(-100,100.01,0.01)
            mask_wrong = abs(ccf_rv) > rv_range
            index_calib[mask_wrong] = 0

            print("\n ------ \n Dico %s analysing \n ------ \n" % (sub_dico))

            flux = []
            conti = []
            flux_err = []
            conti_err = []
            for i, j in enumerate(files):
                file = pd.read_pickle(j)
                flux.append(file["flux" + kw])
                flux_err.append(file["flux_err"])
                conti.append(file[sub_dico]["continuum_" + continuum])
                conti_err.append(file["continuum_err"])

            conti = np.array(conti)
            flux = np.array(flux)
            conti_err = np.array(conti_err)
            flux_err = np.array(flux_err)

            flux_norm, flux_norm_std = myf.flux_norm_std(flux, flux_err, conti, conti_err)
            flux_norm = np.array(flux_norm)
            flux_norm_std = np.sqrt(np.array(flux_norm_std) ** 2 + calib_std**2)

            flux_norm *= np.array(load["correction_factor"])
            flux_norm_std *= np.array(load["correction_factor"])

            for maps in substract_map:
                flux_norm = self.yarara_substract_map(flux_norm, maps, correction_factor=True)

            for maps in add_map:
                flux_norm = self.yarara_add_map(flux_norm, maps, correction_factor=True)

            top_area = []
            bottom_area = []
            top_area_std = []
            bottom_area_std = []

            for k in tqdm(np.arange(len(conti))):
                master = myc.tableXY(grid, flux_norm[k], flux_norm_std[k])
                master.interpolate(new_grid=grid, method="linear", interpolate_x=False)
                top_area.append([np.sum(1 - master.y[j]) for j in idx_lines_top])
                bottom_area.append([np.sum(1 - master.y[j]) for j in idx_lines_bottom])
                top_area_std.append([np.sum(master.yerr[j] ** 2) for j in idx_lines_top])
                bottom_area_std.append([np.sum(master.yerr[j] ** 2) for j in idx_lines_bottom])

            top_area = np.array(top_area)
            bottom_area = np.array(bottom_area)
            top_area_std = np.sqrt(np.array(top_area_std))
            bottom_area_std = np.sqrt(np.array(bottom_area_std))

            bt = np.array(
                [top_area, bottom_area, top_area - bottom_area, top_area + bottom_area]
            ).T
            bt_std = np.array(
                [
                    top_area_std,
                    bottom_area_std,
                    np.sqrt(top_area_std**2 + bottom_area_std**2),
                    np.sqrt(top_area_std**2 + bottom_area_std**2),
                ]
            ).T

            bt = np.array(
                [
                    bt[:, :, 0],
                    bt[:, :, 1],
                    bt[:, :, 2],
                    bt[:, :, 3],
                    (bt[:, :, 0] - calib_lines[:, index_calib, 0])
                    * (1 - mask_wrong.astype("int")),
                    (bt[:, :, 1] - calib_lines[:, index_calib, 1])
                    * (1 - mask_wrong.astype("int")),
                    (bt[:, :, 2] - calib_lines[:, index_calib, 2])
                    * (1 - mask_wrong.astype("int")),
                    (bt[:, :, 3] - calib_lines[:, index_calib, 3])
                    * (1 - mask_wrong.astype("int")),
                ]
            )

            bt_std = np.array(
                [
                    bt_std[:, :, 0],
                    bt_std[:, :, 1],
                    bt_std[:, :, 2],
                    bt_std[:, :, 3],
                    bt_std[:, :, 0],
                    bt_std[:, :, 1],
                    bt_std[:, :, 2],
                    bt_std[:, :, 3],
                ]
            )

            # self.aba[sub_dico] = {'jdb':jdb,'aba_ref':calib_lines,'aba':aba,'aba_std':aba_std,'catalog':kitcat_dico}

            catalog = kitcat_dico.copy()

            self.debug = catalog
            valid = np.product(np.std(bt, axis=2), axis=0) != 0
            catalog["valid"] = np.array(catalog["valid"]) & valid
            self.bt[sub_dico] = {
                "jdb": jdb,
                "bt": bt,
                "bt_std": bt_std,
                "catalog": catalog,
                "mask": kitcat_name,
            }

        myf.pickle_dump(self.bt, open(self.directory + "Analyse_bt_by_bt.p", "wb"))

    def yarara_lbl_iter(
        self,
        kitcat=None,
        master="kitcat",
        continuum="linear",
        windows="large",
        calib_std=1e-3,
        ron=1,
        oversampling=10,
        treshold_telluric=1,
        all_dico="all",
        fast=False,
        substract_map=[],
        add_map=[],
        rm_outliers=True,
        flux_max=0.95,
        normalised=True,
        debug_line=None,
        color_corrected=True,
    ):

        """
        Copute the lbl RV from flux time-series specifying kitcat dictionnary
        col = 0  -> LBL
        col = 1  -> LBL - CCF (planetary free)
        col = 2  -> EW

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        index : 'index' or 'time' if 'time', display the time-series with blank color if no spectra at specific time  (need roughly equidistant time-series spectra)
        zoom : int-type, to improve the resolution of the 2D plot
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution


        """

        directory = self.directory
        self.import_ccf()
        self.import_material()
        self.import_lbl()

        load = self.material
        if not os.path.exists(self.directory + "Analyse_line_by_line_iter.p"):
            self.lbl_iter = {}
        else:
            self.import_lbl_iter()

        planet = self.planet

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        smooth_map = self.smooth_map

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        overshoot_factor = self.fwhm / 2

        file_test = self.import_spectrum()
        wave = np.array(file_test["wave"]).astype("float64")
        dwave = file_test["parameters"]["dwave"]
        dico_found = []
        for dic in self.all_dicos:
            try:
                if "continuum_" + continuum in file_test[dic].keys():
                    dico_found.append(dic)
            except:
                pass

        if kitcat is None:
            kitcat = glob.glob(self.dir_root + "KITCAT/kitcat_mask*.p")[0]
        kitcat_file = pd.read_pickle(kitcat)
        kitcat_dico_backup = kitcat_file["catalogue"]
        kitcat_name = kitcat.split("/")[-1].split(".")[0]
        conti_ref = np.array(kitcat_file["spectre"]["continuum"])

        if type(master) == str:
            ref = kitcat_file["spectre"]["flux"]
        else:
            ref = master * np.array(load["correction_factor"])

        if not normalised:
            ref_spectrum_norm = myc.tableXY(wave, ref)
            ref_spectrum_norm.interpolate(new_grid=oversampling)
            ref_norm = ref_spectrum_norm.y
            ref *= conti_ref

        ref_spectrum = myc.tableXY(wave, ref)
        ref_spectrum.interpolate(new_grid=oversampling)

        ref = ref_spectrum.y
        wave_master = ref_spectrum.x

        if windows == "large":
            wave_left = np.array(kitcat_dico_backup["wave_left"])
            wave_right = np.array(kitcat_dico_backup["wave_right"])
        elif windows == "medium":
            wave_left = np.array(kitcat_dico_backup["wave_fitted"]) - np.array(
                kitcat_dico_backup["win_mic"]
            )
            wave_right = np.array(kitcat_dico_backup["wave_fitted"]) + np.array(
                kitcat_dico_backup["win_mic"]
            )

        # all_left, wave_left, dust = myf.find_nearest(wave,wave_left)
        # all_right, wave_right, dust = myf.find_nearest(wave,wave_right)
        wave_lines = np.array(kitcat_dico_backup["wave_fitted"])

        epsilon = 1e-12

        button_string = False

        if all_dico == "all":
            button_string = True
            all_dico = self.all_dicos
            sub_dicos = np.array(all_dico)[np.in1d(all_dico, dico_found)]
        elif all_dico == "fast":
            button_string = True
            all_dico = self.light_dicos
            sub_dicos = np.array(all_dico)[np.in1d(all_dico, dico_found)]
        else:
            sub_dicos = np.array(all_dico)

        rv = self.import_ccf_timeseries("CCF_" + self.mask_harps, "matching_mad", "rv")
        rv.y -= np.median(rv.y)

        all_dico_name = []
        for sub_dico in sub_dicos:

            kitcat_dico = self.lbl[sub_dico]["catalog"]
            print("\n ------ \n Dico %s analysing \n ------ \n" % (sub_dico))
            all_dico_name.append(sub_dico)

            flux = []
            snr = []
            conti = []
            jdb = []
            flux_err = []
            conti_err = []

            for i, j in enumerate(files):
                self.current_file = j
                file = pd.read_pickle(j)

                snr.append(file["parameters"]["SNR_5500"])

                flux.append(file["flux" + kw])
                flux_err.append(file["flux_err"])
                conti.append(file[sub_dico]["continuum_" + continuum])
                conti_err.append(file["continuum_err"])
                jdb.append(file["parameters"]["jdb"])

            del self.current_file

            snr = np.array(snr)
            jdb = np.array(jdb)

            conti = np.array(conti) + epsilon
            flux = np.array(flux) + epsilon
            conti_err = np.array(conti_err)
            flux_err = np.array(flux_err)

            flux, flux_norm_std = myf.flux_norm_std(flux, flux_err, conti, conti_err)
            std_flux_norm = np.sqrt(np.array(flux_norm_std) ** 2 + calib_std**2)

            flux *= np.array(load["correction_factor"])
            std_flux_norm *= np.array(load["correction_factor"])

            for maps in substract_map:
                flux = self.yarara_substract_map(flux, maps, correction_factor=True)

            for maps in add_map:
                flux = self.yarara_add_map(flux, maps, correction_factor=True)

            if (not normalised) & (color_corrected):
                flux *= conti_ref
                std_flux_norm *= conti_ref
            if not color_corrected:
                flux *= conti_ref
                std_flux_norm *= conti_ref

            all_lbl = []
            all_lbl_std = []
            chi2_Bouchy = []
            sample = range(len(wave_lines))
            if debug_line is not None:
                sample = [debug_line]
            for idx_line in tqdm(sample):

                lm = int(
                    np.round((wave_left[idx_line] - np.min(wave)) / (dwave / oversampling), 0)
                )
                rm = int(
                    np.round((wave_right[idx_line] - np.min(wave)) / (dwave / oversampling), 0)
                )

                overshoot = int(
                    np.mean(wave_master[lm : rm + 1])
                    * overshoot_factor
                    / 3e5
                    / (dwave / oversampling)
                )  # increase the master of 3kms
                lm -= overshoot
                rm += overshoot
                if lm < 0:
                    lm = 0
                if rm > (len(wave_master) - 1):
                    rm = len(wave_master) - 1

                rv_line = []
                rv_line_std = []
                chi2 = []

                master_line = myc.tableXY(wave_master[lm : rm + 1], ref[lm : rm + 1])

                left = wave_left[idx_line]
                right = wave_right[idx_line]

                if flux_max is not None:
                    if not normalised:
                        wave_borders = master_line.x[ref_norm[lm : rm + 1] > flux_max]
                    else:
                        wave_borders = master_line.x[master_line.y > flux_max]
                    if len(wave_borders):
                        wave_borders -= wave_lines[idx_line]
                        if sum(wave_borders < 0):
                            left = np.max(wave_borders[wave_borders < 0]) + wave_lines[idx_line]
                        if sum(wave_borders > 0):
                            right = np.min(wave_borders[wave_borders > 0]) + wave_lines[idx_line]

                l = int(np.round((left - np.min(wave)) / (dwave), 0))
                r = int(np.round((right - np.min(wave)) / (dwave), 0))

                if wave[l + 1] <= wave_master[lm : rm + 1][0]:
                    l += 1
                if wave[r - 1] >= wave_master[lm : rm + 1][-1]:
                    r -= 1

                # mask_master = (wave[l+1:r]>wave_master[lm:rm+1][0])&(wave[l+1:r]<wave_master[lm:rm+1][-1]) #ensure the line range to be smaller than the master

                for time_idx in range(len(jdb)):

                    (
                        normalization_Bouchy,
                        RV_Bouchy,
                        sig_RV_Bouchy,
                        constant_Bouchy,
                        chi_Bouchy,
                    ) = calculate_RV_line_by_line.get_RV_line_by_line2(
                        wave[l + 1 : r],
                        std_flux_norm[time_idx, l + 1 : r] ** 2,
                        flux[time_idx, l + 1 : r],
                        wave_master[lm : rm + 1],
                        ref[lm : rm + 1],
                        ron,
                        rv.y[time_idx],
                        wave_lines[idx_line],
                    )
                    if abs(RV_Bouchy) > 2500:
                        RV_Bouchy = np.sign(RV_Bouchy) * 2500

                    (
                        normalization_Bouchy,
                        RV_Bouchy,
                        sig_RV_Bouchy,
                        constant_Bouchy,
                        chi_Bouchy,
                    ) = calculate_RV_line_by_line.get_RV_line_by_line2(
                        wave[l + 1 : r],
                        std_flux_norm[time_idx, l + 1 : r] ** 2,
                        flux[time_idx, l + 1 : r],
                        wave_master[lm : rm + 1],
                        ref[lm : rm + 1],
                        ron,
                        RV_Bouchy,
                        wave_lines[idx_line],
                    )

                    rv_line.append(RV_Bouchy)
                    rv_line_std.append(sig_RV_Bouchy)
                    chi2.append(chi_Bouchy)

                if debug_line is not None:
                    self.debug_line = {
                        "jdb": jdb,
                        "wave": wave[l + 1 : r],
                        "flux": flux[:, l + 1 : r],
                        "flux_std": std_flux_norm[:, l + 1 : r] ** 2,
                        "wave_master": wave_master[lm : rm + 1],
                        "flux_master": ref[lm : rm + 1],
                        "wave_ref": wave_lines[idx_line],
                        "rv": np.array(rv_line),
                        "rv_std": np.array(rv_line_std),
                    }
                    break

                if False:
                    self.debug_line = {
                        "jdb": jdb,
                        "wave": wave[l + 1 : r],
                        "flux": flux[:, l + 1 : r],
                        "flux_std": std_flux_norm[:, l + 1 : r] ** 2,
                        "wave_master": wave_master[lm : rm + 1],
                        "flux_master": ref[lm : rm + 1],
                        "wave_ref": wave_lines[idx_line],
                        "rv": np.array(rv_line),
                        "rv_std": np.array(rv_line_std),
                    }
                    myf.pickle_dump(
                        self.debug_line,
                        open("/Users/cretignier/Desktop/Bug_line/Line_%.0f.p" % (idx_line), "wb"),
                    )

                all_lbl.append(rv_line)
                all_lbl_std.append(rv_line_std)
                chi2_Bouchy.append(chi2)
            chi2_Bouchy = np.array(chi2_Bouchy)

            all_lbl = np.array(all_lbl)
            all_lbl_std = np.array(all_lbl_std)
            all_lbl_std[np.isnan(all_lbl_std)] = self.lbl[sub_dico]["lbl_std"][0][
                np.isnan(all_lbl_std)
            ]
            all_lbl_std[all_lbl_std == 0] = self.lbl[sub_dico]["lbl_std"][0][all_lbl_std == 0]
            all_lbl[np.isnan(all_lbl)] = self.lbl[sub_dico]["lbl"][0][np.isnan(all_lbl)]

            all_lbl -= np.median(all_lbl, axis=1)[:, np.newaxis]
            if rm_outliers:
                for j in range(len(all_lbl)):
                    mask = myf.rm_outliers(all_lbl[j], m=2, kind="inter")[0]
                    all_lbl[j, ~mask] = self.lbl[sub_dico]["lbl"][0][j, ~mask]

                supress_line = np.sum(abs(self.lbl[sub_dico]["lbl"][0]), axis=1) == 0
                rms_all_line = np.std(all_lbl, axis=1)
                mask, dust = myf.rm_outliers(rms_all_line, m=2, kind="inter")
                all_lbl[~mask] = self.lbl[sub_dico]["lbl"][0][~mask]

                all_lbl[supress_line] = 0

            # all_lbl_std = self.lbl[sub_dico]['lbl_std'][0]
            all_ew = self.lbl[sub_dico]["lbl"][2]
            all_ew_std = self.lbl[sub_dico]["lbl_std"][2]

            nb_valid_points = []
            for j in range(len(all_lbl)):
                a = all_lbl[j]
                nb_valid_points.append(len(jdb) - len(np.unique(a)))

            invalid = np.sum(abs(all_lbl), axis=1) != 0

            nb_valid_points = np.array(nb_valid_points)
            valid = np.ones(len(nb_valid_points))
            valid[nb_valid_points < (np.median(nb_valid_points) - 5 * myf.IQ(nb_valid_points))] = 0
            valid[nb_valid_points > (np.median(nb_valid_points) + 5 * myf.IQ(nb_valid_points))] = 0

            kitcat_dico["nb_valid_points"] = nb_valid_points
            mask = kitcat_dico["valid"].astype("int")
            kitcat_dico["valid"] = (valid * invalid * mask) != 0
            kitcat_dico.loc[kitcat_dico["rel_contam"] > treshold_telluric, "valid"] = False

            rv_table = myc.table(all_lbl)
            rv_table.rms_w(1 / all_lbl_std**2)
            kitcat_dico["rms"] = rv_table.rms
            kitcat_dico["quality"] = kitcat_dico["rms"] / kitcat_dico["med_err"]
            if sub_dico == "matching_brute":
                rms = kitcat_dico.loc[kitcat_dico["valid"] == 1, "rms"]
                quality = kitcat_dico.loc[kitcat_dico["valid"] == 1, "quality"]
                rms_lim = np.nanpercentile(rms, 75) + 1.5 * myf.IQ(rms)
                qual_lim_sup = np.nanpercentile(quality, 75) + 2.5 * myf.IQ(quality)
                qual_lim_inf = np.nanpercentile(quality, 25) - 1.5 * myf.IQ(quality)
                kitcat_dico.loc[kitcat_dico["rms"] > rms_lim, "valid"] = False
                kitcat_dico.loc[
                    (kitcat_dico["quality"] > qual_lim_sup)
                    | (kitcat_dico["quality"] < qual_lim_inf),
                    "valid",
                ] = False

            rvm = rv_table.rv_subselection(
                rv_std=all_lbl_std, selection=np.array(kitcat_dico["valid"])
            )
            # ccf_rv = np.array(self.table_ccf['CCF_'+kitcat_name][sub_dico]['table']['rv']*1000)
            ccf_rv_std = np.array(
                self.table_ccf["CCF_" + kitcat_name]["matching_diff"]["table"]["rv_std"] * 1000
            )

            self.lbl_iter[sub_dico] = {
                "jdb": jdb,
                "lbl": np.array([all_lbl, all_lbl - rvm.y, all_ew]),
                "lbl_std": np.array(
                    [all_lbl_std, np.sqrt(all_lbl_std**2 + ccf_rv_std**2), all_ew_std]
                ),
                "catalog": kitcat_dico.copy(),
                "mask": kitcat_name,
            }

        myf.pickle_dump(self.lbl_iter, open(self.directory + "Analyse_line_by_line_iter.p", "wb"))

        if button_string:
            all_dico = "all"
        self.lbl_to_ccf(all_dico=all_dico, kw_dico="lbl_iter")

    def yarara_lbl(
        self,
        kitcat=None,
        master="kitcat",
        continuum="linear",
        windows="large",
        calib_std=1e-3,
        treshold_telluric=0.4,
        all_dico="all",
        fast=False,
        substract_map=[],
        add_map=[],
    ):

        """
        Copute the lbl RV from flux time-series specifying kitcat dictionnary
        col = 0  -> LBL
        col = 1  -> LBL - CCF (planetary free)
        col = 2  -> EW

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        index : 'index' or 'time' if 'time', display the time-series with blank color if no spectra at specific time  (need roughly equidistant time-series spectra)
        zoom : int-type, to improve the resolution of the 2D plot
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution


        """

        directory = self.directory
        self.import_ccf()
        self.import_material()
        load = self.material
        if not os.path.exists(self.directory + "Analyse_line_by_line.p"):
            self.lbl = {}
        else:
            self.import_lbl()

        planet = self.planet

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        smooth_map = self.smooth_map

        self.import_table()

        files = glob.glob(directory + "RASSI*.p")
        files = np.array(self.table["filename"])  # updated 29.10.21 to allow ins_merged
        files = np.sort(files)

        file_test = self.import_spectrum()
        wave = np.array(file_test["wave"])

        dico_found = []
        for dic in self.all_dicos:
            try:
                if "continuum_" + continuum in file_test[dic].keys():
                    dico_found.append(dic)
            except:
                pass

        if kitcat is None:
            kitcat = glob.glob(self.dir_root + "KITCAT/kitcat_mask*.p")[0]
        kitcat_file = pd.read_pickle(kitcat)
        kitcat_dico_backup = kitcat_file["catalogue"]
        kitcat_name = kitcat.split("/")[-1].split(".")[0]

        if type(master) == str:
            ref = kitcat_file["spectre"]["flux"]
        else:
            ref = master * np.array(load["correction_factor"])

        # r = myc.tableXY(wave,ref)
        # r.interpolate(new_grid=10,method='cubic')
        # r.y = np.gradient(r.y)/np.gradient(r.x)*np.mean(np.gradient(wave))
        # r.interpolate(new_grid=wave)
        # ref_gradient = r.y
        ref_gradient = np.gradient(ref)

        if windows == "large":
            wave_left = np.array(kitcat_dico_backup["wave_left"])
            wave_right = np.array(kitcat_dico_backup["wave_right"])
        elif windows == "medium":
            wave_left = np.array(kitcat_dico_backup["wave_fitted"]) - np.array(
                kitcat_dico_backup["win_mic"]
            )
            wave_right = np.array(kitcat_dico_backup["wave_fitted"]) + np.array(
                kitcat_dico_backup["win_mic"]
            )

        all_left = myf.find_nearest(wave, wave_left)[0]
        all_right = myf.find_nearest(wave, wave_right)[0]

        epsilon = 1e-12

        button_string = False

        if all_dico == "all":
            button_string = True
            all_dico = self.all_dicos
            sub_dicos = np.array(all_dico)[np.in1d(all_dico, dico_found)]
        elif all_dico == "fast":
            button_string = True
            all_dico = self.light_dicos
            sub_dicos = np.array(all_dico)[np.in1d(all_dico, dico_found)]
        else:
            sub_dicos = np.array(all_dico)

        all_dico_name = []
        for sub_dico in sub_dicos:

            kitcat_dico = kitcat_dico_backup.copy()
            print("\n ------ \n Dico %s analysing \n ------ \n" % (sub_dico))
            all_dico_name.append(sub_dico)

            flux = []
            snr = []
            conti = []
            jdb = []
            flux_err = []
            conti_err = []

            for i, j in enumerate(files):
                self.current_file = j
                file = pd.read_pickle(j)

                snr.append(file["parameters"]["SNR_5500"])

                flux.append(file["flux" + kw])
                flux_err.append(file["flux_err"])
                conti.append(file[sub_dico]["continuum_" + continuum])
                conti_err.append(file["continuum_err"])
                jdb.append(file["parameters"]["jdb"])

            del self.current_file

            snr = np.array(snr)
            jdb = np.array(jdb)

            conti = np.array(conti) + epsilon
            flux = np.array(flux) + epsilon
            conti_err = np.array(conti_err)
            flux_err = np.array(flux_err)

            flux, flux_norm_std = myf.flux_norm_std(flux, flux_err, conti, conti_err)
            std_flux_norm = np.sqrt(np.array(flux_norm_std) ** 2 + calib_std**2)

            flux *= np.array(load["correction_factor"])
            std_flux_norm *= np.array(load["correction_factor"])

            for maps in substract_map:
                flux = self.yarara_substract_map(flux, maps, correction_factor=True)

            for maps in add_map:
                flux = self.yarara_add_map(flux, maps, correction_factor=True)

            rv_shift = 0
            if not fast:
                rv = self.import_ccf_timeseries("CCF_" + self.mask_harps, "matching_mad", "rv")
                rv.y -= np.median(rv.y)

                for spec_num in tqdm(range(len(flux))):
                    s = myc.tableXY(myf.doppler_r(wave, rv.y[spec_num])[1], flux[spec_num])
                    s.interpolate(new_grid=wave, method="linear")
                    flux[spec_num] = s.y

                rv_shift = rv.y

            diff = myf.smooth2d(flux - ref, smooth_map)

            dflux = ref_gradient / file["parameters"]["dwave"]

            coeff = 3e8 / wave.astype("float64")
            diff_rv = coeff * diff / (dflux + epsilon)
            map_weight = (wave) ** 2 * np.abs(dflux) ** 2 * np.ones(len(jdb))[:, np.newaxis]
            map_weight /= np.percentile(map_weight, 95)
            diff_rv[:, ref > 0.95] = 0  # remove all weight flux higher than 0.95
            diff_rv[map_weight < 0.1] = 0  # remove all weight smaller than 10%
            diff_rv[abs(diff_rv) > 10000] = 0  # remove absurde RV values
            diff_rv[diff_rv == 0] = np.nan

            diff_rv *= -1

            map_std_rv = coeff * std_flux_norm / np.abs(dflux + epsilon)
            map_std_rv[diff_rv == 0] = 0
            map_weight2 = 1 / map_std_rv**2

            all_lbl = []
            all_lbl_std = []
            all_ew = []
            all_ew_std = []

            f_square = std_flux_norm**2
            mask = np.ones(len(all_left))
            count = -1
            for line in tqdm(range(len(all_left))):
                count += 1
                kept = 1
                lbl_rv = diff_rv[:, all_left[line] : all_right[line] + 1]
                lbl_weight = map_weight2[:, all_left[line] : all_right[line] + 1]
                if sub_dico == "matching_brute":
                    crit = np.sum(
                        np.array(load["mask_brute"])[all_left[line] : all_right[line] + 1]
                    )
                    if crit:
                        kept = 0
                        mask[count] = 0
                all_ew.append(
                    kept * np.nanmean(diff[:, all_left[line] : all_right[line] + 1], axis=1)
                )
                all_ew_std.append(
                    np.sqrt(np.nansum(f_square[:, all_left[line] : all_right[line] + 1], axis=1))
                    / (all_right[line] + 1 - all_left[line])
                )
                all_lbl.append(
                    np.nansum(kept * lbl_rv * lbl_weight, axis=1) / np.nansum(lbl_weight, axis=1)
                )
                all_lbl_std.append(1 / np.sqrt(np.nansum(lbl_weight, axis=1)))

            all_ew = np.array(all_ew)
            all_ew_std = np.array(all_ew_std)
            all_lbl = np.array(all_lbl)
            med_rv = np.median(all_lbl, axis=1)
            all_lbl -= med_rv[:, np.newaxis]
            mask_all_zero = (np.sum(abs(all_lbl), axis=1) != 0).astype("int")
            all_lbl += med_rv[:, np.newaxis]
            all_lbl = all_lbl + rv_shift * mask_all_zero[:, np.newaxis]
            all_lbl_std = np.array(all_lbl_std)

            all_lbl -= np.median(all_lbl, axis=1)[:, np.newaxis]

            nb_valid_points = []
            for j in range(len(all_lbl)):
                a = all_lbl[j]
                nb_valid_points.append(len(jdb) - len(np.unique(a)))

            invalid = np.sum(all_lbl, axis=1) != 0

            nb_valid_points = np.array(nb_valid_points)
            valid = np.ones(len(nb_valid_points))
            valid[nb_valid_points < (np.median(nb_valid_points) - 5 * myf.IQ(nb_valid_points))] = 0
            valid[nb_valid_points > (np.median(nb_valid_points) + 5 * myf.IQ(nb_valid_points))] = 0

            kitcat_dico["nb_valid_points"] = nb_valid_points
            kitcat_dico["valid"] = (valid * mask * invalid) != 0
            kitcat_dico.loc[kitcat_dico["rel_contam"] > treshold_telluric, "valid"] = False

            weights = 1 / all_lbl_std**2
            weights[(valid * mask) == 0] = 0
            weights = np.sum(weights, axis=1)
            weights /= np.sum(weights) / 100  # normalise weights

            rv_table = myc.table(all_lbl)
            rv_table.rms_w(1 / all_lbl_std**2)
            kitcat_dico["weight"] = weights
            kitcat_dico["rms"] = rv_table.rms
            kitcat_dico["med_err"] = np.median(all_lbl_std, axis=1)
            kitcat_dico["quality"] = kitcat_dico["rms"] / kitcat_dico["med_err"]
            if sub_dico == "matching_brute":
                rms = kitcat_dico.loc[kitcat_dico["valid"] == 1, "rms"]
                quality = kitcat_dico.loc[kitcat_dico["valid"] == 1, "quality"]
                rms_lim = np.nanpercentile(rms, 75) + 1.5 * myf.IQ(rms)
                qual_lim_sup = np.nanpercentile(quality, 75) + 2.5 * myf.IQ(quality)
                qual_lim_inf = np.nanpercentile(quality, 25) - 1.5 * myf.IQ(quality)
                kitcat_dico.loc[kitcat_dico["rms"] > rms_lim, "valid"] = False
                kitcat_dico.loc[
                    (kitcat_dico["quality"] > qual_lim_sup)
                    | (kitcat_dico["quality"] < qual_lim_inf),
                    "valid",
                ] = False

            rvm = rv_table.rv_subselection(
                rv_std=all_lbl_std, selection=np.array(kitcat_dico["valid"])
            )
            # ccf_rv = np.array(self.table_ccf['CCF_'+kitcat_name][sub_dico]['table']['rv']*1000)
            ccf_rv_std = np.array(
                self.table_ccf["CCF_" + kitcat_name]["matching_diff"]["table"]["rv_std"] * 1000
            )

            self.lbl[sub_dico] = {
                "jdb": jdb,
                "lbl": np.array([all_lbl, all_lbl - rvm.y, all_ew]),
                "lbl_std": np.array(
                    [all_lbl_std, np.sqrt(all_lbl_std**2 + ccf_rv_std**2), all_ew_std]
                ),
                "catalog": kitcat_dico.copy(),
                "mask": kitcat_name,
            }

        myf.pickle_dump(self.lbl, open(self.directory + "Analyse_line_by_line.p", "wb"))

        if button_string:
            all_dico = "all"
        self.lbl_to_ccf(all_dico=all_dico, kw_dico="lbl")

    def yarara_rv_mask(
        self,
        mask=None,
        continuum="linear",
        calib_std=1e-3,
        sub_dico="matching_diff",
        substract_map=[],
        add_map=[],
    ):

        """
        Compute the lbl RV from flux time-series specifying the boolean mask of the spectrum for extraction

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        index : 'index' or 'time' if 'time', display the time-series with blank color if no spectra at specific time  (need roughly equidistant time-series spectra)
        zoom : int-type, to improve the resolution of the 2D plot
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution


        """

        directory = self.directory
        self.import_ccf()
        self.import_material()
        load = self.material

        planet = self.planet

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        smooth_map = self.smooth_map

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        file_test = self.import_spectrum()
        wave = np.array(file_test["wave"])

        master = pd.read_pickle(self.dir_root + "KITCAT/kitcat_spectrum.p")["flux"]
        ref = master * np.array(load["correction_factor"])

        if mask is None:
            mask = np.ones(len(ref)).astype("bool")

        if len(ref) != len(mask):
            print("The mask given as input has not the same size than the refeence spectrum")

        epsilon = 1e-12

        print("\n ------ \n Dico %s analysing \n ------ \n" % (sub_dico))

        flux = []
        snr = []
        conti = []
        jdb = []
        flux_err = []
        conti_err = []

        for i, j in enumerate(files):
            self.current_file = j
            file = pd.read_pickle(j)

            snr.append(file["parameters"]["SNR_5500"])

            flux.append(file["flux" + kw])
            flux_err.append(file["flux_err"])
            conti.append(file[sub_dico]["continuum_" + continuum])
            conti_err.append(file["continuum_err"])
            jdb.append(file["parameters"]["jdb"])

        del self.current_file

        snr = np.array(snr)
        jdb = np.array(jdb)

        conti = np.array(conti) + epsilon
        flux = np.array(flux) + epsilon
        conti_err = np.array(conti_err)
        flux_err = np.array(flux_err)

        flux, flux_norm_std = myf.flux_norm_std(flux, flux_err, conti, conti_err)
        std_flux_norm = np.sqrt(np.array(flux_norm_std) ** 2 + calib_std**2)

        flux *= np.array(load["correction_factor"])
        std_flux_norm *= np.array(load["correction_factor"])

        for maps in substract_map:
            flux = self.yarara_substract_map(flux, maps, correction_factor=True)

        for maps in add_map:
            flux = self.yarara_add_map(flux, maps, correction_factor=True)

        diff = myf.smooth2d(flux - ref, smooth_map)

        dflux = np.gradient(ref) / file["parameters"]["dwave"]

        coeff = 3e8 / wave.astype("float64")
        diff_rv = coeff * diff / (dflux + epsilon)
        map_weight = (wave) ** 2 * np.abs(dflux) ** 2 * np.ones(len(jdb))[:, np.newaxis]
        map_weight /= np.percentile(map_weight, 95)
        diff_rv[:, ref > 0.95] = 0  # remove all weight flux higher than 0.95
        diff_rv[map_weight < 0.1] = 0  # remove all weight smaller than 10%
        diff_rv[abs(diff_rv) > 2000] = 0  # remove absurde RV values
        diff_rv[diff_rv == 0] = np.nan

        diff_rv *= -1

        map_std_rv = coeff * std_flux_norm / np.abs(dflux + epsilon)
        map_std_rv[diff_rv == 0] = 0
        map_weight2 = 1 / map_std_rv**2

        hl = file_test["parameters"]["hole_left"]
        hr = file_test["parameters"]["hole_right"]

        if hl != -99.9:
            mask_hole = (wave > hl) & (wave < hr)
            mask = mask & (~mask_hole)

        if sub_dico == "matching_brute":
            mask = mask & (~np.array(load["mask_brute"]))

        lbl_rv = diff_rv[:, mask]
        lbl_weight = map_weight2[:, mask]

        all_lbl = np.nansum(lbl_rv * lbl_weight, axis=1) / np.nansum(lbl_weight, axis=1)
        all_lbl_std = 1 / np.sqrt(np.nansum(lbl_weight, axis=1))

        vec = myc.tableXY(jdb, all_lbl, all_lbl_std)

        return vec

    def lbl_polar_1year(self, kw_dico="lbl", deg=0, sub_dico="matching_morpho"):
        fig = plt.figure(figsize=(16, 7))
        mat2, dust = self.lbl_fit_sinus(
            365.25,
            kw_dico=kw_dico,
            sub_dico="matching_diff",
            subplot_name=[fig, 121],
            season=0,
            cmax=10,
            color_axis="K",
            deg=deg,
            plot_proxies=False,
            kde=False,
            light_title="Before YARARA",
            legend=False,
            valid_lines=False,
            fontsize=16,
            pfont=13,
            rfont=13,
        )
        mat4, dust = self.lbl_fit_sinus(
            365.25,
            kw_dico=kw_dico,
            sub_dico=sub_dico,
            subplot_name=[fig, 122],
            season=0,
            cmax=10,
            color_axis="K",
            deg=deg,
            plot_proxies=False,
            kde=False,
            light_title="After YARARA",
            legend=False,
            valid_lines=True,
            fontsize=16,
            pfont=13,
            rfont=13,
        )
        plt.subplots_adjust(left=0.04, right=0.97, top=0.89, bottom=0.10)
        plt.savefig(self.dir_root + "IMAGES/" + str(kw_dico).upper() + "_polar_periodogram.png")

    def lbl_periodogram(
        self,
        period_investigated,
        zone=5,
        sub_dico="matching_diff",
        p_min=0,
        fap=1,
        rv_shift=None,
        plot=False,
        subplot_name=[None, 0],
        hist=0,
        kw_dico="lbl",
        col=0,
        season=0,
        rmax=1,
        cmap="jet",
        cmax=None,
        circle=[None, "r", "-", 2],
        kde=True,
        light_title=False,
        valid_lines=True,
        warning=False,
    ):

        self.import_table()
        self.import_material()

        if kw_dico == "lbl":
            self.import_lbl()
            mat = self.lbl
        elif kw_dico == "lbl_iter":
            kw_dico = "lbl"
            self.import_lbl_iter()
            mat = self.lbl_iter
        elif kw_dico == "dbd":
            self.import_dbd()
            mat = self.dbd

        load = self.material
        jdb = np.array(self.table["jdb"])

        seasons = myf.detect_obs_season(jdb, min_gap=jdb.max() - jdb.min())
        j = 0
        if season:
            seasons = myf.detect_obs_season(jdb, min_gap=40)
            season -= 1
            j = season
            season_label = season
        else:
            season = 0
            season_label = -1

        matrix_rv = mat[sub_dico][kw_dico][col]
        matrix_rv_std = mat[sub_dico][kw_dico + "_std"][col]

        catalog = mat[sub_dico]["catalog"]
        wave = np.array(load["wave"])

        if valid_lines:
            mask_bool = np.array(catalog["valid"])
        else:
            mask_bool = np.ones(len(catalog["valid"]))

        idx = np.arange(len(catalog))[mask_bool.astype("bool")]

        if rv_shift is None:
            rv_shift = np.zeros(len(jdb))
        matrix_rv -= rv_shift

        all_rv = matrix_rv[:, seasons[j, 0] : seasons[j, 1] + 1]
        all_rv_std = matrix_rv_std[:, seasons[j, 0] : seasons[j, 1] + 1]
        all_rv_weights = 1 / all_rv_std**2

        faps = np.zeros(len(catalog))
        power = np.zeros(len(catalog))
        amp = np.zeros(len(catalog))
        phi = np.zeros(len(catalog))
        period_max = np.zeros(len(catalog))

        for i in tqdm(idx):
            lbl = matrix_rv[i]
            lbl_std = matrix_rv_std[i]
            v = myc.tableXY(jdb, lbl, lbl_std)
            if p_min == 0:
                p_min = 0.1 * period_investigated

            try:
                v.periodogram(
                    nb_perm=1,
                    p_min=p_min,
                    Plot=False,
                    level=1 - fap / 100,
                    all_outputs=True,
                    warning=warning,
                )
            except IndexError:
                v.periodogram(
                    nb_perm=1,
                    p_min=p_min * 0.1,
                    Plot=False,
                    level=1 - fap / 100,
                    all_outputs=True,
                    warning=warning,
                )
            try:
                if zone:
                    perio = myc.tableXY(1 / v.freq[::-1], v.power[::-1] / v.fap)
                    perio.find_max()
                    l = perio.x_max[myf.find_nearest(perio.x_max, period_investigated)[0]]
                    if (100 * abs(l - period_investigated) / period_investigated) < zone:
                        faps[i] = v.fap
                        power[i] = perio.y_max[
                            myf.find_nearest(perio.x_max, period_investigated)[0]
                        ]
                        amp[i] = v.amplitude[::-1][
                            perio.index_max[myf.find_nearest(perio.x_max, period_investigated)[0]]
                        ]
                        phi[i] = v.phase[::-1][
                            perio.index_max[myf.find_nearest(perio.x_max, period_investigated)[0]]
                        ]
                        period_max[i] = v.perio_max

                    else:
                        faps[i] = 100
                        power[i] = 0.001
                        amp[i] = 0.001
                        phi[i] = 0.001
                        period_max[i] = 0

                else:
                    l = myf.find_nearest(1 / v.freq, period_investigated)[0]
                    faps[i] = v.fap
                    power[i] = v.power[l] / v.fap
                    amp[i] = v.amplitude[l]
                    phi[i] = v.phase[l]
                    period_max[i] = v.perio_max

            except AttributeError:
                faps[i] = 100
                power[i] = 0.001
                amp[i] = 0.001
                phi[i] = 0.001
                period_max[i] = 0

        catalog["fap"] = faps
        catalog["power"] = power
        catalog["amp"] = amp
        catalog["phi"] = phi
        catalog["max_period"] = period_max

        catalog_backup = catalog.copy()

        if plot:

            catalog = catalog.loc[catalog["fap"] < 99]

            rmax = 1
            if subplot_name[0] is None:
                fig = plt.figure(figsize=(12, 12))
                subplot_name[1] = 111
                button = "on"
            else:
                fig = subplot_name[0]
                button = "off"
            ax = fig.add_subplot(subplot_name[1], projection="polar")
            ax.set_rlabel_position(90)
            if hist:
                N = hist
                bins = np.linspace(0, 2 * np.pi, N + 1)
                dbin = bins[1] - bins[0]
                phi = (np.array(catalog["phi"])) % (2 * np.pi)
                mask = (phi <= bins[1:, np.newaxis]) & (phi > bins[:-1, np.newaxis])
                histo = np.sum(mask, axis=1)
                N_random = np.sum(histo) / N
                histo = (histo - N_random) / (
                    N_random * 4
                )  # touch 0.2 limit circle if 5 times for points in bins than random distribution
                width = (2 * np.pi) / N
                ax.bar(
                    bins[:-1] + dbin / 2,
                    histo * (0.19) * rmax,
                    width=width,
                    bottom=rmax,
                    alpha=0.4,
                    edgecolor="k",
                )

            if type(light_title) == bool:
                if light_title:
                    plt.title("Analysis : %s_%.0f" % (kw_dico.upper(), col), fontsize=14)
                else:
                    plt.title(
                        "Analysis : %s_%.0f        Season : %.0f        Sub dico : %s        Period : %.2f"
                        % (
                            kw_dico.upper(),
                            col,
                            np.round(season_label + 1),
                            sub_dico,
                            period_investigated,
                        ),
                        fontsize=14,
                    )
            elif type(light_title) == str:
                plt.title(light_title, fontsize=14)

            plt.plot(
                np.linspace(0, 2 * np.pi, 100), np.ones(100) * rmax, color="k", ls="-", lw=2.5
            )
            plt.plot(
                np.linspace(0, 2 * np.pi, 100),
                np.median(catalog["fap"]) * np.ones(100),
                color=circle[1],
                ls=circle[2],
                lw=circle[3],
            )

            var_color = np.array(catalog["amp"]).astype("float")
            IQ = myf.IQ(var_color)
            Q3 = np.nanpercentile(var_color, 75)
            Q1 = np.nanpercentile(var_color, 25)
            var_color[np.isnan(var_color)] = np.nanmedian(var_color)
            var_color[(var_color > (Q3 + 3 * IQ)) | (var_color < (Q1 - 3 * IQ))] = np.nanmedian(
                var_color
            )

            color = var_color
            vmax = np.nanpercentile(color, 75) + 1.5 * myf.IQ(color)
            if vmax > np.max(color):
                vmax = np.max(color)

            if cmax is not None:
                vmax = cmax
            theta, r = np.array(catalog["phi"]), np.array(catalog["power"] * catalog["fap"])
            if kde:
                x, y = r * np.sin(theta), r * np.cos(theta)
                test = myc.tableXY(x, y)
                test.kde(levels=["2d", [1]], alpha=0.01)
                x_cs_sig1, y_cs_sig1 = test.vertices_curve[0][:, 0], test.vertices_curve[0][:, 1]
                theta_cs_sig1, r_cs_sig1 = np.arctan2(x_cs_sig1, y_cs_sig1), np.sqrt(
                    x_cs_sig1**2 + y_cs_sig1**2
                )
                r_cs_sig1[r_cs_sig1 > 1] = 1

                test.kde(levels=["2d", [2]], alpha=0.01)
                x_cs_sig2, y_cs_sig2 = test.vertices_curve[0][:, 0], test.vertices_curve[0][:, 1]
                theta_cs_sig2, r_cs_sig2 = np.arctan2(x_cs_sig2, y_cs_sig2), np.sqrt(
                    x_cs_sig2**2 + y_cs_sig2**2
                )
                r_cs_sig2[r_cs_sig2 > 1] = 1

                plt.plot(theta_cs_sig1, r_cs_sig1, color="k", lw=3)
                plt.plot(theta_cs_sig2, r_cs_sig2, color="k", lw=3)
            plt.scatter(
                theta,
                r,
                c=color,
                edgecolor="k",
                cmap=cmap,
                vmin=color.min(),
                vmax=vmax,
                alpha=0.3 * int(kde) + 0.6 * int(1 - kde),
            )
            ax2 = plt.colorbar(orientation="vertical")
            ax2.ax.set_ylabel("Amplitude", fontsize=14)

            if hist:
                ax.set_ylim(0, rmax + 0.19)
            else:
                ax.set_ylim(0, rmax)
            if button == "on":
                plt.subplots_adjust(bottom=0.1, top=0.95, left=0.06, right=0.96)

        return catalog_backup

    def lbl_supress_1year(
        self, sub_dico="matching_morpho", kw_dico="lbl", fap=1, p_min=0, add_step=0, col=0
    ):

        self.import_lbl()
        self.import_lbl_iter()
        self.import_table()
        self.import_dico_tree()

        directory = self.directory
        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        file_test = self.import_spectrum()
        step = file_test[sub_dico]["parameters"]["step"]

        if kw_dico == "lbl":
            imported_lbl = self.lbl
        elif kw_dico == "lbl_iter":
            imported_lbl = self.lbl_iter

        kitcat = imported_lbl[sub_dico]["catalog"].copy()
        lbl = imported_lbl[sub_dico]

        rv_shift = np.zeros(len(self.table["jdb"]))
        n1_val = -1
        n1_v = -1
        while n1_val < 2:
            n1_val += 1
            n1_v += 1
            tab = self.lbl_periodogram(
                365.25,
                sub_dico=sub_dico,
                fap=fap,
                p_min=p_min,
                col=col,
                rv_shift=rv_shift,
                kw_dico=kw_dico,
            )

            valid = kitcat["valid"]
            tab.loc[kitcat["valid"] == False, "power"] = -1
            tab["dist"] = abs(tab["max_period"] - 365.25) * 100 / 365.25
            tab.loc[kitcat["valid"] == False, "dist"] = np.nan

            new_valid = np.array((tab["power"] < 1) & (tab["power"] >= 0))
            new_invalid = np.array(tab["power"] >= 1)
            # new_valid = np.array(tab['dist']>10) ; new_invalid = np.array(tab['dist']<=10)

            tab["new_valid"] = new_valid
            tab.loc[kitcat["valid"] == False, "new_valid"] = np.nan

            # kitcat['valid'] = new_valid.copy()
            vec_t1 = self.lbl_subselection(sub_dico=sub_dico, mask=new_invalid, col=0)
            vec_t2 = self.lbl_subselection(sub_dico=sub_dico, mask=new_valid, col=0)
            vec_t3 = self.lbl_subselection(
                sub_dico=sub_dico, mask=(new_valid) | (new_invalid), col=0
            )

            n1 = sum(new_invalid)
            n2 = sum(new_valid)

            rv_shift = vec_t2.y

            if sum(tab.loc[tab["new_valid"] == False, "weight"]) > 15:
                n1_val -= 1
            if n1_v == 3:
                break

        plt.figure(figsize=(21, 7))
        plt.subplot(2, 1, 1)
        ax = plt.gca()
        plt.title(
            "Nb lines : %.0f (n=%.0f%% / w=%.0f%%)"
            % (n1, 100 * n1 / len(tab), sum(tab.loc[tab["new_valid"] == False, "weight"]))
        )
        if np.sum(~np.isnan(vec_t1.y)):
            vec_t1.periodogram(nb_perm=1, p_min=0.6)
        plt.axvline(x=365.25, color="r", alpha=0.5)
        plt.axvline(x=365.25 / 2, color="r", alpha=0.5)
        plt.axvline(x=365.25 / 3, color="r", alpha=0.5)

        plt.subplot(2, 1, 2, sharex=ax)
        plt.title(
            "Nb lines : %.0f (n=%.0f%% / w=%.0f%%)"
            % (n2, 100 * n2 / len(tab), sum(tab.loc[tab["new_valid"] == True, "weight"]))
        )
        if np.sum(~np.isnan(vec_t3.y)):
            vec_t3.periodogram(nb_perm=1, p_min=0.6, color="gray")
        if np.sum(~np.isnan(vec_t2.y)):
            vec_t2.periodogram(nb_perm=1, p_min=0.6)
        plt.axvline(x=365.25, color="r", alpha=0.5)
        plt.axvline(x=365.25 / 2, color="r", alpha=0.5)
        plt.axvline(x=365.25 / 3, color="r", alpha=0.5)
        plt.subplots_adjust(left=0.07, right=0.95, top=0.95, bottom=0.13, hspace=0.3)
        plt.savefig(self.dir_root + "IMAGES/" + str(kw_dico).upper() + "_1y_suppresion.pdf")

        print(
            "\n [INFO] Nb 1-year significant lines : %.0f (n=%.0f%% / w=%.0f%%)"
            % (n1, 100 * n1 / len(tab), sum(tab.loc[tab["new_valid"] == False, "weight"]))
        )
        print(
            "\n [INFO] Nb valid lines : %.0f (n=%.0f%% / w=%.0f%%)"
            % (n2, 100 * n2 / len(tab), sum(tab.loc[tab["new_valid"] == True, "weight"]))
        )

        print("\n Retropropagation of the valid lines into previous sub_dico...")

        based_dico = np.array(self.dico_tree.loc[self.dico_tree["dico"] == sub_dico, "dico_used"])[
            0
        ]
        imported_lbl[sub_dico]["catalog"]["valid"] = new_valid.copy()
        imported_lbl[based_dico]["catalog"]["valid"] = new_valid.copy()

        if kw_dico == "lbl":
            myf.pickle_dump(imported_lbl, open(self.directory + "Analyse_line_by_line.p", "wb"))
        elif kw_dico == "lbl_iter":
            myf.pickle_dump(
                imported_lbl, open(self.directory + "Analyse_line_by_line_iter.p", "wb")
            )

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file["matching_1y"] = {
                "parameters": {"sub_dico_used": sub_dico, "step": step + 1 + add_step}
            }
            myf.pickle_dump(file, open(j, "wb"))

        self.lbl_to_ccf(all_dico=[sub_dico, based_dico], kw_dico=kw_dico)

    def lbl_subselection(
        self,
        sub_dico="matching_diff",
        kw_dico="lbl",
        mask=None,
        inv=False,
        col=0,
        valid_lines=True,
        wave_min=None,
        wave_max=None,
    ):

        self.import_table()
        self.import_material()

        if kw_dico == "lbl":
            self.import_lbl()
            mat = self.lbl
            name = "lbl"
        elif kw_dico == "lbl_iter":
            self.import_lbl_iter()
            mat = self.lbl_iter
            name = "lbl"
        elif kw_dico == "dbd":
            self.import_dbd()
            mat = self.dbd
            name = "dbd"

        load = self.material

        catalog = mat[sub_dico]["catalog"]
        wave = np.array(load["wave"])
        wave_lines = np.array(catalog["wave"])

        if valid_lines:
            mask_bool = np.array(catalog["valid"])
        else:
            mask_bool = np.ones(len(catalog["valid"])).astype("bool")

        if mask is not None:
            if type(mask) == str:
                match = myf.find_nearest(wave, np.array(catalog["wave"]))[0]
                mask = np.array(load[mask])
                mask = (mask[match] - int(inv)).astype("bool")
                mask_bool = mask_bool & mask
            else:
                if type(mask[0]) == np.bool_:
                    mask_bool = mask_bool & mask
                else:
                    mask_bool = mask_bool & np.in1d(np.arange(len(mask_bool)), mask)

        if wave_min is not None:
            mask_bool = mask_bool & (wave_lines > wave_min)

        if wave_max is not None:
            mask_bool = mask_bool & (wave_lines < wave_max)

        lbl = mat[sub_dico][name][col]
        lbl_std = mat[sub_dico][name + "_std"][col]

        print("Nb lines used : %.0f" % (sum(mask_bool)))

        tab = myc.table(lbl)
        rvm = tab.rv_subselection(rv_std=lbl_std, selection=mask_bool)
        rv_sub = myc.tableXY(self.table.jdb, rvm.y, rvm.yerr)
        return rv_sub

    def lbl_to_ccf(self, all_dico="all", kw_dico="lbl"):

        self.import_table()
        jdb = self.table["jdb"]
        if kw_dico == "lbl":
            self.import_lbl()
        elif kw_dico == "lbl_iter":
            self.import_lbl_iter()

        if kw_dico == "lbl":
            sel_dico = self.lbl.keys()
        elif kw_dico == "lbl_iter":
            sel_dico = self.lbl_iter.keys()

        file_test = self.import_spectrum()
        dico_found = []
        for dic in self.all_dicos:
            try:
                if dic in sel_dico:
                    dico_found.append(dic)
            except:
                pass

        if all_dico == "all":
            all_dico = self.all_dicos
            sub_dicos = np.array(all_dico)[np.in1d(all_dico, dico_found)]
        else:
            sub_dicos = np.array(all_dico)

        kw_table_ccf = [
            "ew",
            "ew_std",
            "contrast",
            "contrast_std",
            "rv",
            "rv_std",
            "rv_std_phot",
            "fwhm",
            "fwhm_std",
            "center",
            "center_std",
            "depth",
            "depth_std",
            "b0",
            "bisspan",
            "bisspan_std",
            "jdb",
        ]

        file_summary_ccf = pd.read_pickle(self.directory + "Analyse_ccf.p")

        for sub_dico in sub_dicos:

            if kw_dico == "lbl":
                import_lbl = self.lbl
            elif kw_dico == "lbl_iter":
                import_lbl = self.lbl_iter

            print("\n ------ \n Dico %s analysing \n ------ \n" % (sub_dico))

            all_lbl = import_lbl[sub_dico]["lbl"][0]
            all_lbl_std = import_lbl[sub_dico]["lbl_std"][0]
            valid = import_lbl[sub_dico]["catalog"]["valid"]
            kitcat_name = import_lbl[sub_dico]["mask"]

            rvm = myc.table(all_lbl)
            like_ccf = rvm.rv_subselection(
                selection=np.array(valid).astype("bool"), rv_std=all_lbl_std
            )
            rv_std = like_ccf.yerr.copy() / 1000
            like_ccf = like_ccf.y * np.ones(len(kw_table_ccf))[:, np.newaxis]
            like_ccf = like_ccf / 1000

            ccf_infos = pd.DataFrame(like_ccf.T, columns=kw_table_ccf)
            ccf_infos["jdb"] = jdb
            ccf_infos["rv_std"] = rv_std
            ccf_infos = {
                "table": ccf_infos,
                "creation_date": datetime.datetime.now().isoformat(),
                "lbl_algo": kw_dico,
            }

            try:
                file_summary_ccf[kw_dico.upper() + "_" + kitcat_name][sub_dico] = ccf_infos
            except KeyError:
                file_summary_ccf[kw_dico.upper() + "_" + kitcat_name] = {sub_dico: ccf_infos}

        myf.pickle_dump(file_summary_ccf, open(self.directory + "Analyse_ccf.p", "wb"))

    def supress_1year_old(self, liste, sub_dico="matching_mad", r_corr=0.4):

        directory = self.directory
        file_test = self.import_spectrum()
        self.import_ccf()
        self.import_table()
        jdb = np.array(self.table.jdb)
        self.table_ccf["CCF_" + self.mask_harps]["matching_1y"] = self.table_ccf[
            "CCF_" + self.mask_harps
        ][sub_dico]
        myf.pickle_dump(self.table_ccf, open(self.directory + "/Analyse_ccf.p", "wb"))

        step = file_test[sub_dico]["parameters"]["step"]
        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        supress = []
        if np.max(jdb) - np.min(jdb) < 365.25:
            print(
                Fore.YELLOW
                + " [WARNING] The baseline is not sufficient for 1 year detection, rcorr increased up to 1.1"
                + Fore.RESET
            )
            myf.make_sound("Warning")
            r_corr = 1
            liste = []
            supress = [0]

        for elem in liste:
            kw, col = elem
            lbl_matrix, dust = self.lbl_fit_sinus(
                365.25, sub_dico=sub_dico, season=0, kw_dico=kw, col=col, num_sim=1
            )

            lim = 1.5 * myf.IQ(lbl_matrix["K"]) + np.nanpercentile(lbl_matrix["K"], 75)
            val = np.array(
                lbl_matrix.loc[
                    (lbl_matrix["K"] > lim) | (lbl_matrix["r_corr"] > r_corr), "kitcat_index"
                ]
            )
            supress.append(val)
            print("Lines supressed for %s and col %.0f : %.0f" % (kw, col, len(val)))
            plt.close()
        supress = np.unique(np.hstack(supress)).astype("int")

        self.import_lbl()
        self.import_dbd()
        self.import_aba()
        self.import_wbw()

        for mat, kw_dico in zip(
            [self.wbw, self.aba, self.dbd, self.lbl], ["wbw", "aba", "dbd", "lbl"]
        ):
            kitcat = mat[sub_dico]["catalog"].copy()
            valid = kitcat["valid"].copy()
            old = sum(valid)
            valid.loc[supress] = False
            new = sum(valid)
            kitcat["valid"] = valid
            print("Number of lines suppressed of " + kw_dico + " : %.0f" % (old - new))

            mat["matching_1y"] = {
                "jdb": mat[sub_dico]["jdb"],
                kw_dico: mat[sub_dico][kw_dico],
                kw_dico + "_std": mat[sub_dico][kw_dico + "_std"],
                "catalog": kitcat,
                "mask": mat[sub_dico]["mask"],
            }

            if kw_dico == "lbl":
                myf.pickle_dump(mat, open(self.directory + "Analyse_line_by_line.p", "wb"))
                self.lbl_to_ccf(all_dico=["matching_1y"])
            elif kw_dico == "dbd":
                myf.pickle_dump(mat, open(self.directory + "Analyse_depth_by_depth.p", "wb"))
            elif kw_dico == "aba":
                myf.pickle_dump(mat, open(self.directory + "Analyse_asym_by_asym.p", "wb"))
            elif kw_dico == "wbw":
                myf.pickle_dump(mat, open(self.directory + "Analyse_width_by_width.p", "wb"))

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file["matching_1y"] = {"parameters": {"sub_dico_used": sub_dico, "step": step + 1}}
            myf.pickle_dump(file, open(j, "wb"))

        return kitcat.loc[kitcat["valid"]], self.lbl[sub_dico]["mask"]

    def lbl_create_proxy(
        self,
        criterion=["period", 36, "r_corr"],
        sub_dico="matching_mad",
        window_bin=20,
        group="uncorr",
        season=0,
        kw_dico="lbl",
        col=0,
        good_morpho=False,
    ):

        self.import_table()

        if kw_dico == "lbl":
            self.import_lbl()
            matrix = self.lbl
        elif kw_dico == "lbl_iter":
            kw_dico = "lbl"
            self.import_lbl_iter()
            matrix = self.lbl_iter
        elif kw_dico == "dbd":
            self.import_dbd()
            matrix = self.dbd
        elif kw_dico == "aba":
            self.import_aba()
            matrix = self.aba

        kitcat = matrix[sub_dico]["catalog"]

        lbl = matrix[sub_dico][kw_dico][col]
        lbl_std = matrix[sub_dico][kw_dico + "_std"][col]

        valid = np.array(kitcat["valid"])
        rv_all = myc.table(lbl)

        if criterion[0] == "period":
            period = criterion[1]
            if period > 0.5 * (np.max(self.table.jdb) - np.min(self.table.jdb)):
                print(
                    Fore.YELLOW
                    + " [WARNING] The baseline of the observations is not long enough to produce a reliable vector"
                    + Fore.RESET
                )
                myf.make_sound("Warning")

            a, b = self.lbl_fit_sinus(
                period,
                season=season,
                kw_dico=kw_dico,
                col=col,
                sub_dico=sub_dico,
                num_sim=1,
                good_morpho=good_morpho,
                radial_axis=criterion[2],
                plot_proxies=False,
                kde=False,
            )

            b = a[["r_corr", "wave", "line_depth", "weight", "kitcat_index", "K", "phi"]].copy()

            if group == "anticorr":
                plt.figure()
                h, x, dust = plt.hist(a["phi"] % np.pi, bins=36)
                phase_curve = myc.tableXY(np.diff(x) / 2 + x[0:-1], h)
                phase_curve.smooth(box_pts=5, shape="savgol")
                plt.plot(phase_curve.x, phase_curve.y, color="r")
                phi0 = phase_curve.x[np.argmax(phase_curve.y)]
                b[criterion[2]] = np.cos((b["phi"].copy() - phi0)) * b[criterion[2]].copy()

            cum = np.array(np.cumsum(b.sort_values(by=criterion[2])["weight"]))
            lim = myf.find_nearest(cum, cum[-1] / 2)[0][0]

            i1 = np.array(b.sort_values(by=criterion[2])["kitcat_index"])[0:lim]
            i2 = np.array(b.sort_values(by=criterion[2])["kitcat_index"])[lim:]
        else:
            period = 365.25
            b = kitcat.copy()
            b["kitcat_index"] = b.index
            b = b.loc[b["valid"] == True]
            cum = np.array(np.cumsum(b.sort_values(by=criterion[0])["weight"]))
            cut = criterion[1]
            lim1 = myf.find_nearest(cum, cum[-1] / cut)[0][0]
            lim2 = myf.find_nearest(cum, cum[-1] / cut * (cut - 1))[0][0]

            i1 = np.array(b.sort_values(by=criterion[0])["kitcat_index"])[0:lim1]
            i2 = np.array(b.sort_values(by=criterion[0])["kitcat_index"])[lim2:]

        alls = rv_all.rv_subselection(rv_std=lbl_std, selection=valid)
        uncorr = rv_all.rv_subselection(rv_std=lbl_std, selection=i1.astype("int"))
        corr = rv_all.rv_subselection(rv_std=lbl_std, selection=i2.astype("int"))

        uncorr.x = self.table.jdb
        corr.x = self.table.jdb

        uncorr.yerr = alls.yerr
        corr.yerr = alls.yerr

        sas = myc.tableXY(uncorr.x, corr.y - uncorr.y, corr.yerr)

        sas_smooth = myc.tableXY(uncorr.x, corr.y - uncorr.y, corr.yerr)
        sas_smooth.modulo(period)
        if window_bin:
            sas_smooth.mod.substract_bin(window_bin, periodic=True, Draw=False)
            sas_smooth.mod.y = sas_smooth.mod.sub_model
        sas_smooth.demodulo()

        plt.figure(figsize=(15, 12))
        plt.subplots_adjust(hspace=0.35)
        plt.subplot(4, 1, 1)
        uncorr.plot(label="uncorr", color="b")
        corr.plot(label="corr", color="r")
        ax = plt.gca()
        plt.ylabel("RV [m/s]")
        plt.legend()

        plt.subplot(4, 1, 2, sharex=ax)
        sas.plot(label="SAS")
        if window_bin:
            sas_smooth.demod.plot(color="b", label="smoothed SAS")
        plt.xlabel("Time [days]")
        plt.ylabel("Corr - uncorr [m/s]")
        plt.legend()

        plt.subplot(4, 1, 3)
        sas.plot(modulo=period, periodic=True)
        if window_bin:
            sas_smooth.demod.plot(
                color="b", modulo=period, periodic=True, zorder=2, cmap="Greys", alpha=0
            )

        plt.xlabel("Time %% %.2f [days]" % (period))
        plt.ylabel("Corr - uncorr [m/s]")

        plt.subplot(4, 1, 4)
        sas.periodogram(nb_perm=1, p_min=0.7)
        if period == 365.25:
            for j in range(1, 6):
                plt.axvline(x=365.25 / j, ls=":", color="k")

        self.sas = sas
        self.sas_smooth = sas_smooth.demod

        return sas, sas_smooth.demod

    def lbl_planet(
        self,
        sub_dico="matching_planet",
        kw_dico="lbl_iter",
        photon_noise=0.0,
        periods=None,
        ext="",
    ):
        if kw_dico == "lbl":
            self.import_lbl()
            imported_lbl_std = self.lbl[sub_dico]["lbl_std"][0].copy()
            kitcat = self.lbl[sub_dico]["catalog"].copy()
            amp = self.lbl[sub_dico]["base_vec"][1:]

        elif kw_dico == "lbl_iter":
            self.import_lbl_iter()
            imported_lbl_std = self.lbl_iter[sub_dico]["lbl_std"][0].copy()
            kitcat = self.lbl_iter[sub_dico]["catalog"].copy()
            amp = self.lbl_iter[sub_dico]["base_vec"][1:]

        amp = 0.5 * (np.max(amp, axis=1) - np.min(amp, axis=1))
        lbl_std = np.median(imported_lbl_std, axis=1)
        lbl_std = np.sqrt(lbl_std**2 + photon_noise**2)
        wave = np.array(kitcat["wave"])

        if periods is None:
            periods = int(
                sum(np.array([len(i.split("s_proxy")) == 2 for i in list(kitcat.keys())])) / 2
            )
            periods = np.arange(1, 1 + periods)
        nb_planet = len(periods)

        gamma = []
        gamma_std = []
        for i in range(1, 1 + nb_planet):
            coeff = np.array(kitcat["s_proxy_%.0f" % (i)])
            coeff_std = np.array(kitcat["s_proxy_%.0f_std" % (i)])

            gamma.append(coeff)
            gamma_std.append(coeff_std)

        gamma = np.array(gamma)
        gamma_std = np.array(gamma_std)
        gamma_weight = 1 / gamma_std**2

        kappa = []
        kappa_white = []
        for i in range(0, nb_planet):
            test = myc.tableXY(wave, gamma[i], gamma_std[i])
            test.supress_nan()
            kappa.append(test.hist_weighted())
            test.y *= 0
            # test.yerr*=1.33
            kappa_white.append(test.hist_weighted())

        kappa = np.array(kappa)
        kappa_white = np.array(kappa_white)

        kappa /= np.mean(kappa, axis=1)[:, np.newaxis]
        kappa *= amp[:, np.newaxis]

        kappa_white *= amp[:, np.newaxis]
        reference = 0.5 * (
            np.percentile(kappa_white, 75, axis=1) - np.percentile(kappa_white, 25, axis=1)
        )

        limite = myf.IQ(kappa) * 4

        kappa2 = kappa - np.mean(kappa, axis=1)[:, np.newaxis]
        kappa2 /= reference[:, np.newaxis]

        delta = (
            0.5 * (np.percentile(kappa2, 75, axis=1) - np.percentile(kappa2, 25, axis=1)) - 1
        ) * 100

        curve_hist = []
        curve_hist_white = []
        for i in range(0, nb_planet):
            a, b, c = plt.hist(kappa[i], bins=np.linspace(-limite, limite, 100), density=True)
            test = myc.tableXY(b[0:-1] + 0.5 * np.diff(b), a)
            test.interpolate(
                new_grid=np.linspace(-limite, limite, 1000), method="cubic", replace=False
            )
            curve_hist.append(test.y_interp)
            a, b, c = plt.hist(
                kappa_white[i], bins=np.linspace(-limite, limite, 100), density=True
            )
            test = myc.tableXY(b[0:-1] + 0.5 * np.diff(b), a)
            test.interpolate(
                new_grid=np.linspace(-limite, limite, 1000), method="cubic", replace=False
            )
            curve_hist_white.append(test.y_interp)
        plt.close()

        plt.figure()
        plt.boxplot(kappa2.T, showcaps=False, showfliers=False, whis=0)
        plt.xticks(
            ticks=np.arange(1, 1 + nb_planet),
            labels=[
                "%s days \n ($\Delta$ = %.0f %%)" % (myf.format_number(i, digit=2), j)
                for i, j in zip(periods, delta)
            ],
        )
        plt.axhline(y=1, color="k", ls=":")
        plt.axhline(y=-1, color="k", ls=":")
        plt.axhline(y=2, color="r", ls="-", alpha=0.5)
        plt.axhline(y=-2, color="r", ls="-", alpha=0.5)
        plt.ylabel(r"$\Delta = \frac{\kappa_{i,j} - <\kappa_j>}{<\epsilon>}$", fontsize=16)
        plt.ylim(-3.5, 3.5)
        plt.subplots_adjust(left=0.15, bottom=0.13, top=0.97)
        plt.savefig(self.dir_root + "KEPLERIAN/Signals_elongations%s.pdf" % (ext))

        if False:
            plt.figure()
            curve_hist = np.array(curve_hist)
            curve_hist_white = np.array(curve_hist_white)
            for i in range(0, nb_planet):
                plt.subplot(nb_planet, 1, i + 1)
                plt.plot(np.linspace(-limite, limite, 1000), curve_hist[i])
                plt.plot(
                    np.linspace(-limite, limite, 1000) + np.median(kappa[i]),
                    curve_hist_white[i],
                    color="k",
                )
            plt.ylim(0, None)

    def lbl_fit_vec(
        self,
        dico_name,
        sub_dico="matching_mad",
        kw_dico="lbl",
        col=0,
        base_vec=["CaII"],
        standardize=True,
        season=0,
        time_detrending=2,
        k_sigma_outliers=5,
        display_time_stat=True,
        add_step=0,
        substract_rv=None,
        save_database=False,
    ):

        self.import_table()
        self.import_ccf()
        file_test = self.import_spectrum()
        step = file_test[sub_dico]["parameters"]["step"]

        files = glob.glob(self.directory + "RASSI*.p")
        files = np.sort(files)

        table = self.table
        jdb = np.array(table["jdb"])
        snr = np.array(table["snr"])

        if substract_rv is None:
            substract_rv = np.zeros(len(table["jdb"]))

        try:
            kep_model = self.model_keplerian_fitted[list(self.model_keplerian_fitted.keys())[-1]]
            if np.sum(abs(kep_model)):
                print(
                    "\n [INFO] A Keplerian model has been found from the mask %s and will be subtracted"
                    % (list(self.model_keplerian_fitted.keys())[-1])
                )
                substract_rv = kep_model.copy()
        except IndexError:
            print("\n [INFO] Keplerians models not found")

        seasons = myf.detect_obs_season(jdb, min_gap=jdb.max() - jdb.min())
        j = 0
        if season:
            seasons = myf.detect_obs_season(jdb, min_gap=80)
            season -= 1
            j = season
        else:
            season = 0

        jdb = jdb[seasons[j, 0] : seasons[j, 1] + 1]
        snr = snr[seasons[j, 0] : seasons[j, 1] + 1]
        substract_rv = substract_rv[seasons[j, 0] : seasons[j, 1] + 1]

        jdb_m = np.array(jdb - np.mean(jdb))
        jdb_m = jdb_m / np.std(jdb_m)

        kw2 = kw_dico
        if kw_dico == "lbl":
            self.import_lbl()
            imported_lbl = self.lbl[sub_dico]["lbl"].copy()
            imported_lbl_std = self.lbl[sub_dico]["lbl_std"].copy()
            lbl = imported_lbl[col]
            lbl_std = imported_lbl_std[col]
            kitcat = self.lbl[sub_dico]["catalog"].copy()
            kitcat_name = self.lbl[sub_dico]["mask"]

        elif kw_dico == "lbl_iter":
            self.import_lbl_iter()
            imported_lbl = self.lbl_iter[sub_dico]["lbl"].copy()
            imported_lbl_std = self.lbl_iter[sub_dico]["lbl_std"].copy()
            lbl = imported_lbl[col]
            lbl_std = imported_lbl_std[col]
            kitcat = self.lbl_iter[sub_dico]["catalog"].copy()
            kitcat_name = self.lbl_iter[sub_dico]["mask"]
            kw2 = "lbl"

        elif kw_dico == "dbd":
            self.import_dbd()
            imported_lbl = self.dbd[sub_dico]["dbd"].copy()
            imported_lbl_std = self.dbd[sub_dico]["dbd_std"].copy()
            lbl = imported_lbl[col]
            lbl_std = imported_lbl_std[col]
            kitcat = self.dbd[sub_dico]["catalog"].copy()
            kitcat_name = self.dbd[sub_dico]["mask"]

        elif kw_dico == "wbw":
            self.import_wbw()
            imported_lbl = self.wbw[sub_dico]["wbw"].copy()
            imported_lbl_std = self.wbw[sub_dico]["wbw_std"].copy()
            lbl = imported_lbl[col]
            lbl_std = imported_lbl_std[col]
            kitcat = self.wbw[sub_dico]["catalog"].copy()
            kitcat_name = self.wbw[sub_dico]["mask"]

        kitcat_keys = list(kitcat.keys())
        dust, keys_to_rm = myf.string_contained_in(kitcat_keys, "_proxy_")
        for k in keys_to_rm:
            del kitcat[k]

        valid = (kitcat["valid"]).astype("bool")
        old_index = np.arange(len(valid))[valid]

        new_lbl = lbl.copy()
        new_lbl_std = lbl_std.copy()

        lbl = lbl[valid, seasons[j, 0] : seasons[j, 1] + 1]
        lbl_std = lbl_std[valid, seasons[j, 0] : seasons[j, 1] + 1]

        max_std = np.max(lbl_std) * 10
        new_lbl_std *= max_std
        mad = myf.mad(lbl, axis=1)
        med = np.median(lbl, axis=1)
        median_matrix = np.ones(len(jdb)) * med[:, np.newaxis]
        max_std_matrix = max_std * np.ones(np.shape(lbl))
        sup_matrix = k_sigma_outliers * np.ones(len(jdb)) * mad[:, np.newaxis]

        outliers = (abs(lbl - median_matrix) - lbl_std) > sup_matrix
        if False:
            plt.figure(figsize=(10, 5))
            plt.subplot(1, 2, 1)
            plt.plot(np.sum(outliers, axis=0))
            plt.ylabel("Nb outliers", fontsize=13)
            plt.xlabel("Time index", fontsize=13)
            plt.subplot(1, 2, 2)
            plt.plot(np.sum(outliers, axis=1))
            plt.ylabel("Nb outliers", fontsize=13)
            plt.xlabel("Line index", fontsize=13)

        lbl_std[outliers] = max_std_matrix[outliers]
        lbl[outliers] = median_matrix[outliers]

        if type(base_vec) == str:
            base_vec = np.array(table[base_vec][seasons[j, 0] : seasons[j, 1] + 1])
            names = [base_vec]

        elif type(base_vec) == list:
            vec = []
            names = []
            counter = 1
            for n in base_vec:
                if type(n) == str:
                    names.append(n)
                    p = myc.tableXY(jdb_m, np.array(table[n][seasons[j, 0] : seasons[j, 1] + 1]))
                    p.substract_polyfit(time_detrending)
                    vec.append(p.detrend_poly.y)
                else:
                    names.append("proxy_" + str(counter))
                    p = myc.tableXY(jdb_m, np.array(n[seasons[j, 0] : seasons[j, 1] + 1]))
                    p.substract_polyfit(time_detrending)
                    vec.append(p.detrend_poly.y)
                    counter += 1
            base_vec = np.array(vec)
        else:
            names = ["proxy_" + str(j) for j in range(len(base_vec))]

        if type(base_vec) == np.ndarray:
            if np.shape(base_vec)[0] == len(jdb_m):
                print(
                    Fore.YELLOW + " [WARNING] Your vectors basis has been transposed" + Fore.RESET
                )
                myf.make_sound("Warning")

                base_vec = base_vec.T
            if np.shape(base_vec)[1] != len(jdb_m):
                print(
                    Fore.YELLOW
                    + " [WARNING] Your vectors basis has not the correct shape"
                    + Fore.RESET
                )
                myf.make_sound("Warning")

        time_vec = np.array([jdb_m**p for p in np.arange(time_detrending + 1)])
        names = ["time" + str(j) for j in range(time_detrending + 1)] + names

        base_vec = np.vstack([time_vec, base_vec])
        if standardize:
            base_vec[1:] = (
                base_vec[1:] / np.std(base_vec[1:], axis=1)[:, np.newaxis]
            )  # standardize vec fitted

        table_to_fit = myc.table(lbl - substract_rv)
        table_to_fit.rms_w(1 / lbl_std**2, axis=1)
        table_to_fit.fit_base(base_vec, weight=1 / lbl_std**2)

        new_lbl[valid, seasons[j, 0] : seasons[j, 1] + 1] = (
            table_to_fit.vec_residues + substract_rv
        )
        new_lbl_std[valid, seasons[j, 0] : seasons[j, 1] + 1] = lbl_std

        uncorrected = myc.table(table_to_fit.table + substract_rv)
        corrected = myc.table(table_to_fit.vec_residues + substract_rv)

        old = uncorrected.rv_subselection(rv_std=lbl_std)
        new = corrected.rv_subselection(rv_std=lbl_std)
        new.rms_w()
        old.rms_w()
        print("\nRMS : old (%.2f m/s) vs new (%.2f m/s)" % (old.rms, new.rms))
        old.x = jdb
        new.x = jdb
        plt.figure(figsize=(15, 8))
        plt.subplot(2, 1, 1)
        old.plot(color="k", label=("Unfitted (rms : %.2f)" % (old.rms)))
        new.plot(color="r", label=("Fitted (rms : %.2f)" % (new.rms)))
        plt.legend()
        plt.title("Correction : %s" % (dico_name))
        plt.xlabel("Time", fontsize=13)
        plt.ylabel("RV [m/s]", fontsize=13)
        plt.subplot(2, 1, 2)
        old.periodogram(nb_perm=1, Norm=True, color="k")
        new.periodogram(nb_perm=1, Norm=True, color="r")

        matrix_r = (
            table_to_fit.coeff_fitted[:, 1:] / table_to_fit.rms[:, np.newaxis]
        )  # exclude fit offset of r matrix
        matrix_s = table_to_fit.coeff_fitted[:, 1:]  # exclude fit offset of r matrix
        matrix_s_std = table_to_fit.coeff_fitted_std[:, 1:]  # exclude fit offset of r matrix

        names = names[1:]

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)
        fig.set_size_inches(18.5, 10.5)

        index = range(len(matrix_r[0]))
        if not display_time_stat:
            index = index[time_detrending:]
        for j in index:
            ax1.hist(
                matrix_r[:, j],
                bins=np.arange(-1, 1, 0.01),
                label=names[j],
                alpha=0.8,
                histtype="step",
            )
            ax1.set_xlim(-1.01, 1.01)
            ax1.set_title("Season : %.0f    Sub dico : %s" % (season, sub_dico))
            ax1.axvline(x=0, color="k", alpha=0.3)
            ax1.set_xlabel(r"$\mathcal{R}_{pearson}$", fontsize=13)
            ax3.hist(
                matrix_r[:, j],
                bins=np.arange(-1, 1, 0.01),
                label=names[j],
                alpha=0.8,
                histtype="step",
                cumulative=-1,
                density=True,
            )
            ax3.set_xlim(-1.01, 1.01)
            ax3.set_xlabel(r"$\mathcal{R}_{pearson}$", fontsize=13)
            ax3.axvline(x=0, color="k", ls=":")
            ax3.axhline(y=0.5, color="k", ls=":")
            ax2.hist(
                abs(matrix_r[:, j]),
                bins=np.arange(0, 1, 0.01),
                label=names[j],
                alpha=0.8,
                histtype="step",
            )
            ax2.set_xlim(-0.01, 1.01)
            ax2.set_xlabel(r"$|\mathcal{R}_{pearson}|$", fontsize=13)
            ax4.hist(
                abs(matrix_r[:, j]),
                bins=np.arange(0, 1, 0.01),
                label=names[j],
                alpha=0.8,
                histtype="step",
                cumulative=-1,
                density=True,
            )
            ax4.set_xlim(-0.01, 1.01)
            ax4.set_ylim(0.01, 1.001)
            ax4.set_yscale("log")
            ax4.set_xlabel(r"$|\mathcal{R}_{pearson}|$", fontsize=13)
        ax1.legend()
        ax2.legend()
        ax3.legend()
        ax4.legend()

        plt.savefig(self.dir_root + "IMAGES/LBL_vec_fitted_%s.png" % (sub_dico))

        r_names = ["r_%s" % (j) for j in names]
        s_names = ["s_%s" % (j) for j in names]

        matrix = pd.DataFrame(matrix_r, columns=r_names, index=old_index)
        matrix_std = pd.DataFrame(
            matrix_r * 0, columns=r_names, index=old_index
        )  # null uncertainties for the moment

        matrix_abs = pd.DataFrame(abs(matrix_r), columns=r_names, index=old_index)
        print(
            "\nSTATISTIC TABLE ABS R\n",
            matrix_abs.describe().sort_values(by="75%", axis=1).loc[["25%", "50%", "75%"]],
        )

        matrix_s = pd.DataFrame(matrix_s, columns=s_names, index=old_index)
        matrix_s_std = pd.DataFrame(matrix_s_std, columns=s_names, index=old_index)

        for n in s_names:
            kitcat[n] = matrix_s[n]
            kitcat[n + "_std"] = matrix_s_std[n]
        for n in r_names:
            kitcat[n] = matrix[n]
            kitcat[n + "_std"] = matrix_std[n]

        if kw_dico == "lbl":
            self.matrix_corr_lbl = kitcat.copy()
        if kw_dico == "lbl_iter":
            self.matrix_corr_lbl = kitcat.copy()
        if kw_dico == "dbd":
            self.matrix_corr_dbd = kitcat.copy()
        if kw_dico == "wbw":
            self.matrix_corr_wbw = kitcat.copy()

        ccf_rv = myc.table(new_lbl)
        ccf = ccf_rv.rv_subselection(
            rv_std=np.array(imported_lbl_std)[0], selection=np.array(kitcat["valid"])
        )
        ccf_rv = ccf.y

        save = {
            "jdb": jdb,
            kw2: np.array([new_lbl, new_lbl - ccf_rv, imported_lbl[2]]),
            kw2 + "_std": imported_lbl_std,
            "catalog": kitcat,
            "mask": kitcat_name,
            "snr_med": np.nanmedian(table["snr"]),
            "base_vec": base_vec,
            "sub_dico_used": sub_dico,
            "date": datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S"),
        }

        if kw_dico == "lbl":
            self.lbl[dico_name] = save
            myf.pickle_dump(self.lbl, open(self.directory + "Analyse_line_by_line.p", "wb"))

        elif kw_dico == "lbl_iter":
            self.lbl_iter[dico_name] = save
            myf.pickle_dump(
                self.lbl_iter, open(self.directory + "Analyse_line_by_line_iter.p", "wb")
            )

        elif kw_dico == "dbd":
            self.dbd[dico_name] = save
            myf.pickle_dump(self.dbd, open(self.directory + "Analyse_depth_by_depth.p", "wb"))

        elif kw_dico == "wbw":
            self.wbw[dico_name] = save
            myf.pickle_dump(self.wbw, open(self.directory + "Analyse_width_by_width.p", "wb"))

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file[dico_name] = {
                "parameters": {"sub_dico_used": sub_dico, "step": step + 1 + add_step}
            }
            myf.pickle_dump(file, open(j, "wb"))

        if (save_database) & ((kw_dico == "lbl_iter") | (kw_dico == "lbl")):
            database = pd.read_pickle(
                self.dir_root.split(self.starname)[0]
                + "/database/%s/%s_pca_vec_fitted.p" % (self.instrument, self.instrument)
            )
            try:
                database[dico_name][self.starname] = save
                myf.pickle_dump(
                    database,
                    open(
                        self.dir_root.split(self.starname)[0]
                        + "/database/%s/%s_pca_vec_fitted.p" % (self.instrument, self.instrument),
                        "wb",
                    ),
                )
            except:
                try:
                    database[dico_name] = {self.starname: save}
                    myf.pickle_dump(
                        database,
                        open(
                            self.dir_root.split(self.starname)[0]
                            + "/database/%s/%s_pca_vec_fitted.p"
                            % (self.instrument, self.instrument),
                            "wb",
                        ),
                    )
                except:
                    error = 1

        if (kw_dico == "lbl") | (kw_dico == "lbl_iter"):
            self.lbl_to_ccf(all_dico=[dico_name], kw_dico=kw_dico)

        plt.show(block=False)

    def lbl_fit_base(
        self,
        dico_name,
        sub_dico="matching_diff",
        kw_dico="lbl",
        proxies=["CaII", "dbd_0"],
        time_detrending=0,
        add_step=0,
        k_sigma_outliers=5,
        substract_rv=None,
    ):

        code_name = []
        for p in proxies:
            if type(p) == str:
                code_name.append(p.split("_")[0])
        code_name = np.unique(code_name)

        self.import_lbl()
        self.import_lbl_iter()
        if "dbd" in code_name:
            self.import_dbd()
        if "aba" in code_name:
            self.import_aba()
        if "bt" in code_name:
            self.import_bt()
        if "wbw" in code_name:
            self.import_wbw()
        self.import_table()

        file_test = self.import_spectrum()
        step = file_test[sub_dico]["parameters"]["step"]

        files = glob.glob(self.directory + "RASSI*.p")
        files = np.sort(files)

        jdb = np.array(self.table["jdb"])
        jdb_m = jdb - np.mean(jdb)

        if substract_rv is None:
            substract_rv = np.zeros(len(jdb))

        try:
            kep_model = self.model_keplerian_fitted[list(self.model_keplerian_fitted.keys())[-1]]
            if np.sum(abs(kep_model)):
                print(
                    "\n [INFO] A Keplerian model has been found from the mask %s and will be subtracted"
                    % (list(self.model_keplerian_fitted.keys())[-1])
                )
                substract_rv = kep_model.copy()
        except AttributeError:
            print("\n [INFO] Keplerians models not found for sub dico %s" % (sub_dico))

        if kw_dico == "lbl":
            imported_lbl = self.lbl[sub_dico]
            lbl = self.lbl[sub_dico]["lbl"][0] - substract_rv
            lbl_std = self.lbl[sub_dico]["lbl_std"][0]
            kitcat = self.lbl[sub_dico]["catalog"].copy()
            kitcat_name = self.lbl[sub_dico]["mask"]
        elif kw_dico == "lbl_iter":
            imported_lbl = self.lbl_iter[sub_dico]
            lbl = self.lbl_iter[sub_dico]["lbl"][0] - substract_rv
            lbl_std = self.lbl_iter[sub_dico]["lbl_std"][0]
            kitcat = self.lbl_iter[sub_dico]["catalog"].copy()
            kitcat_name = self.lbl_iter[sub_dico]["mask"]

        valid = np.array(kitcat["valid"])
        lbl_matrix = lbl.copy()[valid]
        lbl_matrix_std = lbl_std.copy()[valid]

        max_std = np.max(lbl_matrix_std) * 10
        mad = myf.mad(lbl_matrix, axis=1)
        med = np.median(lbl_matrix, axis=1)
        median_matrix = np.ones(len(jdb)) * med[:, np.newaxis]
        max_std_matrix = max_std * np.ones(np.shape(lbl_matrix))
        sup_matrix = k_sigma_outliers * np.ones(len(jdb)) * mad[:, np.newaxis]

        outliers = (abs(lbl_matrix - median_matrix) - lbl_matrix_std) > sup_matrix
        if False:
            plt.figure(figsize=(10, 5))
            plt.subplot(1, 2, 1)
            plt.plot(np.sum(outliers, axis=0))
            plt.ylabel("Nb outliers", fontsize=13)
            plt.xlabel("Time index", fontsize=13)
            plt.subplot(1, 2, 2)
            plt.plot(np.sum(outliers, axis=1))
            plt.ylabel("Nb outliers", fontsize=13)
            plt.xlabel("Line index", fontsize=13)

        lbl_matrix_std[outliers] = max_std_matrix[outliers]
        lbl_matrix[outliers] = median_matrix[outliers]

        base_vec = [
            (jdb_m**k) * np.ones(len(lbl_matrix))[:, np.newaxis]
            for k in range(1 + time_detrending)
        ]

        for prox in proxies:
            if type(prox) == str:
                if len(prox.split("_")) == 2:
                    if len(prox.split("_")[1]) == 1:
                        p1 = prox.split("_")[0]
                        p2 = int(prox.split("_")[1])
                        if p1 == "dbd":
                            vec = self.dbd[sub_dico][p1][p2][valid]
                        elif p1 == "lbl":
                            vec = self.lbl[sub_dico][p1][p2][valid]
                        elif p1 == "aba":
                            vec = self.aba[sub_dico][p1][p2][valid]
                        elif p1 == "bt":
                            vec = self.bt[sub_dico][p1][p2][valid]
                        elif p1 == "wbw":
                            vec = self.wbw[sub_dico][p1][p2][valid]
                        vec = (vec - np.mean(vec)) / np.std(vec)
                    else:
                        vec = np.array(self.table[prox])
                        vec = vec * np.ones(len(lbl_matrix))[:, np.newaxis]
                else:
                    vec = np.array(self.table[prox])
                    vec = vec * np.ones(len(lbl_matrix))[:, np.newaxis]
            else:
                vec = prox * np.ones(len(lbl_matrix))[:, np.newaxis]

            base_vec.append(vec)
        base_vec = np.array(base_vec)

        new_lbl = lbl.copy()
        index = np.arange(len(lbl))[valid]
        crash = []
        for line in tqdm(range(len(lbl_matrix))):
            rvi = myc.tableXY(jdb, lbl_matrix[line], lbl_matrix_std[line])
            vectors = base_vec[:, line, :]
            try:
                rvi.fit_base(vectors)
                new_lbl[index[line], :] = lbl[index[line]] - rvi.vec_fitted.y
            except:
                crash.append(index[line])

        crash = np.array(crash)
        lbl_new = myc.table(new_lbl + substract_rv)
        lbl_new.rms_w(1 / lbl_std**2)

        kitcat["rms"] = lbl_new.rms
        kitcat["med_err"] = np.median(lbl_std, axis=1)
        kitcat["quality"] = kitcat["rms"] / kitcat["med_err"]
        rms = kitcat.loc[kitcat["valid"] == 1, "rms"]
        quality = kitcat.loc[kitcat["valid"] == 1, "quality"]
        rms_lim = np.nanpercentile(rms, 75) + 1.5 * myf.IQ(rms)
        qual_lim_sup = np.nanpercentile(quality, 75) + 2.5 * myf.IQ(quality)
        qual_lim_inf = np.nanpercentile(quality, 25) - 1.5 * myf.IQ(quality)
        kitcat.loc[kitcat["rms"] > rms_lim, "valid"] = False
        kitcat.loc[
            (kitcat["quality"] > qual_lim_sup) | (kitcat["quality"] < qual_lim_inf), "valid"
        ] = False

        lbl_old = myc.table(lbl + substract_rv)
        lbl_old.rms_w(1 / lbl_std**2)

        plt.figure(figsize=(8, 8))
        plt.xlabel("LBL rms old [m/s]", fontsize=13)
        plt.ylabel("Rel. diff LBL [%]", fontsize=13)
        plt.scatter(
            lbl_old.rms[valid],
            (100 * abs(lbl_old.rms - lbl_new.rms) / (lbl_old.rms + 1e-6))[valid],
        )

        old = lbl_old.rv_subselection(rv_std=lbl_std, selection=np.array(kitcat["valid"]))
        new = lbl_new.rv_subselection(rv_std=lbl_std, selection=np.array(kitcat["valid"]))
        new.rms_w()
        old.rms_w()
        print("\nRMS : old (%.2f m/s) vs new (%.2f m/s)" % (old.rms, new.rms))
        old.x = jdb
        new.x = jdb
        plt.figure(figsize=(15, 8))
        plt.subplot(2, 1, 1)
        old.plot(color="k", label=("Unfitted (rms : %.2f)" % (old.rms)))
        new.plot(color="r", label=("Fitted (rms : %.2f)" % (new.rms)))
        plt.legend()
        plt.title("Correction : %s" % (dico_name))
        plt.xlabel("Time", fontsize=13)
        plt.ylabel("RV [m/s]", fontsize=13)
        plt.subplot(2, 1, 2)
        old.periodogram(nb_perm=1, Norm=True, color="k")
        new.periodogram(nb_perm=1, Norm=True, color="r")

        save = {
            "jdb": jdb,
            "lbl": np.array([lbl_new.table, lbl_new.table - new.y, imported_lbl["lbl"][2]]),
            "lbl_std": imported_lbl["lbl_std"],
            "catalog": kitcat,
            "mask": kitcat_name,
        }

        if kw_dico == "lbl":
            self.lbl[dico_name] = save
            myf.pickle_dump(self.lbl, open(self.directory + "Analyse_line_by_line.p", "wb"))

        elif kw_dico == "lbl_iter":
            self.lbl_iter[dico_name] = save
            myf.pickle_dump(
                self.lbl_iter, open(self.directory + "Analyse_line_by_line_iter.p", "wb")
            )

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            file[dico_name] = {
                "parameters": {"sub_dico_used": sub_dico, "step": step + 1 + add_step}
            }
            myf.pickle_dump(file, open(j, "wb"))

        self.lbl_to_ccf(all_dico=[dico_name], kw_dico=kw_dico)

    def planet_fit_base(
        self,
        proxies,
        time_detrending=0,
        nb_phi=18,
        k=np.arange(1, 3.01, 0.1),
        p=5000,
        substract_rv=True,
    ):

        self.import_table()
        mask1 = self.yarara_get_best_mask(sub_dico="matching_mad", poly_deg=2)
        vec_lbl = self.import_ccf_timeseries(mask1, "matching_mad", "rv")

        jdb = np.array(self.table["jdb"])
        jdb_m = jdb - np.mean(jdb)

        if substract_rv:  # no effect of sinus amplitude if no other signal
            k = k[0:2]

        if type(p) == int:
            p = 1 / np.linspace(1 / (np.max(jdb) - np.min(jdb)), 1 / 3, p)

        print("Size of the simulation grid = %.0f" % (nb_phi * len(k) * len(p)))

        phi = np.linspace(0, 2 * np.pi, nb_phi + 1)[:-1]
        lbl_matrix = (
            np.sin(2 * np.pi * jdb_m / p[:, np.newaxis] + phi[:, np.newaxis][:, np.newaxis])
            * k[:, np.newaxis][:, np.newaxis][:, np.newaxis]
        )
        lbl_matrix = np.vstack(lbl_matrix)
        lbl_matrix = np.vstack(lbl_matrix)

        if not substract_rv:
            lbl_matrix += vec_lbl.y

        liste = []
        for ki in k:
            for phii in phi:
                for pi in p:
                    liste.append([ki, phii, pi])
        liste = np.array(liste)

        base_vec = [
            (jdb_m**k) * np.ones(len(lbl_matrix))[:, np.newaxis]
            for k in range(1 + time_detrending)
        ]

        for j in range(len(proxies)):
            prox = myc.tableXY(jdb_m, proxies[j])
            prox.substract_polyfit(time_detrending, replace=True)
            proxies[j] = prox.y

        for prox in proxies:
            if type(prox) == str:
                vec = np.array(self.table[prox])
                vec = vec * np.ones(len(lbl_matrix))[:, np.newaxis]
            else:
                vec = prox * np.ones(len(lbl_matrix))[:, np.newaxis]

            base_vec.append(vec)
        base_vec = np.array(base_vec)

        sin_vec = (
            np.sin(
                2 * np.pi * jdb_m / p[:, np.newaxis]
                + np.zeros(len(phi))[:, np.newaxis][:, np.newaxis]
            )
            * np.ones(len(k))[:, np.newaxis][:, np.newaxis][:, np.newaxis]
        )
        sin_vec = np.vstack(sin_vec)
        sin_vec = np.vstack(sin_vec)

        cos_vec = (
            np.cos(
                2 * np.pi * jdb_m / p[:, np.newaxis]
                + np.zeros(len(phi))[:, np.newaxis][:, np.newaxis]
            )
            * np.ones(len(k))[:, np.newaxis][:, np.newaxis][:, np.newaxis]
        )
        cos_vec = np.vstack(cos_vec)
        cos_vec = np.vstack(cos_vec)

        offset_vec = np.ones(np.shape(sin_vec))

        base_sin = np.array([offset_vec, sin_vec, cos_vec])

        simu = myc.table(lbl_matrix)
        rv_std = vec_lbl.yerr * np.ones(len(lbl_matrix))[:, np.newaxis]
        simu.fit_base(base_vec, weight=1 / rv_std**2)

        vec_fitted = np.dot(simu.coeff_fitted, base_vec[:, 0, :])
        vec_residues = lbl_matrix - vec_fitted

        # vec_fitted2 = np.sum((simu.coeff_fitted.T)[:,:,np.newaxis]*np.ones(len(jdb_m))*base_vec,axis=0) if the basis is different for each vector

        simu_back = myc.table(vec_residues)
        simu_back.fit_base(base_sin, weight=1 / rv_std**2)

        k_back = np.sqrt(simu_back.coeff_fitted[:, 1] ** 2 + simu_back.coeff_fitted[:, 2] ** 2)
        phi_back = np.arctan2(simu_back.coeff_fitted[:, 2], simu_back.coeff_fitted[:, 1])

        liste_back = np.array([k_back, phi_back, liste[:, 0], liste[:, 1], liste[:, 2]]).T

        self.debug = liste_back, p, nb_phi, k

        liste_mean = []
        liste_std = []
        liste_sup = []
        liste_inf = []
        for i in range(len(p)):
            for j in range(len(k)):
                val = np.mean(liste_back[i :: len(p)][j * nb_phi : nb_phi * (j + 1)], axis=0)
                liste_mean.append(val)
                val = np.std(liste_back[i :: len(p)][j * nb_phi : nb_phi * (j + 1)], axis=0)
                liste_std.append(val)
                val = np.percentile(
                    liste_back[i :: len(p)][j * nb_phi : nb_phi * (j + 1)], 84, axis=0
                )
                liste_sup.append(val)
                val = np.percentile(
                    liste_back[i :: len(p)][j * nb_phi : nb_phi * (j + 1)], 16, axis=0
                )
                liste_inf.append(val)

        liste_mean = np.array(liste_mean)
        liste_std = np.array(liste_std)
        liste_sup = np.array(liste_sup)  # minimum absoprtion
        liste_inf = np.array(liste_inf)  # maximum absorption

        liste_mean = np.hstack(
            [liste_mean, (liste_mean[:, 2] - liste_mean[:, 0])[:, np.newaxis]]
        )  # amplitude difference
        liste_mean = np.hstack(
            [liste_mean, (100 * liste_mean[:, -1] / liste_mean[:, 2])[:, np.newaxis]]
        )  # relative amplitude difference

        liste_sup = np.hstack(
            [liste_sup, abs(liste_mean[:, 2] - liste_sup[:, 0])[:, np.newaxis]]
        )  # amplitude difference
        liste_sup = np.hstack(
            [liste_sup, (100 * liste_sup[:, -1] / liste_mean[:, 2])[:, np.newaxis]]
        )  # relative amplitude difference

        liste_inf = np.hstack(
            [liste_sup, abs(liste_mean[:, 2] - liste_inf[:, 0])[:, np.newaxis]]
        )  # amplitude difference
        liste_inf = np.hstack(
            [liste_inf, (100 * liste_inf[:, -1] / liste_mean[:, 2])[:, np.newaxis]]
        )  # relative amplitude difference

        # flat_p = liste_mean[:,4]
        # flat_k = liste_mean[:,2]

        # grid_p = np.reshape(flat_p,(len(p),len(k))).T
        # grid_k = np.reshape(flat_k,(len(p),len(k))).T
        grid_z = np.reshape(liste_mean[:, -1], (len(p), len(k))).T
        grid_z_std = np.reshape(liste_std[:, -1], (len(p), len(k))).T
        grid_z_sup = np.reshape(liste_sup[:, -1], (len(p), len(k))).T
        grid_z_inf = np.reshape(liste_inf[:, -1], (len(p), len(k))).T

        proj = np.mean(grid_z, axis=0)
        proj_std = np.mean(grid_z_std, axis=0)
        proj_sup = np.mean(grid_z_sup, axis=0)
        proj_inf = np.mean(grid_z_inf, axis=0)

        # plt.axes([0.1,0.1,0.8,0.3])
        # plt.xscale('log')
        # plt.plot(p,proj,color='k')
        # plt.ylabel(r'$\Delta$ K [%]',fontsize=14)
        # plt.xlabel('Period [days]',fontsize=14)

        return myc.tableXY(p, proj, proj_inf - proj, proj - proj_sup)

    def planet_simu_absorption(self, planet=None, oversampling=1, sub_dico=None):
        output_file = {}

        simus = myf.touch_pickle(self.dir_root + "KEPLERIAN/Simulation_absorption.p")

        simu_saved = self.simu

        for i in simu_saved.keys():
            simus[i] = {
                "period": simu_saved[i].x,
                "absorption": simu_saved[i].y,
                "sup": simu_saved[i].yerr,
                "inf": simu_saved[i].xerr,
            }
        myf.pickle_dump(simus, open(self.dir_root + "KEPLERIAN/Simulation_absorption.p", "wb"))

        simu = {
            l: myc.tableXY(
                simus[l]["period"], simus[l]["absorption"], simus[l]["sup"], simus[l]["inf"]
            )
            for l in list(simus.keys())
        }

        if sub_dico is not None:
            simu = {sub_dico: simu[sub_dico]}

        plt.figure(figsize=(18, 4.5))

        for i in simu.keys():
            simu_perio = simu[i].copy()
            if oversampling != 1:
                simu_perio.inv()
                simu_perio.interpolate(new_grid=oversampling, replace=True)
                simu_perio.inv()

            simu_perio.plot(color=None, label=i, ls="-")
            plt.xlim(np.min(simu_perio.x), np.max(simu_perio.x))

        if planet is not None:
            for j in planet:
                plt.axvline(x=j, color="k", alpha=0.5, ls="-")
                if i == "bis":
                    absorption = simu[i].y[myf.find_nearest(simu[i].x, j)[0]]
                    plt.text(
                        j * 0.95,
                        95,
                        str(int(absorption)) + "%",
                        fontsize=14,
                        ha="right",
                        color="r",
                    )

        plt.ylim(0, 100)
        # plt.axhline(y=50,color='k',ls='-',alpha=1,label ='50%')
        # plt.axhline(y=25,color='k',ls='-.',alpha=1,label ='25%')
        # plt.axhline(y=10,color='k',ls=':',alpha=1,label ='10%')
        plt.legend()
        plt.xscale("log")
        plt.xlabel("Period [days]", fontsize=14)
        plt.ylabel(r"$\Delta K$ [%]", fontsize=14)
        plt.subplots_adjust(left=0.08, right=0.95, top=0.95, bottom=0.12)
        plt.tick_params(direction="in", top=True, right=True, which="both")
        plt.grid(alpha=0.2)
        plt.savefig(self.dir_root + "IMAGES/Simulation_absorption.pdf")

        colors = []
        plt.figure("dust")
        for j in range(len(simu)):
            p = plt.plot([0, 1])
            colors.append(p[0].get_color())
        plt.close("dust")

        plt.figure(figsize=(18, 2.5 * len(simu) + 2))
        c = 0
        for i in simu.keys():
            c += 1
            if not c - 1:
                plt.subplot(len(simu), 1, c)
                ax = plt.gca()
            else:
                plt.subplot(len(simu), 1, c, sharex=ax, sharey=ax)
            plt.tick_params(direction="in", top=True, right=True, which="both")

            simu_perio = simu[i].copy()
            if oversampling != 1:
                simu_perio.inv()
                simu_perio.interpolate(new_grid=oversampling, replace=True)
                simu_perio.inv()
            simu_perio.fill_between(color=colors[c - 1], label=i, borders=True)
            plt.xlim(np.min(simu_perio.x), np.max(simu_perio.x))

            if planet is not None:
                for j in planet:
                    plt.axvline(x=j, color="k", alpha=0.5)
                    loc = myf.find_nearest(simu_perio.x, j)[0][0]
                    absorption = simu_perio.y[loc]
                    abs_sup = simu_perio.yerr[loc]
                    abs_inf = simu_perio.xerr[loc]
                    plt.text(
                        j * 1.02,
                        90,
                        r""
                        + str(int(absorption))
                        + r" $\pm$ "
                        + str("%.0f" % (0.5 * (abs_sup + abs_inf)))
                        + " %",
                        fontsize=14,
                        ha="left",
                        color="k",
                    )

            plt.ylim(0, 99.9)
            # plt.axhline(y=50,color='k',ls='-',alpha=1,label ='50%')
            # plt.axhline(y=25,color='k',ls='-.',alpha=1,label ='25%')
            # plt.axhline(y=10,color='k',ls=':',alpha=1,label ='10%')
            plt.legend()
            plt.grid(alpha=0.2)
            plt.xscale("log")
            plt.xlabel("Period [days]", fontsize=14)
            plt.ylabel(r"$\Delta K$ [%]", fontsize=14)
        plt.subplots_adjust(left=0.08, right=0.95, top=0.95, bottom=0.12, hspace=0)
        plt.savefig(self.dir_root + "IMAGES/Simulation_absorption2.pdf")

    def lbl_plot(
        self,
        kw_dico="lbl",
        col=0,
        num=None,
        wave=None,
        wave_srf=None,
        recenter=True,
        plot=True,
        modulo=None,
        periodic=False,
        sub_dico=None,
        jdb_min=None,
        jdb_max=None,
    ):
        """To make the plot of a specific line on a specific dictionnary or not"""
        self.import_lbl()

        tab = self.lbl
        ext = ""
        if kw_dico == "dbd":
            self.import_dbd()
            tab = self.dbd
        elif kw_dico == "lbl_iter":
            kw_dico = "lbl"
            ext = "_iter"
            self.import_lbl_iter()
            tab = self.lbl_iter
        elif kw_dico == "aba":
            self.import_aba()
            tab = self.aba
        elif kw_dico == "wbw":
            self.import_wbw()
            tab = self.wbw
        elif kw_dico == "bt":
            self.import_bt()
            tab = self.bt
        elif kw_dico == "bbb":
            self.import_bbb()
            tab = self.bbb

        rv_sys = self.rv_sys

        self.import_table()
        jdb = np.array(self.table["jdb"])
        all_dico = np.array(list(tab.keys()))[::-1]
        file_test = self.import_spectrum()

        ref = pd.read_pickle(self.dir_root + "KITCAT/kitcat_spectrum.p")
        ref_flux = ref["flux"] * ref["correction_factor"]
        ref_wave = ref["wave"]
        if rv_sys is None:
            if file_test["parameters"]["RV_sys"] is not None:
                rv_sys = 1000 * file_test["parameters"]["RV_sys"]
            else:
                rv_sys = 0
        else:
            rv_sys *= 1000

        if wave_srf is not None:
            wave = myf.doppler_r(wave_srf, rv_sys)[1]

        if wave is not None:
            num = myf.find_nearest(np.array(tab[all_dico[0]]["catalog"]["freq_mask0"]), wave)[0][0]
        else:
            wave = -99.9

        if type(sub_dico) == str:
            sub_dico = [sub_dico]
        if sub_dico is None:
            sub_dico = list(tab.keys())[::-1]

        if num is not None:
            wave_ref = tab[sub_dico[0]]["catalog"]["freq_mask0"][num]
            wave_ref_srf = myf.doppler_r(wave_ref, rv_sys)[0]
            self.wave_ref_srf = wave_ref_srf[0]
            if wave == -99.9:
                wave = wave_ref
        else:
            wave_ref = -69

        if abs(wave - wave_ref) < 0.3:
            if plot:
                center = myf.doppler_r(wave_ref, rv_sys)[0]
                plt.figure(figsize=(18, 7))
                plt.subplot(1, 2, 1)
                crit_morpho = ["DOUBTFUL", "OKAY"][tab[sub_dico[0]]["catalog"]["morpho_crit"][num]]
                crit_blend = ["BLENDED", "OKAY"][tab[sub_dico[0]]["catalog"]["blend_crit"][num]]
                crit_telluric = tab[sub_dico[0]]["catalog"]["rel_contam"][num]
                plt.title(
                    "INDEX KITCAT : %.0f    MORPHOLOGY : %s    BLEND : %s    TELLURIC : %.2f"
                    % (num, crit_morpho, crit_blend, crit_telluric)
                )
                plt.plot(ref_wave, ref_flux, color="k")
                plt.axvspan(
                    xmin=tab[sub_dico[0]]["catalog"]["wave_left"][num],
                    xmax=tab[sub_dico[0]]["catalog"]["wave_right"][num],
                    alpha=0.1,
                    color="r",
                )
                plt.axvspan(
                    xmin=tab[sub_dico[0]]["catalog"]["wave_fitted"][num]
                    - tab[sub_dico[0]]["catalog"]["win_mic"][num],
                    xmax=tab[sub_dico[0]]["catalog"]["wave_fitted"][num]
                    + tab[sub_dico[0]]["catalog"]["win_mic"][num],
                    alpha=0.2,
                    color="r",
                )
                plt.axvline(x=center, color="r")
                if kw_dico == "bt":
                    plt.axhline(
                        y=1 - tab[sub_dico[0]]["catalog"]["line_depth"][num] / 2, color="r"
                    )
                plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
                plt.ylabel("Normalised flux", fontsize=14)
                plt.xlim(center - 0.75, center + 0.75)
                plt.ylim(-0.01, 1.01)

            c = 1
            for dico in sub_dico:
                line = myc.tableXY(
                    jdb, tab[dico][kw_dico][col, num, :], tab[dico][kw_dico + "_std"][col, num, :]
                )
                line.clip(min=[jdb_min, None], max=[jdb_max, None])
                if plot:
                    if len(sub_dico) != 1:
                        if c == 1:
                            plt.subplot(len(sub_dico), 2, 2)
                            ax = plt.gca()
                        else:
                            plt.subplot(len(sub_dico), 2, 2 * c, sharex=ax, sharey=ax)
                    if kw_dico == "lbl":
                        plt.axhline(y=0, color="k", alpha=0.5)
                        line.rms_w()
                        plt.ylabel("RV [m/s]", fontsize=14)
                        if recenter:
                            line.recenter(who="Y")
                        line.plot(
                            modulo=modulo,
                            color="k",
                            label="%s (rms : %.2f)" % (dico, line.rms),
                            periodic=periodic,
                        )
                    else:
                        line.plot(modulo=modulo, color="k", label="%s" % (dico), periodic=periodic)
                    plt.legend()
                    if c == 1:
                        plt.title(
                            "Analysis : %s  |  Col : %.0f  |  Wavelength : %.2f (%.2f) | med(err) = %.2f"
                            % (
                                (kw_dico + ext).upper(),
                                col,
                                wave_ref,
                                myf.doppler_r(wave_ref, rv_sys)[0],
                                np.median(line.yerr),
                            )
                        )
                    plt.xlabel("Time [jdb]", fontsize=14)
                    if modulo is not None:
                        plt.xlabel("Time %% %.2f [jdb]" % (modulo), fontsize=14)
                    if c != len(sub_dico):
                        plt.tick_params(labelbottom=False, top=True, direction="in")
                    plt.subplots_adjust(
                        top=0.95, bottom=0.10, left=0.07, right=0.95, hspace=0, wspace=0.2
                    )
                    c += 1

            return line
        else:
            print("The line is not the mask")

        c = 1
        if (wave_ref == -69) & (kw_dico == "lbl"):
            if plot:
                plt.figure(figsize=(16, 5))
                plt.subplot(len(sub_dico), 2, 2)
                ax = plt.gca()
                for dico in sub_dico:
                    mean_rv = np.sum(
                        tab[dico]["lbl"] / tab[dico]["lbl_std"] ** 2, axis=0
                    ) / np.sum(1 / tab[dico]["lbl_std"] ** 2, axis=0)
                    rv_std = self.table["rv_dace_std"]
                    line = myc.tableXY(self.table["jdb"], mean_rv, rv_std)
                    if len(sub_dico) != 1:
                        plt.subplot(len(sub_dico), 1, c, sharex=ax, sharey=ax)
                    line.rms_w()
                    line.plot(
                        modulo=modulo, color=None, label="%s / rms : %.2f" % (dico, line.rms)
                    )
                    plt.legend()
                    if c == 1:
                        plt.title("Wavelength : %.2f" % (wave_ref))
                    plt.xlabel("Time [jdb]", fontsize=14)
                    plt.ylabel("RV [m/s]", fontsize=14)
                    plt.tick_params(labelbottom=False, top=True, direction="in")
                    plt.subplots_adjust(
                        top=0.95, bottom=0.10, left=0.07, right=0.90, hspace=0, wspace=0.4
                    )
                    c += 1
                return line

    def lbl_pca(
        self,
        reduction="pca_scikit",
        sub_dico="matching_diff",
        kw_dico="lbl",
        col=0,
        m=2,
        kind="inter",
        ordering="lbl_var",
        nb_comp=10,
        weighted=True,
        nb_comp_kept="auto",
        ext="",
        color_residues="k",
        recenter=True,
        standardize=False,
        contam_training=True,
        valid_lines=True,
        wave_bins=0,
        depth_bins=0,
        kernel_num=0,
        kernel_file="auto",
        kernel_ext="",
        nb_kernel_split=50,
        kernel=None,
        snr_min=0.5,
        cross_validation=False,
        cv_sim=100,
        cv_percent_rm=10,
        cv_frac_affected=0.01,
    ):
        """ordering : lbl_var or rvm_rms"""
        self.import_table()

        if kw_dico == "lbl":
            self.import_lbl()
            imported_table = self.lbl
        elif kw_dico == "lbl_iter":
            kw_dico = "lbl"
            self.import_lbl_iter()
            imported_table = self.lbl_iter
        elif kw_dico == "dbd":
            self.import_dbd()
            imported_table = self.dbd
        elif kw_dico == "aba":
            self.import_aba()
            imported_table = self.aba
        elif kw_dico == "wbw":
            self.import_wbw()
            imported_table = self.wbw
        elif kw_dico == "bt":
            self.import_bt()
            imported_table = self.bt

        plot_snr = True
        if snr_min == 0:
            plot_snr = False
            snr_min = 1e-6

        matrix_rv = imported_table[sub_dico][kw_dico][col]
        matrix_rv_std = imported_table[sub_dico][kw_dico + "_std"][col]
        jdb = imported_table[sub_dico]["jdb"]
        phase_mod = np.arange(365)[
            np.argmin(
                np.array(
                    [np.max((jdb - k) % 365.25) - np.min((jdb - k) % 365.25) for k in range(365)]
                )
            )
        ]

        if len(jdb) < nb_comp:
            nb_comp = len(jdb) - 1

        valid = imported_table[sub_dico]["catalog"]["valid"].astype("bool")
        valid = np.array(valid)

        if contam_training == False:
            table = imported_table[sub_dico]["catalog"].copy()

            all_keys = table.keys()
            keyword_table = []

            for j in list(all_keys):
                if len(j.split("qc_")) > 1:
                    keyword_table.append(j)

            free = np.array(
                np.product(table[keyword_table], axis=1) != 0
            )  # free of all kwoun contamination
            valid *= free

        if not valid_lines:
            valid_lines *= 0
            valid_lines += 1

        matrix_rv = matrix_rv[valid]
        matrix_rv_std = matrix_rv_std[valid]

        mask = np.std(matrix_rv, axis=1) != 0
        matrix_rv = matrix_rv[mask]
        matrix_rv_std = matrix_rv_std[mask]

        mean_rvi = np.sum(matrix_rv / matrix_rv_std**2, axis=1) / np.sum(
            1 / matrix_rv_std**2, axis=1
        )
        matrix_rv = (
            matrix_rv - mean_rvi[:, np.newaxis]
        )  # because svd sensitive to global offset of each vectors

        mean_rv = np.sum(matrix_rv / matrix_rv_std**2, axis=0) / np.sum(
            1 / matrix_rv_std**2, axis=0
        )
        rv_std = 1 / np.sqrt(np.sum(1 / matrix_rv_std**2, axis=0))

        line = myc.tableXY(jdb, mean_rv, rv_std)
        if recenter:
            matrix_rv = matrix_rv - line.y
            matrix_rv_std = np.sqrt(matrix_rv_std**2 + line.yerr**2)

        matrix = myc.table(matrix_rv)

        nb_lines_per_chuck = np.array(
            [
                np.ones(len(matrix_rv)),
                np.array(imported_table[sub_dico]["catalog"]["wave"])[valid][mask],
                np.array(imported_table[sub_dico]["catalog"]["wave"])[valid][mask],
            ]
        ).T
        dp_s = 0
        if (depth_bins != 0) | (wave_bins != 0):
            new_matrix_rv = []
            new_matrix_rv_std = []
            nb_lines_per_chuck = []
            if not depth_bins:
                depth_bins = 1
            D = np.arange(0, 1.01, depth_bins)

            catalog = imported_table[sub_dico]["catalog"][valid]
            catalog = catalog.reset_index()

            wave_min = np.min(catalog["wave"])
            wave_max = np.max(catalog["wave"])

            if not wave_bins:
                W = np.array([wave_min, wave_max])
                dp_s = 2
            else:
                W = np.arange(wave_min, wave_max + wave_bins, wave_bins)

            d, w = np.meshgrid(D, W)
            d1, d2, w1, w2 = (
                np.ravel(d[0:-1, 0:-1]),
                np.ravel(d[1:, 1:]),
                np.ravel(w[0:-1, 0:-1]),
                np.ravel(w[1:, 1:]),
            )
            for j in range(len(d1)):
                mask = (
                    (catalog["wave"] >= w1[j])
                    & (catalog["wave"] < w2[j])
                    & (catalog["depth_rel"] >= d1[j])
                    & (catalog["depth_rel"] < d2[j])
                )
                nb_lines_per_chuck.append([sum(mask), w1[j], w2[j], d1[j], d2[j]])
                if sum(mask):
                    liste = np.array(catalog.loc[mask].index)
                    a = matrix.rv_subselection(rv_std=matrix_rv_std, selection=liste)
                    new_matrix_rv.append(a.y)
                    new_matrix_rv_std.append(a.yerr)

            new_matrix_rv = np.array(new_matrix_rv)
            new_matrix_rv_std = np.array(new_matrix_rv_std)
            print("Sample size : %.0f" % (len(new_matrix_rv)))
            nb_lines_per_chuck = np.array(nb_lines_per_chuck)

            plt.figure(42, figsize=(18, 6))
            plt.subplot(1, 2, 1)
            for j in range(len(D) - 1):
                plt.plot(
                    np.cumsum(nb_lines_per_chuck[j :: len(D) - 1, 0]),
                    label="%.2f < d < %.2f" % (D[j], D[j + 1]),
                )
            plt.legend()
            plt.ylabel("Cumulative nb lines", fontsize=16)
            plt.xlabel("Wavelength chunck number", fontsize=16)
            plt.subplot(1, 2, 2)
            for j in range(len(D) - 1):
                plt.plot(
                    np.cumsum(nb_lines_per_chuck[j :: len(D) - 1, 0])
                    / np.sum(nb_lines_per_chuck[j :: len(D) - 1, 0]),
                    label="%.2f < d < %.2f" % (D[j], D[j + 1]),
                )
            plt.legend()
            plt.ylabel("Normalised cumulative nb lines", fontsize=16)
            plt.xlabel("Wavelength chunck number", fontsize=16)
            plt.subplots_adjust(left=0.08, right=0.97, top=0.97)

            matrix_rv = new_matrix_rv
            matrix_rv_std = new_matrix_rv_std

        if (kernel_num != 0) | (kernel is not None):
            new_matrix_rv = []
            new_matrix_rv_std = []
            nb_lines_per_chuck = []
            if kernel is None:
                kernels_file = pd.read_pickle(
                    root
                    + "/Python/database/%s/%s_%s_kernels.p"
                    % (self.instrument, self.instrument, kernel_file)
                )
                kernel_name = list(kernels_file.keys())[kernel_num - 1]
                kernel = kernels_file[kernel_name]
            variable = list(kernel["var" + kernel_ext].keys())
            catalog = imported_table[sub_dico]["catalog"][valid]
            catalog = catalog.reset_index()
            if len(variable) == 1:
                model = myc.tableXY(
                    np.array(kernel["var" + kernel_ext][variable[0]]),
                    kernel["strength" + kernel_ext],
                )
                model.interpolate(new_grid=np.array(catalog["wave"]), replace=False)
                strength = model.y_interp
                plt.figure()
                plt.scatter(np.array(catalog["wave"]), strength)
                # pouet

            nb_groups = nb_kernel_split
            cuts = np.linspace(0, 100, nb_groups)
            for i, j in zip(cuts[0:-1], cuts[1:]):
                mask = (strength > np.nanpercentile(strength, i)) & (
                    strength <= np.nanpercentile(strength, j)
                )
                nb_lines_per_chuck.append([sum(mask), i, j])
                if sum(mask):
                    liste = np.array(catalog.loc[mask].index)
                    a = matrix.rv_subselection(rv_std=matrix_rv_std, selection=liste)
                    new_matrix_rv.append(a.y)
                    new_matrix_rv_std.append(a.yerr)

            matrix_rv = np.array(new_matrix_rv)
            matrix_rv_std = np.array(new_matrix_rv_std)
            print("Sample size : %.0f" % (len(new_matrix_rv)))
            nb_lines_per_chuck = np.array(nb_lines_per_chuck)

        mean_bin = 0.5 * (nb_lines_per_chuck[:, 1 + dp_s] + nb_lines_per_chuck[:, 2 + dp_s])
        mean_bin = mean_bin[nb_lines_per_chuck[:, 0] != 0]

        m1 = np.median(myf.mad(matrix_rv, axis=1)[:, np.newaxis] / matrix_rv_std, axis=0)
        m2 = np.median(myf.mad(matrix_rv, axis=0) / matrix_rv_std, axis=1)

        # obs_kept = m1>snr_min
        obs_kept = np.ones(len(m1)).astype(
            "bool"
        )  # cannot apply obs cutoff since otherwise basis not of the good size
        if np.sum(m2 > snr_min) < 10:
            snr_min = 1e-6
            plot_snr = False
        wave_kept = m2 > snr_min

        plt.figure(figsize=(20, 5))
        plt.subplot(1, 2, 1)
        plt.plot(m1, "k.-")
        plt.title(
            "Nb of observations removed : %.1f %%" % (100 - 100 * sum(obs_kept) / len(obs_kept)),
            fontsize=15,
        )
        plt.ylabel("SNR", fontsize=15)
        plt.xlabel("Time index", fontsize=15)
        plt.yscale("log")
        if plot_snr:
            plt.axhline(y=snr_min, ls=":", color="k")
        plt.subplot(1, 2, 2)
        plt.plot(mean_bin, m2, "k.-")
        plt.title(
            "Nb of bins removed : %.1f %%" % (100 - 100 * sum(wave_kept) / len(wave_kept)),
            fontsize=15,
        )
        plt.ylabel("SNR", fontsize=15)
        plt.xlabel("Mean bins variable", fontsize=15)
        plt.yscale("log")
        if plot_snr:
            plt.axhline(y=snr_min, ls=":", color="k")
        plt.subplots_adjust(left=0.07, right=0.98)
        plt.savefig(self.dir_root + "PCA/Observations_removed_%s.pdf" % (ext.split("_")[-1]))

        print(
            "\n [INFO] %.1f%% of the observations removed with criterion SNR %.1f"
            % (100 - 100 * sum(obs_kept) / len(obs_kept), snr_min)
        )
        print(
            "\n [INFO] %.1f%% of the wavelength bins %.0f AA removed with criterion SNR %.1f"
            % (100 - 100 * sum(wave_kept) / len(wave_kept), wave_bins, snr_min)
        )

        matrix = myc.table(matrix_rv[wave_kept][:, obs_kept])

        matrix_weight = 1 / matrix_rv_std[wave_kept][:, obs_kept] ** 2

        if weighted:
            w = matrix_weight
        else:
            w = None

        matrix.replace_outliers(m=m, kind=kind)

        if standardize:
            matrix.rms_w(matrix_rv_std[wave_kept][:, obs_kept])
            norm = matrix.rms
            matrix.table /= norm[:, np.newaxis]

        stop = False
        zscore, phi, base_vec = matrix.dim_reduction(reduction, nb_comp, w)
        if zscore is None:
            stop = True

        percentages = np.zeros(len(base_vec.T))
        if cross_validation:
            cv_base = [base_vec]
            for j in tqdm(np.arange(cv_sim)):
                selection = np.random.choice(
                    np.arange(len(matrix.table)),
                    int((1 - cv_percent_rm / 100) * len(matrix.table)),
                    replace=False,
                )
                matrix_cv = matrix.copy()
                matrix_cv.table = matrix_cv.table[selection]
                zscore, phi, base_vec = matrix_cv.dim_reduction(reduction, nb_comp, w[selection])
                cv_base.append(base_vec)
            cv_base = np.array(cv_base)
            cv_base = np.hstack(cv_base).T
            cv_base = myc.table(cv_base)
            self.debug_test = cv_base
            cv_base.cross_validation2(
                nb_comp,
                frac_affected=cv_frac_affected,
                fig_num=20,
                cv_rm=cv_percent_rm,
                overselection=2,
            )
            plt.figure(20)
            plt.savefig(self.dir_root + "PCA/Cross_validation" + ext + "_matrix.png")
            plt.figure(21)
            plt.savefig(self.dir_root + "PCA/Cross_validation" + ext + "_vectors.png")
            plt.figure(22)
            plt.savefig(self.dir_root + "PCA/Cross_validation" + ext + "_iterations.png")
            self.debug2 = cv_base

            # plt.savefig(self.dir_root+'PCA/SHELL_cross_validation.pdf')
            components = cv_base.cv_components.T
            percentages = cv_base.cv_percentage_norm
            percentages[percentages > 105] = 0
            rmed = cv_base.cv_rmed

            # all_components = myc.table(np.hstack([base_vec,components]))
            # all_components.r_matrix(Plot=False)
            # mat = myf.rm_diagonal(all_components.matrix_corr)

            # ordering_percent = np.argmax(mat[0:len(percentage)],axis=1)-len(base_vec.T)

            # percentages[ordering_percent] = percentage

        percentages_values = percentages.astype("int")
        self.base_cv_percent = percentages_values.copy()
        nb_comp_100 = myf.first_transgression(
            percentages_values, (100 - cv_percent_rm / 4), relation=1
        )

        self.base_cv_comp_100 = nb_comp_100

        self.base_all_vec_pca = base_vec

        percentages = np.array(["%.0f %%" % (p) for p in percentages_values])
        percentages[percentages == "0 %"] = "<%.0f %%" % (cv_frac_affected * 100)

        if not stop:
            plt.figure(69, figsize=(22, 16))
            plt.subplot(2, 2, 1)
            ax = plt.gca()
            plt.xlabel("# components", fontsize=14)
            plt.ylabel("Variance explained", fontsize=14)
            plt.plot(np.arange(1, len(phi) + 1), matrix.var_ratio, marker="o")
            ax2 = plt.gca()
            yspan = ax2.get_ylim()[1] - ax2.get_ylim()[0]

            for n, yplot in enumerate(matrix.var_ratio):
                if percentages[n] != "":
                    plt.annotate(
                        "%s" % (percentages[n]),
                        (n + 1, yplot + 0.2 * yspan),
                        ha="center",
                        color=["k", "r"][int(percentages_values[n]) < (100 - cv_percent_rm)],
                    )
                    plt.plot(
                        [n + 1, n + 1],
                        [matrix.var_ratio[n], yplot + 0.18 * yspan],
                        alpha=0.2,
                        color="k",
                    )

            curve_var = myc.tableXY(np.arange(1, len(matrix.var_ratio) + 1), matrix.var_ratio)

            if nb_comp_kept != "auto":
                button = 0
                pca_comp_kept = nb_comp_kept
            else:
                button = 1
                if cross_validation:
                    pca_comp_kept = nb_comp_100
                    nb_comp_kept = nb_comp_100

                else:
                    curve_var.diff()
                    curve_var.diff()
                    mask = curve_var.x < nb_comp - 2
                    curve_var.masked(mask)

                    iq = myf.IQ(curve_var.y)
                    q3 = np.nanpercentile(curve_var.y, 75)
                    q1 = np.nanpercentile(curve_var.y, 25)

                    l = (curve_var.y > q3 + 1.5 * iq) | (curve_var.y < q1 - 1.5 * iq)
                    l2 = 1 - l[::-1]
                    pca_comp_kept = len(l) - np.min(np.where(l2 == 0)[0])
            if button:
                plt.axvline(
                    x=pca_comp_kept, ls=":", color="k", label="%.0f vectors kept" % (pca_comp_kept)
                )
            plt.legend()
            plt.subplots_adjust(top=0.96, bottom=0.07)

            if not pca_comp_kept:
                pca_comp_kept = 1
            elif pca_comp_kept == nb_comp:
                pca_comp_kept -= 1

            print("Number of PCA vectors kept : %.0f" % (pca_comp_kept))

            line.recenter(who="Y")
            line.rms_w()
            rms = [line.rms]
            plt.figure(figsize=(18, 6))
            plt.subplot(1 + int(ordering == "rvm_rms"), 1, 1)
            plt.title("Var ordering")
            line.periodogram(nb_perm=1, color="k", Norm=True, p_min=0.7)
            for i in range(1, nb_comp):
                line.fit_base(base_vec[:, 0:i].T)
                line.vec_residues.rms_w()
                if i <= nb_comp_kept:
                    line.vec_residues.periodogram(
                        nb_perm=1, color=None, Norm=True, legend="%.0f" % (i), p_min=0.7
                    )
                rms.append(line.vec_residues.rms)
            rms = np.array(rms)
            self.pca_rms_cumu = rms
            plt.legend()

            vectors_indices = np.arange(1, nb_comp)

            if ordering == "rvm_rms":
                print("[INFO] Reordering pca vector by rms improvement")
                vectors_indices = np.argsort(np.diff(self.pca_rms_cumu))
                base_vec = base_vec[:, vectors_indices]
                vectors_indices += 1
                plt.xlabel("")
                ax6 = plt.gca()
                plt.subplot(2, 1, 2, sharex=ax6)
                plt.title("Rms ordering")
                line.periodogram(nb_perm=1, color="k", Norm=True, p_min=0.7)
                for i in range(1, nb_comp):
                    line.fit_base(base_vec[:, 0:i].T)
                    if i <= nb_comp_kept:
                        line.vec_residues.periodogram(
                            nb_perm=1, color=None, Norm=True, legend="%.0f" % (i), p_min=0.7
                        )
                plt.legend()
            plt.subplots_adjust(top=0.94, hspace=0.35, left=0.07, right=0.96)

            plt.figure(2, figsize=(15, 10))
            ax1 = None
            ax2 = None
            ax3 = None

            line.recenter(who="Y")
            line.yerr = np.sqrt(0.7**2 + line.yerr**2)

            line.fit_base(base_vec[:, 0:nb_comp].T)

            for j in range(pca_comp_kept):
                plt.subplot(pca_comp_kept, 3, 3 * j + 1, sharex=ax1)
                if j == 0:
                    ax1 = plt.gca()
                plt.scatter(jdb, base_vec[:, j] * line.coeff_fitted[j + 1])
                plt.subplot(pca_comp_kept, 3, 3 * j + 2, sharex=ax2)
                if j == 0:
                    ax2 = plt.gca()
                plt.scatter((jdb - phase_mod) % 365.25, base_vec[:, j] * line.coeff_fitted[j + 1])
                plt.subplot(pca_comp_kept, 3, 3 * j + 3, sharex=ax3)
                if j == 0:
                    ax3 = plt.gca()
                vec = myc.tableXY(jdb, base_vec[:, j] * line.coeff_fitted[j + 1])
                vec.periodogram(nb_perm=1, Norm=True, ofac=10, p_min=0.7)
                plt.ylabel("Power")
            plt.subplots_adjust(top=0.95, bottom=0.07, left=0.07, right=0.95, hspace=0)

            self.base_vec_pca = base_vec  # [:,0:pca_comp_kept]

            print("Vectors selected : ", list(vectors_indices[0:pca_comp_kept]))

            plt.figure(69, figsize=(22, 16))
            plt.subplot(2, 2, 3, sharex=ax)
            plt.title(sub_dico)
            plt.plot(
                np.arange(0, len(rms)),
                rms,
                marker="o",
                label=reduction + "_%.0f" % (int(contam_training)),
            )
            if button:
                plt.axvline(x=pca_comp_kept, ls=":", color="k")
            plt.xlabel("# components", fontsize=14)
            plt.ylabel("RV rms [m/s]", fontsize=14)
            plt.legend()
            plt.subplots_adjust(top=0.96, bottom=0.07, hspace=0.3, left=0.09, right=0.97)

            line.fit_base(base_vec[:, 0:pca_comp_kept].T)

            plt.subplot(1, 2, 2)
            curve_crit = myc.tableXY(
                np.sqrt(rms[0:-1] ** 2 - rms[1:] ** 2), np.diff(matrix.var_ratio)
            )
            l = plt.scatter(
                curve_crit.x,
                curve_crit.y,
                color=None,
                label=reduction + "_%.0f" % (int(contam_training)),
                zorder=1000,
            )
            curve_crit.myscatter(
                num=False, liste=np.arange(1, len(np.diff(rms)) + 1), color_text="k"
            )
            plt.axhline(y=0, color="k", alpha=0.3)
            plt.axvline(x=0, color="k", alpha=0.3)
            plt.xlabel(r"$\Delta$ RV rms [m/s]", fontsize=14)
            plt.ylabel(r"$\Delta$ Variance explained", fontsize=14)
            plt.legend(loc=3)
            plt.savefig(self.dir_root + "PCA/PCA_var_ratio" + ext + ".png")

            plt.figure(figsize=(20, 14))
            plt.subplot(4, 1, 1)
            plt.title("PCA fit on sub_dico : %s" % (sub_dico), fontsize=14)
            ax1 = plt.gca()
            plt.ylabel("RV [m/s]", fontsize=13)
            plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=13)
            line.rms_w()
            line.vec_residues.rms_w()
            line.plot(color="b", label="Init (rms : %.2f m/s)" % (line.rms), capsize=0)
            line.vec_residues.plot(
                color="g",
                label="PCA residues (rms : %.2f m/s)" % (line.vec_residues.rms),
                capsize=0,
            )
            plt.legend()
            plt.subplot(4, 1, 2)
            ax2 = plt.gca()
            line.periodogram(nb_perm=1, legend="before", color="b", Norm=True, p_min=0.7)
            line.vec_residues.periodogram(
                nb_perm=1, legend="after", color="g", Norm=True, p_min=0.7
            )
            line.plot_lowest_perio(line.vec_residues)
            plt.subplot(4, 1, 3, sharex=ax1)
            line.vec_fitted.rms_w()
            line.vec_fitted.plot(
                color=color_residues,
                label="Vec fitted (rms : %.2f m/s)" % (line.vec_fitted.rms),
                capsize=0,
            )
            plt.legend()
            plt.ylabel(r"$\Delta$RV [m/s]", fontsize=13)
            plt.xlabel(r"Jdb $-$ 2,400,000 [days]", fontsize=13)
            plt.subplot(4, 1, 4, sharex=ax2)
            line.vec_fitted.periodogram(
                nb_perm=1, legend="diff", color=color_residues, Norm=True, p_min=0.7
            )
            plt.subplots_adjust(top=0.95, left=0.08, right=0.96, bottom=0.09, hspace=0.35)
            plt.savefig(self.dir_root + "PCA/PCA_after_versus_before" + ext + ".pdf")

            self.pca_residues = line.vec_residues

            # line.fit_base(base_vec[:,0:pca_comp_kept].T)

    def lbl_slice(
        self,
        reduction="pca",
        sub_dico="matching_shell",
        kw_dico="lbl_iter",
        col=0,
        nb_comp_kept=5,
        nb_comp=5,
        ext="_slice",
        contam_training=True,
        kernel_file="manual",
        kernel_ext="",
        nb_slice_split=10,
        nb_slice=2,
    ):

        kernels_file = pd.read_pickle(
            root
            + "/Python/database/%s/%s_%s_kernels.p"
            % (self.instrument, self.instrument, kernel_file)
        )
        if nb_slice is None:
            nb_slice = len(list(kernels_file.keys()))

        base_kernel = []
        for k in np.arange(1, 1 + nb_slice):
            self.lbl_pca(
                reduction=reduction,
                sub_dico=sub_dico,
                kw_dico=kw_dico,
                col=col,
                nb_comp_kept=nb_comp_kept,
                nb_comp=nb_comp,
                ordering="var_lbl",
                ext=ext,
                color_residues="k",
                contam_training=contam_training,
                recenter=True,
                standardize=True,
                wave_bins=0,
                depth_bins=0,
                kernel_num=k,
                kernel_file=kernel_file,
                nb_kernel_split=nb_slice_split,
                kernel_ext=kernel_ext,
                snr_min=0.5,
            )

            base_kernel.append(self.base_vec_pca[:, 0])
        plt.close("all")

        base_kernel = np.array(base_kernel)
        return base_kernel

    def lbl_slice_gaveup(
        self,
        sub_dico="matching_shell",
        col=0,
        m=2,
        kind="inter",
        nb_comp=3,
        weighted=True,
        dico_name="",
        color_residues="k",
        kernel_file="manual",
        planet=1,
    ):

        """ordering : lbl_var or rvm_rms"""
        self.import_table()
        instrument = self.instrument

        if kw_dico == "lbl":
            self.import_lbl()
            imported_table = self.lbl
        elif kw_dico == "lbl_iter":
            kw_dico = "lbl"
            self.import_lbl_iter()
            imported_table = self.lbl_iter
        elif kw_dico == "dbd":
            self.import_dbd()
            imported_table = self.dbd
        elif kw_dico == "aba":
            self.import_aba()
            imported_table = self.aba
        elif kw_dico == "wbw":
            self.import_wbw()
            imported_table = self.wbw
        elif kw_dico == "bt":
            self.import_bt()
            imported_table = self.bt

        imported_lbl = imported_table[sub_dico][kw_dico]
        imported_lbl_std = imported_table[sub_dico][kw_dico + "_std"]
        jdb = imported_table[sub_dico]["jdb"]

        valid = imported_table[sub_dico]["catalog"]["valid"].astype("bool")
        valid = np.array(valid)

        matrix_rv = imported_lbl[col][valid]
        matrix_rv_std = imported_lbl_std[col][valid]

        matrix_rv += planet * np.sin(2 * np.pi * jdb / 80)

        mask = np.std(matrix_rv, axis=1) != 0
        matrix_rv = matrix_rv[mask]
        matrix_rv_std = matrix_rv_std[mask]

        mean_rvi = np.sum(matrix_rv / matrix_rv_std**2, axis=1) / np.sum(
            1 / matrix_rv_std**2, axis=1
        )
        matrix_rv = (
            matrix_rv - mean_rvi[:, np.newaxis]
        )  # because svd sensitive to global offset of each vectors

        mean_rv = np.sum(matrix_rv / matrix_rv_std**2, axis=0) / np.sum(
            1 / matrix_rv_std**2, axis=0
        )
        rv_std = 1 / np.sqrt(np.sum(1 / matrix_rv_std**2, axis=0))

        line = myc.tableXY(jdb, mean_rv, rv_std)

        matrix = myc.table(matrix_rv)

        kernels_file = root + "/Python/database/%s/%s_%s_kernels.p" % (
            instrument,
            instrument,
            kernel_file,
        )
        kernels = pd.read_pickle(kernels_file)

        plt.figure(figsize=(18, 6))
        kernel_base = []
        kernel_name = list(kernels.keys())
        for kernel in np.arange(nb_comp):
            kw = kernel_name[kernel]
            variable = list(kernels[kw]["var"].keys())
            catalog = imported_table[sub_dico]["catalog"][valid]
            catalog = catalog.reset_index()
            if len(variable) == 1:
                model = myc.tableXY(
                    np.array(kernels[kw]["var"][variable[0]]), kernels[kw]["strength"]
                )
                model.interpolate(new_grid=np.array(catalog["wave"]), replace=False)
                strength = model.y_interp
                kernel_base.append(strength)

                plt.subplot(nb_comp, 1, kernel + 1)
                plt.plot(np.array(catalog["wave"]), strength, color="k", marker="")
                plt.xlim(np.min(model.x_interp), np.max(model.x_interp))
                plt.tick_params(top=True, direction="in")
                plt.axhline(y=0, color="r", alpha=0.4)
                plt.ylim(-2.99, 2.99)
                plt.ylabel("Z score (S%.0f)" % (kernel + 1), fontsize=14)
                # pouet
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.subplots_adjust(hspace=0, left=0.08, right=0.96, top=0.96)
        kernel_base2 = np.array(kernel_base)
        # kernel_base2 -= np.median(kernel_base2,axis=1)[:,np.newaxis] #recenter to kill Doppler shift signatures
        kernel_base = kernel_base2
        # kernel_base = np.vstack([np.ones(len(matrix_rv)),kernel_base2])

        m1 = np.median(myf.mad(matrix_rv, axis=1)[:, np.newaxis] / matrix_rv_std, axis=0)
        m2 = np.median(myf.mad(matrix_rv, axis=0) / matrix_rv_std, axis=1)

        # obs_kept = m1>snr_min
        obs_kept = np.ones(len(m1)).astype(
            "bool"
        )  # cannot apply obs cutoff since otherwise basis not of the good size
        wave_kept = m2 > -1

        matrix = myc.table(matrix_rv[wave_kept][:, obs_kept])
        matrix_weight = (1 / matrix_rv_std[wave_kept][:, obs_kept] ** 2).T

        if weighted:
            w = matrix_weight
        else:
            w = None

        matrix.replace_outliers(m=m, kind=kind)
        matrix.table = matrix.table.T
        matrix.fit_base(kernel_base, weight=w)

        plt.figure()
        plt.plot(np.mean(matrix.vec_fitted, axis=0))

        plt.figure()
        line2 = line.copy()
        line2.y = np.mean(matrix.table, axis=1)
        line3 = line.copy()
        line3.y = np.mean(matrix.vec_residues, axis=1)

        plt.subplot(2, 2, 1)
        line.plot(label="rms %.2f m/s" % (np.std(line.y)))
        plt.legend()
        ax = plt.gca()
        plt.subplot(2, 2, 2)
        line.periodogram(Norm=True)
        plt.subplot(2, 2, 3, sharex=ax, sharey=ax)
        line2.plot(label="rms %.2f m/s" % (np.std(line2.y)), color="r")
        line3.plot(label="rms %.2f m/s" % (np.std(line3.y)), color="b")
        plt.legend()
        plt.subplot(2, 2, 4)
        line2.periodogram(color="r", Norm=True)
        line3.periodogram(color="b", Norm=True)

        # new_lbl = imported_lbl[0].copy()
        # new_lbl[valid] = matrix.vec_residues.T

        # ccf_rv = myc.table(new_lbl)
        # ccf = ccf_rv.rv_subselection(rv_std = np.array(imported_lbl_std)[0], selection=np.arange(len(valid))[valid])
        # ccf.x = jdb
        # ccf_rv = ccf.y

        # save = {'jdb':jdb,
        # kw2:np.array([new_lbl, new_lbl - ccf_rv, imported_lbl[2]]),
        # kw2+'_std':imported_lbl_std,
        # 'catalog':kitcat,
        # 'mask':kitcat_name,
        # 'snr_med':np.nanmedian(table['snr']),
        # 'base_vec':base_vec,
        # 'sub_dico_used':sub_dico,
        # 'date':datetime.datetime.now().strftime("%Y-%m-%d:%H:%M:%S")}

        # if kw_dico=='lbl':
        #     self.lbl[dico_name] = save
        #     myf.pickle_dump(self.lbl,open(self.directory+'Analyse_line_by_line.p','wb'))

        # elif kw_dico=='lbl_iter':
        #     self.lbl_iter[dico_name] = save
        #     myf.pickle_dump(self.lbl_iter,open(self.directory+'Analyse_line_by_line_iter.p','wb'))

        # elif kw_dico=='dbd':
        #     self.dbd[dico_name] = save
        #     myf.pickle_dump(self.dbd,open(self.directory+'Analyse_depth_by_depth.p','wb'))

        # elif kw_dico=='wbw':
        #     self.wbw[dico_name] = save
        #     myf.pickle_dump(self.wbw,open(self.directory+'Analyse_width_by_width.p','wb'))

        # for i,j in enumerate(files):
        #     file = pd.read_pickle(j)
        #     file[dico_name] = {'parameters':{'sub_dico_used':sub_dico,'step':step+1+add_step}}
        #     myf.pickle_dump(file,open(j,'wb'))

        # if (kw_dico=='lbl')|(kw_dico=='lbl_iter'):
        #     self.lbl_to_ccf(all_dico=[dico_name],kw_dico=kw_dico)

        # plt.show(block=False)

    def lbl_color_investigation(
        self,
        sub_dico="matching_diff",
        kw_dico="lbl",
        col=0,
        m=2,
        contam_training=True,
        wave_bins=5,
        ext="",
    ):
        """ordering : lbl_var or rvm_rms"""
        self.import_table()

        jdb = np.array(self.table["jdb"])
        baseline = np.max(jdb) - np.min(jdb)
        sub_dicos = [sub_dico]
        counter = 1
        phase_mod = np.arange(365)[
            np.argmin(
                np.array(
                    [np.max((jdb - k) % 365.25) - np.min((jdb - k) % 365.25) for k in range(365)]
                )
            )
        ]

        if kw_dico == "lbl":
            self.import_lbl()
            imported_table = self.lbl
        elif kw_dico == "lbl_iter":
            kw_dico = "lbl"
            self.import_lbl_iter()
            imported_table = self.lbl_iter
        elif kw_dico == "dbd":
            self.import_dbd()
            imported_table = self.dbd
        elif kw_dico == "aba":
            self.import_aba()
            imported_table = self.aba
        elif kw_dico == "wbw":
            self.import_wbw()
            imported_table = self.wbw
        elif kw_dico == "bt":
            self.import_bt()
            imported_table = self.bt

        matrix_rv = imported_table[sub_dico][kw_dico][col]
        matrix_rv_std = imported_table[sub_dico][kw_dico + "_std"][col]
        valid = imported_table[sub_dico]["catalog"]["valid"].astype("bool")
        valid = np.array(valid)

        if contam_training == False:
            table = imported_table[sub_dico]["catalog"].copy()

            all_keys = table.keys()
            keyword_table = []

            for j in list(all_keys):
                if len(j.split("qc_")) > 1:
                    keyword_table.append(j)

            free = np.array(
                np.product(table[keyword_table], axis=1) != 0
            )  # free of all kwoun contamination
            valid *= free

        matrix_rv = matrix_rv[valid]
        matrix_rv_std = matrix_rv_std[valid]

        mask = np.std(matrix_rv, axis=1) != 0
        matrix_rv = matrix_rv[mask]
        matrix_rv_std = matrix_rv_std[mask]

        mean_rvi = np.sum(matrix_rv / matrix_rv_std**2, axis=1) / np.sum(
            1 / matrix_rv_std**2, axis=1
        )
        matrix_rv = (
            matrix_rv - mean_rvi[:, np.newaxis]
        )  # because svd sensitive to global offset of each vectors

        mean_rv = np.sum(matrix_rv / matrix_rv_std**2, axis=0) / np.sum(
            1 / matrix_rv_std**2, axis=0
        )
        rv_std = 1 / np.sqrt(np.sum(1 / matrix_rv_std**2, axis=0))
        line = myc.tableXY(self.table["jdb"], mean_rv, rv_std)

        matrix = myc.table(matrix_rv)

        new_matrix_rv = []
        new_matrix_rv_std = []
        nb_lines_per_chuck = []
        wave = []
        new_vecs = []

        catalog = imported_table[sub_dico]["catalog"][valid]
        catalog = catalog.reset_index()

        wave_min = np.min(catalog["wave"])
        wave_max = np.max(catalog["wave"])

        D = np.arange(0, 1.01, 1)
        d, w = np.meshgrid(D, np.arange(wave_min, wave_max + wave_bins, wave_bins))
        d1, d2, w1, w2 = (
            np.ravel(d[0:-1, 0:-1]),
            np.ravel(d[1:, 1:]),
            np.ravel(w[0:-1, 0:-1]),
            np.ravel(w[1:, 1:]),
        )

        for j in tqdm(range(len(d1))):
            mask = (
                (catalog["wave"] > w1[j])
                & (catalog["wave"] < w2[j])
                & (catalog["depth_rel"] > d1[j])
                & (catalog["depth_rel"] < d2[j])
            )
            if sum(mask):
                liste = np.array(catalog.loc[mask].index)
                a = matrix.rv_subselection(rv_std=matrix_rv_std, selection=liste)
                new_vecs.append(a)
                a.rms_w()
                new_matrix_rv.append(a.rms)
                new_matrix_rv_std.append(np.median(a.yerr))
                wave.append(w1[j])
                nb_lines_per_chuck.append(sum(mask))

        all_power = []
        for st in tqdm(new_vecs):
            st.substract_polyfit(2, replace=True)
            std_vec = np.nanstd(st.y)
            if np.median(st.yerr) < std_vec / 10:
                st.yerr *= 0
                st.yerr += std_vec / 10
            st.periodogram(nb_perm=1, Norm=True, p_min=0.7, p_max=baseline, Plot=False)
            all_power.append(st.power / st.fap)
        all_power = np.array(all_power)

        new_matrix_rv = np.array(new_matrix_rv)
        new_matrix_rv_std = np.array(new_matrix_rv_std)
        wave = np.array(wave)
        nb_lines_per_chuck = np.array(nb_lines_per_chuck)

        matrix_rv = new_matrix_rv
        matrix_rv_std = new_matrix_rv_std
        matrix_rv_z = matrix_rv / matrix_rv_std

        plt.figure(figsize=(20, 8))
        plt.subplot(5, len(sub_dicos), (0) * len(sub_dicos) + counter)
        ax1 = plt.gca()
        ax1.xaxis.set_minor_locator(MultipleLocator(50))

        plt.tick_params(direction="in", top=True, which="both")
        plt.title("%s" % (sub_dico))
        plt.plot(wave, matrix_rv, color="k")
        plt.scatter(wave, matrix_rv, c=nb_lines_per_chuck, cmap="brg", zorder=10, s=7)
        plt.ylabel("RV rms [m/s]")
        plt.subplot(5, len(sub_dicos), (1) * len(sub_dicos) + counter, sharex=ax1)
        ax1 = plt.gca()
        ax1.xaxis.set_minor_locator(MultipleLocator(50))
        plt.tick_params(direction="in", top=True, which="both")
        increase_noise = (1 - matrix_rv_std[0] / matrix_rv_std[np.argmin(matrix_rv)]) * 100
        plt.plot(wave, matrix_rv_std, color="k")
        plt.scatter(wave, matrix_rv_std, c=nb_lines_per_chuck, cmap="brg", zorder=10, s=7)
        plt.ylabel("med(RV) [m/s]")
        plt.subplot(5, len(sub_dicos), (2) * len(sub_dicos) + counter, sharex=ax1)
        ax1 = plt.gca()
        ax1.xaxis.set_minor_locator(MultipleLocator(50))
        plt.tick_params(direction="in", top=True, which="both")
        plt.plot(wave, matrix_rv_z, color="k")
        plt.scatter(wave, matrix_rv_z, c=nb_lines_per_chuck, cmap="brg", zorder=10, s=7)
        plt.ylim(0, 7)
        plt.axhline(y=1, color="r", alpha=0.5)
        plt.ylabel("Z score RV")
        plt.subplot(5, len(sub_dicos), (3) * len(sub_dicos) + counter, sharex=ax1)
        ax1 = plt.gca()
        ax1.xaxis.set_minor_locator(MultipleLocator(50))
        plt.tick_params(direction="in", top=True, which="both")
        plt.plot(wave, nb_lines_per_chuck, color="k")
        plt.scatter(wave, nb_lines_per_chuck, c=nb_lines_per_chuck, cmap="brg", zorder=10, s=7)
        plt.ylabel(r"Nb lines per %.1f $\AA$" % (wave_bins))
        plt.ylim(-2, None)
        plt.axhline(y=0, color="k", alpha=0.4)
        plt.subplot(5, len(sub_dicos), (4) * len(sub_dicos) + counter, sharex=ax1)
        myf.my_colormesh(wave, np.log10(1 / st.freq), all_power.T, vmin=0.25, vmax=1.5, cmap="jet")
        ax1 = plt.gca()
        ax1.xaxis.set_minor_locator(MultipleLocator(50))
        plt.tick_params(direction="out", top=True, right=True, which="both")

        plt.xlabel(r"Wavelength [$\AA$]")
        plt.ylabel("Log(P) [days]")
        plt.axhline(y=np.log10(365.25), color="white", lw=3, alpha=0.7)
        plt.axhline(y=np.log10(365.25), color="k", alpha=0.9)
        # ax = plt.colorbar(pad=0) ; ax.ax.set_ylabel('Power/FAP(1%)',fontsize=15)

        plt.subplots_adjust(left=0.05, right=0.98, top=0.95, bottom=0.1, hspace=0)

        plt.savefig(self.dir_root + "IMAGES/Chunck_investigation%s.png" % (ext))

    def lbl_treshold_blue(
        self,
        sub_dicos=["matching_diff"],
        kw_dico="lbl",
        col=0,
        contam_training=True,
        wave_bins=4,
        min_nb_lines=100,
    ):
        """ordering : lbl_var or rvm_rms"""
        self.import_table()

        jdb = np.array(self.table["jdb"])

        if kw_dico == "lbl":
            self.import_lbl()
            imported_table = self.lbl
        elif kw_dico == "lbl_iter":
            kw_dico = "lbl"
            self.import_lbl_iter()
            imported_table = self.lbl_iter

        if sub_dicos is None:
            sub_dicos = ["matching_diff", "matching_mad", "matching_empca"]

        plt.figure(figsize=(5 * len(sub_dicos), 10))

        counter = 0
        for sub_dico in sub_dicos:
            counter += 1
            matrix_rv = imported_table[sub_dico][kw_dico][col]
            matrix_rv_std = imported_table[sub_dico][kw_dico + "_std"][col]
            valid = imported_table[sub_dico]["catalog"]["valid"].astype("bool")
            valid = np.array(valid)

            if contam_training == False:
                table = imported_table[sub_dico]["catalog"].copy()

                all_keys = table.keys()
                keyword_table = []

                for j in list(all_keys):
                    if len(j.split("qc_")) > 1:
                        keyword_table.append(j)

                free = np.array(
                    np.product(table[keyword_table], axis=1) != 0
                )  # free of all kwoun contamination
                valid *= free

            matrix_rv = matrix_rv[valid]
            matrix_rv_std = matrix_rv_std[valid]

            mask = np.std(matrix_rv, axis=1) != 0
            matrix_rv = matrix_rv[mask]
            matrix_rv_std = matrix_rv_std[mask]

            mean_rvi = np.sum(matrix_rv / matrix_rv_std**2, axis=1) / np.sum(
                1 / matrix_rv_std**2, axis=1
            )
            matrix_rv = (
                matrix_rv - mean_rvi[:, np.newaxis]
            )  # because svd sensitive to global offset of each vectors

            mean_rv = np.sum(matrix_rv / matrix_rv_std**2, axis=0) / np.sum(
                1 / matrix_rv_std**2, axis=0
            )
            rv_std = 1 / np.sqrt(np.sum(1 / matrix_rv_std**2, axis=0))
            line = myc.tableXY(self.table["jdb"], mean_rv, rv_std)
            matrix = myc.table(matrix_rv)

            new_matrix_rv = []
            new_matrix_rv_std = []
            nb_lines_per_chuck = []
            wave = []

            catalog = imported_table[sub_dico]["catalog"][valid]
            catalog = catalog.reset_index()

            wave_min = np.min(catalog["wave"])
            wave_max = np.max(catalog["wave"])

            D = np.arange(0, 1.01, 1)
            wave_liste = np.arange(wave_min, wave_max + wave_bins, wave_bins)
            d, w = np.meshgrid(D, wave_liste)
            d1, d2, w1, w2 = (
                np.ravel(d[0:-1, 0:-1]),
                np.ravel(d[1:, 1:]),
                np.ravel(w[0:-1, 0:-1]),
                np.ravel(w[1:, 1:]),
            )

            for j in tqdm(range(len(d1))):
                mask = catalog["wave"] >= w1[j]
                if sum(mask) > min_nb_lines:
                    liste = np.array(catalog.loc[mask].index)
                    a = matrix.rv_subselection(rv_std=matrix_rv_std, selection=liste)
                    a.rms_w()
                    new_matrix_rv.append(a.rms)
                    new_matrix_rv_std.append(np.median(a.yerr))
                    wave.append(w1[j])
                    nb_lines_per_chuck.append(sum(mask))

            new_matrix_rv = np.array(new_matrix_rv)
            new_matrix_rv_std = np.array(new_matrix_rv_std)
            nb_lines_per_chuck = np.array(nb_lines_per_chuck)
            wave = np.array(wave)

            matrix_rv = new_matrix_rv
            matrix_rv_std = new_matrix_rv_std
            matrix_rv_z = matrix_rv / matrix_rv_std

            plt.subplot(4, len(sub_dicos), (0) * len(sub_dicos) + counter)
            plt.tick_params(direction="in", top=True)
            plt.title("%s" % (sub_dico))
            plt.plot(
                wave,
                matrix_rv,
                "ko-",
                label=r"$\lambda_{min}$ = %.0f $\AA$ | rms = %.2f m/s (%.2f m/s)"
                % (wave[np.argmin(matrix_rv)], matrix_rv[np.argmin(matrix_rv)], matrix_rv[0]),
            )
            plt.axvline(x=wave[np.argmin(matrix_rv)], color="r")
            plt.legend(loc=2)
            plt.ylabel("RV rms [m/s]")
            plt.subplot(4, len(sub_dicos), (1) * len(sub_dicos) + counter)
            plt.tick_params(direction="in", top=True)
            increase_noise = (1 - matrix_rv_std[0] / matrix_rv_std[np.argmin(matrix_rv)]) * 100
            plt.plot(
                wave,
                matrix_rv_std,
                "ko-",
                label="$\sigma_{\lambda_{min}}/\sigma_{all}$ = %.1f %%" % (increase_noise),
            )
            plt.axvline(x=wave[np.argmin(matrix_rv)], color="r")
            plt.legend(loc=2)
            plt.ylabel("med(RV) [m/s]")
            plt.subplot(4, len(sub_dicos), (2) * len(sub_dicos) + counter)
            plt.tick_params(direction="in", top=True)
            plt.plot(wave, matrix_rv_z, "ko-")
            plt.ylabel("Z score RV")
            plt.subplot(4, len(sub_dicos), (3) * len(sub_dicos) + counter)
            plt.tick_params(direction="in", top=True)
            plt.plot(wave, nb_lines_per_chuck, "ko-")
            plt.xlabel(r"Wavelength $\lambda_{cut}$ [$\AA$]")
            plt.ylabel("Number for lines ($\lambda>\lambda_{cut}$)")
            plt.subplots_adjust(left=0.10, right=0.95, top=0.95, bottom=0.1, hspace=0)

        plt.savefig(self.dir_root + "IMAGES/Blue_cut_RV_rms.pdf")

    def lbl_ica(
        self,
        sub_dico="matching_diff",
        col=0,
        season=0,
        diff=False,
        substract_rvm=False,
        nb_comp=15,
        normalise=False,
        standardize=False,
        num_sim=5000,
    ):

        self.import_lbl()
        self.import_table()
        jdb = self.table["jdb"]

        matrix_rv = self.lbl[sub_dico]["lbl"][col]
        matrix_rv_std = self.lbl[sub_dico]["lbl_std"][col]
        valid = self.lbl[sub_dico]["catalog"]["valid"].astype("bool")
        valid = np.array(valid)

        matrix_rv = matrix_rv[valid]
        matrix_rv_std = matrix_rv_std[valid]

        seasons = myf.detect_obs_season(jdb, min_gap=jdb.max() - jdb.min())
        j = 0
        if season:
            seasons = myf.detect_obs_season(jdb, min_gap=40)
            season -= 1
            j = season
        else:
            season = 0

        matrix_rv = matrix_rv[:, seasons[j, 0] : seasons[j, 1] + 1]
        matrix_rv_std = matrix_rv_std[:, seasons[j, 0] : seasons[j, 1] + 1]

        mean_rv = np.sum(matrix_rv / matrix_rv_std**2, axis=0) / np.sum(
            1 / matrix_rv_std**2, axis=0
        )
        rv_std = self.table["rv_dace_std"]
        line = myc.tableXY(self.table["jdb"], mean_rv, rv_std)

        if substract_rvm:
            matrix_rv = matrix_rv - line.y
            matrix_rv_std = np.sqrt(matrix_rv_std**2 + line.yerr**2)

        if diff:
            indices = np.triu_indices(len(matrix_rv), 1)
            diff_rv = matrix_rv - matrix_rv[:, np.newaxis]
            diff_rv = diff_rv[indices]
            diff_rv_std = np.sqrt(matrix_rv_std**2 + (matrix_rv_std**2)[:, np.newaxis])
            diff_rv_std = diff_rv_std[indices]

        norm = (1 - normalise * np.std(matrix_rv, axis=1)) + normalise * np.std(matrix_rv, axis=1)
        matrix_rv_std /= norm[:, np.newaxis]
        matrix_rv /= norm[:, np.newaxis]

        matrix = myc.table(matrix_rv)
        matrix.ICA(comp_max=nb_comp, standardize=standardize, num_sim=num_sim)

        plt.figure(figsize=(10, 7))

        backup_zscore = matrix.zscore_mixing.copy()
        sorting_zscore = np.argsort(matrix.zscore_mixing)[::-1]
        matrix.zscore_mixing = matrix.zscore_mixing[sorting_zscore]

        backup_phi = matrix.phi_mixing.copy()
        matrix.phi_mixing = abs(matrix.phi_mixing - 0.5)
        sorting_phi = np.argsort(matrix.phi_mixing)[::-1]
        matrix.phi_mixing = matrix.phi_mixing[sorting_phi]

        plt.subplot(2, 2, 1)
        plt.xlabel("# ICA components", fontsize=13)
        plt.ylabel("Z score", fontsize=13)
        plt.plot(np.arange(1, len(matrix.phi_mixing) + 1), matrix.zscore_mixing)
        a = myc.tableXY(np.arange(1, len(matrix.phi_mixing) + 1), matrix.zscore_mixing)
        a.myscatter(num=False, liste=sorting_zscore)

        plt.subplot(2, 2, 2)
        plt.plot(np.arange(1, len(matrix.phi_mixing) + 1), matrix.phi_mixing)
        plt.scatter(np.arange(1, len(matrix.phi_mixing) + 1), matrix.phi_mixing)
        plt.xlabel("# ICA components", fontsize=13)
        plt.ylabel(r"|$\Phi_0-0.5$|", fontsize=13)
        a = myc.tableXY(np.arange(1, len(matrix.phi_mixing) + 1), matrix.phi_mixing)
        a.myscatter(num=False, liste=sorting_phi)
        plt.subplots_adjust(top=0.96, bottom=0.07)

        plt.subplot(2, 1, 2)
        a = myc.tableXY(backup_zscore, backup_phi)
        a.myscatter(num=True)
        plt.ylabel(r"$\Phi_0$", fontsize=13)
        plt.xlabel("Z score", fontsize=13)
        plt.axhline(y=0.5, color="k")
        plt.axvline(x=0, color="k")

        sort = np.argsort(abs(backup_zscore) + abs(backup_phi - 0.5))[::-1]

        plt.figure(figsize=(15, 10))
        plt.subplot(nb_comp, 2, 1)
        ax = plt.gca()

        if nb_comp > 5:
            nb_comp = 5

        for j in range(nb_comp):
            plt.subplot(nb_comp, 3, 3 * j + 1, sharex=ax)
            vec = myc.tableXY(jdb, matrix.ica_vec[:, sort[j]])
            vec.plot(label="%0.f" % (sort[j]))
            plt.legend(loc=2)
            plt.subplot(nb_comp, 3, 3 * j + 2)
            vec.plot(modulo=365.25, label="%0.f" % (sort[j]))
            plt.legend(loc=2)
            plt.subplot(nb_comp, 3, 3 * j + 3)
            vec.periodogram(nb_perm=1, Norm=True)

        plt.subplots_adjust(top=0.95, bottom=0.07, left=0.07, right=0.95, hspace=0)

    def lbl_fit_sinus(
        self,
        period,
        sub_dico="matching_diff",
        season=0,
        kw_dico="lbl",
        col=0,
        deg=1,
        plot=True,
        good_morpho=False,
        blended_lines=True,
        valid_lines=True,
        subplot_name=[None, 0],
        rmax=1,
        fit_lbl=True,
        planet=[0, 26, np.pi / 2],
        cmap="jet",
        radial_axis="r_corr",
        color_axis="slope",
        plot_proxies=True,
        num_sim=1,
        significance=[0, 0],
        periodogram=False,
        fontsize=14,
        rfont=10,
        pfont=10,
        lfont=12,
        nb_col_label=4,
        cmin=None,
        cmax=None,
        circle=[None, "r", "-", 2],
        alpha_rvm=0.1,
        bbox=(-0.1, -0.3),
        proxies_to_display=None,
        legend=True,
        hist=False,
        kde=True,
        light_title=False,
        median_circle=False,
    ):
        """
        Display the periodogram for a fixed period

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        season: to plot a specific chuck of data or not
        plot : draw the polar plot
        good_morpho : to only kept good morphologic line
        subplot_name : subplot canevas
        rmax : maximum radial axis limit
        fit_lbl : True/False compute the lbl periodogram
        planet : introduce a planet
        radial_axis: kw of the radial variable
        color_axis: kw of the color variable
        num_sim: nb of bootstrap
        significance: to cut points with rcorr unsignificant [rmin,sigma]

        """

        all_proxy_liste = [
            "ccf_fwhm",
            "ccf_contrast",
            "ccf_vspan",
            "CaII",
            "CaIIK",
            "CaIIH",
            "NaD",
            "Ha",
            "Hb",
            "Hc",
            "Hd",
            "WB",
            "CB",
            "MgI",
            "HeID3",
        ]

        all_proxies = [
            "ccf_fwhm",
            "ccf_contrast",
            "ccf_vspan",
            "CaII",
            "CaIIK",
            "CaIIH",
            "NaD",
            "Ha",
            "Hb",
            "Hc",
            "Hd",
            "WB",
            "CB",
            "MgI",
            "HeID3",
            "shell1",
            "shell2",
            "shell3",
            "shell4",
            "shell_fitted",
        ]

        if proxies_to_display is None:
            proxies_to_display = all_proxy_liste

        symbols = [
            "*",
            "*",
            "*",
            "*",
            "P",
            "P",
            "P",
            "P",
            "X",
            "X",
            "X",
            "X",
            "s",
            "s",
            "h",
            "h",
            "D",
            "D",
            "D",
            "D",
            "D",
        ]  # one plus because rvm later
        color_marker = ["yellow"] + [None] * (len(symbols) - 1)
        sizes = [450] * 4 + [200] * (len(symbols) - 4)
        sizes2 = [700] * 4 + [300] * (len(symbols) - 4)

        mask_proxy = np.array([True] + list(np.in1d(all_proxies, proxies_to_display)))
        symbols = list(np.array(symbols)[mask_proxy])
        color_marker = list(np.array(color_marker)[mask_proxy])
        sizes = list(np.array(sizes)[mask_proxy])
        sizes2 = list(np.array(sizes2)[mask_proxy])

        if period == 365.25:
            proxies_to_display += ["berv"]
            color_marker += ["pink"]
            symbols += ["*"]
            sizes += [200]
            sizes2 += [300]

        deg = int(deg == 1)

        self.import_table()
        self.import_lbl()

        lbl_matrix = self.lbl
        if kw_dico == "dbd":
            self.import_dbd()
            lbl_matrix = self.dbd
        elif kw_dico == "lbl_iter":
            kw_dico = "lbl"
            self.import_lbl_iter()
            lbl_matrix = self.lbl_iter
        elif kw_dico == "aba":
            self.import_aba()
            lbl_matrix = self.aba
        elif kw_dico == "bbb":
            self.import_bbb()
            lbl_matrix = self.bbb
        elif kw_dico == "wbw":
            self.import_wbw()
            lbl_matrix = self.wbw
        elif kw_dico == "bt":
            self.import_bt()
            lbl_matrix = self.bt

        jdb = self.table["jdb"]
        tab = self.table

        matrix_rv = lbl_matrix[sub_dico][kw_dico][col, :, :]
        matrix_rv_std = lbl_matrix[sub_dico][kw_dico + "_std"][col, :, :]
        valid = np.ones(len(lbl_matrix[sub_dico][kw_dico][col, :, :])).astype("bool")

        if type(valid_lines) == bool:
            if valid_lines:
                valid = lbl_matrix[sub_dico]["catalog"]["valid"].astype("bool")
                valid = np.array(valid)
        elif type(valid_lines) == np.ndarray:
            valid = np.array(valid_lines)

        if good_morpho:
            valid = valid & (lbl_matrix[sub_dico]["catalog"]["morpho_crit"]).astype("int")

        if not blended_lines:
            valid = valid & (lbl_matrix[sub_dico]["catalog"]["blend_crit"]).astype("int")

        line_depth = lbl_matrix[sub_dico]["catalog"]["line_depth"][valid]
        depth_rel = lbl_matrix[sub_dico]["catalog"]["depth_rel"][valid]
        if "depth_vald_corrected" in list(lbl_matrix[sub_dico]["catalog"].keys()):
            depth_vald = lbl_matrix[sub_dico]["catalog"]["depth_vald_corrected"][valid]
        else:
            depth_vald = depth_rel
        wave = lbl_matrix[sub_dico]["catalog"]["wave"][valid]
        wave0 = lbl_matrix[sub_dico]["catalog"]["freq_mask0"][valid]
        ion_pot = lbl_matrix[sub_dico]["catalog"]["ionisation_energy"][valid]
        E_low = lbl_matrix[sub_dico]["catalog"]["E_low"][valid]
        element = lbl_matrix[sub_dico]["catalog"]["element"][valid]
        atomic_number = lbl_matrix[sub_dico]["catalog"]["atomic_number"][valid]
        log_gf = lbl_matrix[sub_dico]["catalog"]["log_gf"][valid]
        lande_mean = lbl_matrix[sub_dico]["catalog"]["lande_mean"][valid]
        line_weight = np.array(lbl_matrix[sub_dico]["catalog"]["weight"])[valid]
        kitcat_index = np.array(lbl_matrix[sub_dico]["catalog"].index)[valid]

        rv_planet = np.array(planet[0] * np.sin(2 * np.pi / planet[1] * tab["jdb"] + planet[2]))

        matrix_rv = matrix_rv[valid]
        matrix_rv_std = matrix_rv_std[valid]

        matrix_rv += rv_planet * np.ones(len(matrix_rv))[:, np.newaxis]

        seasons = myf.detect_obs_season(jdb, min_gap=jdb.max() - jdb.min())
        j = 0
        if season:
            seasons = myf.detect_obs_season(jdb, min_gap=40)
            season -= 1
            j = season
            season_label = season
        else:
            season = 0
            season_label = -1

        all_rv = matrix_rv[:, seasons[j, 0] : seasons[j, 1] + 1]
        all_rv_std = matrix_rv_std[:, seasons[j, 0] : seasons[j, 1] + 1]
        all_rv_weights = 1 / all_rv_std**2

        mean_rv = np.sum(all_rv / all_rv_std**2, axis=0) / np.sum(1 / all_rv_std**2, axis=0)
        mean_rv_std = np.sqrt(1 / np.sum(1 / all_rv_std**2, axis=0))

        mean_time = np.mean(jdb)
        time_jdb = jdb[seasons[j, 0] : seasons[j, 1] + 1]
        mean_time = np.mean(time_jdb)
        time_jdb -= mean_time

        main_points = [mean_rv]
        main_points_std = [mean_rv_std]

        if kw_dico != "lbl":
            main_points = [np.array(tab["ccf_rv"][seasons[season, 0] : seasons[season, 1] + 1])]
            main_points_std = [
                np.array(tab["ccf_rv_std"][seasons[season, 0] : seasons[season, 1] + 1])
            ]

        for proxy in proxies_to_display:
            main_points.append(np.array(tab[proxy][seasons[season, 0] : seasons[season, 1] + 1]))
            if proxy != "berv":
                try:
                    vec_std = np.array(
                        tab[proxy + "_std"][seasons[season, 0] : seasons[season, 1] + 1]
                    )
                    vec_std[vec_std == 0] = np.max(vec_std) + 1
                    main_points_std.append(vec_std)
                except:
                    main_points_std.append(np.std(main_points[-1]) * np.ones(len(main_points[-1])))
            else:
                main_points_std.append(np.std(main_points[-1]) * np.ones(len(main_points[-1])))

        # noise = myc.tableXY(time_jdb,np.random.randn(len(time_jdb)))
        # main_points.append(noise.y)
        # main_points_std.append(noise.yerr)

        proxies_to_display = ["rvm"] + proxies_to_display  # +['white_noise']
        # symbols = symbols + ['.']

        if deg == 1:
            base_vec = np.vstack(
                [
                    np.ones(len(time_jdb)),
                    np.cos(2 * np.pi / period * time_jdb),
                    np.sin(2 * np.pi / period * time_jdb),
                    time_jdb,
                ]
            )
        else:
            base_vec = np.vstack(
                [
                    np.ones(len(time_jdb)),
                    np.cos(2 * np.pi / period * time_jdb),
                    np.sin(2 * np.pi / period * time_jdb),
                ]
            )

        main_points = np.array(main_points)
        main_points_std = np.array(main_points_std)

        main_points[main_points == None] = 0

        main_points_std[main_points_std == None] = 1e6
        main_points_std = np.abs(main_points_std).astype("float")

        remove = np.sum(abs(main_points), axis=1) != 0

        # supress null proxies

        self.debug1 = (proxies_to_display, symbols, main_points, main_points_std)

        if len(np.array(proxies_to_display)[~remove]):
            print(
                "Proxies suppressed because of null vector : ",
                np.array(proxies_to_display)[~remove],
            )
        proxies_to_display = np.array(proxies_to_display)[remove]
        symbols = np.array(symbols)[remove]
        main_points = main_points[remove]
        main_points_std = main_points_std[remove]

        self.debug2 = (proxies_to_display, symbols, main_points, main_points_std)

        proxies = myc.table(main_points.astype("float"))
        proxies.fit_base(base_vec, weight=1 / main_points_std**2)

        print(np.shape(proxies.table), np.shape(base_vec))

        # proxies

        offset_star = proxies.coeff_fitted[:, 0]
        time_drift_star = proxies.coeff_fitted[:, 2 + deg] * deg
        K_star = np.sqrt(proxies.coeff_fitted[:, 1] ** 2 + proxies.coeff_fitted[:, 2] ** 2)
        r_slope_star = K_star / np.nanpercentile(K_star, 95)
        phi_star = np.arctan2(proxies.coeff_fitted[:, 1], proxies.coeff_fitted[:, 2])

        ref_proxy = np.where(proxies_to_display == "CaII")[0][0]
        phi0 = phi_star[ref_proxy]
        phi_star -= phi0

        drift_star = (
            offset_star[:, np.newaxis] + np.array(time_jdb) * time_drift_star[:, np.newaxis]
        )
        rv_lin_detrended_star = proxies.table - drift_star

        eps = np.zeros(np.shape(proxies.vec_fitted))
        eps[((proxies.vec_fitted - drift_star) == 0) & ((K_star[:, np.newaxis]) == 0)] = 1

        model_lin_detrended_star = (proxies.vec_fitted - drift_star) / (
            K_star[:, np.newaxis] + eps
        )

        eps = np.zeros(np.shape(K_star))
        eps[
            ((K_star * np.std(model_lin_detrended_star, axis=1)) == 0)
            & ((np.std(rv_lin_detrended_star, axis=1)) == 0)
        ] = 1

        r_corr_star = (
            K_star
            * np.std(model_lin_detrended_star, axis=1)
            / (np.std(rv_lin_detrended_star, axis=1) + eps)
        )

        mat_star = np.array(
            [
                offset_star,
                0 * offset_star,
                time_drift_star,
                0 * time_drift_star,
                K_star,
                0 * K_star,
                phi_star,
                0 * phi_star,
                r_corr_star,
                0 * r_corr_star,
                r_slope_star,
                0 * r_slope_star,
                0 * phi_star + mean_time,
                0 * phi_star + phi0,
            ]
        ).T

        mmat = pd.DataFrame(
            mat_star,
            index=proxies_to_display,
            columns=[
                "offset",
                "offset_std",
                "drift",
                "drift_std",
                "K",
                "K_std",
                "phi",
                "phi_std",
                "r_corr",
                "r_corr_std",
                "slope",
                "slope_std",
                "mean_time",
                "phi0",
            ],
        )

        self.corr_sinus_proxies = mmat

        # line

        if fit_lbl:
            rv = myc.table(all_rv)
            rv.fit_base(base_vec, weight=all_rv_weights, num_sim=num_sim)

            if num_sim == 1:
                matrix_all_rv = rv.table[:, np.newaxis, :]

                rv_coeff = rv.coeff_fitted
                rv_coeff_std = rv.coeff_fitted_std

                K_mean = np.sqrt(rv_coeff[:, 1] ** 2 + rv_coeff[:, 2] ** 2)
                phi_mean = np.arctan2(rv_coeff[:, 1], rv_coeff[:, 2])

                eps = np.zeros(len(K_mean))
                eps[((rv_coeff_std[:, 1] * rv_coeff[:, 1]) == 0) & (K_mean == 0)] = 1

                K_std = np.sqrt(
                    (rv_coeff_std[:, 1] * rv_coeff[:, 1] / (K_mean + eps)) ** 2
                    + (rv_coeff_std[:, 2] * rv_coeff[:, 2] / (K_mean + eps)) ** 2
                )
                phi_std = np.sqrt(
                    (rv_coeff_std[:, 1] * rv_coeff[:, 2] / (K_mean + eps) ** 2) ** 2
                    + (rv_coeff_std[:, 2] * rv_coeff[:, 1] / (K_mean + eps) ** 2) ** 2
                )

                rv_coeff = rv.coeff_fitted[:, :, np.newaxis]

            else:
                # rv_coeff = rv.coeff_mean
                # rv_coeff_std = rv.coeff_std
                rv_coeff = rv.coeff_resampling
                matrix_all_rv = rv.vec_resampling
                rv_coeff_std = np.nanstd(rv_coeff, axis=2)

                K_mean = np.sqrt(
                    np.nanmean(rv_coeff[:, 1, :], axis=1) ** 2
                    + np.nanmean(rv_coeff[:, 2, :], axis=1) ** 2
                )
                K_std = np.sqrt(
                    (rv_coeff_std[:, 1] * np.nanmean(rv_coeff[:, 1, :], axis=1) / K_mean) ** 2
                    + (rv_coeff_std[:, 2] * np.nanmean(rv_coeff[:, 2, :], axis=1) / K_mean) ** 2
                )
                phi_mean = np.arctan2(
                    np.nanmean(rv_coeff[:, 1, :], axis=1), np.nanmean(rv_coeff[:, 2, :], axis=1)
                )
                phi_std = np.sqrt(
                    (rv_coeff_std[:, 1] * np.nanmean(rv_coeff[:, 2, :], axis=1) / K_mean**2) ** 2
                    + (rv_coeff_std[:, 2] * np.nanmean(rv_coeff[:, 1, :], axis=1) / K_mean**2)
                    ** 2
                )

            phi_mean -= phi0

            offset = rv_coeff[:, 0, :]
            offset_mean = np.nanmean(offset, axis=1)
            offset_std = rv_coeff_std[:, 0]

            time_drift = rv_coeff[:, 2 + deg, :] * deg
            time_drift_mean = np.nanmean(time_drift, axis=1)
            time_drift_std = rv_coeff_std[:, 2 + deg] * deg

            r_slope_mean = K_mean
            r_slope_std = K_std

            drift = (
                offset[:, :, np.newaxis] * np.ones(len(time_jdb))
                + np.array(time_jdb) * time_drift[:, :, np.newaxis]
            )
            rv_lin_detrended = np.nanmean(matrix_all_rv - drift, axis=1)

            eps = np.zeros(np.shape(rv.vec_fitted))
            eps[
                ((rv.vec_fitted - np.nanmean(drift, axis=1)) == 0) & ((K_mean[:, np.newaxis]) == 0)
            ] = 1

            model_lin_detrended = (rv.vec_fitted - np.nanmean(drift, axis=1)) / (
                K_mean[:, np.newaxis] + eps
            )

            # vec_fitted_detrended = base_vec[1]*rv_coeff[:,1,:][:,:,np.newaxis] + base_vec[2]*rv_coeff[:,2,:][:,:,np.newaxis]
            # model_lin_detrended = vec_fitted_detrended/(K_mean*np.ones(num_sim)[:,np.newaxis]).T[:,:,np.newaxis]

            # rv_lin_detrended = all_rv - np.nanmean(drift,axis=1)

            t = myc.table(rv_lin_detrended)
            t.rms_w(1 / all_rv_std**2, axis=1)

            eps = np.zeros(len(K_mean))
            eps[((K_mean * np.std(model_lin_detrended, axis=1)) == 0) & (t.rms == 0)] = 1

            r_corr_mean = (
                K_mean * np.std(model_lin_detrended, axis=1) / (t.rms + eps)
            )  # np.std(rv_lin_detrended, axis=1)
            r_corr_std = (
                K_std * np.std(model_lin_detrended, axis=1) / (t.rms + eps)
            )  # np.std(rv_lin_detrended, axis=1)

            # r_corr_mean = np.nanmean(r_corr, axis=1)
            # r_corr_std = np.nanstd(r_corr, axis=1)
            mat = np.array(
                [
                    offset_mean,
                    offset_std,
                    time_drift_mean,
                    time_drift_std,
                    K_mean,
                    K_std,
                    phi_mean,
                    phi_std,
                    r_corr_mean,
                    r_corr_std,
                    r_slope_mean,
                    r_slope_std,
                    line_depth,
                    depth_rel,
                    depth_vald,
                    wave,
                    wave0,
                    ion_pot,
                    E_low,
                    atomic_number,
                    log_gf,
                    lande_mean,
                    line_weight,
                    kitcat_index,
                ]
            ).T
            # self.mat_bootstrap = np.array([offset, time_drift, K])

            mat = pd.DataFrame(
                mat,
                columns=[
                    "offset",
                    "offset_std",
                    "drift",
                    "drift_std",
                    "K",
                    "K_std",
                    "phi",
                    "phi_std",
                    "r_corr",
                    "r_corr_std",
                    "slope",
                    "slope_std",
                    "line_depth",
                    "depth_rel",
                    "depth_vald_corrected",
                    "wave",
                    "freq_mask0",
                    "ionisation_energy",
                    "E_low",
                    "atomic_number",
                    "log_gf",
                    "lande_mean",
                    "weight",
                    "kitcat_index",
                ],
            )

            mat["element"] = np.array(element)  # outside the main because string type

            if periodogram:
                tab = self.lbl_periodogram(
                    period,
                    sub_dico=sub_dico,
                    fap=1,
                    zone=0,
                    valid_lines=valid_lines,
                    kw_dico=kw_dico,
                )
                mat["r_corr"] = tab["power"] * tab["fap"]
                mat["phi"] = tab["phi"]
                mat["K"] = tab["amp"]

            self.corr_sinus_lbl = mat

            # periodogram criterion

            # phase_to_rvm = ((phi_mean - phi_star[0])+np.pi/2)%(2*np.pi)

            lim = myf.find_nearest(
                np.array(np.cumsum(mat.sort_values(by="r_corr")["weight"])), 50
            )[0][0]
            v_lim = np.array(mat.sort_values(by="r_corr")["r_corr"])[lim]

        if plot:
            if rmax is None:
                if radial_axis == "r_corr":
                    rmax = 1
                elif radial_axis == "K":
                    rmax = 2.5 * myf.IQ(mat["K"]) + np.nanpercentile(mat["K"], 75)

            if subplot_name[0] is None:
                fig = plt.figure(figsize=(12, 12))
                subplot_name[1] = 111
                button = "on"
            else:
                fig = subplot_name[0]
                button = "off"
            ax = fig.add_subplot(subplot_name[1], projection="polar")
            ax.set_rlabel_position(90)
            if hist * int(fit_lbl):
                N = hist
                bins = np.linspace(0, 2 * np.pi, N + 1)
                dbin = bins[1] - bins[0]
                phi = (np.array(mat["phi"])) % (2 * np.pi)
                mask = (phi <= bins[1:, np.newaxis]) & (phi > bins[:-1, np.newaxis])
                histo = np.sum(mask, axis=1)
                N_random = np.sum(histo) / N
                histo = (histo - N_random) / (
                    N_random * 4
                )  # touch 0.2 limit circle if 5 times for points in bins than random distribution
                width = (2 * np.pi) / N
                ax.bar(
                    bins[:-1] + dbin / 2,
                    histo * (0.19) * rmax,
                    width=width,
                    bottom=rmax,
                    alpha=0.4,
                    edgecolor="k",
                )
            if radial_axis == "K":
                rmax *= 1.19

            if type(light_title) == bool:
                if light_title:
                    plt.title("Analysis : %s_%.0f" % (kw_dico.upper(), col), fontsize=fontsize)
                else:
                    plt.title(
                        "Analysis : %s_%.0f        Season : %.0f        Sub dico : %s        Period : %.2f"
                        % (kw_dico.upper(), col, np.round(season_label + 1), sub_dico, period),
                        fontsize=fontsize,
                    )
            elif type(light_title) == str:
                plt.title(light_title + "\n", fontsize=fontsize)

            plt.polar(phi_star, mat_star[:, 4], "k.", alpha=0.0)
            ax_test = plt.gca()
            plt.yticks(fontsize=rfont)
            plt.xticks(fontsize=pfont)

            if radial_axis == "r_corr":
                plt.plot(
                    np.linspace(0, 2 * np.pi, 100), np.ones(100) * rmax, color="k", ls="-", lw=2.5
                )
                if median_circle:
                    plt.plot(
                        np.linspace(0, 2 * np.pi, 100),
                        v_lim * np.ones(100),
                        color="b",
                        ls="-",
                        lw=2.5,
                        alpha=0.4,
                    )

            if circle[0] is not None:
                plt.plot(
                    np.linspace(0, 2 * np.pi, 100),
                    circle[0] * np.ones(100),
                    color=circle[1],
                    ls=circle[2],
                    lw=circle[3],
                )
            # plt.scatter(phi,r_corr,edgecolor='k',facecolor='white',cmap='jet',vmin=0,vmax=20,alpha=1)
            if fit_lbl:
                if color_axis == "slope":
                    name_colorbar = "slope"
                elif color_axis == "K":
                    name_colorbar = "K semi-amplitude [m/s]"
                elif color_axis == "line_depth":
                    name_colorbar = "Line depth"
                elif color_axis == "wave":
                    name_colorbar = "Wavelength [$\AA$]"
                elif color_axis == "E_low":
                    name_colorbar = "$\chi$ [eV]"
                elif color_axis == "lande_mean":
                    name_colorbar = "$g_{eff}$"
                elif color_axis == "ionisation_energy":
                    name_colorbar = "$I$ [eV]"
                # vmax = np.median(color)+2*myf.IQ(color)
                points_kept = mat.loc[
                    abs(mat["r_corr"]) - significance[1] * mat["r_corr_std"] > significance[0]
                ]

                var_color = np.array(points_kept[color_axis]).astype("float")
                IQ = myf.IQ(var_color)
                Q3 = np.nanpercentile(var_color, 75)
                Q1 = np.nanpercentile(var_color, 25)
                var_color[np.isnan(var_color)] = np.nanmedian(var_color)
                var_color[
                    (var_color > (Q3 + 3 * IQ)) | (var_color < (Q1 - 3 * IQ))
                ] = np.nanmedian(var_color)

                color = var_color
                vmax = np.nanpercentile(color, 75) + 1.5 * myf.IQ(color)
                if vmax > np.max(color):
                    vmax = np.max(color)

                if cmax is not None:
                    vmax = cmax

                if cmin is not None:
                    vmin = cmin
                else:
                    vmin = color.min()

                theta, r = np.array(points_kept["phi"]).astype("float"), np.array(
                    points_kept[radial_axis]
                ).astype("float")
                if kde:
                    x, y = r * np.sin(theta), r * np.cos(theta)

                    test = myc.tableXY(x, y)
                    test.kde(levels=["2d", [1]], alpha=0.01)
                    x_cs_sig1, y_cs_sig1 = (
                        test.vertices_curve[0][:, 0],
                        test.vertices_curve[0][:, 1],
                    )
                    theta_cs_sig1, r_cs_sig1 = np.arctan2(x_cs_sig1, y_cs_sig1), np.sqrt(
                        x_cs_sig1**2 + y_cs_sig1**2
                    )
                    if radial_axis == "r_corr":
                        r_cs_sig1[r_cs_sig1 > 1] = 1

                    test.kde(levels=["2d", [2]], alpha=0.01)
                    x_cs_sig2, y_cs_sig2 = (
                        test.vertices_curve[0][:, 0],
                        test.vertices_curve[0][:, 1],
                    )
                    theta_cs_sig2, r_cs_sig2 = np.arctan2(x_cs_sig2, y_cs_sig2), np.sqrt(
                        x_cs_sig2**2 + y_cs_sig2**2
                    )
                    if radial_axis == "r_corr":
                        r_cs_sig2[r_cs_sig2 > 1] = 1

                    plt.plot(theta_cs_sig1, r_cs_sig1, color="k", lw=3)
                    plt.plot(theta_cs_sig2, r_cs_sig2, color="k", lw=3)
                plt.scatter(
                    theta,
                    r,
                    c=color,
                    edgecolor="k",
                    cmap=cmap,
                    vmin=vmin,
                    vmax=vmax,
                    alpha=0.3 * int(kde) + 0.6 * int(1 - kde),
                )
                ax2 = plt.colorbar(orientation="vertical")
                ax2.ax.set_ylabel(name_colorbar, fontsize=fontsize)
            else:
                button = "off"
            k = 0
            if plot_proxies:
                proxies_to_display = [p.replace("shell", r"$\alpha$") for p in proxies_to_display]
                proxies_to_display = [p.replace("fitted", "model") for p in proxies_to_display]

                for sym, name in zip(symbols, proxies_to_display):
                    plt.scatter(
                        phi_star[k],
                        r_corr_star[k],
                        marker=sym,
                        color="white",
                        zorder=99,
                        s=sizes2[k],
                        lw=1.5,
                    )
                    plt.scatter(
                        phi_star[k],
                        r_corr_star[k],
                        marker=sym,
                        color=color_marker[k],
                        edgecolor="k",
                        zorder=100,
                        s=sizes[k],
                        label=name,
                        lw=1,
                    )
                    k += 1

                plt.plot(
                    [phi_star[ref_proxy], phi_star[ref_proxy] + np.pi],
                    [1, 1],
                    color="k",
                    alpha=0.5,
                    lw=1,
                )
                plt.plot(
                    [phi_star[0], phi_star[0] + np.pi], [1, 1], color="k", alpha=alpha_rvm, lw=3
                )

            if hist:
                ax.set_ylim(0, rmax + 0.19)
            else:
                ax.set_ylim(0, rmax)
            if legend:
                plt.legend(loc=3, bbox_to_anchor=bbox, ncol=nb_col_label, fontsize=lfont)
            if button == "on":
                plt.subplots_adjust(bottom=0.1, top=0.95, left=0.06, right=0.96)

        if fit_lbl:
            return mat, mmat
        else:
            return [], mmat

    def lbl_fit_sinus_all(
        self,
        period,
        season=0,
        grid="22",
        all_dico=["matching_diff", "matching_pca", "matching_mad", "matching_morpho"],
        good_morpho=False,
        blended_lines=True,
        rmax=1,
        kw_dico="lbl",
        col=0,
        color_axis="slope",
    ):

        fig = plt.figure(figsize=(21, 17))
        for k, dico in enumerate(all_dico):
            self.lbl_fit_sinus(
                period,
                season=season,
                sub_dico=dico,
                good_morpho=good_morpho,
                blended_lines=blended_lines,
                subplot_name=[fig, int(grid + str(k + 1))],
                rmax=rmax,
                proxies_to_display=["ccf_fwhm", "ccf_contrast", "ccf_vspan", "CaII"],
                legend=False,
                kw_dico=kw_dico,
                col=col,
                color_axis=color_axis,
            )
            plt.subplots_adjust(top=0.96, left=0.07, right=0.93, bottom=0.08)

    def lbl_merge_sinus(
        self,
        period,
        sub_dico="matching_mad",
        season=0,
        good_morpho=False,
        kw_dico=["lbl", "dbd", "aba"],
    ):

        s1 = []
        m1 = []

        if "lbl" in kw_dico:
            for i in [0, 2]:
                lbl, dust = self.lbl_fit_sinus(
                    period,
                    sub_dico=sub_dico,
                    season=season,
                    good_morpho=good_morpho,
                    kw_dico="lbl",
                    col=i,
                )
                s1.append("lbl" + str(i))
                m1.append(lbl)
            plt.close("all")

        if "dbd" in kw_dico:
            for i in [0, 1, 2]:
                dbd, dust = self.lbl_fit_sinus(
                    period,
                    sub_dico=sub_dico,
                    season=season,
                    good_morpho=good_morpho,
                    kw_dico="dbd",
                    col=i,
                )
                s1.append("dbd" + str(i))
                m1.append(dbd)
            plt.close("all")

        if "aba" in kw_dico:
            for i in [0, 1, 2, 3, 4, 5, 6, 7]:
                aba, dust = self.lbl_fit_sinus(
                    period,
                    sub_dico=sub_dico,
                    season=season,
                    good_morpho=good_morpho,
                    kw_dico="aba",
                    col=i,
                )
                s1.append("aba" + str(i))
                m1.append(aba)
            plt.close("all")

        if "bt" in kw_dico:
            for i in [0, 1, 2, 3, 4, 5, 6, 7]:
                bt, dust = self.lbl_fit_sinus(
                    period,
                    sub_dico=sub_dico,
                    season=season,
                    good_morpho=good_morpho,
                    kw_dico="bt",
                    col=i,
                )
                s1.append("bt" + str(i))
                m1.append(bt)
            plt.close("all")

        if "wbw" in kw_dico:
            for i in [0, 1, 2]:
                wbw, dust = self.lbl_fit_sinus(
                    period,
                    sub_dico=sub_dico,
                    season=season,
                    good_morpho=good_morpho,
                    kw_dico="wbw",
                    col=i,
                )
                s1.append("wbw" + str(i))
                m1.append(wbw)
            plt.close("all")

        atomic_column = [
            "kitcat_index",
            "element",
            "freq_mask0",
            "ionisation_energy",
            "E_low",
            "atomic_number",
            "log_gf",
            "lande_mean",
            "weight",
            "line_depth",
            "wave",
        ]
        merge_dataframe = m1[0]

        for s, m in zip(s1, m1):
            m = m.drop(columns=atomic_column[1:])
            merge_dataframe = pd.merge(
                merge_dataframe, m, on="kitcat_index", suffixes=["", "_" + s]
            )

        merge_dataframe = merge_dataframe.drop(
            columns=np.setdiff1d(list(m1[0].keys()), atomic_column)
        )

        self.merge_dataframe = merge_dataframe

        return merge_dataframe

    def lbl_fit_sinus_slider(
        self,
        period=np.arange(20, 50, 0.10),
        frequency=np.linspace(1 / 50, 1 / 25, 100),
        sub_dico="matching_diff",
        season=0,
        good_morpho=False,
        blended_lines=True,
        kw_dico="lbl",
        col=0,
        radial_axis="r_corr",
        planet=[0, 26, np.pi / 2],
        num_sim=1,
    ):
        mat = []
        mat_star = []

        if radial_axis == "r_corr":
            column = 8
        elif radial_axis == "slope":
            column = 10

        if period is None:
            period = (1 / frequency)[::-1]

        for p in tqdm(period):
            a, b = self.lbl_fit_sinus(
                p,
                sub_dico=sub_dico,
                season=season,
                plot=False,
                fit_lbl=True,
                good_morpho=good_morpho,
                blended_lines=blended_lines,
                planet=planet,
                num_sim=num_sim,
                kw_dico=kw_dico,
                col=col,
            )
            mat.append(np.array(a))
            mat_star.append(np.array(b))

        mat = np.array(mat)
        mat_star = np.array(mat_star)

        fig = plt.figure(figsize=(12, 12))
        ax = fig.add_subplot(111, projection="polar")
        l3 = plt.title(
            "Season : %.0f\nSub dico : %s\nPeriod : %.2f [days]" % (season, sub_dico, period[0])
        )
        plt.polar(mat_star[0, :, 6], mat_star[0, :, column], "k.", alpha=0.0)
        # plt.scatter(phi,r_corr,edgecolor='k',facecolor='white',cmap='jet',vmin=0,vmax=20,alpha=1)

        l2 = plt.scatter(
            mat_star[0, 1:, 6],
            mat_star[0, 1:, column],
            edgecolor="k",
            color="orange",
            marker="*",
            zorder=100,
            s=250,
        )
        (l4,) = plt.plot(
            [mat_star[0, 0, 6], mat_star[0, 0, 6] + np.pi], [1, 1], color="k", alpha=0.3, lw=1.5
        )
        l5 = plt.scatter(
            mat_star[0, 0, 6],
            mat_star[0, 0, column],
            edgecolor="k",
            color="red",
            marker="*",
            zorder=100,
            s=250,
        )
        l = plt.scatter(
            mat[0, :, 6],
            mat[0, :, column],
            c=mat[0, :, 4],
            edgecolor="k",
            cmap="jet",
            vmin=0,
            vmax=10,
            alpha=0.6,
            zorder=1,
        )

        plt.colorbar()

        ax.set_ylim(0, 1)

        axcolor = "whitesmoke"
        axtime = plt.axes([0.2, 0.05, 0.57, 0.03], facecolor=axcolor)
        slider_time = Slider(axtime, "Index time", 0, len(period) - 1, valinit=0, valstep=1)

        class Index:
            def update(self, val):
                idx = int(slider_time.val)
                l.set_offsets(np.array([mat[idx, :, 6], mat[idx, :, 8]]).T)
                l2.set_offsets(np.array([mat_star[idx, :, 6], mat_star[idx, :, 8]]).T)
                l4.set_xdata(np.array([mat_star[idx, 4, 6], mat_star[idx, 4, 6] + np.pi]))
                l5.set_offsets(np.array([mat_star[idx, 0, 6], mat_star[idx, 0, 8]]).T)

                fig.canvas.draw_idle()
                l3.set_text(
                    "Season : %.0f\nSub dico : %s\nPeriod : %.2f [days]"
                    % (season, sub_dico, period[idx])
                )

        callback = Index()
        slider_time.on_changed(callback.update)
        plt.show()

        self.corr_sinus_all = {
            "coeff": mat,
            "period": period,
            "sub_dico": sub_dico,
            "season": season,
        }

        if False:
            plt.figure()
            distri = []
            for k in range(len(mat)):
                a, b, c = plt.hist(
                    (mat[k, :, 3] - np.pi) % (2 * np.pi),
                    bins=np.arange(0, 2 * np.pi, 2 * np.pi / 36),
                )
                distri.append(a)
            plt.close()

            distri = np.array(distri)
            bins = 0.5 * (b[1:] + b[0:-1])
            distri = np.vstack([bins, distri])
            self.distri = distri

        #            fig = plt.figure(figsize=(12,12))
        #            l2, = plt.plot(distri[0],distri[1],color='k')
        #            plt.ylim(0,np.max(distri))
        #
        #            axcolor = 'whitesmoke'
        #            axtime = plt.axes([0.2, 0.05, 0.57, 0.03], facecolor = axcolor)
        #            slider_time = Slider(axtime, 'Period [days]', period[0], period[-1], valinit = period[0], valstep=np.diff(period)[0])
        #
        #            class Index():
        #                def update(self,val):
        #                    idx = np.where(period==slider_time.val)[0][0]
        #                    l2.set_ydata(distri[idx+1])
        #                    fig.canvas.draw_idle()
        #
        #            callback = Index()
        #            slider_time.on_changed(callback.update)
        #            plt.show()

        return mat

    def lbl_fit_sinus_periodogram_simu(
        self,
        liste,
        name_ext="",
        period=myf.my_ruler(10, 500, 0.25, 2.5),
        p_min=4,
        p_max=500,
        nb_p=1000,
        good_morpho=False,
        fit_lbl=True,
        num_sim=1,
        planet=[0, 26, np.pi],
    ):

        """give a list to lauche the periodogram under format [[season, dico, kw_dico, col],[...]]"""

        if period is None:
            frequency = np.linspace(1 / p_max, 1 / p_min, nb_p)
            period = (1 / frequency)[::-1]

        for season, dico, kw_dico, col in liste:
            all_period = []
            all_period_proxies = []
            for p in tqdm(period):
                mat, mat_star = self.lbl_fit_sinus(
                    p,
                    sub_dico=dico,
                    season=season,
                    plot=False,
                    col=col,
                    good_morpho=good_morpho,
                    fit_lbl=fit_lbl,
                    num_sim=num_sim,
                    kw_dico=kw_dico,
                    planet=planet,
                )

                if len(mat):
                    b = mat[
                        ["r_corr", "r_corr_std", "K", "K_std", "phi", "phi_std", "weight"]
                    ].copy()
                    b["period"] = p
                    all_period.append(np.array(b))

                a = mat_star[
                    ["r_corr", "r_corr_std", "K", "K_std", "phi", "phi_std", "mean_time", "phi0"]
                ].copy()
                a["period"] = p
                all_period_proxies.append(np.array(a))

            all_period = np.array(all_period)
            all_period_proxies = np.array(all_period_proxies)
            if len(all_period):
                pmin = np.min(period)
                pmax = np.max(period)
                myf.pickle_dump(
                    {"matrice": all_period},
                    open(
                        self.dir_root
                        + "PERIODOGRAM/%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f"
                        % (dico, season, kw_dico, col, pmin, pmax)
                        + name_ext
                        + ".p",
                        "wb",
                    ),
                )
                print(
                    "\n File saved : %s"
                    % (
                        self.dir_root
                        + "PERIODOGRAM/%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f"
                        % (dico, season, kw_dico, col, pmin, pmax)
                        + name_ext
                        + ".p"
                    )
                )
                time.sleep(1)
            if np.shape(all_period_proxies)[1]:
                pmin = np.min(period)
                pmax = np.max(period)
                myf.pickle_dump(
                    {"matrice": all_period_proxies, "proxies": list(a.index)},
                    open(
                        self.dir_root
                        + "PERIODOGRAM/%s_%.0f_proxies_pmin_%.0f_pmax_%.0f"
                        % (dico, season, pmin, pmax)
                        + name_ext
                        + ".p",
                        "wb",
                    ),
                )
                print(
                    "\n File saved : %s"
                    % (
                        self.dir_root
                        + "PERIODOGRAM/%s_proxies_%.0f_pmin_%.0f_pmax_%.0f"
                        % (dico, season, pmin, pmax)
                        + name_ext
                        + ".p"
                    )
                )
                time.sleep(1)

    def lbl_new_periodogram(self, filename, use_dbd=True):
        self.import_lbl()
        self.import_dbd()
        self.import_table()
        jdb = np.array(self.table.jdb)

        file_proxies = np.sort(
            glob.glob(self.dir_root + "PERIODOGRAM/*proxies*" + filename + ".p")
        )[0]
        dico_kw = file_proxies.split("/")[-1].split("_")[1]

        lbl = self.lbl["matching_" + dico_kw]["lbl"][0]
        lbl_std = self.lbl["matching_" + dico_kw]["lbl_std"][0]

        dbd = self.dbd["matching_" + dico_kw]["dbd"][1]
        dbd_std = self.dbd["matching_" + dico_kw]["dbd_std"][1]

        mask = np.std(lbl, axis=1) != 0
        lbl = lbl[mask]
        lbl_std = lbl_std[mask]
        dbd = dbd[mask]
        dbd_std = dbd_std[mask]

        proxies = pd.read_pickle(file_proxies)["matrice"]
        proxies_name = pd.read_pickle(file_proxies)["proxies"]
        phi0 = proxies[:, 0, 7]
        tmean = proxies[0, 0, 6]

        proxies_rows = np.where(np.in1d(np.array(proxies_name), np.array(["CaII"])))[0]
        proxies_name = np.array(proxies_name)[proxies_rows]

        file_lbl = glob.glob(
            self.dir_root + "PERIODOGRAM/*" + dico_kw + "*lbl_0*" + filename + ".p"
        )[0]
        file_dbd = glob.glob(
            self.dir_root + "PERIODOGRAM/*" + dico_kw + "*dbd_1*" + filename + ".p"
        )[0]

        # lbl
        matrice_lbl = pd.read_pickle(file_lbl)["matrice"]

        period = matrice_lbl[:, 0, -1]
        k = matrice_lbl[:, :, 2]
        k_std = matrice_lbl[:, :, 3]
        phi = matrice_lbl[:, :, 4] % (2 * np.pi)
        phi_std = matrice_lbl[:, :, 5]

        #        x = np.cos(phi)*k
        #        y = np.sin(phi)*k
        #        x_std = np.sqrt((k*np.cos(phi)*phi_std)**2+(k_std*np.sin(phi))**2)
        #        y_std = np.sqrt((k*np.sin(phi)*phi_std)**2+(k_std*np.cos(phi))**2)
        #        x_mean = np.sum(x/x_std**2,axis=1)/np.sum(1/x_std**2,axis=1)
        #        y_mean = np.sum(y/y_std**2,axis=1)/np.sum(1/y_std**2,axis=1)
        #        r_mean = np.sqrt(x_mean**2+y_mean**2)
        #        theta_mean = np.arctan2(y_mean,x_mean)
        #        theta_mean += phi0
        #        theta_mean = (theta_mean-2*np.pi*tmean/period)%(2*np.pi)

        k_mean = np.sum(k / k_std**2, axis=1) / np.sum(1 / k_std**2, axis=1)
        phi_mean = np.sum(phi / phi_std**2, axis=1) / np.sum(1 / phi_std**2, axis=1)
        phi_mean += phi0
        phi_mean = (phi_mean - 2 * np.pi * tmean / period) % (2 * np.pi)

        perio = pd.DataFrame({"period": period, "phi_mean_lbl": phi_mean, "k_mean_lbl": k_mean})

        # dbd
        matrice_dbd = pd.read_pickle(file_dbd)["matrice"]

        period = matrice_dbd[:, 0, -1]
        k = matrice_dbd[:, :, 2]
        k_std = matrice_dbd[:, :, 3]
        phi = matrice_dbd[:, :, 4] % (2 * np.pi)
        phi_std = matrice_dbd[:, :, 5]

        k_mean = np.sum(k / k_std**2, axis=1) / np.sum(1 / k_std**2, axis=1)
        phi_mean = np.sum(phi / phi_std**2, axis=1) / np.sum(1 / phi_std**2, axis=1)
        phi_mean += phi0
        phi_mean = (phi_mean - 2 * np.pi * tmean / period) % (2 * np.pi)

        perio["phi_mean_dbd"] = phi_mean
        perio["k_mean_dbd"] = k_mean
        perio["phi_dist"] = 1 - myf.dist_modulo(perio["phi_mean_lbl"], perio["phi_mean_dbd"]) / 180
        perio["dist_qtl"] = myf.transform_prim(period, perio["phi_dist"])[1]

        sinus_fitted = np.array(perio.k_mean_lbl) * np.sin(
            2 * np.pi * jdb[:, np.newaxis] / np.array(perio.period) + np.array(perio.phi_mean_lbl)
        )
        # sinus_fitted2 = r_mean*np.sin(2*np.pi*jdb[:,np.newaxis]/period+theta_mean)

        dim = len(lbl) * len(lbl[0])

        chi2_l_0 = np.sum(lbl**2 / lbl_std**2) / dim
        chi2_l = np.zeros(len(period))

        for i in tqdm(range(len(chi2_l))):
            vec = sinus_fitted[:, i]

            # res = lbl - vec[np.newaxis,:]
            # offset_l = np.sum(res/lbl_std**2,axis=1)/np.sum(1/lbl_std**2,axis=1)
            # res = dbd - vec[np.newaxis,:]
            # offset_r = np.sum(res/dbd_std**2,axis=1)/np.sum(1/dbd_std**2,axis=1)

            chi2_lbl = np.sum((lbl - vec[np.newaxis, :]) ** 2 / (lbl_std) ** 2) / dim
            chi2_l[i] = chi2_lbl

        psd_lbl = 1 - chi2_l / chi2_l_0
        psd_lbl[psd_lbl < 0] = 0

        plt.figure()
        plt.plot(period, psd_lbl * (perio.dist_qtl > 0.75))
        plt.xscale("log")

    def lbl_fit_sinus_periodogram_compute_metrics(self, filename, nbins=72):

        file_proxies = np.sort(
            glob.glob(self.dir_root + "PERIODOGRAM/*proxies*" + filename + ".p")
        )[0]
        file = np.sort(glob.glob(self.dir_root + "PERIODOGRAM/*" + filename + ".p"))
        file = np.setdiff1d(file, file_proxies)

        proxies = pd.read_pickle(file_proxies)["matrice"]
        proxies_name = pd.read_pickle(file_proxies)["proxies"]

        proxies_rows = np.where(
            np.in1d(
                np.array(proxies_name),
                np.array(["CaII", "Ha", "Hb", "WB", "ccf_vspan", "ccf_fwhm", "ccf_contrast"]),
            )
        )[0]
        proxies_name = np.array(proxies_name)[proxies_rows]
        proxies_phi = proxies[:, proxies_rows, 4]

        Mass_earth = 5.97e24
        Mass_jupiter = 1.89e27
        conv = Mass_jupiter / Mass_earth

        database = "Exoplanets.org"
        exoplanet_database = Exoplanet.query_database(
            filters={
                "db_info_name": {"contains": database},
                "obj_id_catname": {"contains": self.starname},
            }
        )
        names = exoplanet_database["obj_id_catname"]
        mass = np.array(exoplanet_database["obj_phys_msini_mjup"])
        periods = exoplanet_database["obj_orb_period_day"]

        table = pd.DataFrame(
            {"Name": np.array(names), "Period": np.array(periods), "Mass": np.array(mass) * conv}
        )
        table = table.sort_values(by="Period")
        table = table.round(1)

        count = 0

        save = {"temp": 0}

        if len(file) != 0:
            for f in file:
                print(f.split("/")[-1])
                count += 1
                split_name = f.split("/")[-1].split("_")
                matrice = pd.read_pickle(f)["matrice"]
                period = matrice[:, 0, -1]
                r_corr = matrice[:, :, 0]
                k = matrice[:, :, 2]
                k_std = matrice[:, :, 3]
                phi = matrice[:, :, 4] % (2 * np.pi)
                phi_std = matrice[:, :, 5]
                weight = matrice[:, :, 6]

                k_mean = np.sum(k / k_std**2, axis=1) / np.sum(1 / k_std**2, axis=1)
                phi_mean = np.sum(phi / phi_std**2, axis=1) / np.sum(1 / phi_std**2, axis=1)

                x_mean = np.cos(phi_mean) * k_mean
                y_mean = np.sin(phi_mean) * k_mean

                # cardinal metric

                x = np.cos(phi) * k
                y = np.sin(phi) * k

                x_std = np.sqrt((k * np.cos(phi) * phi_std) ** 2 + (k_std * np.sin(phi)) ** 2)
                y_std = np.sqrt((k * np.sin(phi) * phi_std) ** 2 + (k_std * np.cos(phi)) ** 2)

                x -= x_mean[:, np.newaxis]
                y -= y_mean[:, np.newaxis]

                eig_val_svd = []
                angle_svd = []

                eig_val_wpca = []
                var_ratio_wpca = []
                angle_wpca = []

                eig_val_uapca = []
                var_ratio_uapca = []
                angle_uapca = []

                for j in range(len(period)):
                    vec = np.array([x[j], y[j]])
                    vec_std = np.array([x_std[j], y_std[j]])
                    tab = myc.table(vec)
                    # tabxy = myc.tableXY(vec[:,0],vec[:,1],vec_std[:,0],vec_std[:,1])
                    # svd
                    tab.SVD()
                    eig_val_svd.append(tab.eigen_values / np.sum(tab.eigen_values))
                    ang_svd.append(np.arctan2(tab.svd_vec[1, 0], tab.svd_vec[0, 0]))
                    # wpca
                    tab.WPCA("wpca", weight=1 / vec_std**2)
                    eig_val_wpca.append(np.std(tab.components, axis=0))
                    var_ratio_wpca.append(tab.var_ratio)
                    ang_wpca.append(np.arctan2(tab.vec[1, 0], tab.vec[0, 0]))
                    # uapca
                    tab.UAPCA(weight=1 / vec_std**2)
                    eig_val_uapca.append(np.std(tab.cuapca_coeff, axis=0))
                    var_ratio_uapca.append(tab.uapca_lambda / np.sum(tab.uapca_lambda))
                    ang_uapca.append(np.arctan2(tab.uapca_vec[1, 0], tab.uapca_vec[0, 0]))

                eig_val_svd = np.array(eig_val_svd)
                eig_val_wpca = np.array(eig_val_wpca)
                eig_val_uapca = np.array(eig_val_uapca)

                var_ratio_pca = np.array(var_ratio_pca)
                var_ratio_uapca = np.array(var_ratio_uapca)

                ang_svd = np.array(ang_svd)
                ang_wpca = np.array(ang_wpca)
                ang_uapca = np.array(ang_uapca)

                asym_svd = (eig_val_svd[:, 0] - eig_val_svd[:, 1]) / np.min(eig_val_svd, axis=1)
                asym_wpca = (eig_val_wpca[:, 0] - eig_val_wpca[:, 1]) / np.min(
                    eig_val_wpca, axis=1
                )
                asym_uapca = (eig_val_uapca[:, 0] - eig_val_uapca[:, 1]) / np.min(
                    eig_val_uapca, axis=1
                )

                # polar metric

                phi_rel = phi_mean[:, np.newaxis] - proxies_phi
                phi_rel_bij = phi_rel.copy()
                phi_rel_bij = np.array(
                    [
                        myf.bij_modulo_merge(phi_rel_bij[:, i].copy(), offset=np.pi / 2, max_cut=9)
                        for i in range(len(phi_rel_bij[0]))
                    ]
                ).T
                phi_rel_bij = np.array(
                    [
                        myf.smooth(phi_rel_bij[:, i].copy(), box_pts=3)
                        for i in range(len(phi_rel_bij[0]))
                    ]
                ).T
                phi_rel_grad = np.gradient(phi_rel_bij, axis=0)

                rlim = []
                for simu in range(len(r_corr)):
                    sort = np.argsort(r_corr[simu])
                    cum = np.cumsum(weight[simu][sort])
                    cum = cum / np.max(cum) * 100
                    split = myf.find_nearest(cum, 50)[0][0]
                    rlim.append(r_corr[simu][sort][split])
                rlim = np.array(rlim)

                phi_binned = []
                N = nbins
                if not N % 2:
                    N += 1

                N_random = len(phi[0]) / N  # number of expected occurence for random phases
                bins = np.linspace(0, 2 * np.pi, N + 1)
                for simu in range(len(phi)):
                    mask = (phi[simu] <= bins[1:, np.newaxis]) & (
                        phi[simu] > bins[:-1, np.newaxis]
                    )
                    phi_binned.append(np.sum(mask, axis=1))
                phi_binned = np.array(phi_binned)

                bin_max = np.argmax(phi_binned, axis=1)
                phi_shift = bins[bin_max] - np.pi
                for j in range(len(phi_binned)):
                    phi_binned[j] = phi_binned[
                        j, np.roll(np.arange(nbins + 1), int(nbins / 2) - bin_max[j])
                    ]

                phi_max = []
                phi_center = []
                for j in range(len(phi_binned)):
                    phi_smoothed = myf.smooth(
                        (phi_binned[j] - N_random) / N_random + 1,
                        shape="savgol",
                        box_pts=int(nbins / 5),
                    )
                    phi_max.append(np.max(phi_smoothed))
                    phi_center.append(np.argmax(phi_smoothed))
                phi_max = np.array(phi_max)
                phi_center = (bins[np.array(phi_center)] + phi_shift) % (2 * np.pi)

                #                modulo_dist = myf.dist_modulo(phi_center[:,np.newaxis],proxies_phi)
                #                modulo_dist_smooth = np.array([myf.smooth(modulo_dist[:,i],3) for i in range(len(modulo_dist[0]))]).T
                #
                #                modulo_dist_prim = np.mean(np.array([myf.transform_prim(period,abs(np.gradient(modulo_dist_smooth[:,i])))[1] for i in range(len(modulo_dist[0]))]).T,axis=1)

                phi_abs = []
                phi_diff = []
                for j in range(int(N / 2)):
                    phi_abs.append(phi_binned[:, j] + phi_binned[:, int(j + N / 2)])
                    phi_diff.append(phi_binned[:, j] - phi_binned[:, int(j + N / 2)])
                phi_abs = np.array(phi_abs).T
                phi_diff = np.array(phi_diff).T / phi_abs

                phi_area_abs = np.sum(abs(phi_abs - 2 * N_random), axis=1) / (2 * N * N_random)
                phi_area_diff = np.mean(abs(phi_diff), axis=1)
                median_r_corr = np.nanmedian(r_corr, axis=1)
                median_k = np.nanmedian(k, axis=1)
                iq_r_corr = myf.IQ(r_corr, axis=1)
                iq_k = myf.IQ(k, axis=1)

                z_k = median_k / iq_k
                z_r_corr = median_r_corr / iq_r_corr

                scores = np.array(
                    [
                        period,
                        phi_max,
                        phi_center,
                        phi_area_abs,
                        phi_area_diff,
                        median_r_corr,
                        median_k,
                        iq_r_corr,
                        iq_k,
                        z_r_corr,
                        z_k,
                        asym_svd,
                        asym_pca,
                        phi_rel_grad,
                        k_mean,
                        phi_mean,
                    ]
                ).T

                save[
                    split_name[0] + "_" + split_name[1] + "_" + split_name[3] + "_" + split_name[4]
                ] = pd.DataFrame(
                    scores,
                    columns=[
                        "period",
                        "phi_max",
                        "phi_center",
                        "phi_area_abs",
                        "phi_area_diff",
                        "median_r_corr",
                        "median_k",
                        "iq_r_corr",
                        "iq_k",
                        "z_r_corr",
                        "z_k",
                        "phi_grad",
                        "k_mean",
                        "phi_mean",
                    ],
                )

            del save["temp"]

            myf.pickle_dump(
                save,
                open(
                    self.dir_root
                    + "PERIODOGRAM/periodogram_%s_%s" % (split_name[6], split_name[8]),
                    "wb",
                ),
            )
            print(
                "File saved : %s"
                % (
                    self.dir_root
                    + "PERIODOGRAM/periodogram_%s_%s" % (split_name[6], split_name[8])
                )
            )

    def yarara_new_periodogram(
        self, file, quantile=None, kw=None, loss="add", transform="min_max", detrending=False
    ):

        print("File %s loaded" % (glob.glob(self.dir_root + "PERIODOGRAM/" + file)[0]))
        file = pd.read_pickle(glob.glob(self.dir_root + "PERIODOGRAM/" + file)[0])

        self.dataframe = file

        score_planet = []
        score_activity = []
        score_syst = []

        weights_planet = {
            "aba_0": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_1": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_2": {"phi_max": 1, "phi_area_diff": 1, "iq_k": np.nan, "z_k": np.nan},
            "aba_3": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_4": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_5": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_6": {"phi_max": 0, "phi_area_diff": 0, "iq_k": 0, "z_k": 0},
            "aba_7": {"phi_max": 0, "phi_area_diff": 0, "iq_k": 0, "z_k": 0},
            "dbd_0": {"phi_max": 0, "phi_area_diff": 0, "iq_k": 0, "z_k": 0},
            "dbd_1": {"phi_max": 1, "phi_area_diff": 1, "iq_k": 0, "z_k": 1},
            "dbd_2": {"phi_max": 0, "phi_area_diff": 0, "iq_k": 0, "z_k": 0},
            "lbl_0": {"phi_max": 1, "phi_area_diff": 1, "iq_k": 0, "z_k": 1},
            "lbl_1": {"phi_max": 0, "phi_area_diff": 0, "iq_k": 0, "z_k": 0},
            "lbl_2": {"phi_max": 0, "phi_area_diff": 0, "iq_k": 0, "z_k": 0},
        }

        weights_syst = {
            "aba_0": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_1": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_2": {"phi_max": 1, "phi_area_diff": 0, "iq_k": 1, "z_k": 0},
            "aba_3": {"phi_max": 1, "phi_area_diff": 0, "iq_k": 1, "z_k": 0},
            "aba_4": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_5": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_6": {"phi_max": 1, "phi_area_diff": 0, "iq_k": 1, "z_k": 0},
            "aba_7": {"phi_max": 1, "phi_area_diff": 0, "iq_k": 1, "z_k": 0},
            "dbd_0": {"phi_max": 1, "phi_area_diff": 0, "iq_k": 1, "z_k": 0},
            "dbd_1": {"phi_max": 1, "phi_area_diff": 0, "iq_k": 1, "z_k": 0},
            "dbd_2": {"phi_max": 1, "phi_area_diff": 0, "iq_k": 1, "z_k": 0},
            "lbl_0": {"phi_max": 1, "phi_area_diff": 0, "iq_k": 1, "z_k": 0},
            "lbl_1": {"phi_max": 1, "phi_area_diff": 0, "iq_k": 1, "z_k": 0},
            "lbl_2": {"phi_max": 1, "phi_area_diff": 0, "iq_k": 1, "z_k": 0},
        }

        weights_activity = {
            "aba_0": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_1": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_2": {"phi_max": 1, "phi_area_diff": 1, "iq_k": np.nan, "z_k": np.nan},
            "aba_3": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_4": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_5": {"phi_max": np.nan, "phi_area_diff": np.nan, "iq_k": np.nan, "z_k": np.nan},
            "aba_6": {"phi_max": 1, "phi_area_diff": 1, "iq_k": 1, "z_k": 0},
            "aba_7": {"phi_max": 1, "phi_area_diff": 1, "iq_k": 1, "z_k": 0},
            "dbd_0": {"phi_max": 1, "phi_area_diff": 1, "iq_k": 1, "z_k": 1},
            "dbd_1": {"phi_max": 1, "phi_area_diff": 1, "iq_k": 1, "z_k": 1},
            "dbd_2": {"phi_max": 1, "phi_area_diff": 1, "iq_k": 1, "z_k": 1},
            "lbl_0": {"phi_max": 1, "phi_area_diff": 1, "iq_k": 1, "z_k": 1},
            "lbl_1": {"phi_max": 1, "phi_area_diff": 1, "iq_k": 1, "z_k": 1},
            "lbl_2": {"phi_max": 1, "phi_area_diff": 1, "iq_k": 1, "z_k": 0},
        }

        keys = np.array(list(file.keys()))
        if kw is not None:
            kept = np.array([k.split("_")[1] == kw for k in keys])
            keys = keys[kept]
        else:
            liste = np.array(
                [
                    "matching_1y_lbl_0",
                    "matching_1y_lbl_1",
                    "matching_pca_lbl_2",
                    "matching_pca_dbd_0",
                    "matching_pca_dbd_1",
                    "matching_pca_dbd_2",
                    "matching_pca_aba_2",
                    "matching_pca_aba_6",
                    "matching_pca_aba_7",
                ]
            )
            keys = liste[np.in1d(liste, keys)]

        count = 0
        for f in np.sort(keys):
            print("Observable used : ", f)
            count += 1
            code = "_".join(f.split("_")[2:])
            if code == "lbl_0":
                ref = f

            period = np.array(file[f]["period"])

            if detrending:
                temp = (
                    file[f]
                    .rolling(int(len(period) / 20), center=True, min_periods=1)
                    .quantile(0.5)
                )
                temp = temp - temp.median()
                table = file[f] - temp
                table["z_k"] = table["median_k"] / table["iq_k"]
                table["z_r_corr"] = table["median_r_corr"] / table["iq_r_corr"]
            else:
                table = file[f].copy()

            table_score = table[["phi_max", "z_k", "phi_area_diff", "iq_k"]].copy()

            for elem in table_score.keys():
                new = myf.transform_min_max(table_score[elem])
                if transform == "prim":
                    p, new = myf.transform_prim(period, new)

                table_score[elem] = new

            qtl_pos = table_score.copy()
            qtl_neg = 1 - table_score.copy()
            if quantile is not None:
                qtl_pos = (table_score > quantile).astype("int")
                qtl_neg = 1 - qtl_pos.copy()

            for kw_check in table_score.keys():
                sign_planet = weights_planet[code][kw_check]
                sign_syst = weights_syst[code][kw_check]
                sign_activity = weights_activity[code][kw_check]

                score_planet.append(
                    np.array(qtl_pos[kw_check]) * sign_planet
                    + np.array(qtl_neg[kw_check]) * (1 - sign_planet)
                )
                score_syst.append(
                    np.array(qtl_pos[kw_check]) * sign_syst
                    + np.array(qtl_neg[kw_check]) * (1 - sign_syst)
                )
                score_activity.append(
                    np.array(qtl_pos[kw_check]) * sign_activity
                    + (1 - np.array(qtl_neg[kw_check])) * (1 - sign_activity)
                )

        if loss == "add":
            score_planet = np.nansum(score_planet, axis=0)
            score_syst = np.nansum(score_syst, axis=0)
            score_activity = np.nansum(score_activity, axis=0)
        else:
            score_planet = np.nanprod(score_planet, axis=0)
            score_syst = np.nanprod(score_syst, axis=0)
            score_activity = np.nanprod(score_activity, axis=0)

        score_planet -= np.nanmin(score_planet)
        score_syst -= np.nanmin(score_syst)
        score_activity -= np.nanmin(score_activity)

        score_planet /= np.nanmax(score_planet)
        score_syst /= np.nanmax(score_syst)
        score_activity /= np.nanmax(score_activity)

        all_score = np.array([period, score_planet, score_syst, score_activity])
        self.new_periodogram = all_score

        plt.figure()
        plt.subplot(4, 1, 1)
        plt.plot(period, file[ref]["phi_max"], label="", color="k")
        plt.xscale("log")
        ax = plt.gca()
        plt.subplot(4, 1, 2, sharex=ax)
        plt.plot(period, score_planet, label="metric_planet", color="g")
        plt.subplot(4, 1, 3, sharex=ax)
        plt.plot(period, score_syst, label="metric_syst", color="b")
        plt.subplot(4, 1, 4, sharex=ax)
        plt.plot(period, score_activity, label="metric_activity", color="r")

    def lbl_fit_sinus_periodogram_scores(
        self,
        file,
        kw=None,
        marker="o",
        detrending=True,
        period_special=np.array([365.25]),
        rotation=0,
        smooth_box=1,
    ):

        print("File %s loaded" % (glob.glob(self.dir_root + "PERIODOGRAM/" + file)[0]))
        file = pd.read_pickle(glob.glob(self.dir_root + "PERIODOGRAM/" + file)[0])
        self.dataframe = file

        keys = np.array(list(file.keys()))
        if kw is not None:
            kept = np.array([k.split("_")[1] == kw for k in keys])
            keys = keys[kept]

        fig_num = np.random.randint(0, 100)

        output = {}
        output2 = {}

        count = 0
        for f in np.sort(keys):
            count += 1
            code = "_".join(f.split("_")[2:])

            save = []
            save2 = []

            counter = 0
            name = []
            period = np.array(file[f]["period"])

            loc = myf.find_nearest(period, period_special)[0]
            loc = loc[abs(period[loc] - period_special) / period_special * 100 < 10].astype("int")

            table = file[f].copy()

            if detrending:
                temp = (
                    file[f]
                    .rolling(int(len(period) / 20), center=True, min_periods=1)
                    .quantile(0.5)
                )
                temp = temp - temp.median()
                table_rolled = file[f] - temp
                table_rolled["z_k"] = table_rolled["median_k"] / table_rolled["iq_k"]
                table_rolled["z_r_corr"] = (
                    table_rolled["median_r_corr"] / table_rolled["iq_r_corr"]
                )
                table[
                    ["median_r_corr", "median_k", "iq_r_corr", "iq_k", "z_r_corr", "z_k"]
                ] = table_rolled[
                    ["median_r_corr", "median_k", "iq_r_corr", "iq_k", "z_r_corr", "z_k"]
                ]

            table_score = table[["phi_max", "phi_area_abs", "phi_area_diff", "iq_k", "z_k"]].copy()

            for i, j in enumerate(table_score.keys()):

                vec = np.array(table_score[j])
                vec = myf.transform_min_max(vec)
                vec = myf.smooth(vec, box_pts=smooth_box, shape="gaussian")
                p2, vec2 = myf.transform_prim(period, vec.copy())

                if (
                    (j == "phi_max")
                    | (j == "phi_area_abs")
                    | (j == "z_k")
                    | (j == "phi_area_diff")
                    | (j == "iq_k")
                ):

                    name.append(j)
                    counter += 1

                    save.append(np.array([np.mean(vec[l : l + 1]) for l in loc]))
                    save2.append(np.array([np.mean(vec2[l : l + 1]) for l in loc]))

            save = np.array(save)
            save2 = np.array(save2)

            if np.shape(save)[1]:
                plt.figure(fig_num, figsize=(10, 10))
                plt.subplot(3, 3, count)

                plt.title(f)
                # plt.boxplot(distribution.T,positions=[0,1,2,3],zorder=0,showcaps=False,showfliers=False)

                # plt.scatter(np.arange(len(name)),score_metric,color='yellow',marker='*',edgecolor='k',zorder=100,s=60)
                for j in range(len(save[0])):
                    plt.plot(
                        save[:, j],
                        marker=marker,
                        color=["r", "g", "b", "purple", "pink", "orange", "cyan", "brown"][j],
                        label="%.2f" % (period[loc[j]]),
                        zorder=10,
                    )
                plt.axhline(y=0, color="k", lw=2, ls=":")
                plt.axhline(y=0.5, color="k", lw=2, ls=":")
                plt.axhline(y=1, color="k", lw=2, ls=":")

                plt.xticks(np.arange(len(name)), name, rotation=rotation)
                plt.ylim(-0.1, 1.1)
                plt.xlim(-0.25, 0.25 + len(name) - 1)
                plt.ylabel("Metrics normed")

                plt.subplots_adjust(hspace=0.35, top=0.95, left=0.07, right=0.96)

                if not count - 1:
                    plt.legend()

            if np.shape(save2)[1]:
                plt.figure(fig_num + 1, figsize=(10, 10))
                plt.subplot(3, 3, count)
                plt.title(f)
                # plt.boxplot(distribution.T,positions=[0,1,2,3],zorder=0,showcaps=False,showfliers=False)

                # plt.scatter(np.arange(len(name)),score_metric,color='yellow',marker='*',edgecolor='k',zorder=100,s=60)
                for j in range(len(save2[0])):
                    plt.plot(
                        save2[:, j],
                        marker=marker,
                        color=["r", "g", "b", "purple", "pink", "orange", "cyan", "brown"][j],
                        label="%.2f" % (period[loc[j]]),
                        zorder=10,
                    )
                plt.axhline(y=0, color="k", lw=2, ls=":")
                plt.axhline(y=0.5, color="k", lw=2, ls=":")
                plt.axhline(y=1, color="k", lw=2, ls=":")

                plt.xticks(np.arange(len(name)), name, rotation=rotation)
                plt.ylim(-0.1, 1.1)
                plt.xlim(-0.25, 0.25 + len(name) - 1)
                plt.ylabel("Metric qtl")
                plt.subplots_adjust(hspace=0.40, top=0.95, left=0.07, right=0.96)

                if not count - 1:
                    plt.legend()

            for num, k in enumerate(name):
                output[code + "_" + k] = save[num]
                output2[code + "_" + k] = save2[num]
        return [output, output2]

    def lbl_fit_sinus_periodogram_plot_metrics(
        self,
        file,
        kw=None,
        obs=None,
        nbins=72,
        seed=69,
        detrending=True,
        period_special=np.array([365.25]),
        transform="min_max",
        smooth_box=1,
    ):

        file = pd.read_pickle(self.dir_root + "PERIODOGRAM/" + file)

        Mass_earth = 5.97e24
        Mass_jupiter = 1.89e27
        conv = Mass_jupiter / Mass_earth

        database = "Exoplanets.org"
        exoplanet_database = Exoplanet.query_database(
            filters={
                "db_info_name": {"contains": database},
                "obj_id_catname": {"contains": self.starname},
            }
        )
        names = exoplanet_database["obj_id_catname"]
        mass = np.array(exoplanet_database["obj_phys_msini_mjup"])
        periods = exoplanet_database["obj_orb_period_day"]

        table = pd.DataFrame(
            {"Name": np.array(names), "Period": np.array(periods), "Mass": np.array(mass) * conv}
        )
        table = table.sort_values(by="Period")
        table = table.round(1)

        count = 0
        color = None

        self.dataframe = file

        keys = np.array(list(file.keys()))
        if kw is not None:
            kept = np.array([k.split("_")[1] == kw for k in keys])
            keys = keys[kept]

        if obs is not None:
            kept = np.array([k.split("_")[2] == obs for k in keys])
            keys = keys[kept]

        if len(keys) != 0:

            count = 0

            fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(
                nrows=3, ncols=2, sharex=True, figsize=(24, 8)
            )

            if len(table) > 0:
                curve_phi = myc.tableXY(
                    file["matching_1y_lbl_0"]["period"], file["matching_1y_lbl_0"]["phi_area_diff"]
                )
                curve_phi.find_max(vicinity=5)
                find = myf.find_nearest(curve_phi.x_max, np.array(table["Period"]))
                index_star = find[0][100 * abs(find[2]) / find[1] < 10]

            for f in np.sort(keys):

                period = np.array(file[f]["period"])

                if detrending:
                    temp = (
                        file[f]
                        .rolling(int(len(period) / 20), center=True, min_periods=1)
                        .quantile(0.5)
                    )
                    temp = temp - temp.median()
                    table_rolled = file[f] - temp
                    table_rolled["z_k"] = table_rolled["median_k"] / table_rolled["iq_k"]
                    table_rolled["z_r_corr"] = (
                        table_rolled["median_r_corr"] / table_rolled["iq_r_corr"]
                    )
                    table2 = file[f]
                    table2[
                        ["median_r_corr", "median_k", "iq_r_corr", "iq_k", "z_r_corr", "z_k"]
                    ] = table_rolled[
                        ["median_r_corr", "median_k", "iq_r_corr", "iq_k", "z_r_corr", "z_k"]
                    ]

                else:
                    table2 = file[f]

                count += 1
                title = f

                scores = np.array(table2)
                scores[:, 0] = period
                (
                    period,
                    phi_max,
                    phi_area_abs,
                    phi_area_diff,
                    median_r_corr,
                    median_k,
                    iq_r_corr,
                    iq_k,
                    z_r_corr,
                    z_k,
                ) = scores.T

                def maxima_plot(col1, colx=3, coly=1, nb_max=10, color="b", sign=1):
                    ref = myc.tableXY(period, scores[:, col1])
                    ref.find_max(vicinity=5)
                    kept = ref.y_max.argsort()[::-1][0:nb_max]
                    sub_scores = scores[ref.index_max][kept]
                    maxima = myc.tableXY(sub_scores[:, colx], sub_scores[:, coly])
                    ax = plt.gca()

                    y_min, y_max = ax.get_ylim()
                    x_min, x_max = ax.get_xlim()

                    coordonnes = [y_min, y_max, y_min]

                    int_x = x_max - x_min
                    int_y = y_max - y_min

                    s = int_y / 50
                    s2 = int_x / 30

                    sum_dist = []
                    min_dist = []
                    conv = 0
                    for j in range(100):
                        np.random.seed(seed + j)
                        new_y = maxima.y + sign * (np.random.rand(len(maxima.x))) * abs(
                            maxima.y - coordonnes[sign]
                        )
                        dist = np.sqrt(
                            ((maxima.x - maxima.x[:, np.newaxis]) / int_x) ** 2
                            + ((new_y - new_y[:, np.newaxis]) / int_y) ** 2
                        )
                        dist_abs = dist[np.triu_indices(len(maxima.x), k=1)] * 100
                        if np.product(dist_abs > 10):
                            conv = 1
                            break
                        sum_dist.append(sum(dist_abs))
                        min_dist.append(min(dist_abs))
                    if not conv:
                        j = np.argmax(min_dist)
                    np.random.seed(seed + j)
                    new_y = maxima.y + sign * (np.random.rand(len(maxima.x))) * abs(
                        maxima.y - coordonnes[sign]
                    )

                    plt.scatter(maxima.x, maxima.y, color=color)
                    for j in range(len(maxima.x)):
                        plt.plot(
                            [maxima.x[j]] * 2, [maxima.y[j], new_y[j]], color=color, alpha=0.3
                        )
                    maxima.y = new_y
                    maxima.myscatter(
                        num=False,
                        liste=["%.1f" % (a) for a, b in zip(sub_scores[:, 0], sub_scores[:, 5])],
                        factor=100,
                        alpha=0,
                        x_offset=-s2,
                        y_offset=s * (sign - 1),
                    )

                    return myc.tableXY(sub_scores[:, 0], sub_scores[:, col1])

                plt.figure(figsize=(20, 10))
                plt.axes([0.05, 0.95, 0.90, 0.01])
                plt.axis("off")
                plt.title(title, fontsize=14)

                coly_bottom = 3
                curve = myc.tableXY(scores[:, 0], scores[:, coly_bottom])
                curve.find_max(vicinity=5)

                if len(table) > 0:
                    scores_planet = scores[curve_phi.index_max][index_star]
                else:
                    scores_planet = np.nan * scores[0:2]

                # 0) period,     1) phi_max,     2) phi_area_abs,  3) phi_area_diff,  4) median_r_corr,
                # 5) median_k,   6) iq_r_corr,   7) iq_k,          8) z_r_corr,       9) z_k

                coly = 9
                colx = 3
                plt.axes([0.05, 0.35, 0.40, 0.60])
                plt.scatter(
                    scores[:, colx], scores[:, coly], color="k", marker=".", s=5, alpha=0.1
                )
                optx, opty = myf.optimal_plot_axis(scores[:, colx], scores[:, coly])
                plt.xlim(optx)
                plt.ylim(opty)
                plt.plot(scores[:, colx], scores[:, coly], color="k", alpha=0.1)
                plt.scatter(
                    scores[curve.index_max, colx], scores[curve.index_max, coly], color="k"
                )
                refe = maxima_plot(3, colx=colx, coly=coly, nb_max=10, color="b", sign=-1)
                refe2 = maxima_plot(7, colx=colx, coly=coly, nb_max=5, color="r", sign=+1)
                plt.scatter(
                    scores_planet[:, colx],
                    scores_planet[:, coly],
                    marker="*",
                    color="yellow",
                    edgecolor="k",
                    s=100,
                    zorder=69,
                )
                plt.ylabel("Z score K", fontsize=14)
                plt.xlabel("Phi asym", fontsize=14)
                plt.tick_params(right=True, direction="in")

                coly = 9
                colx = 7
                plt.axes([0.55, 0.35, 0.40, 0.60])
                plt.scatter(
                    scores[:, colx], scores[:, coly], color="k", marker=".", s=5, alpha=0.1
                )
                plt.plot(scores[:, colx], scores[:, coly], color="k", alpha=0.1)
                plt.scatter(
                    scores[curve.index_max, colx], scores[curve.index_max, coly], color="k"
                )
                optx, opty = myf.optimal_plot_axis(scores[:, colx], scores[:, coly])
                plt.xlim(optx)
                plt.ylim(opty)
                refe = maxima_plot(3, colx=colx, coly=coly, nb_max=10, color="b", sign=+1)
                refe2 = maxima_plot(7, colx=colx, coly=coly, nb_max=5, color="r", sign=-1)
                plt.scatter(
                    scores_planet[:, colx],
                    scores_planet[:, coly],
                    marker="*",
                    color="yellow",
                    edgecolor="k",
                    s=100,
                    zorder=69,
                )
                plt.ylabel("Z score K", fontsize=14)
                plt.xlabel("IQ K", fontsize=14)
                plt.tick_params(right=True, direction="in")

                plt.axes([0.05, 0.05, 0.90, 0.2])
                plt.plot(scores[:, 0], scores[:, coly_bottom], color="k", alpha=0.1)
                plt.scatter(
                    scores[:, 0], scores[:, coly_bottom], color="k", alpha=0.1, s=5, marker="."
                )
                plt.scatter(curve.x_max, curve.y_max, color="k", zorder=10, s=10)
                plt.scatter(refe.x, refe.y, color="b", zorder=11, s=20)
                plt.scatter(
                    scores_planet[:, 0],
                    scores_planet[:, coly_bottom],
                    marker="*",
                    color="yellow",
                    edgecolor="k",
                    s=100,
                    zorder=69,
                )

                for j in table["Period"]:
                    plt.axvline(x=j, color="orange", alpha=0.6)

                plt.xscale("log")
                plt.ylabel("Phi asym", fontsize=14)
                plt.xlabel("Period [days]", fontsize=14)
                plt.xlim(np.min(period), np.max(period))

                plt.subplots_adjust(hspace=0.2, top=0.95, left=0.07, right=0.97, bottom=0.10)
                plt.savefig(self.dir_root + "PERIODOGRAM/New_periodogram_" + title + ".pdf")

                names = [
                    "med(K)",
                    "IQ(K)",
                    "Z(K)",
                    "Phi_max",
                    "Phi area",
                    "Phi_asym_area",
                ]
                sub_fig = [ax1, ax3, ax5, ax2, ax4, ax6]
                for n, metric in enumerate(
                    [median_k, iq_k, z_k, phi_max, phi_area_abs, phi_area_diff]
                ):
                    if transform == "min_max":
                        metric = myf.transform_min_max(metric)
                    if smooth_box != 1:
                        metric = myf.smooth(metric, box_pts=smooth_box, shape="gaussian")
                    sub_fig[n].plot(period, metric, label=title, color=color)
                    sub_fig[n].set_ylabel(names[n], fontsize=13)
                    sub_fig[n].set_xlabel("Period [days]", fontsize=13)
                    sub_fig[n].tick_params(top=True, direction="in")
                    for j in period_special:
                        sub_fig[n].axvline(x=j, color="k", alpha=0.6)
                    for j in table["Period"]:
                        sub_fig[n].axvline(x=j, color="orange", alpha=0.6)

            if color is not None:
                ax3.scatter(refe.x, refe.y, color="b", zorder=11, s=20)
                ax5.scatter(refe2.x, refe2.y, color="r", zorder=11, s=20)

            ax1.legend()
            ax1.set_xscale("log")
            ax1.set_xlim(np.min(period), np.max(period))
            fig.subplots_adjust(hspace=0, top=0.95, left=0.07, right=0.93, bottom=0.10)
            if color is not None:
                fig.savefig(
                    self.dir_root + "PERIODOGRAM/New_metrics_phase_periodogram_" + title + ".pdf"
                )
            else:
                fig.savefig(self.dir_root + "PERIODOGRAM/New_metrics_phase_periodogram.pdf")
        else:
            print("File not found, the following files are availible")
            for f in np.sort(glob.glob(self.dir_root + "PERIODOGRAM/*.p")):
                print(f)

    def lbl_fit_sinus_film(
        self,
        period=myf.my_ruler(10, 500, 0.25, 2.5),
        frequency=np.linspace(1 / 400, 1 / 10, 2000),
        sub_dico=["matching_diff"],
        seasons=[0],
        good_morpho=False,
        blended_lines=True,
        kw_dico="lbl",
        col=0,
        valid_lines=True,
        color_axis="K",
        radial_axis="r_corr",
        cmin=None,
        cmax=None,
        rmax=1,
        plot_proxies=True,
        deg=1,
        bbox=(-0.1, -0.3),
        fit_lbl=True,
        loop=0,
        delay=10,
        planet=[0, 26, np.pi],
        circle=[0.0, "r", "-", 2],
        hist=72,
        kde=True,
        cloud=True,
        barycentre=True,
    ):

        if period is None:
            period = (1 / frequency)[::-1]

        pmin = period.min()
        pmax = period.max()

        for season in seasons:
            for dico in sub_dico:
                if cloud:
                    if not os.path.exists(
                        self.dir_root
                        + "FILM/film_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f"
                        % (dico, season, kw_dico, col, pmin, pmax)
                    ):
                        os.system(
                            "mkdir "
                            + self.dir_root
                            + "FILM/film_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f"
                            % (dico, season, kw_dico, col, pmin, pmax)
                        )

                    dir_output = (
                        self.dir_root
                        + "FILM/film_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f/"
                        % (dico, season, kw_dico, col, pmin, pmax)
                    )

                    counter = 0
                    for p in tqdm(period):
                        counter += 1
                        self.lbl_fit_sinus(
                            p,
                            sub_dico=dico,
                            season=season,
                            plot=True,
                            kw_dico=kw_dico,
                            col=col,
                            plot_proxies=plot_proxies,
                            good_morpho=good_morpho,
                            fit_lbl=fit_lbl,
                            num_sim=1,
                            rmax=rmax,
                            kde=kde,
                            blended_lines=blended_lines,
                            valid_lines=valid_lines,
                            planet=planet,
                            color_axis=color_axis,
                            radial_axis=radial_axis,
                            cmax=cmax,
                            circle=circle,
                            hist=hist,
                            bbox=bbox,
                        )
                        plt.savefig(
                            dir_output
                            + "film_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f_%s_%.2f.png"
                            % (dico, season, kw_dico, col, pmin, pmax, str(counter).zfill(4), p)
                        )
                        plt.close()

                    os.system(
                        "convert -delay "
                        + str(delay)
                        + " -loop "
                        + str(loop)
                        + " "
                        + dir_output
                        + "film_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f*.png "
                        % (dico, season, kw_dico, col, pmin, pmax)
                        + self.dir_root
                        + "FILM/film_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f.gif"
                        % (dico, season, kw_dico, col, pmin, pmax)
                    )
                    os.system(
                        "convert -delay 1 -quality 100 "
                        + dir_output
                        + "film_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f*.png "
                        % (dico, season, kw_dico, col, pmin, pmax)
                        + self.dir_root
                        + "FILM/film_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f.mpeg"
                        % (dico, season, kw_dico, col, pmin, pmax)
                    )

                if barycentre:
                    if not os.path.exists(
                        self.dir_root
                        + "FILM/bary_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f"
                        % (dico, season, kw_dico, col, pmin, pmax)
                    ):
                        os.system(
                            "mkdir "
                            + self.dir_root
                            + "FILM/bary_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f"
                            % (dico, season, kw_dico, col, pmin, pmax)
                        )

                    dir_output = (
                        self.dir_root
                        + "FILM/bary_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f/"
                        % (dico, season, kw_dico, col, pmin, pmax)
                    )

                    fig = plt.figure(figsize=(12, 3))
                    plt.subplots_adjust(left=0.06, right=0.97, top=0.95, bottom=0.20)
                    counter = 0
                    key = {"lbl": "rv", "lbl_iter": "rv", "dbd": "contrast", "wbw": "fwhm"}[
                        kw_dico
                    ]
                    bary = self.import_ccf_timeseries(
                        "CCF_kitcat_mask_" + self.starname, dico, kw=key
                    )
                    if season:
                        bary.split_seasons()
                        bary = bary.seasons_splited[season - 1].copy()
                    bary.periodogram(
                        p_min=np.min(period) / 5,
                        p_max=np.max(period) * 5,
                        Norm=True,
                        legend=key.upper(),
                    )
                    plt.ylim(0, None)
                    plt.legend()
                    vert = plt.axvline(x=100, color="r")
                    for p in tqdm(period):
                        counter += 1
                        vert.set_xdata([p, p])
                        plt.savefig(
                            dir_output
                            + "bary_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f_%s_%.2f.png"
                            % (dico, season, kw_dico, col, pmin, pmax, str(counter).zfill(4), p)
                        )

                    os.system(
                        "convert -delay "
                        + str(delay)
                        + " -loop "
                        + str(loop)
                        + " "
                        + dir_output
                        + "bary_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f*.png "
                        % (dico, season, kw_dico, col, pmin, pmax)
                        + self.dir_root
                        + "FILM/bary_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f.gif"
                        % (dico, season, kw_dico, col, pmin, pmax)
                    )
                    os.system(
                        "convert -delay 1 -quality 100 "
                        + dir_output
                        + "bary_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f*.png "
                        % (dico, season, kw_dico, col, pmin, pmax)
                        + self.dir_root
                        + "FILM/bary_%s_%.0f_%s_%.0f_pmin_%.0f_pmax_%.0f.mpeg"
                        % (dico, season, kw_dico, col, pmin, pmax)
                    )

    def lbl_correlation_activity(
        self, sub_dico="matching_diff", proxy="CaII", season=0, max_lag=10
    ):
        self.import_table()
        self.import_lbl()

        jdb = self.table["jdb"]
        updated_table = self.lbl.copy()
        lbl_matrix = self.lbl
        tab = self.table

        matrix_rv = lbl_matrix[sub_dico]["lbl"][0]
        matrix_rv_std = lbl_matrix[sub_dico]["lbl_std"][0]
        valid = lbl_matrix[sub_dico]["catalog"]["valid"].astype("bool")
        valid = np.array(valid)

        index_valid = np.arange(len(matrix_rv))[valid]
        catalog = lbl_matrix[sub_dico]["catalog"]

        matrix_rv = matrix_rv[valid]
        matrix_rv_std = matrix_rv_std[valid]

        seasons = myf.detect_obs_season(jdb, min_gap=jdb.max() - jdb.min())
        j = 0
        if season:
            seasons = myf.detect_obs_season(jdb, min_gap=40)
            j = season
        else:
            season = 0
        border = len(jdb[seasons[season, 0] : seasons[season, 1] + 1]) - max_lag
        idx_border = np.array([seasons[season, 0], seasons[season, 1]])
        jdb_border = np.array(jdb)[idx_border]

        proxy_activity = myc.tableXY(
            jdb[seasons[j, 0] : seasons[j, 1] + 1], tab[proxy][seasons[j, 0] : seasons[j, 1] + 1]
        )
        proxy_activity.substract_polyfit(2, replace=True)
        proxy_activity.yerr *= 0
        infos = []
        infos0 = []
        print("Season %.0f analysed \n " % (j + 1))
        time.sleep(0.5)
        all_rv = matrix_rv[:, seasons[j, 0] : seasons[j, 1] + 1]
        all_rv_std = matrix_rv_std[:, seasons[j, 0] : seasons[j, 1] + 1]
        for k in tqdm(range(len(all_rv))):
            line = myc.tableXY(jdb[seasons[j, 0] : seasons[j, 1] + 1], all_rv[k], all_rv_std[k])
            line.substract_polyfit(2, replace=True)
            line.rms_w()
            vec = myc.tableXY(proxy_activity.y, line.y, line.yerr)
            vec.circular_corr(grid=line.x.astype("int"), border=border)
            best = np.argmax(abs(vec.circ_rho))
            infos.append(
                [
                    int(index_valid[k]),
                    catalog["line_depth"][int(index_valid[k])],
                    catalog["freq_mask0"][int(index_valid[k])],
                    best - max_lag + 1,
                    vec.circ_rho[best],
                    vec.circ_corr[best],
                    vec.circ_errcorr[best],
                    vec.circ_slope[best],
                    vec.circ_errslope[best],
                    vec.circ_nbpoints[best],
                    line.rms,
                    np.std(line.y),
                    np.median(line.yerr),
                ]
            )
            infos0.append(
                [
                    int(index_valid[k]),
                    catalog["line_depth"][int(index_valid[k])],
                    catalog["freq_mask0"][int(index_valid[k])],
                    vec.circ_rho[max_lag],
                    vec.circ_corr[max_lag],
                    vec.circ_errcorr[max_lag],
                    vec.circ_slope[max_lag],
                    vec.circ_errslope[max_lag],
                    line.rms,
                    np.std(line.y),
                    np.median(line.yerr),
                ]
            )

        infos = np.array(infos)
        infos0 = np.array(infos0)

        all_infos_best = pd.DataFrame(
            infos,
            columns=[
                "line",
                "line_depth",
                "wave",
                "lag",
                "rho_spearman",
                "r_pearson",
                "r_pearson_std",
                "r_slope",
                "r_slope_std",
                "nb_pts",
                "rmsw",
                "rms",
                "med_err",
            ],
        )
        all_infos_0 = pd.DataFrame(
            infos0,
            columns=[
                "line",
                "line_depth",
                "wave",
                "rho_spearman",
                "r_pearson",
                "r_pearson_std",
                "r_slope",
                "r_slope_std",
                "rmsw",
                "rms",
                "med_err",
            ],
        )
        dico = {"best": all_infos_best, "rest": all_infos_0}

        self.corr_summary = dico

        dico_best = dico["best"]
        dico_rest = dico["rest"]

        save_best = dico_best.drop(columns=["line_depth", "wave"])
        save_rest = dico_rest.drop(columns=["line_depth", "wave"])

        save_best.index = save_best["line"]
        save_rest.index = save_rest["line"]

        save_best = save_best.drop(columns=["line"])
        save_rest = save_rest.drop(columns=["line"])

        updated_table[sub_dico]["parameters"] = {
            "secteur": season,
            "idx_first_time": idx_border[0],
            "idx_last_time": idx_border[1],
            "first_time": jdb_border[0],
            "last_time": jdb_border[1],
        }
        self.new_lbl = updated_table

        for keys in save_best.keys():
            updated_table[sub_dico]["catalog"][keys] = np.nan
            updated_table[sub_dico]["catalog"].loc[
                save_best.index.astype("int"), keys
            ] = save_best[keys]
        for keys in save_rest.keys():
            updated_table[sub_dico]["catalog"][keys + "0"] = np.nan
            updated_table[sub_dico]["catalog"].loc[
                save_rest.index.astype("int"), keys + "0"
            ] = save_rest[keys]

        self.lbl = updated_table
        myf.pickle_dump(self.lbl, open(self.directory + "Analyse_line_by_line.p", "wb"))
        del self.new_lbl

        data_sum = []
        data_density = []
        plt.figure()
        for j in np.unique(dico_best["lag"]):
            a, b, c = plt.hist(
                dico_best.loc[dico_best["lag"] == j, "rho_spearman"],
                bins=np.arange(-1.025, 1.026, 0.05),
            )
            data_sum.append(a)
            a, b, c = plt.hist(
                dico_best.loc[dico_best["lag"] == j, "rho_spearman"],
                bins=np.arange(-1.025, 1.026, 0.05),
                density=True,
            )
            data_density.append(a)

        plt.close()
        data_sum = np.array(data_sum)
        data_density = np.array(data_density)

        lags = np.unique(dico_best["lag"]) - 0.5
        bin_center = 0.5 * (b[0:-1] + b[1:]) - 0.025

        plt.figure()
        plt.scatter(save_best["lag"], save_best["nb_pts"])
        plt.axhline(y=np.max(save_best["nb_pts"]) / 2, color="k", ls="-")
        plt.xlabel("Time lag [days]", fontsize=14)
        plt.ylabel("Nb pts", fontsize=14)

        plt.figure(figsize=(15, 6))
        plt.subplot(1, 2, 1)
        myf.my_colormesh(lags, bin_center, data_sum.T, cmap="jet")
        ax = plt.colorbar()
        plt.axhline(y=-0.4, color="w", ls="-.")
        plt.axhline(y=0.4, color="w", ls="-.")
        plt.axvline(x=0, color="k")
        plt.axhline(y=0, color="k")
        plt.xlabel("Time lag [days]", fontsize=14)
        plt.ylabel(r"Coefficient correlation ($RV_i$ vs activity proxy)", fontsize=14)
        ax.ax.set_ylabel("Nb of lines", fontsize=14)
        plt.subplot(1, 2, 2)
        plt.axhline(y=-0.4, color="w", ls="-.")
        plt.axhline(y=0.4, color="w", ls="-.")
        plt.axvline(x=0, color="k")
        plt.axhline(y=0, color="k")
        myf.my_colormesh(lags, bin_center, data_density.T, cmap="jet")
        ax = plt.colorbar()
        ax.ax.set_ylabel(r"Nb of lines / $\Sigma$ nb of lines", fontsize=14)
        plt.xlabel("Time lag [days]", fontsize=14)
        plt.ylabel(r"Coefficient correlation ($RV_i$ vs activity proxy)", fontsize=14)
        plt.subplots_adjust(left=0.08, right=0.95, top=0.95, bottom=0.1)

        plt.figure(figsize=(12, 12))
        plt.subplot(2, 2, 1)
        ax = plt.gca()
        plt.title("Linear correlation")
        plt.xlim(-1, 1)
        plt.axvline(x=-0.4, color="k", ls="-.")
        plt.axvline(x=0.4, color="k", ls="-.")
        plt.grid()
        plt.hist(dico_rest["rho_spearman"], bins=100, alpha=0.5, label="spearman")
        plt.hist(dico_rest["r_pearson"], bins=100, alpha=0.5, label="pearson")
        plt.xlabel("Correlation coefficient", fontsize=14)
        plt.legend()

        plt.subplot(2, 2, 2)
        ax2 = plt.gca()
        plt.title("Linear correlation")
        plt.xlim(0, 1)
        plt.axvline(x=-0.4, color="k", ls="-.")
        plt.axvline(x=0.4, color="k", ls="-.")
        plt.grid()
        plt.hist(
            abs(dico_rest["rho_spearman"]), bins=50, alpha=0.5, cumulative=True, label="spearman"
        )
        plt.hist(abs(dico_rest["r_pearson"]), bins=50, alpha=0.5, cumulative=True, label="pearson")
        plt.xlabel("Abs(Correlation coefficient)", fontsize=14)

        plt.subplot(2, 2, 3, sharex=ax, sharey=ax)
        plt.xlim(-1, 1)
        plt.axvline(x=-0.4, color="k", ls="-.")
        plt.axvline(x=0.4, color="k", ls="-.")
        plt.grid()
        plt.title("Circular correlation")
        plt.hist(dico_best["rho_spearman"], bins=100, alpha=0.5, label="spearman")
        plt.hist(dico_best["r_pearson"], bins=100, alpha=0.5, label="pearson")
        plt.subplots_adjust(hspace=0.35, top=0.95, bottom=0.09)
        plt.xlabel("Correlation coefficient", fontsize=14)

        plt.subplot(2, 2, 4, sharex=ax2, sharey=ax2)
        plt.title("Circular correlation")
        plt.xlim(0, 1)
        plt.axvline(x=-0.4, color="k", ls="-.")
        plt.axvline(x=0.4, color="k", ls="-.")
        plt.grid()
        plt.hist(
            abs(dico_best["rho_spearman"]), bins=50, alpha=0.5, cumulative=True, label="spearman"
        )
        plt.hist(abs(dico_best["r_pearson"]), bins=50, alpha=0.5, cumulative=True, label="pearson")
        plt.xlabel("Abs(Correlation coefficient)", fontsize=14)

    def light_curve_periodogram(
        self,
        name="yarara_v2_lbl",
        table_keplerian=None,
        std_p=3,
        nb_period=1000,
        std_phase=1 / 18,
        pts_transit=15,
        exclude_out=True,
    ):

        if table_keplerian is None:
            table_keplerian = self.planet_fitted[name]

        try:
            self.photometry.light_curve_periodogram(
                table_keplerian,
                std_p=std_p,
                nb_period=nb_period,
                std_phase=std_phase,
                pts_transit=pts_transit,
                exclude_out=exclude_out,
            )
            plt.savefig(self.dir_root + "KEPLERIAN/Light_curve_keplerian.pdf")
            self.transit_model = self.photometry.transit_model
        except AttributeError:
            pass

    def lbl_kolmo_alfev(
        self,
        base,
        sub_dico="matching_wpca",
        kw_dico="lbl",
        species="wave",
        cut_lim=5250,
        alpha_kolmo=1,
        z_lim=0.25,
        ext="",
    ):

        self.import_table()
        jdb = np.array(self.table.jdb)
        phase_mod = np.arange(365)[
            np.argmin(
                np.array(
                    [np.max((jdb - k) % 365.25) - np.min((jdb - k) % 365.25) for k in range(365)]
                )
            )
        ]

        c_alpha = np.sqrt(-np.log(alpha_kolmo / 2 / 100) * 0.5)

        if kw_dico == "lbl":
            self.import_lbl()
            lbl_matrix = self.lbl
        if kw_dico == "lbl_iter":
            self.import_lbl_iter()
            lbl_matrix = self.lbl_iter
        elif kw_dico == "dbd":
            self.import_dbd()
            lbl_matrix = self.dbd
        elif kw_dico == "aba":
            self.import_aba()
            lbl_matrix = self.aba
        elif kw_dico == "bbb":
            self.import_bbb()
            lbl_matrix = self.bbb
        elif kw_dico == "wbw":
            self.import_wbw()
            lbl_matrix = self.wbw
        elif kw_dico == "bt":
            self.import_bt()
            lbl_matrix = self.bt

        table = lbl_matrix[sub_dico]["catalog"].copy()

        all_keys = table.keys()
        keyword = ["wave", "line_depth"]  # ,'E_low','log_gf']
        skip = len(keyword)
        name = []
        for j in all_keys:
            if j[0:7] == "s_proxy":
                keyword.append(j)
                name.append(j[2:])
        name = name[::2]

        tab = table[keyword].copy()
        tab = tab.dropna()
        kw = list(tab.keys())
        tab_par = tab[kw[0:skip]]
        tab_coeff = tab[kw[skip::2]]
        tab_coeff_std = tab[kw[skip + 1 :: 2]]

        t = myc.table(tab_coeff)
        t.rms_w(1 / tab_coeff_std**2, axis=0)

        tab_coeff /= t.rms
        tab_coeff_std /= t.rms

        # alfev
        #        wave = np.array(tab_par['wave'])
        #        rms_binned = []
        #        binning = np.arange(5,300,5) # number of box size
        #        dbin = 10 #number of shift of a box
        #
        #        table_alfev = myc.table(np.array(tab_coeff.copy()))
        #        table_alfev_std = np.array(tab_coeff_std.copy())
        #
        #        table_alfev.table = np.hstack([table_alfev.table, np.random.randn(len(wave))[:,np.newaxis]])
        #        table_alfev_std = np.hstack([table_alfev_std,table_alfev_std[:,0][:,np.newaxis]])
        #
        #        for j in binning:
        #            r = []
        #            for i in np.linspace(0,j,dbin):
        #                group = np.sum(wave>=np.arange(wave.min()-i,wave.max()+j-i,j)[:,np.newaxis],axis=0)
        #
        #                new = []
        #                new_std = []
        #                wave_binned = []
        #                for g in np.unique(group):
        #                    liste = np.where(group==g)
        #                    vec = table_alfev.rv_subselection(rv_std = table_alfev_std, selection=liste)
        #                    new.append(vec.y)
        #                    new_std.append(vec.yerr)
        #                    wave_binned.append(np.mean(wave[liste]))
        #                new = np.array(new)
        #                new_std = np.array(new_std)
        #                wave_binned = np.array(wave_binned)
        #
        #                binned_data = myc.table(new)
        #                binned_data.rms_w(1/new_std**2,axis=0)
        #                r.append(binned_data.rms)
        #            rms_binned.append(r)
        #        rms_binned = np.array(rms_binned)
        #        rms_binned = np.mean(rms_binned,axis=1) # mean of the shifted box for a same box size
        #        rms_binned = np.vstack([np.ones(len(t.rms)+1),rms_binned])
        #        rms_binned_white = rms_binned[:,-1]
        #        rms_binned = rms_binned[:,:-1]
        #
        #        maxi_alfev = np.max(rms_binned - rms_binned_white[:,np.newaxis],axis=0)
        #
        #        plt.figure(figsize=(10,10))
        #        for j in range(len(maxi_alfev)):
        #            plt.plot(np.hstack([np.min(abs(np.diff(wave))),binning]),rms_binned[:,j]-rms_binned_white,label='Vector %s (max = %.2f)'%(name[j],np.max(rms_binned[:,j]-rms_binned_white)))
        #        plt.xlabel('Bin size window [$\AA$]',fontsize=14)
        #        plt.ylabel('std(coeff) - std(white noise)',fontsize=14)
        #        plt.legend()
        #        plt.savefig(self.dir_root+'IMAGES/Alfev.pdf')

        # kolmogorov ST test
        t_coeff1 = tab_coeff.loc[tab_par[species] < cut_lim]
        t_coeff2 = tab_coeff.loc[tab_par[species] >= cut_lim]

        alpha = []
        plt.figure(figsize=(24, 3 + 3 * (len(base) - 1)))
        plt.subplot(len(base), 4, 4)
        ax = plt.gca()
        plt.subplot(len(base), 4, 3)
        ax1 = plt.gca()

        med_d = []
        for i, col in enumerate(t_coeff1.keys()):

            vector = myc.tableXY(jdb, base[i])

            plt.subplot(len(base), 4, 4 * i + 3, sharex=ax1)
            vector.periodogram(nb_perm=1, Norm=True)
            plt.axvline(x=365.25, color="b", alpha=0.5, ls="-")
            plt.axvline(x=365.25 / 2, color="b", alpha=0.5, ls="-")
            plt.ylabel(None)
            plt.ylim(0, None)

            x1, y1 = myf.transform_prim(t_coeff1[col], t_coeff1[col])
            x2, y2 = myf.transform_prim(t_coeff2[col], t_coeff2[col])

            med1 = x1[myf.find_nearest(y1, 0.5)[0]]
            med2 = x2[myf.find_nearest(y2, 0.5)[0]]

            t = myc.tableXY(x1, y1, 0 * x1)
            t.interpolate(new_grid=x2, replace=True, method="linear", interpolate_x=False)
            dist = np.max(abs(t.y - y2))
            dist_idx = np.argmax(abs(t.y - y2))

            n = len(t_coeff1)
            m = len(t_coeff2)

            alpha.append(2 * np.exp(-2 * dist**2 / ((n + m) / (n * m))) * 100)
            treshold = c_alpha * np.sqrt((n + m) / (n * m))

            plt.subplot(len(base), 4, 4 * i + 4, sharex=ax, sharey=ax)
            plt.title(
                "Kolmo = %.3f (%.3f)    Max Alfev = %.2f"
                % (dist, treshold, 2 * np.abs(med1 - med2) / np.abs(med1 + med2))
            )  # maxi_alfev[i]))
            med_d.append(np.abs(med1 - med2) / np.abs(0.5 * (med1 + med2)))
            plt.plot(x1, y1, color="b", label="%.2f" % (med1))
            plt.plot(x2, y2, color="r", label="%.2f" % (med2))
            plt.legend()
            plt.axvline(x=t.x[dist_idx], color="gray", ls=":", alpha=0.5)
            plt.axvline(x=0, color="k", ls="-")
            plt.axhline(y=0.5, color="k", ls="-")
            plt.tick_params(labelbottom=False, direction="in")
            if i == (len(t_coeff1.keys()) - 1):
                plt.xlabel("Coeff/std(Coeff)", fontsize=14)
            plt.ylim(0, 1)

            criterion = int((alpha[-1] < alpha_kolmo) & (med_d[-1] > z_lim))

            plt.subplot(len(base), 4, 4 * i + 1)
            plt.title("Vector %s" % (name[i]))
            plt.scatter(vector.x, vector.y, color=["r", "k"][criterion])
            plt.tick_params(labelbottom=True, direction="in")
            if i == (len(t_coeff1.keys()) - 1):
                plt.xlabel("Time", fontsize=14)
            plt.subplot(len(base), 4, 4 * i + 2)
            plt.scatter((vector.x - phase_mod) % 365.25, vector.y, color=["r", "k"][criterion])
            plt.tick_params(labelbottom=False, direction="in")
            if i == (len(t_coeff1.keys()) - 1):
                plt.xlabel("Time%365.25", fontsize=14)

        plt.subplots_adjust(top=0.97, bottom=0.05, hspace=0.30, left=0.07, right=0.93, wspace=0.3)
        plt.savefig(self.dir_root + "IMAGES/PCA_vectors" + ext + ".pdf")

        self.alpha_kolmogorov = np.array(alpha)
        self.z_median = np.ravel(med_d)
        # self.alfev_maxi = np.array(maxi_alfev)

        alpha = np.log10(alpha)
        alpha[alpha < -2] = -2

        plt.figure(figsize=(8, 13))
        plt.subplot(1, 1, 1)
        ax = plt.gca()
        plt.plot(np.arange(len(alpha)), alpha, "ko-")
        plt.axhline(
            y=np.log10(alpha_kolmo),
            color="k",
            ls="-.",
            label="%.0f percent alpha value" % (alpha_kolmo),
        )
        plt.axhline(y=-2, color="r", ls=":")
        plt.legend()
        plt.xlabel("Vector index")
        plt.ylabel("Kolmogorov log10(alpha significance)")
        ax.set_xticks(np.arange(len(name)))
        ax.set_xticklabels(name, rotation=90)

    #        plt.subplot(2,1,1,sharex=ax)
    #        plt.plot(np.arange(len(alpha)), maxi_alfev,'ko-')
    #        plt.ylabel('Alfev max')
    #        ax = plt.gca()
    #        ax.set_xticks(np.arange(len(name)))
    #        ax.set_xticklabels(name,rotation=90)

    def lbl_xgb_cat(
        self,
        table=None,
        sub_dico="matching_morpho",
        kw_dico="lbl_iter",
        period=None,
        season=0,
        deg=0,
        ext="",
        abs_value=False,
        min_r_percentile=0,
        percentile=25,
        var="s",
        analysis="morpho-atomic-instru-pixel",
        Plot=True,
        my_dpi=192,
        wave_min=0,
        wave_max=100000,
        depth_min=0,
        depth_max=1,
    ):

        """analysis = 'morpho','atomic','instru','morpho-atomic','morpho-instru'"""
        atomic = [
            "wave",
            "depth_rel",
            "atomic_number",
            "log_gf",
            "E_low",
            "lande_mean",
            "ionisation_energy",
            "zeeman",
        ]
        morpho = [
            "wave",
            "depth_rel",
            "dist_continuum",
            "diff_continuum_signed",
            "contrast_deri_norm",
            "asym_ddflux_norm",
        ]
        instru = ["qc_stitching", "qc_telluric", "qc_ghost_a", "qc_ghost_b", "qc_merged"]
        pixel = ["pixels_l1", "pixels_l2", "pixels_l3"]

        liste = []
        if type(analysis) == list:
            liste = analysis
        else:
            if analysis == "all":
                analysis = "morpho-atomic-instru-pixel"
            a = analysis.split("-")
            for analysis in a:
                if analysis == "morpho":
                    liste.append(morpho)
                elif analysis == "atomic":
                    liste.append(atomic)
                elif analysis == "instru":
                    liste.append(instru)
                elif analysis == "pixel":
                    liste.append(pixel)

        liste = list(np.unique(np.hstack(liste)))
        liste = list(np.sort(liste))

        self.import_table()
        self.import_material()

        jdb = np.array(self.table.jdb)

        if kw_dico == "lbl":
            self.import_lbl()
            lbl_matrix = self.lbl
        if kw_dico == "lbl_iter":
            self.import_lbl_iter()
            kw_dico = "lbl"
            lbl_matrix = self.lbl_iter
        elif kw_dico == "dbd":
            self.import_dbd()
            lbl_matrix = self.dbd
        elif kw_dico == "aba":
            self.import_aba()
            lbl_matrix = self.aba
        elif kw_dico == "bbb":
            self.import_bbb()
            lbl_matrix = self.bbb
        elif kw_dico == "wbw":
            self.import_wbw()
            lbl_matrix = self.wbw
        elif kw_dico == "bt":
            self.import_bt()
            lbl_matrix = self.bt

        os.system("rm " + self.dir_root + "PCA/*%s_%s*" % (sub_dico, var) + ".png")
        os.system(
            "rm " + self.dir_root + "PCA/Analyse*%s*XGB*%s*" % (kw_dico.upper(), sub_dico) + ".png"
        )

        liste = list(
            np.array(liste)[
                np.in1d(np.array(liste), np.array(lbl_matrix[sub_dico]["catalog"].keys()))
            ]
        )

        valid_lines = np.array(lbl_matrix[sub_dico]["catalog"]["valid"])

        if period is not None:
            mat, dust = self.lbl_fit_sinus(
                period,
                sub_dico=sub_dico,
                kw_dico=kw_dico,
                season=season,
                deg=deg,
                valid_lines=valid_lines,
                plot=Plot,
            )
            mat["phi"] -= np.nanmedian(mat["phi"])
            if not abs_value:
                mat["r_corr"] = np.cos(mat["phi"]) * mat["r_corr"]
                mat["K"] = np.cos(mat["phi"]) * mat["K"]

            table = lbl_matrix[sub_dico]["catalog"].copy()

            name_var = "p%.0fs%.0f" % (period, season)
            table.loc[np.array(mat["kitcat_index"].astype("int")), "s_" + name_var] = np.array(
                mat["K"]
            )
            table.loc[
                np.array(mat["kitcat_index"].astype("int")), "s_" + name_var + "_std"
            ] = np.array(mat["K_std"])
            table.loc[np.array(mat["kitcat_index"].astype("int")), "r_" + name_var] = np.array(
                mat["r_corr"]
            )
            table.loc[
                np.array(mat["kitcat_index"].astype("int")), "r_" + name_var + "_std"
            ] = np.array(mat["r_corr_std"])

        if table is None:
            table = lbl_matrix[sub_dico]["catalog"].copy()
        table_rv = myc.table(lbl_matrix[sub_dico][kw_dico][0].copy())
        table_rv.table -= np.nanmedian(table_rv.table, axis=1)[:, np.newaxis]
        table_rv_std = lbl_matrix[sub_dico][kw_dico + "_std"][0].copy()

        self.table_xgb = table.copy()

        all_keys = table.keys()

        name = []
        name_coeff = []
        kept_kw = []

        for j in all_keys:
            if j.split("_")[0] == "r":
                kept_kw.append(j.split("_")[1])

        for j in all_keys:
            if j.split("_")[0] == var:
                name.append(j[2:])
                name_coeff.append(j)

        name = name[::2]
        name_coeff = name_coeff[::2]

        feature_relevance = []
        for species_col in name_coeff:

            valid_lines = np.array(lbl_matrix[sub_dico]["catalog"]["valid"])
            print("Nominal sample size %.0f" % (len(lbl_matrix[sub_dico]["catalog"])))

            tab = table.copy()

            remove = list(
                np.array(liste)[
                    np.in1d(
                        liste,
                        [
                            "atomic_number",
                            "ionisation_energy",
                            "qc_borders_pxl",
                            "qc_ghost_a",
                            "qc_ghost_b",
                            "qc_stitching",
                            "qc_telluric",
                            "qc_thar",
                            "qc_merged",
                            "pixels_l1",
                            "pixels_l2",
                            "pixels_l3",
                            "orders_l1",
                            "orders_l2",
                            "orders_l3",
                        ],
                    )
                ]
            )
            tab_all = tab[liste].copy()
            tab_all = tab_all.dropna()
            old_index = np.array(tab_all.index)

            if len(remove) > 0:
                t1 = myc.table(tab_all.drop(columns=remove))
            else:
                t1 = myc.table(tab_all)

            mask_wave = np.array(
                (tab.loc[old_index, "wave"] < wave_min) | (tab.loc[old_index, "wave"] > wave_max)
            )
            mask_depth = np.array(
                (tab.loc[old_index, "depth_rel"] < depth_min)
                | (tab.loc[old_index, "depth_rel"] > depth_max)
            )
            mask_noise = ~(mask_wave | mask_depth)
            t1.table = t1.table[mask_noise]
            print("Noise precision criterion : %.0f" % (len(t1.table)))

            if len(t1.table.columns):
                print("Outiers rejection performed on : ", list(t1.table.keys()))
                t1.rm_outliers(m=5, kind="inter")

                tab = table.loc[t1.table.index]
                valid_lines = valid_lines[tab.index.astype("int")]

            print("Sample size after outliers rejection : %.0f" % (len(tab)))

            if min_r_percentile:
                if species_col[1:7] == "_proxy":
                    min_rcorr = np.nanpercentile(
                        np.array(abs(tab[species_col.replace("s_proxy", "r_proxy")])),
                        min_r_percentile,
                    )
                    print("Min R corr accepted : %.2f" % (min_rcorr))
                    tab.loc[
                        abs(tab[species_col.replace("s_proxy", "r_proxy")]) < min_rcorr
                    ] = np.nan
                    print(
                        "Sample size after R cutoff reduced from %.0f to %.0f"
                        % (
                            len(tab),
                            sum(~np.isnan(tab[species_col.replace("s_proxy", "r_proxy")])),
                        )
                    )

            species = tab[species_col].copy()

            species /= np.sign(np.nanmedian(species))

            if abs_value:
                species = np.abs(species)

            vmin = np.nanpercentile(species, percentile, axis=0)
            vmax = np.nanpercentile(species, 100 - percentile, axis=0)

            tab_all = tab[liste].copy()
            tab_all["species"] = np.nan
            tab_all = tab_all.loc[valid_lines]

            print("Sample size after invalid lines rejected : %.0f" % (len(tab_all)))

            tab_all.loc[species < vmin, "species"] = "0"
            tab_all.loc[species > vmax, "species"] = "1"

            idx1 = tab_all.loc[tab_all["species"] == "0"].index
            idx2 = tab_all.loc[tab_all["species"] == "1"].index

            w1 = self.lbl_weight_selection(idx1, sub_dico="matching_mad")
            w2 = self.lbl_weight_selection(idx2, sub_dico="matching_mad")

            name_group = [["anticorr", "uncorr"][int(abs_value)], "corr"]

            print("Nb lines group %s : %.0f (weight=%.1f)" % (name_group[0], len(idx1), w1))
            print("Nb lines group %s : %.0f (weight=%.1f)\n" % (name_group[1], len(idx2), w2))

            deep_rv = table_rv.rv_subselection(rv_std=table_rv_std, selection=idx1)
            deep_rv.x = jdb
            shallow_rv = table_rv.rv_subselection(rv_std=table_rv_std, selection=idx2)
            shallow_rv.x = jdb
            deep_rv.recenter(who="Y")
            shallow_rv.recenter(who="Y")
            sas = myc.tableXY(
                jdb, shallow_rv.y - deep_rv.y, np.sqrt(deep_rv.yerr**2 + shallow_rv.yerr**2)
            )
            sas.substract_polyfit(5, replace=False)

            species = species[~np.isnan(species)]

            columns_to_remove = []
            for j in tab_all.dropna().keys():
                if len(np.unique(tab_all.dropna()[j])) == 1:
                    columns_to_remove.append(j)

            if len(columns_to_remove):
                print(
                    "Columns will be removed because unique values with the train sample : ",
                    columns_to_remove,
                )
                tab_all = tab_all.drop(columns=columns_to_remove)

            t1 = myc.table(tab_all)

            if Plot:
                plt.figure(figsize=(22, 16))
                plt.axes([0.04, 0.44, 0.93, 0.13])
                deep_rv.plot(color="b", alpha=1, label=name_group[0])
                shallow_rv.plot(color="g", alpha=1, label=name_group[1])
                plt.xlabel("Time")
                plt.title(kw_dico.upper())
                plt.legend()
                plt.axes([0.04, 0.26, 0.93, 0.13])
                sas.plot(color="gray", label="diff")
                sas.detrend_poly.plot(color="k", label="diff(detrend)")
                plt.xlabel("Time")
                plt.legend()
                plt.axes([0.04, 0.07, 0.93, 0.13])
                sas.periodogram(color="gray", Norm=True, p_min=0.75)
                sas.detrend_poly.periodogram(color="k", Norm=True, p_min=0.75)
                species = myf.rm_outliers(species, m=5, kind="inter")[1]
                plt.axes([0.04, 0.63, 0.37, 0.345])
                sp = myc.histoXY(species, species, xlabel=species_col)
                sp.plot1(color="k", alpha=0.3)
                plt.ylabel("Nb lines")
                if var == "s":
                    plt.xlim(
                        np.nanpercentile(species, 25) - myf.IQ(species) * 2.5,
                        np.nanpercentile(species, 75) + myf.IQ(species) * 2.5,
                    )
                else:
                    plt.xlim(-1 + int(abs_value), 1)
                if abs_value:
                    plt.xlim(0, None)

                plt.ylim(None, plt.gca().get_ylim()[1] * 1.3)
                if var == "r":
                    if plt.gca().get_xlim()[1] > 1:
                        plt.xlim(None, 1)
                    if plt.gca().get_xlim()[0] < -1:
                        plt.xlim(-1, None)
                plt.axvline(
                    vmin, color="b", label="%s: %.0f (%.1f)" % (name_group[0], len(idx1), w1)
                )
                plt.axvline(
                    vmax, color="g", label="%s: %.0f (%.1f)" % (name_group[1], len(idx2), w2)
                )
                plt.axvline(
                    np.nanmedian(species), color="k", label="median=%.2e" % (np.nanmedian(species))
                )
                plt.title("Total lines : %.0f" % (len(species)))
                plt.axvline(0, color="k", ls=":")
                plt.legend()

                ax1 = plt.axes([0.60, 0.63, 0.37, 0.345])
                # ax1 = plt.axes([0.40,0.63,0.21,0.345])
                # ax2 = plt.axes([0.76,0.63,0.21,0.345])
            else:
                ax1 = None
                ax2 = None

            col_before = list(t1.table.keys())[:-1]

            t1.Machine_learning(
                "species",
                test=[0, "xgb"],
                logi=False,
                plot_imp=True,
                outliers=["inter", 0],
                ax1=ax1,
                pred_full=True,
            )  # , ax2=ax2)

            beta1 = {}
            for i in list(t1.feature_importance.index):
                ypred = np.array(t1.table.species).astype("float")
                z = np.array(t1.table[i])
                test = myc.tableXY(z, ypred)
                test.supress_nan()
                test.fit_line(Draw=False)
                beta1[i] = np.sign(test.lin_slope_w)

            table_median = t1.table.copy()
            med = table_median.median()
            print("\n")
            for i in list(med.index):
                table_median[i] = med[i]

            beta2 = {}
            for i in list(col_before):
                table_test = table_median.copy()
                table_test[i] = t1.table[i]
                table_test = table_test[col_before]
                y_pred = t1.model.predict_proba(table_test)[:, 1]
                z = np.array(t1.table[i])
                test = myc.tableXY(z, y_pred)
                test.supress_nan()
                test.recenter(who="Y")
                test.fit_line(Draw=False)
                beta2[i] = np.sign(test.lin_slope_w)

            t1.feature_importance = t1.feature_importance.sort_values(by="gain", ascending=False)
            t1.feature_importance["mean"] = t1.feature_importance.mean(axis=1)
            t1.feature_importance["beta1"] = 0
            t1.feature_importance["beta2"] = 0
            for j in t1.feature_importance.index:
                t1.feature_importance.loc[j, "beta1"] = beta1[j]
                t1.feature_importance.loc[j, "beta2"] = beta2[j]

            feature_relevance.append(t1.feature_importance)

            col_after = list(t1.feature_importance.index)

            for i, j in enumerate(col_after[::-1]):
                if np.array(t1.feature_importance.loc[j]["beta2"] > 0):
                    ax1.patches[i].set_facecolor((0.80, 0.13, 0.11, 1.0))

            if len(col_before) != len(col_after):
                print(
                    " [WARNING] A column has disappeared during XGB fitting : ",
                    np.setdiff1d(np.array(col_before), np.array(col_after)),
                )

            self.xgb_table = t1

            if Plot:
                plt.savefig(
                    self.dir_root
                    + "PCA/XGB_vec_%s_%s_%s_analyse" % (kw_dico, sub_dico, species_col)
                    + ext
                    + ".png"
                )

                # plt.figure(figsize=(3+len(t1.feature_importance),6))
                # color = ['r','b','g'][0:len(t1.feature_importance.columns)-1]+['k']
                # alpha = [0.5,0.5,0.5][0:len(t1.feature_importance.columns)-1]+[0.8]
                # lw = [1,1,1][0:len(t1.feature_importance.columns)-1]+[7]

                # for i,j in enumerate(t1.feature_importance.columns):
                #     plt.plot(np.arange(len(t1.feature_importance)),t1.feature_importance[j],'-o',label=j,color=color[i],alpha=alpha[i],lw=lw[i])
                # plt.legend(loc=1)
                # plt.xticks(ticks=np.arange(len(t1.feature_importance)),labels=list(t1.feature_importance.index),rotation=45,ha='right')
                # plt.ylabel('Coefficient',fontsize=14)
                # plt.subplots_adjust(left=0.1,right=0.97,top=0.95,bottom=0.3)
                # plt.savefig(self.dir_root+'PCA/Vec_%s_%s_%s_features_ranking'%(sub_dico,species_col,kw_dico)+ext+'.pdf')

                sub_table = myc.table(
                    t1.table[
                        ["species"]
                        + list(
                            t1.feature_importance["gain"]
                            .copy()
                            .sort_values(ascending=False)
                            .index[0:5]
                        )
                    ]
                )

                plt.figure(figsize=(22, 16))
                sub_table.pairplot(col_species="species", n_levels=["2d", [1]], alpha_s=0.25)
                plt.savefig(
                    self.dir_root
                    + "PCA/XGB_vec_%s_%s_%s_pairplot" % (kw_dico, sub_dico, species_col)
                    + ext
                    + ".png"
                )

                if len(name_coeff) > 1:
                    plt.close("all")

            if Plot:
                sp_name = species_col.split("_")[-1]
                f0 = np.sort(
                    glob.glob(
                        self.dir_root + "PCA/PCA_var_ratio*%s*.png" % (sub_dico.split("_")[1])
                    )
                )
                f1 = np.sort(
                    glob.glob(
                        self.dir_root + "PCA/PCA_vec_%s*vectors*%s*.png" % (sub_dico, sp_name)
                    )
                )
                f2 = np.sort(
                    glob.glob(
                        self.dir_root
                        + "PCA/XGB_vec_%s_%s*%s_analyse.png" % (kw_dico, sub_dico, sp_name)
                    )
                )
                f3 = np.sort(
                    glob.glob(
                        self.dir_root
                        + "PCA/XGB_vec_%s_%s*%s_pairplot.png" % (kw_dico, sub_dico, sp_name)
                    )
                )

                if len(f0) | len(f1) | len(f2) | len(f3):
                    plt.ioff()
                    plt.figure(
                        figsize=(
                            1680 * np.max([len(f2), len(f3)]) / my_dpi,
                            917 * ((len(f1) != 0) + (len(f2) != 0) + (len(f3) != 0)) / my_dpi,
                        ),
                        dpi=my_dpi,
                    )
                    # plt.figure(figsize=(11*np.max([len(f2),len(f3)]),32+len(f1)*16))

                    plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0, hspace=0)
                    if len(f1):
                        plt.subplot(3, 2, 1)
                        if len(f0):
                            a = plt.imread(f0[0])
                            plt.imshow(a)
                            plt.axis("off")

                        plt.subplot(3, 2, 2)
                        a = plt.imread(f1[0])
                        plt.imshow(a)
                        plt.axis("off")

                    for j in range(len(f2)):
                        plt.subplot(
                            (len(f2) != 0) + len(f1) + (len(f3) != 0),
                            len(f2),
                            1 + len(f2) * len(f1) + j,
                        )
                        a = plt.imread(f2[j])
                        plt.imshow(a)
                        plt.axis("off")

                    for j in range(len(f3)):
                        plt.subplot(
                            (len(f2) != 0) + len(f1) + (len(f3) != 0),
                            len(f3),
                            1 + len(f2) + len(f3) * len(f1) + j,
                        )
                        a = plt.imread(f3[j])
                        plt.imshow(a)
                        plt.axis("off")
                    plt.savefig(
                        self.dir_root
                        + "PCA/Analyse_%s_XGB_%s_%s" % (kw_dico.upper(), sub_dico, sp_name)
                        + ext
                        + ".png"
                    )
                    plt.close()
                    plt.ion()
        if Plot:
            plt.figure("parameters", figsize=(17, 4))
            for n, tab in enumerate(feature_relevance[::-1]):
                plt.subplot(1, len(feature_relevance), len(feature_relevance) - n)
                test = myc.tableXY(np.arange(len(tab)), np.array(tab["gain"]))
                test.null()
                test.plot(fmt="ko-")
                test.myannotate(
                    list(tab.index),
                    draw_line=True,
                    direction="y",
                    reference="data",
                    frac_shift=0.2,
                    n_first=5,
                    fontsize=9,
                )
                plt.xlabel("Parameter ordered", fontsize=15)
                plt.title(name_coeff[len(feature_relevance) - n - 1])
                plt.axhline(y=1, color="k", ls=":")
                plt.ylim(0.01, None)
            plt.ylabel(r"F-score (gain)", fontsize=15)
            plt.subplots_adjust(left=0.05, right=0.98, wspace=0, bottom=0.13, top=0.93)
            plt.savefig(
                self.dir_root
                + "PCA/Parameters_%s_XGB_%s" % (kw_dico.upper(), sub_dico)
                + ext
                + ".pdf"
            )

            self.xgb_feature_importance = feature_relevance

    def yarara_xgb_proxy(
        self,
        proxy_base,
        sub_dico="matching_pca",
        name="matching_xgb",
        kw_dico="lbl_iter",
        analysis="morpho-atomic-pixel",
    ):
        alpha_crit = 1  # 1% of significance alpha
        z_crit = 0.25
        self.lbl_fit_vec(
            name,
            sub_dico=sub_dico,
            kw_dico=kw_dico,
            base_vec=proxy_base,
            time_detrending=0,
            add_step=9,
            save_database=False,
            col=0,
        )
        self.lbl_kolmo_cat(
            proxy_base,
            sub_dico=name,
            kw_dico=kw_dico,
            alpha_kolmo=alpha_crit,
            z_lim=z_crit,
            ext=name,
            depth_var="depth_rel",
        )
        self.lbl_xgb_cat(
            sub_dico=name,
            kw_dico=kw_dico,
            analysis=analysis,
            var="r",
            percentile=20,
            Plot=True,
            my_dpi=192,
        )
        self.lbl_xgb_cat(
            sub_dico=name,
            kw_dico=kw_dico,
            analysis=analysis,
            var="s",
            percentile=20,
            Plot=True,
            my_dpi=192,
        )

    def yarara_multilinear(self, slope_vec, analysis="morpho-atomic-pixel", flux_max=0.99):
        atomic = [
            "wave",
            "depth_rel",
            "atomic_number",
            "log_gf",
            "E_low",
            "lande_mean",
            "zeeman",
            "ionisation_energy",
        ]
        morpho = [
            "wave",
            "depth_rel",
            "dist_continuum",
            "diff_continuum_signed",
            "contrast_deri_norm",
            "asym_ddflux_norm",
        ]
        instru = ["qc_stitching", "qc_telluric", "qc_ghost_a", "qc_ghost_b", "qc_merged"]
        pixel = ["pixels_l1", "pixels_l2", "pixels_l3"]

        liste = []
        if type(analysis) == list:
            liste = analysis
        else:
            if analysis == "all":
                analysis = "morpho-atomic-instru-pixel"
            a = analysis.split("-")
            for analysis in a:
                if analysis == "morpho":
                    liste.append(morpho)
                elif analysis == "atomic":
                    liste.append(atomic)
                elif analysis == "instru":
                    liste.append(instru)
                elif analysis == "pixel":
                    liste.append(pixel)

        liste = list(np.unique(np.hstack(liste)))
        liste = list(np.sort(liste))

        self.import_material()
        self.import_kitcat()
        load = self.material
        kitcat = self.kitcat["catalogue"]

        kitcat_vec = np.array(self.material.kitcat_vec)
        mask = ~np.isnan(kitcat_vec)

        f = np.array(load["reference_spectrum"])
        df = np.hstack([0, np.diff(f)])
        ddf = np.hstack([0, np.diff(df)])
        df[f == 0] = 0
        ddf[f == 0] = 0

        wave = np.array(load["wave"])

        liste = ["E_low", "zeeman", "ionisation_energy", "atomic_number", "depth_rel"]

        base = [f, df, ddf]
        for i in liste:
            new_vec = np.nan * np.ones(len(f))
            new_vec[mask] = np.array(kitcat[i].loc[kitcat_vec[mask]])
            base.append(new_vec)

        base = np.array(base)
        mask_spec = ~np.isnan(np.sum(base, axis=0))
        mask_spec = mask_spec & (f < flux_max) & (wave > 5000)

        slope = myc.tableXY(wave[mask_spec], slope_vec[mask_spec])
        slope.fit_base(base[:, mask_spec])

        plt.figure(figsize=(15, 7))
        test = slope.corr(slope.vec_fitted, Draw=False)
        plt.axes([0.1, 0.44, 0.8, 0.25])
        plt.plot(slope.x, slope.y, "ko-", label="kernel")
        plt.plot(
            slope.x,
            slope.vec_fitted.y,
            "bo-",
            label="atomic multilinear model (R=%.2f)" % (test.r_pearson_w),
        )
        plt.legend()
        ax = plt.gca()
        plt.axes([0.1, 0.72, 0.8, 0.25], sharex=ax)
        plt.plot(wave, f)
        for i, j, k in zip(
            np.array(kitcat["wave"]),
            1 - np.array(kitcat["line_depth"]),
            np.array(kitcat["element"]),
        ):
            if type(k) == str:
                plt.annotate(k, (i, j))

        plt.axes([0.1, 0.08, 0.8, 0.25], sharex=ax, sharey=ax)
        plt.plot(slope.x, slope.vec_residues.y, "o-", color="purple")
        plt.axhline(y=0, color="r", alpha=0.4)

        plt.figure()
        table = np.vstack([slope.vec_residues.y, base[:, mask_spec]])
        tab = myc.table(table.T)
        tab.pairplot(color_param=0, kde_plot=False, fraction=0.1)

        plt.figure()
        table = np.vstack([slope.y, base[:, mask_spec]])
        tab = myc.table(table.T)
        tab.pairplot(color_param=0, kde_plot=False, fraction=0.1)

    def ccf_order_per_order(
        self,
        maskname,
        sub_dico="matching_mad",
        continuum="linear",
        weighted=True,
        ccf_oversampling=1,
        mask_col="weight_rv",
        treshold_telluric=1,
        color=0,
    ):
        self.import_table()
        self.import_material()
        self.import_kitcat(clean=(maskname.split("kitcat_")[-1].split("_")[0] == "cleaned"))

        jdb = np.array(self.table.jdb)
        wave = np.array(self.material.wave)

        orders = self.yarara_get_orders()

        orders_num = np.unique(orders[orders != 0])
        rv = []
        fwhm = []
        contrast = []
        wmean = []
        wrange = []
        if type(maskname) == str:
            if maskname.split(".")[-1] == "p":
                loc_mask = self.dir_root + "KITCAT/"
                mask_loc = loc_mask + maskname
                dico = pd.read_pickle(mask_loc)["catalogue"]
                dico = dico.loc[dico["rel_contam"] < treshold_telluric]
                if "valid" in dico.keys():
                    dico = dico.loc[dico["valid"]]
                mask = np.array([np.array(dico["freq_mask0"]), np.array(dico[mask_col])]).T
                mask = mask[mask[:, 1] != 0]
            else:
                mask_loc = root + "/Python/MASK_CCF/" + maskname + ".txt"
                mask = np.genfromtxt(mask_loc)
                mask = np.array([0.5 * (mask[:, 0] + mask[:, 1]), mask[:, 2]]).T

        maskname = maskname.split("_mask")[0]

        freq_mask0 = self.kitcat["catalogue"]["freq_mask0"]
        lbl_v0 = []
        lbl_v1 = []
        dbd_v0 = []
        dbd_v1 = []

        for j in orders_num:
            loc = np.unique(np.sort(np.where(orders == j)[0]))
            wmin = np.min(wave[loc])
            wmax = np.max(wave[loc])
            wrange.append("%.0f-%.0f" % (wmin, wmax))

            print("\nOrders : %.0f" % (j))
            if maskname.split("_")[0] == "kitcat":
                mask_lbl = np.array((freq_mask0 > wmin) & (freq_mask0 < wmax))

                lbl_v1.append(
                    self.lbl_subselection(
                        sub_dico=sub_dico, mask=mask_lbl, valid_lines=True, kw_dico="lbl_iter"
                    ).y
                )
                lbl_v0.append(
                    self.lbl_subselection(
                        sub_dico=sub_dico, mask=mask_lbl, valid_lines=False, kw_dico="lbl_iter"
                    ).y
                )
                dbd_v1.append(
                    self.lbl_subselection(
                        sub_dico=sub_dico, mask=mask_lbl, valid_lines=True, kw_dico="dbd"
                    ).y
                )
                dbd_v0.append(
                    self.lbl_subselection(
                        sub_dico=sub_dico, mask=mask_lbl, valid_lines=False, kw_dico="dbd"
                    ).y
                )

            mask2 = mask[(mask[:, 0] > wmin) & (mask[:, 0] < wmax)]
            wmean.append(np.mean(mask2[:, 0]))

            if len(mask2) > 2:
                try:
                    output = self.yarara_ccf(
                        sub_dico=sub_dico,
                        continuum=continuum,
                        mask=mask2,
                        weighted=weighted,
                        plot=False,
                        save=False,
                        ccf_oversampling=ccf_oversampling,
                        display_ccf=False,
                        rv_range=5 * int(self.fwhm),
                        wave_max=None,
                        wave_min=None,
                        force_brute=True,
                    )

                    rv.append(output["rv"].y)
                    fwhm.append(output["fwhm"].y)
                    contrast.append(output["contrast"].y)
                except ValueError:
                    rv.append(np.nan * jdb)
                    fwhm.append(np.nan * jdb)
                    contrast.append(np.nan * jdb)
            else:
                rv.append(np.nan * jdb)
                fwhm.append(np.nan * jdb)
                contrast.append(np.nan * jdb)

        rv = np.array(rv)
        fwhm = np.array(fwhm)
        contrast = np.array(contrast)
        lbl_v1 = np.array(lbl_v1)
        lbl_v0 = np.array(lbl_v0)
        dbd_v1 = np.array(dbd_v1)
        dbd_v0 = np.array(dbd_v0)

        self.debug1 = (rv, fwhm, contrast, lbl_v1, lbl_v0, dbd_v1, dbd_v0)

        rv = myc.tableXY(orders_num, np.std(rv, axis=1), 0 * orders_num)
        fwhm = myc.tableXY(orders_num, np.std(fwhm, axis=1), 0 * orders_num)
        contrast = myc.tableXY(orders_num, np.std(contrast, axis=1), 0 * orders_num)

        if maskname.split("_")[0] == "kitcat":
            rv1_lbl = myc.tableXY(orders_num, np.std(lbl_v1, axis=1), 0 * orders_num)
            contrast1_lbl = myc.tableXY(orders_num, np.std(dbd_v1, axis=1), 0 * orders_num)
            rv0_lbl = myc.tableXY(orders_num, np.std(lbl_v0, axis=1), 0 * orders_num)
            contrast0_lbl = myc.tableXY(orders_num, np.std(dbd_v0, axis=1), 0 * orders_num)
        else:
            rv1_lbl = rv.copy()
            rv1_lbl.y *= np.nan
            contrast1_lbl = rv.copy()
            contrast1_lbl.y *= np.nan
            rv0_lbl = rv.copy()
            rv0_lbl.y *= np.nan
            contrast0_lbl = rv.copy()
            contrast0_lbl.y *= np.nan

        tmin = np.min(rv.x[rv.y != np.nan])
        tmax = np.max(rv.x[rv.y != np.nan])

        plt.figure(1, figsize=(18, 12))
        plt.subplot(3, 1, 1)
        plt.xticks(ticks=orders_num)
        if not color:
            plt.ylim(0.0001, myf.IQ(rv.y) * 1.5 + np.nanpercentile(rv.y, 75))
        ax = plt.gca()
        rv.plot_outliers_cut(color=color, label=maskname + "_" + sub_dico.split("_")[1])
        if maskname.split("_")[0] == "kitcat":
            rv1_lbl.plot(
                ls=["-", ":"][int(sub_dico == "matching_diff")],
                label="LBL_" + maskname + "_" + sub_dico.split("_")[1],
            )
        plt.ylabel("RV rms [m/s]")
        plt.tick_params(top=True, direction="in")
        plt.grid(alpha=0.2)
        plt.legend()
        plt.xlim(tmin - 1, tmax + 1)
        ax2 = ax.twiny()
        ax2.set_xticks(orders_num)
        # ax2.set_xticklabels(['%.0f'%(i) for i in wmean],rotation=90)
        ax2.set_xticklabels(["%s" % (i) for i in wrange], rotation=90)
        ax2.set_xlim(ax.get_xlim())

        plt.subplot(3, 1, 2, sharex=ax)
        if not color:
            plt.ylim(0.0001, myf.IQ(fwhm.y) * 1.5 + np.nanpercentile(fwhm.y, 75))
        fwhm.plot_outliers_cut(color=color)
        plt.ylabel("FWHM rms [km/s]")
        plt.tick_params(top=True, direction="in")
        plt.grid(alpha=0.2)

        plt.subplot(3, 1, 3, sharex=ax)
        if not color:
            plt.ylim(0.0001, myf.IQ(contrast.y) * 1.5 + np.nanpercentile(contrast.y, 75))
        contrast.plot_outliers_cut(color=color)
        plt.ylabel("Contrast rms [%]")
        plt.xlabel("Orders")
        plt.tick_params(top=True, direction="in")
        plt.grid(alpha=0.2)
        plt.subplots_adjust(hspace=0, left=0.08, right=0.97, bottom=0.1)

    def lbl_kolmo_cat(
        self,
        base,
        sub_dico="matching_empca",
        quarte=3,
        kw_dico="lbl",
        alpha_kolmo=1,
        z_lim=0.25,
        ext="",
        depth_var="depth_rel",
    ):

        self.import_table()
        self.import_material()

        orders = self.yarara_get_orders()
        kitcat_vec = self.material["kitcat_vec"]

        os.system("rm " + self.dir_root + "PCA/PCA_vec_%s*" % (sub_dico) + ".png")

        jdb = np.array(self.table.jdb)
        phase_mod = np.arange(365)[
            np.argmin(
                np.array(
                    [np.max((jdb - k) % 365.25) - np.min((jdb - k) % 365.25) for k in range(365)]
                )
            )
        ]

        c_alpha = np.sqrt(-np.log(alpha_kolmo / 2 / 100) * 0.5)

        if kw_dico == "lbl":
            self.import_lbl()
            lbl_matrix = self.lbl
        if kw_dico == "lbl_iter":
            self.import_lbl_iter()
            lbl_matrix = self.lbl_iter
        elif kw_dico == "dbd":
            self.import_dbd()
            lbl_matrix = self.dbd
        elif kw_dico == "aba":
            self.import_aba()
            lbl_matrix = self.aba
        elif kw_dico == "bbb":
            self.import_bbb()
            lbl_matrix = self.bbb
        elif kw_dico == "wbw":
            self.import_wbw()
            lbl_matrix = self.wbw
        elif kw_dico == "bt":
            self.import_bt()
            lbl_matrix = self.bt

        table = lbl_matrix[sub_dico]["catalog"].copy()
        line_depth = self.kitcat_match_property(depth_var)
        line_depth[np.isnan(line_depth)] = -1
        depths = np.arange(0, 1.01, 0.1)
        for d1, d2 in zip(depths[0:-1], depths[1:]):
            line_depth[(line_depth > d1) & (line_depth < d2)] = 0.5 * (d1 + d2)

        all_keys = table.keys()
        keyword = []
        keyword_table = []

        for j in list(all_keys):
            if len(j.split("qc_")) > 1:
                keyword.append(j.split("qc_")[1])
                keyword_table.append(j)

        free = np.product(table[keyword_table], axis=1) != 0  # free of all kwoun contamination

        skip = len(keyword)
        name = []
        name_coeff = []

        kept_kw = []
        for j in all_keys:
            if j.split("_")[0] == "s":
                kept_kw.append(j.split("_")[1])

        for j in all_keys:
            if len(j.split("_")) > 1:
                if j.split("_")[1] in kept_kw:
                    keyword_table.append(j)
                    if j[0] == "s":
                        name.append(j[2:])
                        name_coeff.append(j)

        tab = table[keyword_table].copy()
        tab = tab.dropna()
        kw = list(tab.keys())
        tab_par = tab[kw[0:skip]]

        name = name[::2]
        name_coeff = name_coeff[::2]
        tab_coeff = tab[kw[skip::2]].copy()
        tab_coeff_std = tab[kw[skip + 1 :: 2]].copy()
        tab_coeff /= np.sign(tab_coeff.median())  # to get positive median value

        for j in name_coeff:
            t = myc.tableXY(
                np.arange(len(tab_coeff)),
                np.array(tab_coeff[j]),
                np.array(tab_coeff_std[j + "_std"]),
            )
            t.rms_w()
            tab_coeff_std[j + "_std"] = t.yerr / t.rms
            tab_coeff[j] = t.y / t.rms

        # kolmogorov ST test

        nb_lines = []
        for j in np.unique(orders):
            if j != 0:
                idx_lines = kitcat_vec[(orders[:, 0] == j) | (orders[:, 1] == j)]
                idx_lines = np.unique(idx_lines[np.in1d(idx_lines, tab_coeff.index)]).astype("int")
                nb_lines.append(len(idx_lines))

        nb_lines_depth = []
        for j in np.unique(line_depth):
            if j != -1:
                idx_lines_depth = kitcat_vec[(line_depth == j)]
                idx_lines_depth = np.unique(
                    idx_lines_depth[np.in1d(idx_lines_depth, tab_coeff.index)]
                ).astype("int")
                nb_lines_depth.append(len(idx_lines_depth))

        ax = None
        for i, col2 in enumerate(name):
            plt.figure(figsize=(18, 10))
            plt.subplots_adjust(left=0.05, right=0.97, top=0.94, bottom=0.09, hspace=0.4)
            vector = myc.tableXY(jdb, base[i])
            plt.subplot(4, 4, 4)
            vector.periodogram(p_min=0.7)
            if vector.perio_max > 1.2:
                plt.title("P = %s days" % (myf.format_number(vector.perio_max, digit=3)))
            else:
                periodogram = myc.tableXY(
                    1 / vector.freq[vector.freq < (1 / 1.2)][::-1],
                    vector.power[vector.freq < (1 / 1.2)][::-1],
                )
                p2 = periodogram.x[np.argmax(periodogram.y)]
                plt.title(
                    "P = %s (%s) days"
                    % (
                        myf.format_number(vector.perio_max, digit=3),
                        myf.format_number(p2, digit=3),
                    )
                )
                del periodogram

            plt.scatter(vector.perio_max, vector.power_max, color="b", zorder=10)
            plt.axvline(x=365.25, color="b", alpha=0.3)
            plt.axvline(x=365.25 / 2, color="b", alpha=0.3)
            plt.axvline(x=365.25 / 3, color="b", alpha=0.3)
            plt.axvline(x=365.25 / 4, color="b", alpha=0.3)
            vector.y = (vector.y - np.nanmedian(vector.y)) / (
                np.nanstd(vector.y - np.nanmedian(vector.y))
            )
            # vector.y *= np.nanmedian(np.array(tab_coeff[col2]))

            plt.subplot(4, 4, 1)
            plt.scatter(vector.x, vector.y, color="k", s=10)
            plt.subplot(4, 4, 2)
            plt.scatter((vector.x - phase_mod) % 365.25, vector.y, color="k", s=10)
            plt.subplot(4, 4, 3)
            vector.null()
            vector.plot(modulo=vector.perio_max, periodic=1)
            for p, variable in enumerate(["s", "r"]):
                col = variable + "_" + col2

                plt.subplot(4, 1, 2 + p)
                plt.ylabel(["S", r"$\mathcal{R}$"][p], fontsize=14)
                plt.xlabel(["", "# order"][p], fontsize=14)
                plt.title(col, fontsize=16)
                plt.axhline(y=0, color="r")
                plt.axhline(y=np.nanmedian(np.array(tab_coeff[col])), color="k", ls=":")
                std = []
                std2 = []
                median = []
                for j in np.unique(orders):
                    if j != 0:

                        idx_lines = kitcat_vec[(orders[:, 0] == j) | (orders[:, 1] == j)]
                        idx_lines = np.unique(
                            idx_lines[np.in1d(idx_lines, tab_coeff.index)]
                        ).astype("int")
                        box = plt.boxplot(
                            tab_coeff.loc[idx_lines][col], positions=[int(j)], showfliers=False
                        )
                        median.append(np.median(tab_coeff.loc[idx_lines][col]))
                        std.append(np.std(tab_coeff.loc[idx_lines][col]))
                        std2.append(
                            0.5 * (box["caps"][1].get_ydata()[0] - box["caps"][0].get_ydata()[0])
                        )
                plt.xticks(rotation=90)

                plt.plot(
                    np.arange(1, len(median) + 1), median, "ko", ls=None, alpha=0.8, zorder=10
                )
                plt.plot(np.arange(1, len(std) + 1), std, "ro", ls=None, alpha=0.2)
                plt.plot(np.arange(1, len(std2) + 1), std2, "go", ls=None, alpha=0.2)

                plt.subplot(4, 2, 7 + p)
                plt.ylabel(["S", r"$\mathcal{R}$"][p], fontsize=14)
                plt.xlabel("depth_rel", fontsize=14)
                plt.title(col, fontsize=16)
                plt.axhline(y=0, color="r")
                plt.axhline(y=np.nanmedian(np.array(tab_coeff[col])), color="k", ls=":")
                std = []
                std2 = []
                median = []
                depths = []
                for k, j in enumerate(np.unique(line_depth)):
                    if j != -1:
                        depths.append("%.2f" % (j))
                        idx_lines = kitcat_vec[line_depth == j]
                        idx_lines = np.unique(
                            idx_lines[np.in1d(idx_lines, tab_coeff.index)]
                        ).astype("int")
                        box = plt.boxplot(
                            tab_coeff.loc[idx_lines][col], positions=[k], showfliers=False
                        )
                        median.append(np.median(tab_coeff.loc[idx_lines][col]))
                        std.append(np.std(tab_coeff.loc[idx_lines][col]))
                        std2.append(
                            0.5 * (box["caps"][1].get_ydata()[0] - box["caps"][0].get_ydata()[0])
                        )
                plt.xticks(ticks=np.arange(1, len(median) + 1), labels=depths, rotation=90)

                plt.plot(
                    np.arange(1, len(median) + 1), median, "ko", ls=None, alpha=0.8, zorder=10
                )
                plt.plot(np.arange(1, len(std) + 1), std, "ro", ls=None, alpha=0.2)
                plt.plot(np.arange(1, len(std2) + 1), std2, "go", ls=None, alpha=0.2)

            plt.savefig(
                self.dir_root + "PCA/PCA_vec_" + ext + "_vectors_%.0f" % ((i + 1)) + ".png"
            )

        plt.figure(figsize=(24, 3 + 3 * (len(base) - 1)))
        ax = None
        ax1 = None
        ax2 = None
        alpha = []

        med_d = []
        for i, col in enumerate(name):
            col = variable + "_" + col
            vector = myc.tableXY(jdb, base[i])

            plt.subplot(len(base), 4, 4 * i + 3, sharex=ax1)
            if i == 0:
                ax1 = plt.gca()
            vector.periodogram(nb_perm=1, Norm=True, p_min=0.7)
            if vector.perio_max > 1.2:
                plt.title("P = %s days" % (myf.format_number(vector.perio_max, digit=3)))
            else:
                periodogram = myc.tableXY(
                    1 / vector.freq[vector.freq < (1 / 1.2)][::-1],
                    vector.power[vector.freq < (1 / 1.2)][::-1],
                )
                p2 = periodogram.x[np.argmax(periodogram.y)]
                plt.title(
                    "P = %s (%s) days"
                    % (
                        myf.format_number(vector.perio_max, digit=3),
                        myf.format_number(p2, digit=3),
                    )
                )
                del periodogram
            plt.scatter(vector.perio_max, vector.power_max, color="b", zorder=10)
            plt.axvline(x=365.25, color="b", alpha=0.5, ls="-")
            plt.axvline(x=365.25 / 2, color="b", alpha=0.5, ls="-")
            plt.ylabel(None)
            if i != (len(tab_coeff.keys()) - 1):
                plt.xlabel(None)

            vector.y = (vector.y - np.nanmedian(vector.y)) / (
                np.nanstd(vector.y - np.nanmedian(vector.y))
            )
            plt.subplot(len(base), 4, 4 * i + 1, sharex=ax)
            plt.tick_params(labelbottom=(i == (len(tab_coeff.keys()) - 1)), direction="in")
            if i == 0:
                ax = plt.gca()
            plt.title("Vector %s" % (name[i]))
            plt.scatter(vector.x, vector.y, color="k", s=10)
            if i == (len(tab_coeff.keys()) - 1):
                plt.xlabel("Time", fontsize=14)
            plt.subplot(len(base), 4, 4 * i + 2, sharex=ax2)
            if i == 0:
                ax2 = plt.gca()
            plt.scatter((vector.x - phase_mod) % 365.25, vector.y, color="k", s=10)
            plt.tick_params(labelbottom=False, direction="in")
            if i == (len(tab_coeff.keys()) - 1):
                plt.xlabel("Time%365.25", fontsize=14)

            plt.subplot(len(base), 4, 4 * i + 4)
            plt.axhline(y=0, color="k", ls=":")
            label = []
            for num, k in enumerate(keyword):
                mask = (tab_par["qc_" + k] == np.min(tab_par["qc_" + k])).astype("bool")
                t_coeff1 = tab_coeff.loc[mask]
                t_coeff2 = tab_coeff.loc[free]

                plt.boxplot(t_coeff1[col], positions=[num - 0.15], showfliers=False)
                plt.boxplot(t_coeff2[col], positions=[num + 0.15], showfliers=False)

                fig = plt.figure(99, figsize=(3, 3))
                y1, x1, dust = plt.hist(
                    t_coeff1[col],
                    bins=np.arange(-5, 5, 0.2),
                    density=True,
                    cumulative=True,
                    histtype="step",
                )
                y2, x2, dust = plt.hist(
                    t_coeff2[col],
                    bins=np.arange(-5, 5, 0.2),
                    density=True,
                    cumulative=True,
                    histtype="step",
                )

                x1 = x1[0:-1] + np.diff(x1) * 0.5
                x2 = x2[0:-1] + np.diff(x2) * 0.5

                med1 = np.median(t_coeff1[col])
                med2 = np.median(t_coeff2[col])

                phi1 = abs(0.5 - y1[myf.find_nearest(x1, 0)[0]])
                phi2 = abs(0.5 - y2[myf.find_nearest(x2, 0)[0]])

                dist = np.max(abs(y1 - y2))
                dist_idx = np.argmax(abs(y1 - y2))

                n = len(t_coeff1)
                m = len(t_coeff2)

                alpha.append(2 * np.exp(-2 * dist**2 / ((n + m) / (n * m))) * 100)
                treshold = c_alpha * np.sqrt((n + m) / (n * m))
                med_d.append(np.abs(med1 - med2) / np.abs(0.5 * (med1 + med2)))
                if i == (len(tab_coeff.keys()) / 2 - 1):
                    label.append(
                        "%.2f \n%s \n%.2f " % (10 * dist, k[0] + k[-1], 10 * treshold)
                    )  # ,2*np.abs(med1-med2)/np.abs(med1+med2)))
                else:
                    label.append("%.2f " % (10 * dist))  # ,2*np.abs(med1-med2)/np.abs(med1+med2)))

                plt.close(fig)

                if dist > treshold:
                    crit = int(phi2 > phi1)
                    plt.axvline(x=num, color=["r", "b"][crit], alpha=0.4)
                    plt.scatter(num, x1[dist_idx], color=["r", "b"][crit])

            if variable == "s":
                plt.ylim(-5, 5)
            elif variable == "r":
                plt.ylim(-0.7, 0.7)

            plt.xticks(ticks=np.arange(len(keyword)), labels=label)
            # plt.ylabel('Coeff/std(Coeff)',fontsize=14)

            # med_d.append(np.abs(med1-med2)/np.abs(0.5*(med1+med2)))

            # criterion = int((alpha[-1]<alpha_kolmo)&(med_d[-1]>z_lim))

        plt.subplots_adjust(top=0.97, bottom=0.09, hspace=0.40, left=0.07, right=0.93, wspace=0.25)
        plt.savefig(self.dir_root + "PCA/PCA_vectors_" + ext + ".pdf")

        self.alpha_kolmogorov = np.array(alpha)
        self.z_median = np.ravel(med_d)
        # self.alfev_maxi = np.array(maxi_alfev)

        # alpha = np.log10(alpha)
        # alpha[alpha<-2] = -2

        # plt.figure(figsize=(8,13))
        # plt.subplot(1,1,1)
        # ax = plt.gca()
        # plt.plot(np.arange(len(alpha)), alpha,'ko-')
        # plt.axhline(y=np.log10(alpha_kolmo),color='k',ls='-.',label='%.0f percent alpha value'%(alpha_kolmo))
        # plt.axhline(y=-2,color='r',ls=':')
        # plt.legend()
        # plt.xlabel('Vector index')
        # plt.ylabel('Kolmogorov log10(alpha significance)')
        # ax.set_xticks(np.arange(len(name)))
        # ax.set_xticklabels(name,rotation=90)

    def lbl_pairplot(
        self,
        columns,
        sub_dico="matching_wpca",
        kw_dico="lbl",
        col_species=None,
        fraction=1.0,
        n_levels=["2d", [1]],
    ):

        if kw_dico == "lbl":
            self.import_lbl()
            lbl_matrix = self.lbl
        elif kw_dico == "dbd":
            self.import_dbd()
            lbl_matrix = self.dbd
        elif kw_dico == "aba":
            self.import_aba()
            lbl_matrix = self.aba
        elif kw_dico == "bbb":
            self.import_bbb()
            lbl_matrix = self.bbb
        elif kw_dico == "wbw":
            self.import_wbw()
            lbl_matrix = self.wbw
        elif kw_dico == "bt":
            self.import_bt()
            lbl_matrix = self.bt

        table = lbl_matrix[sub_dico]["catalog"].copy()
        if col_species is not None:
            table = table.dropna(subset=[col_species])
            table["species"] = "g2"
            table.loc[table[col_species] < np.median(table[col_species]), "species"] = "g1"
            columns = columns + ["species"]
            col_species = "species"

        table = table[columns]
        tab = myc.table(table)
        plt.figure(figsize=(15, 12))
        tab.pairplot(fraction=fraction, col_species=col_species, n_levels=n_levels)

    def dbd_temp_atomic(
        self,
        period,
        season=0,
        element=["Fe1"],
        sub_dico="matching_pca",
        good_morpho=False,
        blended_lines=True,
        num_sim=100,
        rlim=[0.55, 2],
        expo=2,
    ):

        matrix, dust, crit = self.lbl_fit_sinus(
            period,
            season=season,
            kw_dico="dbd",
            col=0,
            sub_dico=sub_dico,
            good_morpho=good_morpho,
            blended_lines=blended_lines,
            radial_axis="r_corr",
            color_axis="line_depth",
            num_sim=num_sim,
        )

        matrix = matrix.loc[matrix["lande_mean"] < 1.7]  # remove magnetic sensitive lines
        matrix = matrix.loc[matrix["wave"] > 4400]  # remove inaccurate blue line depth

        matrix["r_corr_signed"] = matrix["r_corr"] * np.sign(matrix["phi"] + np.pi / 2)
        matrix["K_signed"] = matrix["K"] * np.sign(matrix["phi"] + np.pi / 2)
        matrix_significatif = matrix.loc[
            matrix["r_corr"] - matrix["r_corr_std"] * rlim[1] > rlim[0]
        ]
        matrix_significatif = matrix_significatif.loc[matrix_significatif["r_corr_signed"] < 0]

        elements = matrix_significatif["element"].value_counts()
        print(elements)
        plt.figure(2)
        plt.subplot(len(element), 1, 1)
        ax3 = plt.gca()

        for num, elem in enumerate(element):

            matrix_elem = matrix.loc[matrix["element"] == elem]
            mat_elem = matrix_significatif.loc[matrix_significatif["element"] == elem]

            #        plt.figure(figsize=(7,21))
            #        plt.subplot(3,1,1)
            #        ax1 = plt.gca()
            #        for k, kw in enumerate(['line_depth','wave','E_low']):
            #            cmap = ['brg','jet','brg'][k]
            #            plt.subplot(3,1,k+1,sharex=ax1,sharey=ax1)
            #            plt.ylabel(r'$\mathcal{R}_{pearson}$',fontsize=13)
            #            plt.xlabel(r'$\Delta R /R [\%]$',fontsize=13)
            #            test = myc.tableXY(matrix['K_signed']*100/matrix['line_depth'], matrix['r_corr_signed'], matrix['K_std']*100/matrix['line_depth'], matrix['r_corr_std'])
            #            test.plot()
            #            test.scatter(c=np.array(matrix[kw]),cmap=cmap)
            #            ax = plt.colorbar()
            #            ax.ax.set_ylabel(kw,fontsize=13)
            #        plt.subplots_adjust(top=0.95,bottom=0.08,left=0.12,right=0.95,hspace=0.35,wspace=0.35)

            s = np.array(mat_elem["K_signed"] * 100 / mat_elem["line_depth"])
            s_std = np.array(mat_elem["K_std"] * 100 / mat_elem["line_depth"])

            vmin = -0.5
            vmax = 0
            table = myc.tableXY(mat_elem["line_depth"], mat_elem["E_low"])
            plt.figure(1)
            plt.subplot(2, len(element), 1 + num)
            plt.title(
                "element : %s (I=%.2f eV)" % (elem, np.array(mat_elem["ionisation_potential"])[0])
            )
            plt.scatter(
                matrix_elem["line_depth"],
                matrix_elem["E_low"],
                c=matrix_elem["K_signed"] * 100 / matrix_elem["line_depth"],
                vmin=vmin,
                vmax=vmax,
                cmap="brg",
            )
            plt.xlabel("Line depth", fontsize=13)
            plt.ylabel(r"$\chi$ [eV]", fontsize=13)
            ax = plt.colorbar()
            ax.ax.set_ylabel(r"$\Delta R /R [\%]$")
            plt.subplot(2, len(element), 1 + len(element) + num)
            table.fit_poly2d(
                s,
                s_std,
                expo1=expo,
                expo2=expo,
                Draw=True,
                cmap="brg",
                vmin=vmin,
                vmax=vmax,
                ax_label=r"$\Delta R /R [\%]$",
            )
            plt.xlabel("Line depth", fontsize=13)
            plt.ylabel(r"$\chi$ [eV]", fontsize=13)
            plt.subplots_adjust(wspace=0.4, top=0.95, bottom=0.08, left=0.04, right=0.96)

            #            plt.subplot(3,len(element),1+2*len(element)+num)
            #            table2 = myc.tableXY(mat_elem['line_depth'],mat_elem['wave'])
            #            table2.fit_poly2d(table.z_res, s_std, Draw=True,cmap='seismic',vmin=-0.2,vmax=0.2,ax_label=r'$\Delta R /R [\%]$')
            #            plt.xlabel('Line depth',fontsize=13)
            #            plt.ylabel(r'Wavelength',fontsize=13)

            plt.figure(2)
            plt.subplot(len(element), 1, num + 1, sharex=ax3)
            plt.hist(table.z_res, bins=30)
            plt.title(
                "element : %s \nstd : %.2f iq : %.2f"
                % (elem, np.std(table.z_res), myf.IQ(table.z_res))
            )

            # ax = plt.colorbar()
            # ax.ax.set_ylabel(r'$\Delta R /R [\%]$')

        plt.subplots_adjust(hspace=0.3, top=0.95, bottom=0.08)

    def dbd_analyse_atomic(
        self,
        period,
        season=0,
        sub_dico="matching_pca",
        good_morpho=False,
        blended_lines=True,
        num_sim=100,
        rlim=[0.4, 2],
    ):

        matrix, dust, crit = self.lbl_fit_sinus(
            period,
            season=season,
            kw_dico="dbd",
            col=0,
            sub_dico=sub_dico,
            good_morpho=good_morpho,
            blended_lines=blended_lines,
            radial_axis="r_corr",
            color_axis="line_depth",
            num_sim=num_sim,
        )

        matrix["r_corr_signed"] = matrix["r_corr"] * np.sign(matrix["phi"] + np.pi / 2)
        matrix["K_signed"] = matrix["K"] * np.sign(matrix["phi"] + np.pi / 2)
        matrix_significatif = matrix.loc[
            matrix["r_corr"] - matrix["r_corr_std"] * rlim[1] > rlim[0]
        ]
        matrix_significatif = matrix_significatif.loc[matrix_significatif["r_corr_signed"] < 0]

        elements = matrix_significatif["element"].value_counts()
        print(elements)
        elements = elements[elements > 75].keys()

        # Fe = matrix_significatif.loc[matrix_significatif['element']=='Fe1']

        if len(elements) > 0:
            plt.figure(figsize=(15, 15))
            size = np.where((np.array([x**2 for x in range(8)]) >= len(elements)) == 1)[0][0]

            for k, e in enumerate(elements):
                smat = matrix_significatif.loc[matrix_significatif["element"] == e]
                vmin = np.percentile(smat["K"] * 100, 16)
                vmax = np.percentile(smat["K"] * 100, 84)
                plt.subplot(size, size, k + 1)
                I = np.array(smat["ionisation_potential"])[0]
                plt.title("Element : %s (I = %.1f [eV])" % (e, I), fontsize=13)
                test = myc.tableXY(smat["line_depth"], smat["E_low"])
                test.fit_poly2d(
                    smat["K"] * 100 / smat["line_depth"],
                    Draw=True,
                    cmap="plasma",
                    vmin=vmin,
                    vmax=vmax,
                    ax_label=r"$\Delta$ R / R [%]",
                )
                plt.xlabel("Line depth", fontsize=13)
                plt.ylabel(r"$\chi$ [eV]", fontsize=13)
            plt.subplots_adjust(
                top=0.95, bottom=0.08, left=0.08, right=0.95, hspace=0.35, wspace=0.35
            )

        plt.figure(figsize=(15, 7))
        plt.subplot(1, 2, 1)
        ax1 = plt.gca()
        matrix_significatif = matrix_significatif.dropna(subset=["E_low"])
        test = myc.tableXY(matrix_significatif["line_depth"], matrix_significatif["E_low"])
        test.fit_poly2d(
            matrix_significatif["r_corr"],
            vmin=np.percentile(matrix_significatif["r_corr"], 16),
            vmax=np.percentile(matrix_significatif["r_corr"], 84),
            Draw=True,
            cmap="plasma",
            ax_label=r"$\mathcal{R}_{pearson}$",
        )
        plt.xlabel("Line depth", fontsize=13)
        plt.ylabel(r"$\chi$ [eV]", fontsize=13)
        plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)
        test = myc.tableXY(matrix_significatif["line_depth"], matrix_significatif["E_low"])
        test.fit_poly2d(
            matrix_significatif["K"] * 100,
            vmin=vmin,
            vmax=vmax,
            Draw=True,
            cmap="plasma",
            ax_label=r"$\Delta R /R [\%]$",
        )
        plt.xlabel("Line depth", fontsize=13)
        plt.ylabel(r"$\chi$ [eV]", fontsize=13)

        matrix = matrix.dropna()
        plt.figure(figsize=(7, 21))
        plt.subplot(3, 1, 1)
        ax1 = plt.gca()
        for k, kw in enumerate(["line_depth", "wave", "E_low"]):
            cmap = ["brg", "jet", "brg"][k]
            plt.subplot(3, 1, k + 1, sharex=ax1, sharey=ax1)
            plt.ylabel(r"$\mathcal{R}_{pearson}$", fontsize=13)
            plt.xlabel(r"$\Delta R /R [\%]$", fontsize=13)
            test = myc.tableXY(
                matrix["K_signed"] * 100,
                matrix["r_corr_signed"],
                matrix["K_std"] * 100,
                matrix["r_corr_std"],
            )
            test.plot()
            test.scatter(c=np.array(matrix[kw]), cmap=cmap)
            ax = plt.colorbar()
            ax.ax.set_ylabel(kw, fontsize=13)
        plt.subplots_adjust(top=0.95, bottom=0.08, left=0.12, right=0.95, hspace=0.35, wspace=0.35)

        return matrix

        # plt.scatter(matrix['K_signed']/matrix['line_depth'],matrix['r_corr_signed'],c=matrix['line_depth'],cmap='brg')

        # plt.subplot(1,3,1)
        # plt.scatter(matrix['line_depth'],matrix['wave'],c=matrix['r_corr_signed'],cmap='seismic')
        # plt.subplot(1,3,2)
        # plt.scatter(matrix['line_depth'],matrix['E_low'],c=matrix['r_corr_signed'],cmap='seismic')
        # plt.subplot(1,3,3)
        # plt.scatter(matrix['line_depth'],matrix['E_low'],s=matrix['ionisation_potential'], c=matrix['r_corr_signed'],cmap='seismic')

    # =============================================================================
    # INTERFERENCE CORRECTION
    # =============================================================================

    def yarara_correct_pattern(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        wave_min=6000,
        wave_max=6100,
        reference="median",
        width_range=[0.1, 20],
        correct_blue=True,
        correct_red=True,
        jdb_range=None,
    ):

        """
        Supress interferency pattern produced by a material of a certain width by making a Fourier filtering.
        Width_min is the minimum width possible for the material in mm.

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : Minimum x axis limit for the plot
        wave_max : Maximum x axis limit for the plot
        zoom : int-type, to improve the resolution of the 2D plot
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum usedin the difference
        cmap : cmap of the 2D plot
        width_min : minimum width in mm above which to search for a peak in Fourier space
        correct_blue : enable correction of the blue detector for HARPS
        correct_blue : enable correction of the red detector for HARPS

        """

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        cmap = self.cmap
        planet = self.planet
        epsilon = 1e-6
        self.import_material()
        self.import_table()
        load = self.material

        myf.print_box("\n---- RECIPE : CORRECTION FRINGING ----\n")

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        flux = []
        conti = []
        snr = []
        jdb = []
        hl = []
        hr = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
            snr.append(file["parameters"]["SNR_5500"])
            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            try:
                hl.append(file["parameters"]["hole_left"])
            except:
                hl.append(None)
            try:
                hr.append(file["parameters"]["hole_right"])
            except:
                hr.append(None)

            flux.append(file["flux" + kw] / file[sub_dico]["continuum_" + continuum])
            conti.append(file[sub_dico]["continuum_" + continuum])

        step = file[sub_dico]["parameters"]["step"]

        wave = np.array(wave)
        flux = np.array(flux)
        conti = np.array(conti)
        snr = np.array(snr)
        jdb = np.array(jdb)

        idx_min = int(myf.find_nearest(wave, wave_min)[0])
        idx_max = int(myf.find_nearest(wave, wave_max)[0])

        mask = np.zeros(len(snr)).astype("bool")
        if jdb_range is not None:
            mask = (np.array(self.table.jdb) > jdb_range[0]) & (
                np.array(self.table.jdb) < jdb_range[1]
            )

        if reference == "median":
            if sum(~mask) < 50:
                print("Not enough spectra out of the temporal specified range")
                mask = np.zeros(len(snr)).astype("bool")
            else:
                print(
                    "%.0f spectra out of the specified temporal range can be used for the median"
                    % (sum(~mask))
                )

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            print("[INFO] Reference spectrum : median")
            ref = np.median(flux[~mask], axis=0)
        elif reference == "master":
            print("[INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print("[INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = flux[reference]
        else:
            ref = 0 * np.median(flux, axis=0)

        print("[INFO] Pattern analysis for range mm : ", width_range)
        # low = np.percentile(flux-ref,2.5)
        # high = np.percentile(flux-ref,97.5)
        old_diff = myf.smooth2d(flux - ref, smooth_map)
        low = np.percentile(flux / (ref + epsilon), 2.5)
        high = np.percentile(flux / (ref + epsilon), 97.5)

        diff = myf.smooth2d(flux / (ref + epsilon), smooth_map) - 1  # changed for a ratio 21-01-20
        diff[diff == -1] = 0
        diff_backup = diff.copy()

        if jdb_range is None:
            fig = plt.figure(figsize=(18, 6))

            plt.axes([0.06, 0.28, 0.7, 0.65])

            myf.my_colormesh(
                wave[idx_min:idx_max],
                np.arange(len(diff)),
                diff[:, idx_min:idx_max],
                zoom=zoom,
                vmin=low,
                vmax=high,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            ax = plt.gca()
            cbaxes = fig.add_axes([0.7 + 0.06, 0.28, 0.01, 0.65])
            plt.colorbar(cax=cbaxes)

            plt.axes([0.82, 0.28, 0.07, 0.65], sharey=ax)
            plt.plot(snr, np.arange(len(snr)), "k-")
            plt.tick_params(direction="in", top=True, right=True, labelleft=False)
            plt.xlabel("SNR", fontsize=14)

            plt.axes([0.90, 0.28, 0.07, 0.65], sharey=ax)
            plt.plot(jdb, np.arange(len(snr)), "k-")
            plt.tick_params(direction="in", top=True, right=True, labelleft=False)
            plt.xlabel("jdb", fontsize=14)

            plt.axes([0.06, 0.08, 0.7, 0.2], sharex=ax)
            plt.plot(wave[idx_min:idx_max], flux[snr.argmax()][idx_min:idx_max], color="k")
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
            plt.ylabel("Flux normalised", fontsize=14)
            plt.tick_params(direction="in", top=True, right=True)

            plt.show(block=False)
            index_sphinx = int(myf.sphinx("Which index present a clear pattern ?"))
            index_sphinx2 = int(myf.sphinx("Which index present no pattern ?"))
            plt.close()
        else:
            snr = np.array(self.table.snr)

            if sum(mask):
                i1 = np.argmax(snr[mask])
                index_sphinx = np.arange(len(snr))[mask][i1]

                if sum(mask) == len(mask):
                    index_sphinx2 = -1
                else:
                    i2 = np.argmax(snr[~mask])
                    index_sphinx2 = np.arange(len(snr))[~mask][i2]
            else:
                index_sphinx = -1
                index_sphinx2 = -1
                print("[INFO] No spectrum contaminated by interference pattern")

        print("Index spectrum containing a pattern : %.0f" % (index_sphinx))
        print("Index spectrum not containing a pattern : %.0f" % (index_sphinx2))

        time.sleep(1)

        if index_sphinx >= 0:

            for j in tqdm(range(len(diff))):
                diff_pattern = myc.tableXY(wave, diff[j].copy(), 0 * wave)
                diff_pattern.rolling(
                    window=10000, median=False
                )  # HARDCODE PARAMETER rolling filter to remove telluric power in the fourrier space
                diff_pattern.y[abs(diff_pattern.y) > 3 * diff_pattern.roll_IQ] = (
                    0
                    * np.random.randn(np.sum(abs(diff_pattern.y) > 3 * diff_pattern.roll_IQ))
                    * np.median(diff_pattern.roll_IQ)
                )
                diff[j] = diff_pattern.y

            diff_pattern = myc.tableXY(2 / wave[::-1], diff[index_sphinx][::-1])

            if np.float(index_sphinx2) >= 0:
                diff_flat = myc.tableXY(2 / wave[::-1], diff[index_sphinx2][::-1], 0 * wave)
            else:
                diff_flat = myc.tableXY(2 / wave[::-1], np.median(diff, axis=0)[::-1], 0 * wave)

            new_grid = np.linspace(diff_pattern.x.min(), diff_pattern.x.max(), len(diff_pattern.x))
            diff_pattern.interpolate(new_grid=new_grid, interpolate_x=False)
            diff_flat.interpolate(new_grid=new_grid, interpolate_x=False)

            # diff_pattern.rolling(window=1000) #HARDCODE PARAMETER rolling filter to remove telluric power in the fourrier space
            # diff_pattern.y[abs(diff_pattern.y)>3*diff_pattern.roll_IQ] = np.median(diff_pattern.y)
            # diff_flat.rolling(window=1000) #rolling filter to remove telluric power in the fourrier space
            # diff_flat.y[abs(diff_flat.y)>3*diff_flat.roll_IQ] = np.median(diff_flat.y)

            dl = np.diff(new_grid)[0]

            fft_pattern = np.fft.fft(diff_pattern.y)
            fft_flat = np.fft.fft(diff_flat.y)

            plt.figure()
            plt.plot(np.abs(fft_pattern))
            plt.plot(np.abs(fft_flat))

            diff_fourrier = np.abs(fft_pattern) - np.abs(fft_flat)
            diff_fourrier_pos = diff_fourrier[0 : int(len(diff_fourrier) / 2) + 1]

            width = (
                1e-7
                * abs(np.fft.fftfreq(len(diff_fourrier)))[0 : int(len(diff_fourrier) / 2) + 1]
                / dl
            )  # transformation of the frequency in mm of the material

            maximum = np.argmax(
                diff_fourrier_pos[(width > width_range[0]) & (width < width_range[1])]
            )
            freq_maximum_ref = width[(width > width_range[0]) & (width < width_range[1])][maximum]

            plt.figure()
            plt.plot(width, diff_fourrier_pos, color="k")
            print(
                "\n[INFO] The interference pattern is produced by material with a width of %.3f mm"
                % (freq_maximum_ref)
            )

            new_diff = diff.copy()
            hard_window = 50  # window extraction of the fourier power excess

            index_corrected_pattern_red = []
            index_corrected_pattern_blue = []
            timer_red = -1
            timer_blue = -1

            for j in range(len(diff)):

                diff_pattern = myc.tableXY(2 / wave[::-1], diff[j][::-1], 0 * wave)
                diff_pattern.interpolate(new_grid=new_grid, interpolate_x=False)
                diff_pattern_backup = diff_pattern.y.copy()
                # diff_pattern.rolling(window=1000)  #HARDCODE PARAMETER rolling filter to remove telluric power in the fourrier space
                # diff_pattern.y[abs(diff_pattern.y)>3*diff_pattern.roll_IQ] = np.median(diff_pattern.y)
                emergency = 1

                if (hl[j] == -99.9) | (hr[j] == -99.9):
                    emergency = 0
                    highest = [0, 0, 0]
                else:
                    dstep_clust = abs(np.mean(np.diff(diff[j])))
                    if dstep_clust == 0:
                        dstep_clust = np.mean(abs(np.mean(np.diff(diff, axis=1), axis=1)))
                    mask = myf.clustering(np.cumsum(diff[j]), dstep_clust, 0)[-1]
                    highest = mask[mask[:, 2].argmax()]

                    left = (
                        2 / wave[int(highest[0] - 1.0 / np.diff(wave)[0])]
                    )  # add 1.0 angstrom of security around the gap
                    right = (
                        2 / wave[int(highest[1] + 1.0 / np.diff(wave)[0])]
                    )  # add 1.0 angstrom of security around the gap

                    if hr[j] is not None:
                        if (hr[j] - wave[int(highest[1])]) > 10:
                            print(
                                "The border right of the gap is not the same than the one of the header"
                            )
                            emergency = 0
                    if hl[j] is not None:
                        if (hl[j] - wave[int(highest[0])]) > 10:
                            print(
                                "The border left of the gap is not the same than the one of the header"
                            )
                            emergency = 0

                if (highest[2] > 1000) & (emergency):  # because gap between ccd is large
                    left = myf.find_nearest(diff_pattern.x, left)[0]
                    right = myf.find_nearest(diff_pattern.x, right)[0]

                    left, right = right[0], left[0]  # because xaxis is reversed in 1/lambda space

                    fft_pattern_left = np.fft.fft(diff_pattern.y[0:left])
                    fft_flat_left = np.fft.fft(diff_flat.y[0:left])
                    diff_fourrier_left = np.abs(fft_pattern_left) - np.abs(fft_flat_left)

                    fft_pattern_right = np.fft.fft(diff_pattern.y[right:])
                    fft_flat_right = np.fft.fft(diff_flat.y[right:])
                    diff_fourrier_right = np.abs(fft_pattern_right) - np.abs(fft_flat_right)

                    width_right = (
                        1e-7
                        * abs(np.fft.fftfreq(len(diff_fourrier_right)))[
                            0 : int(len(diff_fourrier_right) / 2) + 1
                        ]
                        / dl
                    )  # transformation of the frequency in mm of the material
                    width_left = (
                        1e-7
                        * abs(np.fft.fftfreq(len(diff_fourrier_left)))[
                            0 : int(len(diff_fourrier_left) / 2) + 1
                        ]
                        / dl
                    )  # transformation of the frequency in mm of the material

                    diff_fourrier_pos_left = diff_fourrier_left[
                        0 : int(len(diff_fourrier_left) / 2) + 1
                    ]
                    diff_fourrier_pos_right = diff_fourrier_right[
                        0 : int(len(diff_fourrier_right) / 2) + 1
                    ]

                    maxima_left = myc.tableXY(
                        width_left[(width_left > width_range[0]) & (width_left < width_range[1])],
                        myf.smooth(
                            diff_fourrier_pos_left[
                                (width_left > width_range[0]) & (width_left < width_range[1])
                            ],
                            3,
                        ),
                    )
                    maxima_left.find_max(vicinity=int(hard_window / 2))

                    maxima_right = myc.tableXY(
                        width_right[
                            (width_right > width_range[0]) & (width_right < width_range[1])
                        ],
                        myf.smooth(
                            diff_fourrier_pos_right[
                                (width_right > width_range[0]) & (width_right < width_range[1])
                            ],
                            3,
                        ),
                    )
                    maxima_right.find_max(vicinity=int(hard_window / 2))

                    five_maxima_left = maxima_left.x_max[np.argsort(maxima_left.y_max)[::-1]][0:10]
                    five_maxima_right = maxima_right.x_max[np.argsort(maxima_right.y_max)[::-1]][
                        0:10
                    ]

                    five_maxima_left_y = maxima_left.y_max[np.argsort(maxima_left.y_max)[::-1]][
                        0:10
                    ]
                    five_maxima_right_y = maxima_right.y_max[np.argsort(maxima_right.y_max)[::-1]][
                        0:10
                    ]

                    thresh_left = 10 * np.std(diff_fourrier_pos_left)
                    thresh_right = 10 * np.std(diff_fourrier_pos_right)

                    five_maxima_left = five_maxima_left[five_maxima_left_y > thresh_left]
                    five_maxima_right = five_maxima_right[five_maxima_right_y > thresh_right]

                    if len(five_maxima_left) > 0:
                        where, freq_maximum_left, dust = myf.find_nearest(
                            five_maxima_left, freq_maximum_ref
                        )
                        maximum_left = maxima_left.index_max[np.argsort(maxima_left.y_max)[::-1]][
                            0:10
                        ]
                        maximum_left = maximum_left[five_maxima_left_y > thresh_left][where]
                    else:
                        freq_maximum_left = 0
                    if len(five_maxima_right) > 0:
                        where, freq_maximum_right, dust = myf.find_nearest(
                            five_maxima_right, freq_maximum_ref
                        )
                        maximum_right = maxima_right.index_max[
                            np.argsort(maxima_right.y_max)[::-1]
                        ][0:10]
                        maximum_right = maximum_right[five_maxima_right_y > thresh_right][where]
                    else:
                        freq_maximum_right = 0

                    offset_left = np.where(
                        ((width_left > width_range[0]) & (width_left < width_range[1])) == True
                    )[0][0]
                    offset_right = np.where(
                        ((width_right > width_range[0]) & (width_right < width_range[1])) == True
                    )[0][0]

                    if ((abs(freq_maximum_left - freq_maximum_ref) / freq_maximum_ref) < 0.05) & (
                        correct_red
                    ):
                        print(
                            "[INFO] Correcting night %.0f (r). Interference produced a width of %.3f mm"
                            % (j, freq_maximum_left)
                        )
                        timer_red += 1
                        index_corrected_pattern_red.append(timer_red)

                        # left
                        smooth = myc.tableXY(
                            np.arange(len(diff_fourrier_pos_left)),
                            np.ravel(
                                pd.DataFrame(diff_fourrier_pos_left)
                                .rolling(hard_window, min_periods=1, center=True)
                                .std()
                            ),
                        )
                        smooth.find_max(vicinity=int(hard_window / 2))
                        maxi = myf.find_nearest(smooth.x_max, maximum_left + offset_left)[1]

                        smooth.diff(replace=False)
                        smooth.deri.rm_outliers(m=5, kind="sigma")

                        loc_slope = np.arange(len(smooth.deri.mask))[~smooth.deri.mask]
                        cluster = myf.clustering(loc_slope, hard_window / 2, 0)[
                            0
                        ]  # half size of the rolling window
                        dist = np.ravel([np.mean(k) - maxi for k in cluster])
                        closest = np.sort(np.abs(dist).argsort()[0:2])

                        mini1 = cluster[closest[0]].min()
                        mini2 = cluster[closest[1]].max()

                        fft_pattern_left[mini1:mini2] = fft_flat_left[mini1:mini2]
                        fft_pattern_left[-mini2:-mini1] = fft_flat_left[-mini2:-mini1]
                        diff_pattern.y[0:left] = np.real(np.fft.ifft(fft_pattern_left))
                    if ((abs(freq_maximum_right - freq_maximum_ref) / freq_maximum_ref) < 0.05) & (
                        correct_blue
                    ):
                        print(
                            "[INFO] Correcting night %.0f (b). Interference produced a width of %.3f mm"
                            % (j, freq_maximum_right)
                        )
                        # right
                        timer_blue += 1
                        index_corrected_pattern_blue.append(timer_blue)

                        smooth = myc.tableXY(
                            np.arange(len(diff_fourrier_pos_right)),
                            np.ravel(
                                pd.DataFrame(diff_fourrier_pos_right)
                                .rolling(hard_window, min_periods=1, center=True)
                                .std()
                            ),
                        )
                        smooth.find_max(vicinity=int(hard_window / 2))
                        maxi = myf.find_nearest(smooth.x_max, maximum_right + offset_right)[1]
                        smooth.diff(replace=False)
                        smooth.deri.rm_outliers(
                            m=5, kind="sigma"
                        )  # find peak in fourier space and width from derivative

                        loc_slope = np.arange(len(smooth.deri.mask))[~smooth.deri.mask]
                        cluster = myf.clustering(loc_slope, hard_window / 2, 0)[
                            0
                        ]  # half size of the rolling window
                        dist = np.ravel([np.mean(k) - maxi for k in cluster])
                        closest = np.sort(np.abs(dist).argsort()[0:2])

                        mini1 = cluster[closest[0]].min()
                        mini2 = cluster[closest[1]].max()

                        fft_pattern_right[mini1:mini2] = fft_flat_right[mini1:mini2]
                        fft_pattern_right[-mini2:-mini1] = fft_flat_right[-mini2:-mini1]
                        diff_pattern.y[right:] = np.real(np.fft.ifft(fft_pattern_right))

                        # final

                    correction = myc.tableXY(
                        diff_pattern.x, diff_pattern_backup - diff_pattern.y, 0 * diff_pattern.x
                    )
                    correction.interpolate(new_grid=2 / wave[::-1], interpolate_x=False)
                    correction.x = 2 / correction.x[::-1]
                    correction.y = correction.y[::-1]
                    # diff_pattern.x = 2/diff_pattern.x[::-1]
                    # diff_pattern.y = diff_pattern.y[::-1]
                    # diff_pattern.interpolate(new_grid=wave)
                    new_diff[j] = diff_backup[j] - correction.y

                else:
                    fft_pattern = np.fft.fft(diff_pattern.y)
                    diff_fourrier = np.abs(fft_pattern) - np.abs(fft_flat)
                    diff_fourrier_pos = diff_fourrier[0 : int(len(diff_fourrier) / 2) + 1]
                    width = (
                        1e-7
                        * abs(np.fft.fftfreq(len(diff_fourrier)))[
                            0 : int(len(diff_fourrier) / 2) + 1
                        ]
                        / dl
                    )  # transformation of the frequency in mm of the material

                    maxima = myc.tableXY(
                        width[(width > width_range[0]) & (width < width_range[1])],
                        diff_fourrier_pos[(width > width_range[0]) & (width < width_range[1])],
                    )
                    maxima.find_max(vicinity=50)
                    five_maxima = maxima.x_max[np.argsort(maxima.y_max)[::-1]][0:5]
                    match = int(np.argmin(abs(five_maxima - freq_maximum_ref)))
                    freq_maximum = five_maxima[match]
                    maximum = maxima.index_max[np.argsort(maxima.y_max)[::-1]][0:5][match]
                    offset = np.where(
                        ((width > width_range[0]) & (width < width_range[1])) == True
                    )[0][0]

                    if (abs(freq_maximum - freq_maximum_ref) / freq_maximum) < 0.10:
                        print(
                            "[INFO] Correcting night %.0f. The interference pattern is produced by material with a width of %.3f mm"
                            % (j, freq_maximum)
                        )
                        timer_blue += 1
                        index_corrected_pattern_red.append(timer_red)

                        smooth = myc.tableXY(
                            np.arange(len(diff_fourrier_pos)),
                            np.ravel(
                                pd.DataFrame(diff_fourrier_pos)
                                .rolling(100, min_periods=1, center=True)
                                .std()
                            ),
                        )
                        smooth.find_max(vicinity=30)
                        maxi = myf.find_nearest(smooth.x_max, maximum + offset)[1]

                        smooth.diff(replace=False)
                        smooth.deri.rm_outliers(m=5, kind="sigma")

                        loc_slope = np.arange(len(smooth.deri.mask))[~smooth.deri.mask]
                        cluster = myf.clustering(loc_slope, 50, 0)[
                            0
                        ]  # half size of the rolling window
                        dist = np.ravel([np.mean(k) - maxi for k in cluster])

                        closest = np.sort(np.abs(dist).argsort()[0:2])

                        mini1 = cluster[closest[0]].min()
                        if len(closest) > 1:
                            mini2 = cluster[closest[1]].max()
                        else:
                            mini2 = mini1 + 1

                        fft_pattern[mini1:mini2] = fft_flat[mini1:mini2]
                        fft_pattern[-mini2:-mini1] = fft_flat[-mini2:-mini1]
                        diff_pattern.y = np.real(np.fft.ifft(fft_pattern))
                        diff_pattern.x = 2 / diff_pattern.x[::-1]
                        diff_pattern.y = diff_pattern.y[::-1]
                        diff_pattern.interpolate(new_grid=wave, interpolate_x=False)
                        new_diff[j] = diff_pattern.y

            self.index_corrected_pattern_red = index_corrected_pattern_red
            self.index_corrected_pattern_blue = index_corrected_pattern_blue

            correction = diff_backup - new_diff

            ratio2_backup = new_diff + 1

            new_conti = conti * flux / (ref * ratio2_backup + epsilon)
            new_continuum = new_conti.copy()
            new_continuum[flux == 0] = conti[flux == 0]

            diff2_backup = flux * conti / new_continuum - ref

            new_conti = flux * conti / (diff2_backup + ref + epsilon)

            new_continuum = new_conti.copy()
            new_continuum[flux == 0] = conti[flux == 0]

            low_cmap = self.low_cmap * 100
            high_cmap = self.high_cmap * 100

            fig = plt.figure(figsize=(21, 9))
            plt.axes([0.05, 0.66, 0.90, 0.25])
            myf.my_colormesh(
                wave[idx_min:idx_max],
                np.arange(len(diff)),
                100 * old_diff[:, idx_min:idx_max],
                zoom=zoom,
                vmin=low_cmap,
                vmax=high_cmap,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            ax = plt.gca()
            cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
            ax1 = plt.colorbar(cax=cbaxes)
            ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

            plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
            myf.my_colormesh(
                wave[idx_min:idx_max],
                np.arange(len(new_diff)),
                100 * diff2_backup[:, idx_min:idx_max],
                zoom=zoom,
                vmin=low_cmap,
                vmax=high_cmap,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            ax = plt.gca()
            cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
            ax2 = plt.colorbar(cax=cbaxes2)
            ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

            plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
            myf.my_colormesh(
                wave[idx_min:idx_max],
                np.arange(len(new_diff)),
                100 * old_diff[:, idx_min:idx_max] - 100 * diff2_backup[:, idx_min:idx_max],
                vmin=low_cmap,
                vmax=high_cmap,
                zoom=zoom,
                cmap=cmap,
            )
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
            ax3 = plt.colorbar(cax=cbaxes3)
            ax3.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

            plt.savefig(self.dir_root + "IMAGES/Correction_pattern.png")

            pre_map = np.zeros(np.shape(diff2_backup))
            if sub_dico == "matching_fourier":
                spec = self.import_spectrum()
                sub_dico = spec[sub_dico]["parameters"]["sub_dico_used"]
                step -= 1
                pre_map = pd.read_pickle(self.dir_root + "CORRECTION_MAP/map_matching_fourier.p")[
                    "correction_map"
                ]

            correction_pattern = old_diff - diff2_backup
            to_be_saved = {"wave": wave, "correction_map": correction_pattern + pre_map}
            myf.pickle_dump(
                to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_fourier.p", "wb")
            )

            print("\nComputation of the new continua, wait ... \n")
            time.sleep(0.5)

            i = -1
            for j in tqdm(files):
                i += 1
                file = pd.read_pickle(j)
                output = {"continuum_" + continuum: new_continuum[i]}
                file["matching_fourier"] = output
                file["matching_fourier"]["parameters"] = {
                    "pattern_width": freq_maximum_ref,
                    "width_cutoff": width_range,
                    "index_corrected_red": np.array(index_corrected_pattern_red),
                    "index_corrected_blue": np.array(index_corrected_pattern_blue),
                    "reference_pattern": index_sphinx,
                    "reference_flat": index_sphinx2,
                    "reference_spectrum": reference,
                    "sub_dico_used": sub_dico,
                    "step": step + 1,
                }
                ras.save_pickle(j, file)

            self.dico_actif = "matching_fourier"

            plt.show(block=False)

            self.fft_output = np.array([diff, new_diff, conti, new_continuum])

    # def yarara_correct_berv_line_backup(self,sub_dico='matching_diff',continuum='linear', reference='median', berv_shift='berv',
    #                              wave_min = 5730, wave_max = 5750, jdb_range=[54988,55021,2,1], method='weighted'):

    #     """recipes to remove residual straight line induced by the fourier pattern, jdb_range defined by cen B spectra HARPS03"""

    #     myf.print_box('\n---- RECIPE : CORRECTION BERV LINE ----\n')

    #     directory = self.directory

    #     zoom = self.zoom
    #     smooth_map = self.smooth_map
    #     low_cmap = self.low_cmap*100
    #     high_cmap = self.high_cmap*100
    #     cmap = self.cmap
    #     planet = self.planet

    #     self.import_material()
    #     self.import_table()
    #     load = self.material

    #     jdb = np.array(self.table.jdb)

    #     epsilon = 1e-12

    #     kw = '_planet'*planet
    #     if kw!='':
    #         print('\n---- PLANET ACTIVATED ----')

    #     if sub_dico is None:
    #         sub_dico = self.dico_actif
    #     print('---- DICO %s used ----'%(sub_dico))

    #     files = glob.glob(directory+'RASSI*.p')
    #     files = np.sort(files)

    #     flux = []
    #     flux_err = []
    #     conti = []
    #     snr = []
    #     berv = []
    #     rv_shift = []
    #     for i,j in enumerate(files):
    #         file = pd.read_pickle(j)
    #         if not i:
    #             wave = file['wave']
    #         snr.append(file['parameters']['SNR_5500'])

    #         f = file['flux'+kw]
    #         f_std = file['flux_err']
    #         c = file[sub_dico]['continuum_'+continuum]
    #         c_std = file['continuum_err']
    #         f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
    #         flux.append(f_norm)
    #         flux_err.append(f_norm_std)
    #         conti.append(c)
    #         try:
    #             berv.append(file['parameters'][berv_shift])
    #         except:
    #             berv.append(0)
    #         try:
    #             rv_shift.append(file['parameters']['RV_shift'])
    #         except:
    #             rv_shift.append(0)

    #     step = file[sub_dico]['parameters']['step']

    #     snr = np.array(snr)
    #     wave = np.array(wave)
    #     flux = np.array(flux)
    #     flux_err = np.array(flux_err)
    #     conti = np.array(conti)
    #     berv = np.array(berv)
    #     rv_shift = np.array(rv_shift)
    #     mean_berv = np.mean(berv)
    #     berv = berv - mean_berv - rv_shift

    #     if reference=='snr':
    #         ref = flux[snr.argmax()]
    #     elif reference=='median':
    #         ref = np.median(flux,axis=0)
    #     elif reference=='master':
    #         ref = np.array(load['reference_spectrum'])
    #     elif type(reference)==int:
    #         ref = flux[reference]
    #     else:
    #         ref = 0*np.median(flux,axis=0)

    #     idx_min = 0
    #     idx_max = len(wave)

    #     if wave_min is not None:
    #         idx_min = myf.find_nearest(wave,wave_min)[0]
    #     if wave_max is not None:
    #         idx_max = myf.find_nearest(wave,wave_max)[0]+1

    #     new_wave = wave[int(idx_min):int(idx_max)]

    #     diff_backup = myf.smooth2d(flux-ref,smooth_map)

    #     diff = diff_backup.copy()
    #     if np.sum(abs(berv))!=0:
    #         for j in tqdm(np.arange(len(flux))):
    #             test = myc.tableXY(wave,flux[j]-ref, flux_err[j])
    #             test.x = myf.doppler_r(test.x,berv[j]*1000)[1]
    #             test.interpolate(new_grid=wave, method='cubic', replace=True,interpolate_x=False)
    #             diff[j] = test.y
    #             flux_err[j] = test.yerr

    #     weight = 1/flux_err**2

    #     correction = np.zeros(np.shape(diff_backup))

    #     for jdb_ranges in jdb_range:
    #         m_out = jdb_ranges[2]
    #         smooth_box = jdb_ranges[3]
    #         flag = (jdb>jdb_ranges[0])&(jdb<jdb_ranges[1])
    #         if sum(flag)<5:
    #             print('[INFO] Less than 5 spectra in the specified jdb range (%.0f), recipes skipped'%(sum(flag)))
    #         else:
    #             mean1 = np.zeros(len(diff[0]))
    #             mean2 = np.zeros(len(diff[0]))

    #             matrice = diff.copy() - correction
    #             if m_out:
    #                 mask = ~(myf.rm_outliers(diff,m=2,kind='inter')[0])
    #                 matrice[mask] = np.nan

    #             if sum(flag):
    #                 if method!='median':
    #                     mean1 = np.nansum((matrice*weight)[flag],axis=0)/np.sum(weight[flag],axis=0)
    #                 else:
    #                     mean1 = np.nanmedian(matrice[flag],axis=0)

    #             if sum(~flag):
    #                 if method!='median':
    #                     mean2 = np.nansum((matrice*weight)[~flag],axis=0)/np.sum(weight[~flag],axis=0)
    #                 else:
    #                     mean2 = np.nanmedian(matrice[~flag],axis=0)

    #             value = (mean1 - mean2)

    #             idx1 = myf.find_nearest(wave,myf.doppler_r(wave[0],np.max(berv)*1000)[0])[0][0]
    #             idx2 = myf.find_nearest(wave,myf.doppler_r(wave[-1],np.min(berv)*1000)[0])[0][0]
    #             value = myf.smooth(value,smooth_box,shape='savgol')
    #             value[0:int(idx1+1)] = 0
    #             value[int(idx2):] = 0
    #             self.berv_vec = value

    #             correction[flag] += value*np.ones(sum(flag))[:,np.newaxis]

    #     correction_backup = correction.copy()
    #     if np.sum(abs(berv))!=0:
    #         for j in tqdm(np.arange(len(flux))):
    #             test = myc.tableXY(wave,correction[j],0*wave)
    #             test.x = myf.doppler_r(test.x,berv[j]*1000)[0]
    #             test.interpolate(new_grid=wave,method='cubic',replace=True,interpolate_x=False)
    #             correction_backup[j] = test.y

    #     diff_ref2 = diff_backup - correction_backup

    #     new_conti = conti*(diff_backup+ref)/(diff_ref2+ref+epsilon)
    #     new_continuum = new_conti.copy()
    #     new_continuum[flux==0] = conti[flux==0]
    #     new_continuum[new_continuum!=new_continuum] = conti[new_continuum!=new_continuum] #to supress mystic nan appearing

    #     new_wave = wave[int(idx_min):int(idx_max)]

    #     fig = plt.figure(figsize=(21,9))

    #     plt.axes([0.05,0.66,0.90,0.25])
    #     myf.my_colormesh(new_wave, np.arange(len(diff_backup)), 100*diff_backup[:,int(idx_min):int(idx_max)], vmin=low_cmap, vmax=high_cmap, cmap=cmap)
    #     plt.tick_params(direction='in',top=True,right=True,labelbottom=False)
    #     plt.ylabel('Spectra  indexes (time)',fontsize=14)
    #     plt.ylim(0,None)
    #     ax = plt.gca()
    #     cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
    #     ax1 = plt.colorbar(cax = cbaxes)
    #     ax1.ax.set_ylabel(r'$\Delta$ flux normalised [%]',fontsize=14)

    #     plt.axes([0.05,0.375,0.90,0.25],sharex=ax,sharey=ax)
    #     myf.my_colormesh(new_wave, np.arange(len(diff_backup)), 100*diff_ref2[:,int(idx_min):int(idx_max)], vmin=low_cmap, vmax=high_cmap, cmap=cmap)
    #     plt.tick_params(direction='in',top=True,right=True,labelbottom=False)
    #     plt.ylabel('Spectra  indexes (time)',fontsize=14)
    #     plt.ylim(0,None)
    #     ax = plt.gca()
    #     cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
    #     ax2 = plt.colorbar(cax = cbaxes2)
    #     ax2.ax.set_ylabel(r'$\Delta$ flux normalised [%]',fontsize=14)

    #     plt.axes([0.05,0.09,0.90,0.25],sharex=ax,sharey=ax)
    #     myf.my_colormesh(new_wave, np.arange(len(diff_backup)), 100*correction_backup[:,int(idx_min):int(idx_max)], vmin=low_cmap, vmax=high_cmap, cmap=cmap)
    #     plt.tick_params(direction='in',top=True,right=True,labelbottom=True)
    #     plt.ylabel('Spectra  indexes (time)',fontsize=14)
    #     plt.xlabel(r'Wavelength [$\AA$]',fontsize=14)
    #     plt.ylim(0,None)
    #     ax = plt.gca()
    #     cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
    #     ax3 = plt.colorbar(cax = cbaxes3)
    #     ax3.ax.set_ylabel(r'$\Delta$ flux normalised [%]',fontsize=14)

    #     plt.savefig(self.dir_root+'IMAGES/Correction_berv_lines.png')

    #     to_be_saved = {'wave':wave,'correction_map':correction_backup}
    #     myf.pickle_dump(to_be_saved,open(self.dir_root+'CORRECTION_MAP/map_matching_berv.p','wb'))

    #     print('Computation of the new continua, wait ... \n')
    #     time.sleep(0.5)
    #     i = -1
    #     for j in tqdm(files):
    #         i+=1
    #         file = pd.read_pickle(j)
    #         output = {'continuum_'+continuum:new_continuum[i]}
    #         file['matching_berv'] = output
    #         file['matching_berv']['parameters'] = {'reference_spectrum':reference, 'sub_dico_used':sub_dico, 'step':step+1}
    #         ras.save_pickle(j,file)

    #     self.dico_actif = 'matching_berv'

    def yarara_correct_berv_line(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        reference="median",
        berv_shift="berv",
        wave_min=5730,
        wave_max=5750,
        jdb_range=[54988, 55021, 2, 1],
        method="weighted",
    ):

        """recipes to remove residual straight line induced by the fourier pattern, jdb_range defined by cen B spectra HARPS03"""

        myf.print_box("\n---- RECIPE : CORRECTION BERV LINE ----\n")

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100
        cmap = self.cmap
        planet = self.planet

        self.import_material()
        self.import_table()
        load = self.material

        jdb = np.array(self.table.jdb)

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("---- DICO %s used ----" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        flux = []
        flux_err = []
        conti = []
        snr = []
        berv = []
        rv_shift = []
        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
            snr.append(file["parameters"]["SNR_5500"])

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            flux.append(f_norm)
            flux_err.append(f_norm_std)
            conti.append(c)
            try:
                berv.append(file["parameters"][berv_shift])
            except:
                berv.append(0)
            try:
                rv_shift.append(file["parameters"]["RV_shift"])
            except:
                rv_shift.append(0)

        step = file[sub_dico]["parameters"]["step"]

        snr = np.array(snr)
        wave = np.array(wave)
        flux = np.array(flux)
        flux_err = np.array(flux_err)
        conti = np.array(conti)
        berv = np.array(berv)
        rv_shift = np.array(rv_shift)
        mean_berv = np.mean(berv)
        berv = berv - mean_berv - rv_shift

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            ref = np.median(flux, axis=0)
        elif reference == "master":
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            ref = flux[reference]
        else:
            ref = 0 * np.median(flux, axis=0)

        idx_min = 0
        idx_max = len(wave)

        if wave_min is not None:
            idx_min = myf.find_nearest(wave, wave_min)[0]
        if wave_max is not None:
            idx_max = myf.find_nearest(wave, wave_max)[0] + 1

        new_wave = wave[int(idx_min) : int(idx_max)]

        diff_backup = myf.smooth2d(flux / (ref + epsilon) - 1, smooth_map)
        diff_backup[diff_backup == -1] = 0
        diff = diff_backup.copy()
        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(
                    wave, flux[j] / (ref + epsilon) - 1, flux_err[j] / (ref + epsilon)
                )
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.y[test.y == -1] = 0
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                diff[j] = test.y
                flux_err[j] = test.yerr

        weight = 1 / flux_err**2

        correction = np.zeros(np.shape(diff_backup))

        for jdb_ranges in jdb_range:
            m_out = jdb_ranges[2]
            smooth_box = jdb_ranges[3]
            flag = (jdb > jdb_ranges[0]) & (jdb < jdb_ranges[1])
            if sum(flag) < 5:
                print(
                    "[INFO] Less than 5 spectra in the specified jdb range (%.0f), recipes skipped"
                    % (sum(flag))
                )
            else:
                mean1 = np.zeros(len(diff[0]))
                mean2 = np.zeros(len(diff[0]))

                matrice = diff.copy() - correction
                if m_out:
                    mask = ~(myf.rm_outliers(diff, m=2, kind="inter")[0])
                    matrice[mask] = np.nan

                if sum(flag):
                    if method != "median":
                        mean1 = np.nansum((matrice * weight)[flag], axis=0) / np.sum(
                            weight[flag], axis=0
                        )
                    else:
                        mean1 = np.nanmedian(matrice[flag], axis=0)

                if sum(~flag):
                    if method != "median":
                        mean2 = np.nansum((matrice * weight)[~flag], axis=0) / np.sum(
                            weight[~flag], axis=0
                        )
                    else:
                        mean2 = np.nanmedian(matrice[~flag], axis=0)

                value = mean1 - mean2

                idx1 = myf.find_nearest(wave, myf.doppler_r(wave[0], np.max(berv) * 1000)[0])[0][0]
                idx2 = myf.find_nearest(wave, myf.doppler_r(wave[-1], np.min(berv) * 1000)[0])[0][
                    0
                ]
                value = myf.smooth(value, smooth_box, shape="savgol")
                value[0 : int(idx1 + 1)] = 0
                value[int(idx2) :] = 0
                self.berv_vec = value

                correction[flag] += value * np.ones(sum(flag))[:, np.newaxis]

        self.debug = correction.copy(), diff_backup.copy()

        correction_backup = correction.copy()
        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, correction[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[0]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                correction_backup[j] = test.y

        ratio2_backup = 1 + diff_backup - correction_backup

        new_conti = conti * flux / (ref * ratio2_backup + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[flux == 0] = conti[flux == 0]

        diff_ref1 = flux - ref
        diff_ref2 = flux * conti / new_continuum - ref

        new_conti = conti * flux / (diff_ref2 + ref + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[flux == 0] = conti[flux == 0]
        new_continuum[new_continuum != new_continuum] = conti[
            new_continuum != new_continuum
        ]  # to supress mystic nan appearing

        new_wave = wave[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))

        plt.axes([0.05, 0.66, 0.90, 0.25])
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * diff_ref1[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
        ax1 = plt.colorbar(cax=cbaxes)
        ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * diff_ref2[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
        ax2 = plt.colorbar(cax=cbaxes2)
        ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * (diff_ref1 - diff_ref2)[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
        ax3 = plt.colorbar(cax=cbaxes3)
        ax3.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.savefig(self.dir_root + "IMAGES/Correction_berv_lines.png")

        to_be_saved = {"wave": wave, "correction_map": correction_backup}
        myf.pickle_dump(
            to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_berv.p", "wb")
        )

        print("Computation of the new continua, wait ... \n")
        time.sleep(0.5)
        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_berv"] = output
            file["matching_berv"]["parameters"] = {
                "reference_spectrum": reference,
                "sub_dico_used": sub_dico,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.dico_actif = "matching_berv"

    def yarara_correct_smooth(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        reference="median",
        wave_min=4200,
        wave_max=4300,
        window_ang=5,
    ):

        myf.print_box("\n---- RECIPE : CORRECTION SMOOTH ----\n")

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100
        cmap = self.cmap
        planet = self.planet

        self.import_material()
        self.import_table()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        flux = []
        # flux_err = []
        conti = []
        snr = []
        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
                dgrid = file["parameters"]["dwave"]
                try:
                    hl = file["parameters"]["hole_left"]
                except:
                    hl = None
                try:
                    hr = file["parameters"]["hole_right"]
                except:
                    hr = None

            snr.append(file["parameters"]["SNR_5500"])

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            flux.append(f_norm)
            # flux_err.append(f_norm_std)
            conti.append(c)

        step = file[sub_dico]["parameters"]["step"]

        snr = np.array(snr)
        wave = np.array(wave)
        all_flux = np.array(flux)
        # all_flux_std = np.array(flux_err)
        conti = np.array(conti)

        if reference == "snr":
            ref = all_flux[snr.argmax()]
        elif reference == "median":
            print("[INFO] Reference spectrum : median")
            ref = np.median(all_flux, axis=0)
        elif reference == "master":
            print("[INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print("[INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = all_flux[reference]
        else:
            ref = 0 * np.median(all_flux, axis=0)

        diff_ref = all_flux.copy() - ref

        flux_ratio = (all_flux.copy() / (ref + epsilon)) - 1

        box_pts = int(window_ang / dgrid)

        for k in tqdm(range(len(all_flux))):
            spec_smooth = myf.smooth(
                myf.smooth(flux_ratio[k], box_pts=box_pts, shape=50),
                box_pts=box_pts,
                shape="savgol",
            )
            if hl is not None:
                i1 = int(myf.find_nearest(wave, hl)[0])
                i2 = int(myf.find_nearest(wave, hr)[0])
                spec_smooth[i1 - box_pts : i2 + box_pts] = 0

            flux_ratio[k] -= spec_smooth

        flux_ratio += 1
        flux_ratio *= ref

        diff_ref2 = flux_ratio - ref

        del flux_ratio

        correction_smooth = diff_ref - diff_ref2

        new_conti = conti * (diff_ref + ref) / (diff_ref2 + ref + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[flux == 0] = conti[flux == 0].copy()
        new_continuum[new_continuum != new_continuum] = conti[
            new_continuum != new_continuum
        ].copy()  # to supress mystic nan appearing
        new_continuum[np.isnan(new_continuum)] = conti[np.isnan(new_continuum)].copy()
        new_continuum[new_continuum == 0] = conti[new_continuum == 0].copy()
        new_continuum = self.uncorrect_hole(new_continuum, conti)

        idx_min = 0
        idx_max = len(wave)

        if wave_min is not None:
            idx_min = myf.find_nearest(wave, wave_min)[0]
        if wave_max is not None:
            idx_max = myf.find_nearest(wave, wave_max)[0] + 1

        if (idx_min == 0) & (idx_max == 1):
            idx_max = myf.find_nearest(wave, np.min(wave) + 200)[0] + 1

        new_wave = wave[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))

        plt.axes([0.05, 0.66, 0.90, 0.25])
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_ref)),
            100 * diff_ref[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
        ax1 = plt.colorbar(cax=cbaxes)
        ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_ref)),
            100 * diff_ref2[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
        ax2 = plt.colorbar(cax=cbaxes2)
        ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_ref)),
            100
            * (
                diff_ref[:, int(idx_min) : int(idx_max)]
                - diff_ref2[:, int(idx_min) : int(idx_max)]
            ),
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
        ax3 = plt.colorbar(cax=cbaxes3)
        ax3.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        if sub_dico == "matching_diff":
            plt.savefig(self.dir_root + "IMAGES/Correction_diff.png")
            spec = self.import_spectrum()
            name = "diff"
            recenter = spec[sub_dico]["parameters"]["recenter"]
            ref_name = spec[sub_dico]["parameters"]["reference_continuum"]
            savgol_window = spec[sub_dico]["parameters"]["savgol_window"]
            sub_dico = spec[sub_dico]["parameters"]["sub_dico_used"]
        else:
            plt.savefig(self.dir_root + "IMAGES/Correction_smooth.png")
            to_be_saved = {"wave": wave, "correction_map": correction_smooth}
            myf.pickle_dump(
                to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_smooth.p", "wb")
            )
            name = "smooth"
            recenter = False
            ref_name = str(reference)
            savgol_window = 0

        # diff_ref2 = flux_ratio - ref
        # correction_smooth = diff_ref - diff_ref2
        # new_conti = conti*(diff_ref+ref)/(diff_ref2+ref+epsilon)
        # new_continuum = new_conti.copy()
        # new_continuum = self.uncorrect_hole(new_continuum,conti)

        print("Computation of the new continua, wait ... \n")
        time.sleep(0.5)
        count_file = -1
        for j in tqdm(files):
            count_file += 1
            file = pd.read_pickle(j)
            conti = new_continuum[count_file]
            mask = yarara_artefact_suppressed(
                file[sub_dico]["continuum_" + continuum], conti, larger_than=50, lower_than=-50
            )
            conti[mask] = file[sub_dico]["continuum_" + continuum][mask]
            output = {"continuum_" + continuum: conti}
            file["matching_" + name] = output
            file["matching_" + name]["parameters"] = {
                "reference_continuum": ref_name,
                "sub_dico_used": sub_dico,
                "savgol_window": savgol_window,
                "window_ang": window_ang,
                "step": step + 1,
                "recenter": recenter,
            }
            ras.save_pickle(j, file)

    def yarara_retropropagation_correction(
        self, correction_map="matching_smooth", sub_dico="matching_cosmics", continuum="linear"
    ):

        # we introduce the continuum correction (post-processing of rassine normalisation) inside the cosmics correction
        # it allow to not lose the output product of rassine (no need of backup)
        # allow to rerun the code iteratively from the beginning
        # do not use matching_diff + substract_map['cosmics','smooth'] simultaneously otherwise 2 times corrections
        # rurunning the cosmics recipes will kill this correction, therefore a sphinx warning is included in the recipes
        # when a loop is rerun (beginning at fourier correction or water correction), make sure to finish completely the loop

        myf.print_box("\n---- RECIPE : RETROPROPAGATION CORRECTION MAP ----\n")

        directory = self.directory

        planet = self.planet

        self.import_material()
        self.import_table()
        file_test = self.import_spectrum()

        try:
            hl = file_test["parameters"]["hole_left"]
        except:
            hl = None
        try:
            hr = file_test["parameters"]["hole_right"]
        except:
            hr = None

        wave = np.array(self.material["wave"])
        if hl is not None:
            i1 = int(myf.find_nearest(wave, hl)[0])
            i2 = int(myf.find_nearest(wave, hr)[0])

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        correction_retro = pd.read_pickle(
            self.dir_root + "CORRECTION_MAP/map_" + correction_map + ".p"
        )["correction_map"]

        m = pd.read_pickle(
            self.dir_root + "CORRECTION_MAP/map_" + sub_dico + ".p"
        )  # allow the iterative process to be run
        m["correction_map"] += correction_retro
        myf.pickle_dump(m, open(self.dir_root + "CORRECTION_MAP/map_" + sub_dico + ".p", "wb"))

        print("\nComputation of the new continua, wait ... \n")
        time.sleep(0.5)
        count_file = -1
        for j in tqdm(files):
            count_file += 1
            file = pd.read_pickle(j)
            conti = file["matching_cosmics"]["continuum_" + continuum]
            flux = file["flux" + kw]

            flux_norm_corrected = flux / conti - correction_retro[count_file]
            new_conti = flux / (flux_norm_corrected + epsilon)  # flux/flux_norm_corrected

            new_continuum = new_conti.copy()
            new_continuum[flux == 0] = conti[flux == 0]
            new_continuum[new_continuum != new_continuum] = conti[
                new_continuum != new_continuum
            ]  # to supress mystic nan appearing
            new_continuum[np.isnan(new_continuum)] = conti[np.isnan(new_continuum)]
            new_continuum[new_continuum == 0] = conti[new_continuum == 0]
            if hl is not None:
                new_continuum[i1:i2] = conti[i1:i2]

            file[sub_dico]["continuum_" + continuum] = new_continuum
            ras.save_pickle(j, file)

    def yarara_correct_noise(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        reference="median",
        wave_min=5730,
        wave_max=5750,
        smooth_box=5,
    ):

        myf.print_box("\n---- RECIPE : CORRECTION NOISE ----\n")

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        low_cmap = self.low_cmap
        high_cmap = self.high_cmap
        cmap = self.cmap
        planet = self.planet

        self.import_material()
        self.import_table()
        rv_dace = self.import_dace_sts(substract_model=False)

        rv_range = np.max(rv_dace.y) - np.min(rv_dace.y)
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("---- DICO %s used ----" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        flux = []
        flux_err = []
        conti = []
        snr = []
        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
                dgrid = file["parameters"]["dwave"]
            snr.append(file["parameters"]["SNR_5500"])

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            flux.append(f_norm)
            flux_err.append(f_norm_std)
            conti.append(c)

        step = file[sub_dico]["parameters"]["step"]

        snr = np.array(snr)
        wave = np.array(wave)
        flux = np.array(flux)
        flux_err = np.array(flux_err)
        conti = np.array(conti)

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            ref = np.median(flux, axis=0)
        elif reference == "master":
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            ref = flux[reference]
        else:
            ref = 0 * np.median(flux, axis=0)

        m_spec = myc.tableXY(wave, ref, 0 * wave)
        m_spec_s = myc.tableXY(myf.doppler_r(wave, rv_range)[0], ref, 0 * ref)
        m_spec_s.interpolate(new_grid=wave, interpolate_x=False)

        shift_calib_savgol = [m_spec.y - m_spec_s.y]
        shift_calib_box = [m_spec.y - m_spec_s.y]

        if smooth_box < 20:
            smooth_box = 20
            print(
                Fore.YELLOW
                + " [WARNING] Smooth_box too small, parameters reduced to 20"
                + Fore.RESET
            )
            myf.make_sound("Warning")

        for j in np.arange(20, 201, 1):
            shift_calib_savgol.append(
                m_spec_s.y - myf.smooth(m_spec.y / m_spec_s.y, shape="savgol", box_pts=j)
            )
            shift_calib_box.append(m_spec_s.y - myf.smooth(m_spec.y / m_spec_s.y, box_pts=j))

        shift_calib_savgol = np.array(shift_calib_savgol)
        shift_calib_box = np.array(shift_calib_box)

        # convert to rv
        def conv_dflux_rv(delta_flux):
            dflux = np.gradient(m_spec.y) / dgrid
            coeff = 3e8 / wave.astype("float64")
            map_weight = (wave) ** 2 * np.abs(dflux) ** 2
            map_weight /= np.percentile(map_weight, 95)
            diff_rv = coeff * delta_flux / dflux
            diff_rv[:, m_spec.y > 0.95] = 0  # remove all weight smaller than 10%
            diff_rv[:, map_weight < 0.1] = 0  # remove all weight smaller than 10%
            diff_rv[abs(diff_rv) > 1000] = 0
            diff_rv[diff_rv == 0] = np.nan
            rv_curve = np.nansum(diff_rv * map_weight, axis=1) / np.nansum(map_weight)
            return rv_curve

        rv_curve_savgol = conv_dflux_rv(shift_calib_savgol)
        rv_curve_box = conv_dflux_rv(shift_calib_box)

        rv_savgol_norm = 100 * rv_curve_savgol / rv_curve_savgol[0]
        rv_box_norm = 100 * rv_curve_box / rv_curve_box[0]
        config_used = [rv_box_norm[smooth_box], rv_savgol_norm[smooth_box]][
            int(kernel == "savgol")
        ]
        plt.figure()
        plt.plot(np.arange(0, 21, 1), rv_savgol_norm, label="savgol")
        plt.plot(np.arange(0, 21, 1), rv_box_norm, label="rectangular")
        plt.scatter(
            smooth_box,
            config_used,
            color="k",
            label="config used (K output = %.0f %% K input)" % (config_used),
            zorder=10,
        )
        plt.ylabel("K RV output / K RV input [%]", fontsize=14)
        plt.xlabel("Kernel window", fontsize=14)
        plt.legend()

        # master_diff = master_spectrum.diff(master_spectrum_shifted)

        idx_min = 0
        idx_max = len(wave)

        if wave_min is not None:
            idx_min = myf.find_nearest(wave, wave_min)[0]
        if wave_max is not None:
            idx_max = myf.find_nearest(wave, wave_max)[0] + 1

        new_wave = wave[int(idx_min) : int(idx_max)]

        diff_ref = myf.smooth2d(flux - ref, smooth_map)
        diff_backup = diff_ref.copy()

        smoothed = []
        for j in tqdm(range(len(flux))):
            spec = myc.tableXY(wave, flux[j], flux_err[j])
            spec.smooth(shape=kernel, box_pts=smooth_box)
            smoothed.append(spec.y)
        ref = myf.smooth(ref, shape=kernel, box_pts=smooth_box)
        diff_ref2 = np.array(smoothed) - ref

        correction = diff_ref - diff_ref2

        new_conti = conti * (diff_ref + ref) / (diff_ref2 + ref + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[flux == 0] = conti[flux == 0]
        new_continuum[new_continuum != new_continuum] = conti[
            new_continuum != new_continuum
        ]  # to supress mystic nan appearing

        new_wave = wave[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))

        plt.axes([0.05, 0.66, 0.90, 0.25])
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_ref)),
            diff_backup[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
        plt.colorbar(cax=cbaxes)

        plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_ref)),
            diff_ref2[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
        plt.colorbar(cax=cbaxes2)

        plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_ref)),
            correction[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
        plt.colorbar(cax=cbaxes3)
        plt.savefig(self.dir_root + "IMAGES/Correction_smooth.png")

        to_be_saved = {"wave": wave, "correction_map": correction}
        myf.pickle_dump(
            to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_smooth.p", "wb")
        )

        print("Computation of the new continua, wait ... \n")
        time.sleep(0.5)
        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_smooth"] = output
            file["matching_smooth"]["parameters"] = {
                "reference_spectrum": reference,
                "sub_dico_used": sub_dico,
                "smooth_box": smooth_box,
                "kernel": kernel,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.yarara_analyse_summary()

        self.dico_actif = "matching_smooth"

        plt.show(block=False)

    # =============================================================================
    #  TELLURIC CORRECTION
    # =============================================================================

    def yarara_correct_telluric_proxy(
        self,
        sub_dico="matching_fourier",
        sub_dico_output="telluric",
        continuum="linear",
        wave_min=5700,
        wave_max=5900,
        reference="master",
        berv_shift="berv",
        smooth_corr=1,
        proxies_corr=["h2o_depth", "h2o_fwhm"],
        proxies_detrending=None,
        wave_min_correction=4400,
        wave_max_correction=None,
        min_r_corr=0.40,
        sigma_ext=2,
    ):

        """
        Display the time-series spectra with proxies and its correlation

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : Minimum x axis limit
        wave_max : Maximum x axis limit
        zoom : int-type, to improve the resolution of the 2D plot
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum usedin the difference
        berv_shift : True/False to move in terrestrial rest-frame
        proxy1_corr : keyword  of the first proxies from RASSINE dictionnary to use in the correlation
        proxy1_detrending : Degree of the polynomial fit to detrend the proxy
        proxy2_corr : keyword  of the second proxies from RASSINE dictionnary to use in the correlation
        proxy2_detrending : Degree of the polynomial fit to detrend the proxy
        cmap : cmap of the 2D plot
        min_wave_correction : wavelength limit above which to correct
        min_r_corr : minimum correlation coefficient of one of the two proxies to consider a line as telluric
        dwin : window correction increase by dwin to slightly correct above around the peak of correlation
        positive_coeff : The correction can only be absorption line profile moving and no positive


        """

        if sub_dico_output == "telluric":
            myf.print_box("\n---- RECIPE : CORRECTION TELLURIC WATER ----\n")
            name = "water"
        elif sub_dico_output == "oxygen":
            myf.print_box("\n---- RECIPE : CORRECTION TELLURIC OXYGEN ----\n")
            name = "oxygen"
        else:
            myf.print_box("\n---- RECIPE : CORRECTION TELLURIC PROXY ----\n")
            name = "telluric"

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        low_cmap = self.low_cmap
        high_cmap = self.high_cmap
        cmap = self.cmap
        planet = self.planet

        self.import_material()
        self.import_table()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        flux = []
        err_flux = []
        snr = []
        conti = []
        prox = []
        jdb = []
        berv = []
        rv_shift = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
                hole_left = file["parameters"]["hole_left"]
                hole_right = file["parameters"]["hole_right"]
                dgrid = file["parameters"]["dwave"]
            snr.append(file["parameters"]["SNR_5500"])
            for proxy_name in proxies_corr:
                prox.append(file["parameters"][proxy_name])

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            flux.append(f_norm)
            conti.append(c)
            err_flux.append(f_norm_std)

            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            try:
                berv.append(file["parameters"][berv_shift])
            except:
                berv.append(0)
            try:
                rv_shift.append(file["parameters"]["RV_shift"])
            except:
                rv_shift.append(0)

        step = file[sub_dico]["parameters"]["step"]

        wave = np.array(wave)
        flux = np.array(flux)
        err_flux = np.array(err_flux)
        conti = np.array(conti)
        snr = np.array(snr)
        proxy = np.array(prox)
        proxy = np.reshape(proxy, (len(proxy) // len(proxies_corr), len(proxies_corr)))

        jdb = np.array(jdb)
        berv = np.array(berv)
        rv_shift = np.array(rv_shift)
        mean_berv = np.mean(berv)
        berv = berv - mean_berv - rv_shift

        if proxies_detrending is None:
            proxies_detrending = [0] * len(proxies_corr)

        for k in range(len(proxies_corr)):
            proxy1 = myc.tableXY(jdb, proxy[:, k])
            proxy1.substract_polyfit(proxies_detrending[k])
            proxy[:, k] = proxy1.detrend_poly.y

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            print("[INFO] Reference spectrum : median")
            ref = np.median(flux, axis=0)
        elif reference == "master":
            print("[INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print("[INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = flux[reference]
        else:
            ref = 0 * np.median(flux, axis=0)

        low = np.percentile(flux - ref, 2.5)
        high = np.percentile(flux - ref, 97.5)

        ratio = myf.smooth2d(flux / (ref + 1e-6), smooth_map)
        ratio_backup = ratio.copy()

        diff_backup = myf.smooth2d(flux - ref, smooth_map)

        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, ratio[j], err_flux[j] / (ref + 1e-6))
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                ratio[j] = test.y
                err_flux[j] = test.yerr

        t = myc.table(ratio)
        t.rms_w(1 / (err_flux) ** 2, axis=0)

        rslope = []
        rcorr = []
        for k in range(len(proxies_corr)):
            rslope.append(
                np.median(
                    (ratio - np.mean(ratio, axis=0))
                    / ((proxy[:, k] - np.mean(proxy[:, k]))[:, np.newaxis]),
                    axis=0,
                )
            )
            rcorr.append(abs(rslope[-1] * np.std(proxy[:, k]) / (t.rms + epsilon)))

        # rcorr1 = abs(rslope1*np.std(proxy1)/np.std(ratio,axis=0))
        # rcorr2 = abs(rslope2*np.std(proxy2)/np.std(ratio,axis=0))

        rslope = np.array(rslope)
        rcorr = np.array(rcorr)

        rcorr = np.max(rcorr, axis=0)
        r_corr = myc.tableXY(wave, rcorr)
        r_corr.smooth(box_pts=smooth_corr, shape="savgol", replace=True)
        rcorr = r_corr.y

        if wave_min_correction is None:
            wave_min_correction = np.min(wave)

        if wave_max_correction is None:
            wave_max_correction = np.max(wave)

        if min_r_corr is None:
            min_r_corr = np.percentile(rcorr[wave < 5400], 75) + 1.5 * myf.IQ(rcorr[wave < 5400])
            print(
                "\n [INFO] Significative R Pearson detected as %.2f based on wavelength smaller than 5400 \AA"
                % (min_r_corr)
            )

        first_guess_position = (
            (rcorr > min_r_corr) & (wave > wave_min_correction) & (wave < wave_max_correction)
        )  # only keep >0.4 and redder than 4950 AA
        second_guess_position = first_guess_position

        # fwhm_telluric = np.median(self.table['telluric_fwhm'])
        fwhm_telluric = self.star_info["FWHM"][""]  # 09.08.21
        val, borders = myf.clustering(first_guess_position, 0.5, 1)
        val = np.array([np.product(v) for v in val]).astype("bool")
        borders = borders[val]
        wave_tel = wave[(0.5 * (borders[:, 0] + borders[:, 1])).astype("int")]
        extension = np.round(sigma_ext * fwhm_telluric / 3e5 * wave_tel / dgrid, 0).astype("int")
        borders[:, 0] -= extension
        borders[:, 1] += extension
        borders[:, 2] = borders[:, 1] - borders[:, 0] + 1
        borders = myf.merge_borders(borders)
        second_guess_position = myf.flat_clustering(len(wave), borders).astype("bool")

        guess_position = np.arange(len(second_guess_position))[second_guess_position]

        correction = np.zeros((len(wave), len(jdb)))

        len_segment = 10000
        print("\n")
        for k in range(len(guess_position) // len_segment + 1):
            print(
                " [INFO] Segment %.0f/%.0f being reduced\n"
                % (k + 1, len(guess_position) // len_segment + 1)
            )
            second_guess_position = guess_position[k * len_segment : (k + 1) * len_segment]
            # print(second_guess_position)

            collection = myc.table(ratio.T[second_guess_position])

            base_vec = np.vstack(
                [np.ones(len(flux))] + [proxy[:, k] for k in range(len(proxies_corr))]
            )
            # rm outliers and define weight for the fit
            weights = (1 / (err_flux / (ref + 1e-6)) ** 2).T[second_guess_position]
            IQ = myf.IQ(collection.table, axis=1)
            Q1 = np.nanpercentile(collection.table, 25, axis=1)
            Q3 = np.nanpercentile(collection.table, 75, axis=1)
            sup = Q3 + 1.5 * IQ
            inf = Q1 - 1.5 * IQ
            out = (collection.table > sup[:, np.newaxis]) | (collection.table < inf[:, np.newaxis])
            weights[out] = np.min(weights) / 100

            collection.fit_base(base_vec, weight=weights, num_sim=1)

            correction[second_guess_position] = collection.coeff_fitted.dot(base_vec)

        correction = np.transpose(correction)
        correction[correction == 0] = 1

        correction_backup = correction.copy()
        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, correction[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[0]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                correction_backup[j] = test.y

        index_min_backup = int(
            myf.find_nearest(wave, myf.doppler_r(wave[0], berv.max() * 1000)[0])[0]
        )
        index_max_backup = int(
            myf.find_nearest(wave, myf.doppler_r(wave[-1], berv.min() * 1000)[0])[0]
        )
        correction_backup[:, 0:index_min_backup] = 1
        correction_backup[:, index_max_backup:] = 1
        index_hole_right = int(
            myf.find_nearest(wave, hole_right + 1)[0]
        )  # correct 1 angstrom band due to stange artefact at the border of the gap
        index_hole_left = int(
            myf.find_nearest(wave, hole_left - 1)[0]
        )  # correct 1 angstrom band due to stange artefact at the border of the gap
        correction_backup[:, index_hole_left : index_hole_right + 1] = 1

        #        if positive_coeff:
        #            correction_backup[correction_backup>0] = 0

        ratio2_backup = ratio_backup - correction_backup + 1

        # print(psutil.virtual_memory().percent)

        del correction_backup
        del correction
        del err_flux

        new_conti = conti * flux / (ref * ratio2_backup + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[flux == 0] = conti[flux == 0]

        del ratio2_backup
        del ratio_backup

        diff2_backup = flux * conti / new_continuum - ref

        # plot end

        idx_min = 0
        idx_max = len(wave)

        if wave_min is not None:
            idx_min = myf.find_nearest(wave, wave_min)[0]
        if wave_max is not None:
            idx_max = myf.find_nearest(wave, wave_max)[0] + 1

        new_wave = wave[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))

        plt.axes([0.05, 0.66, 0.90, 0.25])
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            diff_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
        plt.colorbar(cax=cbaxes)

        plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            diff2_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
        plt.colorbar(cax=cbaxes2)

        plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            (diff_backup - diff2_backup)[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
        plt.colorbar(cax=cbaxes3)

        plt.savefig(self.dir_root + "IMAGES/Correction_" + name + ".png")

        correction_water = diff_backup - diff2_backup
        to_be_saved = {"wave": wave, "correction_map": correction_water}
        myf.pickle_dump(
            to_be_saved,
            open(self.dir_root + "CORRECTION_MAP/map_matching_" + sub_dico_output + ".p", "wb"),
        )

        print("\nComputation of the new continua, wait ... \n")
        time.sleep(0.5)

        if sub_dico == "matching_" + sub_dico_output:
            spec = self.import_spectrum()
            sub_dico = spec[sub_dico]["parameters"]["sub_dico_used"]

        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_" + sub_dico_output] = output
            file["matching_" + sub_dico_output]["parameters"] = {
                "reference_spectrum": reference,
                "sub_dico_used": sub_dico,
                "proxies": proxies_corr,
                "min_wave_correction ": wave_min_correction,
                "minimum_r_corr": min_r_corr,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.dico_actif = "matching_" + sub_dico_output

    # =============================================================================
    # OXYGENE CORRECTION
    # =============================================================================

    def yarara_correct_oxygen(
        self,
        sub_dico="matching_telluric",
        continuum="linear",
        berv_shift="berv",
        reference="master",
        wave_min=5760,
        wave_max=5850,
        oxygene_bands=[[5787, 5835], [6275, 6340], [6800, 6950]],
    ):

        """
        Display the time-series spectra with proxies and its correlation

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : Minimum x axis limit
        wave_max : Maximum x axis limit
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution
        berv_shift : True/False to move in terrestrial rest-frame
        cmap : cmap of the 2D plot
        low_cmap : vmin cmap colorbar
        high_cmap : vmax cmap colorbar

        """

        myf.print_box("\n---- RECIPE : CORRECTION TELLURIC OXYGEN ----\n")

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        cmap = self.cmap
        planet = self.planet
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100
        self.import_material()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        flux = []
        flux_err = []
        conti = []
        snr = []
        jdb = []
        berv = []
        rv_shift = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
                hole_left = file["parameters"]["hole_left"]
                hole_right = file["parameters"]["hole_right"]
            snr.append(file["parameters"]["SNR_5500"])

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            flux.append(f_norm)
            flux_err.append(f_norm_std)
            conti.append(c)
            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            if type(berv_shift) != np.ndarray:
                try:
                    berv.append(file["parameters"][berv_shift])
                except:
                    berv.append(0)
            else:
                berv = berv_shift
            try:
                rv_shift.append(file["parameters"]["RV_shift"])
            except:
                rv_shift.append(0)

        step = file[sub_dico]["parameters"]["step"]

        wave = np.array(wave)
        flux = np.array(flux)
        flux_err = np.array(flux_err)
        conti = np.array(conti)
        snr = np.array(snr)
        jdb = np.array(jdb)
        rv_shift = np.array(rv_shift)
        berv = np.array(berv)
        mean_berv = np.mean(berv)
        berv = berv - mean_berv - rv_shift

        def idx_wave(wavelength):
            return int(myf.find_nearest(wave, wavelength)[0])

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            ref = np.median(flux, axis=0)
        elif reference == "master":
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            ref = flux[reference]
        else:
            ref = 0 * np.median(flux, axis=0)

        diff_ref = myf.smooth2d(flux - ref, smooth_map)
        ratio_ref = myf.smooth2d(flux / (ref + epsilon), smooth_map)

        diff_backup = diff_ref.copy()
        ratio_backup = ratio_ref.copy()

        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, ratio_ref[j], flux_err[j] / (ref + epsilon))
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                ratio_ref[j] = test.y
                flux_err[j] = test.yerr

        inside_oxygene_mask = np.zeros(len(ratio_ref.T))
        for k in range(len(oxygene_bands)):
            first = myf.find_nearest(wave, oxygene_bands[k][0])[0]
            last = myf.find_nearest(wave, oxygene_bands[k][1])[0]
            inside_oxygene_mask[int(first) : int(last)] = 1
        # inside_oxygene[wave>6600] = 0  #reject band [HYPERPARAMETER HARDCODED]
        inside_oxygene = inside_oxygene_mask.astype("bool")

        vec = ratio_ref.T[inside_oxygene]
        collection = myc.table(vec)

        print(np.shape(flux_err))

        weights = 1 / (flux_err) ** 2
        weights = weights.T[inside_oxygene]
        IQ = myf.IQ(collection.table, axis=1)
        Q1 = np.nanpercentile(collection.table, 25, axis=1)
        Q3 = np.nanpercentile(collection.table, 75, axis=1)
        sup = Q3 + 1.5 * IQ
        inf = Q1 - 1.5 * IQ
        out = (collection.table > sup[:, np.newaxis]) | (collection.table < inf[:, np.newaxis])
        weights[out] = np.min(weights) / 100

        base_vec = np.vstack(
            [np.ones(len(flux)), jdb - np.median(jdb), jdb**2 - np.median(jdb**2)]
        )  # fit offset + para trend par oxygene line (if binary drift substract)
        collection.fit_base(base_vec, weight=weights, num_sim=1)
        correction = np.zeros((len(wave), len(jdb)))
        correction[inside_oxygene] = collection.coeff_fitted.dot(base_vec)
        correction = np.transpose(correction)
        correction[correction == 0] = 1
        correction_backup = correction.copy()

        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, correction[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[0]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                correction_backup[j] = test.y

        index_min_backup = int(
            myf.find_nearest(wave, myf.doppler_r(wave[0], berv.max() * 1000)[0])[0]
        )
        index_max_backup = int(
            myf.find_nearest(wave, myf.doppler_r(wave[-1], berv.min() * 1000)[0])[0]
        )
        correction_backup[:, 0:index_min_backup] = 1
        correction_backup[:, index_max_backup:] = 1
        index_hole_right = int(
            myf.find_nearest(wave, hole_right + 1)[0]
        )  # correct 1 angstrom band due to stange artefact at the border of the gap
        index_hole_left = int(
            myf.find_nearest(wave, hole_left - 1)[0]
        )  # correct 1 angstrom band due to stange artefact at the border of the gap
        correction_backup[:, index_hole_left : index_hole_right + 1] = 1

        del flux_err
        del weights
        del correction

        ratio2_backup = ratio_backup - correction_backup + 1

        del correction_backup
        del ratio_backup

        new_conti = conti * flux / (ref * ratio2_backup + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[flux == 0] = conti[flux == 0]

        del ratio2_backup

        diff2_backup = flux * conti / new_continuum - ref

        # plot end

        idx_min = 0
        idx_max = len(wave)

        if wave_min is not None:
            idx_min = myf.find_nearest(wave, wave_min)[0]
        if wave_max is not None:
            idx_max = myf.find_nearest(wave, wave_max)[0] + 1

        new_wave = wave[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))

        plt.axes([0.05, 0.66, 0.90, 0.25])
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * diff_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
        ax1 = plt.colorbar(cax=cbaxes)
        ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * diff2_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
        ax2 = plt.colorbar(cax=cbaxes2)
        ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * (diff_backup - diff2_backup)[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
        ax3 = plt.colorbar(cax=cbaxes3)
        ax3.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.savefig(self.dir_root + "IMAGES/Correction_oxygen.png")

        pre_map = np.zeros(np.shape(diff_backup))
        if sub_dico == "matching_oxygen":
            spec = self.import_spectrum()
            sub_dico = spec[sub_dico]["parameters"]["sub_dico_used"]
            step -= 1
            pre_map = pd.read_pickle(self.dir_root + "CORRECTION_MAP/map_matching_oxygen.p")[
                "correction_map"
            ]

        correction_oxygen = diff_backup - diff2_backup
        to_be_saved = {"wave": wave, "correction_map": correction_oxygen + pre_map}
        myf.pickle_dump(
            to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_oxygen.p", "wb")
        )

        print("\nComputation of the new continua, wait ... \n")
        time.sleep(0.5)

        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_oxygen"] = output
            file["matching_oxygen"]["parameters"] = {
                "reference_spectrum": reference,
                "sub_dico_used": sub_dico,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.dico_actif = "matching_oxygen"

    # =============================================================================
    #     CORRECT OXYGEN TELLURIC BANDS
    # =============================================================================

    def yarara_correct_oxygen_bands(
        self,
        sub_dico="matching_diff",
        berv_shift="berv",
        berv_offset=0,
        continuum="linear",
        iteration=3,
        broadening=0,
        wave_min=5760,
        wave_max=5850,
        debug_plot=False,
        percentile=50,
        step_flux=0.05,
        bands=[[6840, 7000], [7530, 7800]],
    ):
        """
        Display the time-series spectra with proxies and its correlation

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : Minimum x axis limit
        wave_max : Maximum x axis limit
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution
        berv_shift : True/False to move in terrestrial rest-frame
        cmap : cmap of the 2D plot
        low_cmap : vmin cmap colorbar
        high_cmap : vmax cmap colorbar

        """

        myf.print_box("\n---- RECIPE : CORRECTION TELLURIC OXYGEN BANDS ----\n")

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        cmap = self.cmap
        planet = self.planet
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100
        self.import_material()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        snr = []
        jdb = []
        berv = []
        rv_shift = []
        flux_backup = []
        flux_corr = []
        conti = []
        flux_err = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
                hole_left = file["parameters"]["hole_left"]
                hole_right = file["parameters"]["hole_right"]
            snr.append(file["parameters"]["SNR_5500"])

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            flux_corr.append(f_norm)
            conti.append(c)
            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            if type(berv_shift) != np.ndarray:
                try:
                    berv.append(file["parameters"][berv_shift])
                except:
                    berv.append(0)
            else:
                berv = berv_shift
            try:
                rv_shift.append(file["parameters"]["RV_shift"])
            except:
                rv_shift.append(0)

        step = file[sub_dico]["parameters"]["step"]

        wave = np.array(wave)
        dgrid = np.median(np.diff(wave))
        flux = np.array(flux_corr)
        conti = np.array(conti)
        snr = np.array(snr)
        jdb = np.array(jdb)
        rv_shift = np.array(rv_shift)
        berv = np.array(berv)
        berv = berv - rv_shift - berv_offset

        model = pd.read_pickle(root + "/Python/Material/model_telluric_oxygen.p")

        if False:  # to find the best PSF broadening for an instrument
            res = []
            for l in np.linspace(0.032, 0.038, 11):
                model_tel = myc.tableXY(model["wave"], model["flux_norm"])
                model_tel.clip(min=[6800, None], max=[7800, None], replace=True)
                model_tel.y = myf.broadGaussFast(model_tel.x, model_tel.y, l, maxsig=5)
                model_tel.y[0:50] = 1
                model_tel.y[-50:] = 1
                model_tel.interpolate(new_grid=wave, method="linear")
                model_tel.clip(min=[6800, None], max=[7800, None], replace=True)
                tellurics = []
                for j in tqdm(range(len(flux_corr))):
                    telluric_model = myc.tableXY(
                        myf.doppler_r(model_tel.x, berv[j] * 1000)[0], model_tel.y
                    )
                    telluric_model.interpolate(new_grid=wave, interpolate_x=False, method="linear")
                    tellurics.append(telluric_model.y)
                tellurics = np.array(tellurics)

                ref = np.median(flux_corr - tellurics, axis=0)
                res.append(np.std(flux_corr - tellurics - ref))
            plt.plot(np.linspace(0.032, 0.038, 11), res)

        if False:  # produce static product
            telluric = myc.tableXY(model["wave"], model["flux_norm"])
            for w in np.hstack([np.arange(6857, 6867, 0.5), np.arange(7580, 7592, 0.5)]):
                loc = myf.find_nearest(telluric.x, w)[0][0]
                telluric.y[loc] = 1.002 + 1e-5 * np.random.randn(1)
            telluric.find_max(vicinity=5)
            telluric.max_extremum.find_max(vicinity=2)
            telluric.max_extremum.max_extremum.smooth(replace=False, box_pts=3)
            diff = myc.tableXY(
                telluric.max_extremum.max_extremum.x,
                telluric.max_extremum.max_extremum.y
                - telluric.max_extremum.max_extremum.smoothed.y,
            )
            diff.clip(min=[6840, None], max=[7800, None])
            diff.masked(diff.y > -0.05)
            diff.y = abs(diff.y)
            # diff.masked(diff.y<=0.07)
            diff.masked(diff.y > 1e-6)
            diff.masked(~((diff.x > 6868.25) & (diff.x < 6869.25)))
            diff.masked(~((diff.x > 6889) & (diff.x < 6890)))
            plt.figure()
            plt.plot(telluric.x, telluric.y)
            for j in diff.x:
                plt.axvline(x=j, color="k")
            match = myf.match_nearest(telluric.x, diff.x)

            # np.savetxt(root+'/Python/Material/model_telluric_oxygen_bands_continuum.txt',np.array([telluric.x[match[:,0].astype('int')],telluric.y[match[:,0].astype('int')]]).T,fmt=['%.4f','%.4f'])

        conti_loc = np.genfromtxt(
            root + "/Python/Material/model_telluric_oxygen_bands_continuum.txt"
        )

        i1 = [myf.find_nearest(wave, i[0])[0][0] for i in bands]
        i2 = [myf.find_nearest(wave, i[1])[0][0] for i in bands]

        b1 = [i[0] for i in bands]
        b2 = [i[1] for i in bands]

        model = myc.tableXY(model["wave"], model["flux_norm"])
        if broadening:
            model.y = myf.broadGaussFast(model.x, model.y, broadening, maxsig=5)
            model.y[0:50] = 1
            model.y[-50:] = 1

        flux_correction = []
        for j in tqdm(range(len(flux))):
            spectrum = myc.tableXY(wave, flux[j])

            telluric_model = myc.tableXY(myf.doppler_r(model.x, berv[j] * 1000)[0], model.y)
            telluric_model.interpolate(wave)

            telluric_adjusted = np.ones(len(spectrum.x))
            # percentile = 50
            # step = 0.05

            for lmin, lmax, imin, imax in zip(b1, b2, i1, i2):
                l0 = np.median(
                    np.hstack(
                        [
                            spectrum.y[imin - int(10 / dgrid) : imin],
                            spectrum.y[imax : imax + int(10 / dgrid)],
                        ]
                    )
                )
                spectrum_cut = spectrum.copy()
                index = spectrum.copy()
                index.y = np.arange(len(spectrum.x))
                telluric = telluric_model.copy()
                spectrum_cut.clip(min=[lmin, None], max=[lmax, None])
                telluric.clip(min=[lmin, None], max=[lmax, None])

                index.clip(min=[lmin, None], max=[lmax, None])
                spectrum_copy = spectrum_cut.copy()
                spectrum_copy.y[telluric.y > 0.9997] = l0

                spectrum_copy.smooth(box_pts=10, shape="savgol", replace=False)
                spectrum_copy.smoothed.find_max(vicinity=5)
                spectrum_copy.smoothed.max_extremum.clip(min=[None, 0.3])

                match1 = myf.match_nearest(
                    spectrum_copy.x, myf.doppler_r(conti_loc[:, 0], berv[j] * 1000)[0]
                )[:, 0].astype("int")

                for loop in range(iteration):
                    telluric.max_extremum = myc.tableXY(telluric.x[match1], telluric.y[match1])
                    telluric.x_max = telluric.max_extremum.x
                    telluric.y_max = telluric.max_extremum.y

                    match = myf.match_nearest(spectrum_copy.smoothed.x_max, telluric.x_max)

                    if l0 > 0.95:

                        continuum_correction = myc.tableXY(
                            telluric.x_max[match[:, 1].astype("int")],
                            spectrum_copy.smoothed.y_max[match[:, 0].astype("int")]
                            / telluric.y_max[match[:, 1].astype("int")],
                        )

                        continuum_correction.smooth(
                            box_pts=1, shape="rectangular", replace=False
                        )  # not used anymore
                        continuum_correction.smoothed.interpolate(
                            new_grid=telluric.x[telluric.y != 1], method="linear"
                        )
                        continuum_correction.smoothed.y[
                            continuum_correction.smoothed.x
                            < telluric.x_max[match[0, 1].astype("int")]
                        ] = 1
                        continuum_correction.smoothed.y[
                            continuum_correction.smoothed.x
                            > telluric.x_max[match[-1, 1].astype("int")]
                        ] = 1
                        telluric_modified = telluric.y * continuum_correction.smoothed.y
                    else:
                        telluric_modified = telluric.y

                    calib = myc.tableXY(telluric.y, spectrum_copy.y - telluric_modified)
                    calib.supress_zero()

                    self.debug1 = calib

                    tab = myf.bands_binning(
                        np.arange(0, 1.001, step_flux), calib.x, calib.y, binning=percentile
                    )
                    tab = tab.dropna()
                    correction = myc.tableXY(tab["x"], tab["y"], 0 * tab["x"])
                    correction.interpolate(new_grid=telluric_modified, method="linear")

                    telluric_correction = telluric_modified + correction.y
                    if False:
                        plt.axhline(y=0, color="k", zorder=10)
                        plt.plot(
                            spectrum_cut.x,
                            spectrum_cut.y - telluric_correction,
                            label="std=%.2e" % (np.std(spectrum_cut.y - telluric_correction)),
                        )
                        plt.legend()

                    telluric.y = telluric_correction
                if l0 < 0.95:
                    telluric_adjusted[index.y] = telluric_correction + (1 - l0)
                else:
                    telluric_adjusted[index.y] = telluric_correction

                # plt.subplot(4,1,j+1)
                # plt.plot(spectrum_cut.x,telluric.y,color='b')
                # plt.plot(spectrum_cut.x,spectrum_cut.y,color='r')
                # plt.plot(spectrum_cut.x,spectrum_cut.y-telluric.y+1,color='k')
                # plt.ylim(0,1.1)

                if debug_plot:
                    plt.figure(figsize=(15, 9))
                    plt.subplot(4, 1, 1)
                    plt.scatter(
                        spectrum_copy.smoothed.x_max, spectrum_copy.smoothed.y_max, zorder=10
                    )
                    plt.scatter(telluric.x_max, telluric.y_max, zorder=10)
                    plt.plot(spectrum_copy.x, spectrum_copy.y, label="input spectrum")
                    plt.plot(telluric.x, telluric.y, label="telluric model", color="k", alpha=0.25)
                    plt.plot(
                        spectrum_copy.x,
                        telluric_modified,
                        label="modified model",
                        color="k",
                        alpha=0.5,
                    )
                    plt.legend()
                    ax = plt.gca()

                    plt.subplot(4, 1, 2)
                    plt.scatter(calib.x, calib.y, s=1, color="k")
                    correction.plot(ls="-", color="b")
                    plt.scatter(tab["x"], tab["y"], color="g", zorder=10)
                    plt.ylim(-0.1, 0.2)

                    plt.subplot(4, 1, 3, sharex=ax)
                    plt.plot(spectrum_copy.x, spectrum_copy.y, label="input spectrum")
                    plt.plot(
                        spectrum_copy.x, telluric.y, label="telluric model", color="k", alpha=0.25
                    )
                    plt.plot(
                        spectrum_copy.x,
                        telluric_modified,
                        label="modified model",
                        color="k",
                        alpha=0.50,
                    )
                    plt.plot(
                        spectrum_copy.x, telluric_correction, label="adjusted model", color="k"
                    )
                    plt.legend()

                    plt.subplot(4, 1, 4, sharex=ax)

                    plt.axhline(y=0, color="k", zorder=10)
                    plt.plot(
                        spectrum_cut.x,
                        spectrum_cut.y - telluric.y,
                        label="std=%.2e" % (np.std(spectrum_cut.y - telluric.y)),
                    )
                    plt.legend()

            flux_correction.append(telluric_adjusted)

        flux_correction = np.array(flux_correction)

        diff_flux = flux - flux_correction + 1
        ref = np.median(diff_flux, axis=0)
        diff_backup = flux - ref
        diff2_backup = diff_flux - ref

        new_conti = conti * flux / (diff_flux)
        new_continuum = new_conti.copy()
        new_continuum[flux == 0] = conti[flux == 0]

        # plot end
        fig = plt.figure(figsize=(21, 9))
        c = -1
        for wave_min, wave_max in zip([6840, 7570], [7000, 7735]):
            c += 1

            idx_min = 0
            idx_max = len(wave)

            if wave_min is not None:
                idx_min = myf.find_nearest(wave, wave_min)[0]
            if wave_max is not None:
                idx_max = myf.find_nearest(wave, wave_max)[0] + 1

            new_wave = wave[int(idx_min) : int(idx_max)]

            plt.subplot(2, 2, 1 + c)
            ax = plt.gca()
            plt.plot(new_wave, (flux[:, int(idx_min) : int(idx_max)]).T)
            plt.plot(
                new_wave, np.median(flux[:, int(idx_min) : int(idx_max)], axis=0), color="k", lw=2
            )
            plt.ylim(-0.1, 1.1)
            plt.xlim(wave_min, wave_max)
            plt.title("Before YARARA", fontsize=14)
            plt.ylabel("Flux normalised", fontsize=14)
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)

            plt.subplot(2, 2, 3 + c, sharex=ax, sharey=ax)
            plt.plot(new_wave, (diff_flux[:, int(idx_min) : int(idx_max)]).T)
            plt.plot(
                new_wave,
                np.median(diff_flux[:, int(idx_min) : int(idx_max)], axis=0),
                color="k",
                lw=2,
            )
            plt.title("After YARARA", fontsize=14)
            plt.ylabel("Flux normalised", fontsize=14)
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
            # for j in conti_loc[:,0]:
            #     plt.axvline(x=j,color='k',alpha=0.1)

        plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.09, hspace=0.4)
        plt.savefig(self.dir_root + "IMAGES/Comparison_oxygen_bands.png")

        fig = plt.figure(figsize=(21, 9))
        plt.axes([0.05, 0.66, 0.90, 0.25])
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * diff_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
        ax1 = plt.colorbar(cax=cbaxes)
        ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * diff2_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
        ax2 = plt.colorbar(cax=cbaxes2)
        ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_backup)),
            100 * (diff_backup - diff2_backup)[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
        ax3 = plt.colorbar(cax=cbaxes3)
        ax3.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.savefig(self.dir_root + "IMAGES/Correction_oxygen_bands.png")

        correction_bands = diff_backup - diff2_backup
        to_be_saved = {"wave": wave, "correction_map": correction_bands}
        myf.pickle_dump(
            to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_oxy_bands.p", "wb")
        )

        print("\nComputation of the new continua, wait ... \n")
        time.sleep(0.5)

        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_oxy_bands"] = output
            file["matching_oxy_bands"]["parameters"] = {
                "reference_spectrum": ref,
                "sub_dico_used": sub_dico,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.dico_actif = "matching_oxy_bands"

    # =============================================================================
    # TELLRUCI CORRECTION V2
    # =============================================================================

    def yarara_correct_telluric_gradient(
        self,
        sub_dico_detection="matching_fourier",
        sub_dico_correction="matching_oxygen",
        continuum="linear",
        wave_min_train=4200,
        wave_max_train=5000,
        wave_min_correction=4400,
        wave_max_correction=6600,
        smooth_map=1,
        berv_shift="berv",
        reference="master",
        inst_resolution=110000,
        debug=False,
        equal_weight=True,
        nb_pca_comp=20,
        nb_pca_comp_kept=None,
        nb_pca_max_kept=5,
        calib_std=1e-3,
    ):

        """
        Display the time-series spectra with proxies and its correlation

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : Minimum x axis limit
        wave_max : Maximum x axis limit
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution
        berv_shift : True/False to move in terrestrial rest-frame
        cmap : cmap of the 2D plot
        low_cmap : vmin cmap colorbar
        high_cmap : vmax cmap colorbar

        """

        myf.print_box("\n---- RECIPE : CORRECTION TELLURIC PCA ----\n")

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        cmap = self.cmap
        planet = self.planet
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100
        self.import_material()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico_correction is None:
            sub_dico_correction = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico_correction))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        snr = []
        jdb = []
        berv = []
        rv_shift = []
        flux_backup = []
        flux_corr = []
        flux_det = []
        conti = []
        flux_err = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
                hole_left = file["parameters"]["hole_left"]
                hole_right = file["parameters"]["hole_right"]
            snr.append(file["parameters"]["SNR_5500"])
            flux_backup.append(file["flux" + kw])
            flux_det.append(file["flux" + kw] / file[sub_dico_detection]["continuum_" + continuum])

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico_correction]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            flux_err.append(f_norm_std)
            flux_corr.append(f_norm)
            conti.append(c)
            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            if type(berv_shift) != np.ndarray:
                try:
                    berv.append(file["parameters"][berv_shift])
                except:
                    berv.append(0)
            else:
                berv = berv_shift
            try:
                rv_shift.append(file["parameters"]["RV_shift"])
            except:
                rv_shift.append(0)

        step = file[sub_dico_correction]["parameters"]["step"]

        wave = np.array(wave)
        flux = np.array(flux_det)
        flux_backup = np.array(flux_backup)
        flux_to_correct = np.array(flux_corr)
        flux_err = np.array(flux_err) + calib_std
        conti = np.array(conti)
        snr = np.array(snr)
        jdb = np.array(jdb)
        rv_shift = np.array(rv_shift)
        berv = np.array(berv)
        mean_berv = np.mean(berv)
        berv = berv - mean_berv - rv_shift

        if len(snr) < nb_pca_comp:
            nb_pca_comp = len(snr) - 1
            print(
                "Nb component too high compared to number of observations, nc reduced to %.0f"
                % (len(snr) - 2)
            )

        def idx_wave(wavelength):
            return int(myf.find_nearest(wave, wavelength)[0])

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            print("[INFO] Reference spectrum : median")
            ref = np.median(flux, axis=0)
        elif reference == "master":
            print("[INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print("[INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = flux[reference]
        else:
            ref = 0 * np.median(flux, axis=0)

        diff = myf.smooth2d(flux, smooth_map)
        diff_ref = myf.smooth2d(flux - ref, smooth_map)
        diff_ref_to_correct = myf.smooth2d(flux_to_correct - ref, smooth_map)
        ratio_ref = myf.smooth2d(flux_to_correct / (ref + epsilon), smooth_map)
        diff_backup = diff_ref.copy()
        ratio_backup = ratio_ref.copy()

        del flux_to_correct

        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, diff[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                diff[j] = test.y
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, diff_ref[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                diff_ref[j] = test.y
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, ratio_ref[j], flux_err[j] / (ref + epsilon))
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                ratio_ref[j] = test.y
                flux_err[j] = test.yerr

        med_wave_gradient_f = np.median(np.gradient(diff)[1], axis=0)
        med_time_gradient_f = np.median(np.gradient(diff)[0], axis=0)
        mad_time_gradient_f = np.median(np.gradient(diff)[0] - med_time_gradient_f, axis=0)

        med_wave_gradient_df = np.median(np.gradient(diff_ref)[1], axis=0)
        med_time_gradient_df = np.median(np.gradient(diff_ref)[0], axis=0)
        mad_time_gradient_df = np.median(np.gradient(diff_ref)[0] - med_time_gradient_df, axis=0)

        med_df = np.median(diff_ref, axis=0)
        med_f = np.median(diff, axis=0)

        par1 = np.log10(abs(med_time_gradient_f + med_wave_gradient_f) + 1e-6)
        par2 = np.log10(abs(med_time_gradient_df + med_wave_gradient_df) + 1e-6)

        par3 = np.log10(abs(med_df) + 1e-6)
        par4 = np.log10(1 - abs(med_f) + 1e-6)

        par5 = np.log10(abs(med_wave_gradient_f / (med_time_gradient_f + 1e-4) + 1e-2))
        par6 = np.log10(abs(med_wave_gradient_f / (mad_time_gradient_f + 1e-4) + 1e-2))

        par7 = np.log10(abs(med_wave_gradient_df / (med_time_gradient_df + 1e-4) + 1e-6))
        par8 = np.log10(abs(med_wave_gradient_df / (mad_time_gradient_df + 1e-4) + 1e-6))

        par9 = np.log10(abs(med_time_gradient_f / (mad_time_gradient_f + 1e-4) + 1e-2))
        par10 = np.log10(abs(med_time_gradient_df / (mad_time_gradient_df + 1e-4) + 1e-2))

        table = pd.DataFrame(
            {
                "wave": wave,
                "med_df": med_df,
                "med_f": med_f,
                "gt_f": med_time_gradient_f,
                "gl_f": med_wave_gradient_f,
                "gt_df": med_time_gradient_df,
                "gl_df": med_wave_gradient_df,
                "par1": par1,
                "par2": par2,
                "par3": par3,
                "par4": par4,
                "par5": par5,
                "par6": par6,
                "par7": par7,
                "par8": par8,
                "par9": par9,
                "par10": par10,
            }
        )

        table["classes"] = "full_spectrum"
        table.loc[
            (table["wave"] > wave_min_train) & (table["wave"] < wave_max_train), "classes"
        ] = "telluric_free"

        med1 = np.median(10 ** np.array(table.loc[table["classes"] == "telluric_free", "par2"]))
        mad1 = 1.48 * np.median(
            abs(med1 - 10 ** np.array(table.loc[table["classes"] == "telluric_free", "par2"]))
        )

        med2 = np.median(10 ** np.array(table.loc[table["classes"] == "telluric_free", "par3"]))
        mad2 = 1.48 * np.median(
            abs(med1 - 10 ** np.array(table.loc[table["classes"] == "telluric_free", "par3"]))
        )

        med3 = np.median(10 ** np.array(table.loc[table["classes"] == "telluric_free", "par8"]))
        mad3 = 1.48 * np.median(
            abs(med1 - 10 ** np.array(table.loc[table["classes"] == "telluric_free", "par8"]))
        )

        crit1 = 10**par2 - med1
        crit2 = 10**par3 - med2
        crit3 = 10**par8 - med3

        z1 = crit1 / mad1
        z2 = crit2 / mad2
        z3 = crit3 / mad3

        ztot = z1 + z2 + z3
        ztot -= np.median(ztot)

        dint = int(3 / np.mean(np.diff(wave)))
        criterion = myc.tableXY(wave, ztot)
        criterion.rolling(window=dint)
        iq = np.percentile(ztot, 75) - np.percentile(ztot, 25)

        # telluric detection

        inside = (
            myf.smooth(ztot, 5, shape="savgol") > 1.5 * iq
        )  # &(criterion.y>criterion.roll_median) # hyperparameter
        pos_peaks = (inside > np.roll(inside, 1)) & (inside > np.roll(inside, -1))
        inside = inside * (1 - pos_peaks)
        neg_peaks = (inside < np.roll(inside, 1)) & (inside < np.roll(inside, -1))
        inside = inside + neg_peaks

        # comparison with Molecfit

        model = pd.read_pickle(root + "/Python/Material/model_telluric.p")
        wave_model = myf.doppler_r(model["wave"], mean_berv * 1000)[0]
        telluric_model = model["flux_norm"]
        model = myc.tableXY(wave_model, telluric_model, 0 * wave_model)
        model.interpolate(new_grid=wave, interpolate_x=False)
        model.find_min(vicinity=5)
        mask_model = np.zeros(len(wave))
        mask_model[model.index_min.astype("int")] = 1 - model.y_min
        mask_model[mask_model < 1e-5] = 0  # hyperparameter

        inside_model = (1 - model.y) > 3e-4
        pos_peaks = (inside_model > np.roll(inside_model, 1)) & (
            inside_model > np.roll(inside_model, -1)
        )
        inside_model = inside_model * (1 - pos_peaks)

        completness = mask_model * inside
        completness = completness[completness != 0]
        completness_max = mask_model[mask_model != 0]
        completness2 = mask_model * (1 - inside)
        completness2 = completness2[completness2 != 0]

        # self.debug1 = completness, completness_max, completness2, inside

        plt.figure()
        val = plt.hist(
            np.log10(completness_max),
            bins=np.linspace(-5, 0, 50),
            cumulative=-1,
            histtype="step",
            lw=3,
            color="k",
            label="telluric model",
        )
        val2 = plt.hist(
            np.log10(completness),
            bins=np.linspace(-5, 0, 50),
            cumulative=-1,
            histtype="step",
            lw=3,
            color="r",
            label="telluric detected",
        )
        val3 = plt.hist(
            np.log10(completness2),
            bins=np.linspace(-5, 0, 50),
            cumulative=1,
            histtype="step",
            lw=3,
            color="g",
            label="telluric undetected",
        )
        plt.close()

        # comp_percent = 100*(1 - (val[0]-val2[0])/(val[0]+1e-12)) #update 10.06.21 to complicated metric
        comp_percent = val3[0] * 100 / np.max(val3[0])
        tel_depth_grid = val[1][0:-1] + 0.5 * (val[1][1] - val[1][0])

        plt.figure(12, figsize=(8.5, 7))
        plt.plot(tel_depth_grid, comp_percent, color="k")
        plt.axhline(y=100, color="b", ls="-.")
        plt.grid()
        if len(np.where(comp_percent == 100)[0]) > 0:
            plt.axvline(
                x=tel_depth_grid[np.where(comp_percent == 100)[0][0]],
                color="b",
                label="100%% Completeness : %.2f [%%]"
                % (100 * 10 ** (tel_depth_grid[np.where(comp_percent == 100)[0][0]])),
            )
            plt.axvline(
                x=tel_depth_grid[myf.find_nearest(comp_percent, 90)[0]],
                color="b",
                ls=":",
                label="90%% Completeness : %.2f [%%]"
                % (100 * 10 ** (tel_depth_grid[myf.find_nearest(comp_percent, 90)[0]])),
            )
        plt.ylabel("Completness [%]", fontsize=16)
        plt.xlabel(r"$\log_{10}$(Telluric depth)", fontsize=16)
        plt.title("Telluric detection completeness versus MolecFit model", fontsize=16)
        plt.ylim(-5, 105)
        plt.legend(prop={"size": 14})
        plt.savefig(self.dir_root + "IMAGES/telluric_detection.pdf")

        # extraction telluric

        telluric_location = inside.copy()
        telluric_location[wave < wave_min_correction] = 0  # reject shorter wavelength
        telluric_location[wave > wave_max_correction] = 0  # reject band

        # self.debug = telluric_location

        # extraction of uncontaminated telluric

        plateau, cluster = myf.clustering(telluric_location, 0.5, 1)
        plateau = np.array([np.product(j) for j in plateau]).astype("bool")
        cluster = cluster[plateau]
        # med_width = np.median(cluster[:,-1])
        # mad_width = np.median(abs(cluster[:,-1] - med_width))*1.48
        telluric_kept = cluster  # cluster[(cluster[:,-1]>med_width-mad_width)&(cluster[:,-1]<med_width+mad_width),:]
        telluric_kept[:, 1] += 1
        # telluric_kept = np.hstack([telluric_kept,wave[telluric_kept[:,0],np.newaxis]])
        # plt.figure();plt.hist(telluric_kept[:,-1],bins=100)
        telluric_kept = telluric_kept[
            telluric_kept[:, -1]
            > np.nanmedian(telluric_kept[:, -1]) - myf.mad(telluric_kept[:, -1])
        ]
        min_telluric_size = wave / inst_resolution / np.gradient(wave)
        telluric_kept = telluric_kept[
            min_telluric_size[telluric_kept[:, 0]] < telluric_kept[:, -1]
        ]

        if debug:
            plt.figure(1)
            plt.subplot(3, 2, 1)
            plt.plot(wave, ref, color="k")
            (l4,) = plt.plot(5500 * np.ones(2), [0, 1], color="r")
            ax = plt.gca()
            plt.subplot(2, 2, 2, sharex=ax)
            plt.plot(wave, ztot, color="k")
            ax = plt.gca()
            border_y = ax.get_ylim()
            (l,) = plt.plot(5500 * np.ones(2), border_y, color="r")
            idx = myf.find_nearest(wave, 5500)[0].astype("int")
            plt.ylim(border_y)
            for j in range(len(telluric_kept)):
                plt.axvspan(
                    xmin=wave[telluric_kept[j, 0].astype("int")],
                    xmax=wave[telluric_kept[j, 1].astype("int")],
                    alpha=0.3,
                    color="r",
                )
            plt.subplot(2, 2, 4, sharex=ax)
            plt.imshow(
                ratio_ref,
                aspect="auto",
                cmap="plasma",
                vmin=0.99,
                vmax=1.01,
                extent=[wave[0], wave[-1], 0, len(jdb)],
            )

            (l2,) = plt.plot(5500 * np.ones(2), [0, len(jdb)], color="k")

            plt.subplot(3, 2, 5)
            l5, (), (bars5,) = plt.errorbar(
                jdb % 365.25, ratio_ref[:, idx], 0.001 * np.ones(len(jdb)), fmt="ko"
            )
            plt.ylim(0.99, 1.01)
            ax3 = plt.gca()

            plt.subplot(3, 2, 3)
            l3, (), (bars3,) = plt.errorbar(
                jdb, ratio_ref[:, idx], 0.001 * np.ones(len(jdb)), fmt="ko"
            )
            plt.ylim(0.99, 1.01)
            ax4 = plt.gca()

            class Index:
                def update_data(self, newx, newy):
                    idx = myf.find_nearest(wave, newx)[0].astype("int")
                    l.set_xdata(newx * np.ones(len(l.get_xdata())))
                    l2.set_xdata(newx * np.ones(len(l.get_xdata())))
                    l4.set_xdata(newx * np.ones(len(l.get_xdata())))
                    l3.set_ydata(ratio_ref[:, idx])
                    l5.set_ydata(ratio_ref[:, idx])
                    new_segments = [
                        np.array([[x, yt], [x, yb]])
                        for x, yt, yb in zip(
                            jdb, ratio_ref[:, idx] + 0.001, ratio_ref[:, idx] - 0.001
                        )
                    ]
                    bars3.set_segments(new_segments)
                    bars5.set_segments(new_segments)
                    ax3.set_ylim(
                        np.min(ratio_ref[:, idx]) - 0.002, np.max(ratio_ref[:, idx]) + 0.002
                    )
                    ax4.set_ylim(
                        np.min(ratio_ref[:, idx]) - 0.002, np.max(ratio_ref[:, idx]) + 0.002
                    )
                    plt.gcf().canvas.draw_idle()

            t = Index()

            def onclick(event):
                newx = event.xdata
                newy = event.ydata
                if event.dblclick:
                    print(newx)
                    t.update_data(newx, newy)

            plt.gcf().canvas.mpl_connect("button_press_event", onclick)
        else:
            plt.figure(1)
            plt.subplot(3, 1, 1)
            plt.plot(ref, color="k")
            ax = plt.gca()
            plt.subplot(3, 1, 2, sharex=ax)
            plt.plot(ztot, color="k")
            for j in range(len(telluric_kept)):
                plt.axvspan(
                    xmin=telluric_kept[j, 0].astype("int"),
                    xmax=telluric_kept[j, 1].astype("int"),
                    alpha=0.3,
                    color="r",
                )
            plt.subplot(3, 1, 3, sharex=ax)
            plt.imshow(diff_ref, aspect="auto", vmin=-0.005, vmax=0.005)

        telluric_extracted_ratio_ref = []
        telluric_extracted_ratio_ref_std = []

        ratio_ref_std2 = flux_err**2
        for j in range(len(telluric_kept)):

            norm = telluric_kept[j, 1] + 1 - telluric_kept[j, 0]
            val = np.nanmean(ratio_ref[:, telluric_kept[j, 0] : telluric_kept[j, 1] + 1], axis=1)
            val_std = (
                np.sqrt(
                    np.nansum(
                        ratio_ref_std2[:, telluric_kept[j, 0] : telluric_kept[j, 1] + 1], axis=1
                    )
                )
                / norm
            )

            telluric_extracted_ratio_ref.append(val)
            telluric_extracted_ratio_ref_std.append(val_std)

        telluric_extracted_ratio_ref = np.array(telluric_extracted_ratio_ref).T
        telluric_extracted_ratio_ref_std = np.array(telluric_extracted_ratio_ref_std).T
        telluric_extracted_ratio_ref -= np.median(telluric_extracted_ratio_ref, axis=0)

        plt.figure(2, figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.imshow(telluric_extracted_ratio_ref, aspect="auto", vmin=-0.005, vmax=0.005)
        plt.title("Water lines")
        plt.xlabel("Pixels extracted", fontsize=14)
        plt.ylabel("Time", fontsize=14)

        plt.subplot(1, 2, 2)
        plt.imshow(
            telluric_extracted_ratio_ref / np.std(telluric_extracted_ratio_ref, axis=0),
            aspect="auto",
            vmin=-0.005,
            vmax=0.005,
        )
        plt.title("Water lines")
        plt.xlabel("Pixels extracted", fontsize=14)
        plt.ylabel("Time", fontsize=14)

        c = int(equal_weight)

        X_train = (
            telluric_extracted_ratio_ref
            / ((1 - c) + c * np.std(telluric_extracted_ratio_ref, axis=0))
        ).T
        X_train_std = (
            telluric_extracted_ratio_ref_std
            / ((1 - c) + c * np.std(telluric_extracted_ratio_ref, axis=0))
        ).T

        # self.debug = (X_train, X_train_std)
        # myf.pickle_dump({'jdb':np.array(self.table.jdb),'ratio_flux':X_train,'ratio_flux_std':X_train_std},open(root+'/Python/datasets/telluri_cenB.p','wb'))

        test2 = myc.table(X_train)

        test2.WPCA("wpca", weight=1 / X_train_std**2, comp_max=nb_pca_comp)

        phase_mod = np.arange(365)[
            np.argmin(
                np.array(
                    [np.max((jdb - k) % 365.25) - np.min((jdb - k) % 365.25) for k in range(365)]
                )
            )
        ]

        plt.figure(4, figsize=(10, 14))
        plt.subplot(3, 1, 1)
        plt.xlabel("# PCA components", fontsize=13)
        plt.ylabel("Variance explained", fontsize=13)
        plt.plot(np.arange(1, len(test2.phi_components) + 1), test2.var_ratio)
        plt.scatter(np.arange(1, len(test2.phi_components) + 1), test2.var_ratio)
        plt.subplot(3, 1, 2)
        plt.xlabel("# PCA components", fontsize=13)
        plt.ylabel("Z score", fontsize=13)
        plt.plot(np.arange(1, len(test2.phi_components) + 1), test2.zscore_components)
        plt.scatter(np.arange(1, len(test2.phi_components) + 1), test2.zscore_components)
        z_max = test2.zscore_components[-10:].max()
        z_min = test2.zscore_components[-10:].min()
        vec_relevant = np.arange(len(test2.zscore_components)) * (
            (test2.zscore_components > z_max) | (test2.zscore_components < z_min)
        )
        pca_comp_kept2 = int(np.where(vec_relevant != np.arange(len(vec_relevant)))[0][0])

        plt.axhspan(ymin=z_min, ymax=z_max, alpha=0.2, color="k")
        plt.axhline(y=0, color="k")
        plt.subplot(3, 1, 3)
        plt.xlabel("# PCA components", fontsize=13)
        plt.ylabel(r"$\Phi(0)$", fontsize=13)
        plt.plot(np.arange(1, len(test2.phi_components) + 1), test2.phi_components)
        plt.scatter(np.arange(1, len(test2.phi_components) + 1), test2.phi_components)
        plt.axhline(y=0.5, color="k")
        phi_max = test2.phi_components[-10:].max()
        phi_min = test2.phi_components[-10:].min()
        plt.axhspan(ymin=phi_min, ymax=phi_max, alpha=0.2, color="k")
        vec_relevant = np.arange(len(test2.phi_components)) * (
            (test2.phi_components > phi_max) | (test2.phi_components < phi_min)
        )
        pca_comp_kept = int(np.where(vec_relevant != np.arange(len(vec_relevant)))[0][0])
        pca_comp_kept = np.max([pca_comp_kept, pca_comp_kept2])

        if nb_pca_comp_kept is not None:
            pca_comp_kept = nb_pca_comp_kept

        if pca_comp_kept > nb_pca_max_kept:
            pca_comp_kept = nb_pca_max_kept

        print(" [INFO] Nb PCA comp kept : %.0f" % (pca_comp_kept))

        plt.savefig(self.dir_root + "IMAGES/telluric_PCA_variances.pdf")

        plt.figure(figsize=(15, 10))
        for j in range(pca_comp_kept):
            plt.subplot(pca_comp_kept, 2, 2 * j + 1)
            plt.scatter(jdb, test2.vec[:, j])
            plt.subplot(pca_comp_kept, 2, 2 * j + 2)
            plt.scatter((jdb - phase_mod) % 365.25, test2.vec[:, j])
        plt.subplots_adjust(top=0.95, bottom=0.07, left=0.07, right=0.95, hspace=0)
        plt.savefig(self.dir_root + "IMAGES/telluric_PCA_vectors.pdf")

        to_be_fit = ratio_ref / (np.std(ratio_ref, axis=0) + epsilon)

        rcorr = np.zeros(len(wave))
        for j in range(pca_comp_kept):
            proxy1 = test2.vec[:, j]
            rslope1 = np.median(
                (to_be_fit - np.mean(to_be_fit, axis=0))
                / ((proxy1 - np.mean(proxy1))[:, np.newaxis]),
                axis=0,
            )

            rcorr1 = abs(rslope1 * np.std(proxy1) / (np.std(to_be_fit, axis=0) + epsilon))
            rcorr = np.nanmax([rcorr1, rcorr], axis=0)
        rcorr[np.isnan(rcorr)] = 0
        rcorr_telluric_free = rcorr[
            int(myf.find_nearest(wave, 4800)[0]) : int(myf.find_nearest(wave, 5000)[0])
        ]
        rcorr_telluric = rcorr[
            int(myf.find_nearest(wave, 5800)[0]) : int(myf.find_nearest(wave, 6000)[0])
        ]

        plt.figure(figsize=(8, 6))
        bins_contam, bins, dust = plt.hist(
            rcorr_telluric, label="contaminated region", bins=np.linspace(0, 1, 100), alpha=0.5
        )
        bins_control, bins, dust = plt.hist(
            rcorr_telluric_free, bins=np.linspace(0, 1, 100), label="free region", alpha=0.5
        )
        plt.legend()
        plt.yscale("log")
        bins = bins[0:-1] + np.diff(bins) * 0.5
        sum_a = np.sum(bins_contam[bins > 0.40])
        sum_b = np.sum(bins_control[bins > 0.40])
        crit = int(sum_a > (2 * sum_b))
        check = ["r", "g"][crit]  # five times more correlation than in the control group
        plt.xlabel(r"|$\mathcal{R}_{pearson}$|", fontsize=14, fontweight="bold", color=check)
        plt.title("Density", color=check)
        myf.plot_color_box(color=check)

        plt.savefig(self.dir_root + "IMAGES/telluric_control_check.pdf")
        print(" [INFO] %.0f versus %.0f" % (sum_a, sum_b))

        if crit:
            print(" [INFO] Control check sucessfully performed: telluric")
        else:
            print(
                Fore.YELLOW
                + " [WARNING] Control check failed. Correction may be poorly performed for: telluric"
                + Fore.RESET
            )

        collection = myc.table(
            ratio_ref.T[telluric_location.astype("bool")]
        )  # do fit only on flag position

        weights = 1 / (flux_err) ** 2
        weights = weights.T[telluric_location.astype("bool")]
        IQ = myf.IQ(collection.table, axis=1)
        Q1 = np.nanpercentile(collection.table, 25, axis=1)
        Q3 = np.nanpercentile(collection.table, 75, axis=1)
        sup = Q3 + 1.5 * IQ
        inf = Q1 - 1.5 * IQ
        out = (collection.table > sup[:, np.newaxis]) | (collection.table < inf[:, np.newaxis])
        weights[out] = np.min(weights) / 100

        # base_vec = np.vstack([np.ones(len(flux)), jdb-np.median(jdb), test2.vec[:,0:pca_comp_kept].T])
        base_vec = np.vstack([np.ones(len(flux)), test2.vec[:, 0:pca_comp_kept].T])
        collection.fit_base(base_vec, weight=weights, num_sim=1)
        # collection.coeff_fitted[:,3] = 0 #supress the linear trend fitted

        del weights
        del flux_err

        correction = np.zeros((len(wave), len(jdb)))
        correction[telluric_location.astype("bool")] = collection.coeff_fitted.dot(base_vec)
        correction = np.transpose(correction)

        correction[correction == 0] = 1
        correction_backup = correction.copy()

        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, correction[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[0]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                correction_backup[j] = test.y

        index_min_backup = int(
            myf.find_nearest(wave, myf.doppler_r(wave[0], berv.max() * 1000)[0])[0]
        )
        index_max_backup = int(
            myf.find_nearest(wave, myf.doppler_r(wave[-1], berv.min() * 1000)[0])[0]
        )
        correction_backup[:, 0:index_min_backup] = 1
        correction_backup[:, index_max_backup:] = 1
        index_hole_right = int(
            myf.find_nearest(wave, hole_right + 1)[0]
        )  # correct 1 angstrom band due to stange artefact at the border of the gap
        index_hole_left = int(
            myf.find_nearest(wave, hole_left - 1)[0]
        )  # correct 1 angstrom band due to stange artefact at the border of the gap
        correction_backup[:, index_hole_left : index_hole_right + 1] = 1

        del correction

        ratio2_backup = ratio_backup - correction_backup + 1

        del correction_backup

        new_conti = flux_backup / (ref * ratio2_backup + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[flux == 0] = conti[flux == 0]
        new_continuum[np.isnan(new_continuum)] = conti[np.isnan(new_continuum)]
        new_continuum[new_continuum == 0] = conti[new_continuum == 0]

        del ratio2_backup
        del ratio_backup

        diff2_backup = flux_backup / new_continuum - ref

        idx_min = myf.find_nearest(wave, 5700)[0]
        idx_max = myf.find_nearest(wave, 5900)[0] + 1

        new_wave = wave[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))
        plt.axes([0.05, 0.55, 0.90, 0.40])
        ax = plt.gca()
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_ref)),
            100 * diff_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=16)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.55, 0.01, 0.40])
        ax1 = plt.colorbar(cax=cbaxes)
        ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.1, 0.90, 0.40], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_ref)),
            100 * diff2_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=16)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=16)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.1, 0.01, 0.40])
        ax2 = plt.colorbar(cax=cbaxes2)
        ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.savefig(self.dir_root + "IMAGES/Correction_telluric.png")

        correction_pca = diff_ref_to_correct - diff2_backup
        to_be_saved = {"wave": wave, "correction_map": correction_pca}
        myf.pickle_dump(
            to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_pca.p", "wb")
        )

        print("Computation of the new continua, wait ... \n")
        time.sleep(0.5)

        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_pca"] = output
            file["matching_pca"]["parameters"] = {
                "reference_spectrum": reference,
                "sub_dico_used": sub_dico_correction,
                "nb_pca_component": pca_comp_kept,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.dico_actif = "matching_pca"

        plt.show(block=False)

    # =============================================================================
    #    CONTAM TH FIBER B CORRECTION
    # =============================================================================

    def yarara_correct_contam_TH(
        self,
        sub_dico="matching_activity",
        continuum="linear",
        wave_max_train=4500,
        smooth_map=1,
        berv_shift="berv",
        reference="median",
        nb_pca_comp=25,
        equal_weight=True,
        nb_pca_max_kept=3,
        detection="frog",
    ):

        """
        Display the time-series spectra with proxies and its correlation

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : Minimum x axis limit
        wave_max : Maximum x axis limit
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution
        berv_shift : True/False to move in terrestrial rest-frame
        cmap : cmap of the 2D plot
        low_cmap : vmin cmap colorbar
        high_cmap : vmax cmap colorbar

        """

        myf.print_box("\n---- RECIPE : CORRECTION THORIUM CONTAM ----\n")

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        cmap = self.cmap
        planet = self.planet
        low_cmap = self.low_cmap
        high_cmap = self.high_cmap
        self.import_material()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("---- DICO %s used ----" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        snr = []
        jdb = []
        berv = []
        flux = []
        flux_std = []
        conti = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
                hole_left = file["parameters"]["hole_left"]
                hole_right = file["parameters"]["hole_right"]
            snr.append(file["parameters"]["SNR_5500"])
            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            flux.append(f_norm)
            flux_std.append(f_norm_std)
            conti.append(c)
            jdb.append(file["parameters"]["jdb"])
            if type(berv_shift) != np.ndarray:
                try:
                    berv.append(file["parameters"][berv_shift])
                except:
                    berv.append(0)
            else:
                berv = berv_shift

        step = file[sub_dico]["parameters"]["step"]

        wave = np.array(wave)
        flux = np.array(flux)
        flux_std = np.array(flux_std)
        conti = np.array(conti)
        snr = np.array(snr)
        jdb = np.array(jdb)
        berv = np.array(berv)
        mean_berv = np.mean(berv)
        berv = berv - mean_berv

        def idx_wave(wavelength):
            return int(myf.find_nearest(wave, wavelength)[0])

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            ref = np.median(flux, axis=0)
        elif reference == "master":
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            ref = flux[reference]
        else:
            ref = 0 * np.median(flux, axis=0)

        diff_ref = myf.smooth2d(flux - ref, smooth_map)
        diff_backup = diff_ref.copy()

        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, diff_ref[j], flux_std[j])
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                diff_ref[j] = test.y
                flux_std[j] = test.yerr

        diff_ref[:, 0 : idx_wave(myf.doppler_r(wave[0], berv.max() * 1000)[0])] = 0
        diff_ref[:, idx_wave(myf.doppler_r(wave[-1], berv.min() * 1000)[0]) :] = 0

        weights = 1 / flux_std**2
        weights[:, 0 : idx_wave(myf.doppler_r(wave[0], berv.max() * 1000)[0])] = np.median(weights)
        weights[:, idx_wave(myf.doppler_r(wave[-1], berv.min() * 1000)[0]) :] = np.median(weights)

        average_weighted = np.sum(diff_ref * weights, axis=0) / np.sum(weights, axis=0)
        criterion = myc.tableXY(wave, average_weighted)

        # telluric detection

        if detection == "frog":
            inside = (np.array(load["thar"]) != 0) & (load["wave"] < wave_max_train)
            ext = "_1"
        else:
            inside = criterion.y > 5 * myf.mad(
                criterion.y
            )  # assume mean around 0 (should be fine)
            ext = "_2"
        # extraction telluric

        telluric_location = inside.copy()

        telluric_extracted_diff_ref = diff_ref[:, telluric_location]
        telluric_extracted_diff_ref -= np.median(telluric_extracted_diff_ref, axis=0)

        plt.figure(2, figsize=(12, 12))
        plt.subplot(1, 2, 1)
        plt.imshow(telluric_extracted_diff_ref, aspect="auto", vmin=-0.005, vmax=0.005)
        plt.title("TH pixels lines")
        plt.xlabel("Pixels extracted", fontsize=14)
        plt.ylabel("Time", fontsize=14)

        plt.subplot(1, 2, 2)
        plt.imshow(
            telluric_extracted_diff_ref / np.std(telluric_extracted_diff_ref, axis=0),
            aspect="auto",
            vmin=-0.005,
            vmax=0.005,
        )
        plt.title("TH pixels lines")
        plt.xlabel("Pixels extracted", fontsize=14)
        plt.ylabel("Time", fontsize=14)

        c = int(equal_weight)
        test2 = myc.table(
            (
                telluric_extracted_diff_ref
                / ((1 - c) + c * np.std(telluric_extracted_diff_ref, axis=0))
            ).T
        )
        test2.PCA(comp_max=nb_pca_comp)

        phase_mod = np.arange(365)[
            np.argmin(
                np.array(
                    [np.max((jdb - k) % 365.25) - np.min((jdb - k) % 365.25) for k in range(365)]
                )
            )
        ]

        plt.figure(4, figsize=(10, 14))
        plt.subplot(3, 1, 1)
        plt.xlabel("# PCA components", fontsize=13)
        plt.ylabel("Variance explained", fontsize=13)
        plt.plot(np.arange(1, len(test2.phi_components) + 1), test2.var_ratio)
        plt.scatter(np.arange(1, len(test2.phi_components) + 1), test2.var_ratio)
        plt.subplot(3, 1, 2)
        plt.xlabel("# PCA components", fontsize=13)
        plt.ylabel("Z score", fontsize=13)
        plt.plot(np.arange(1, len(test2.phi_components) + 1), test2.zscore_components)
        plt.scatter(np.arange(1, len(test2.phi_components) + 1), test2.zscore_components)
        z_max = test2.zscore_components[-15:].max()
        z_min = test2.zscore_components[-15:].min()
        vec_relevant = np.arange(len(test2.zscore_components)) * (
            (test2.zscore_components > z_max) | (test2.zscore_components < z_min)
        )
        pca_comp_kept2 = int(np.where(vec_relevant != np.arange(len(vec_relevant)))[0][0])

        plt.axhspan(ymin=z_min, ymax=z_max, alpha=0.2, color="k")
        plt.axhline(y=0, color="k")
        plt.subplot(3, 1, 3)
        plt.xlabel("# PCA components", fontsize=13)
        plt.ylabel(r"$\Phi(0)$", fontsize=13)
        plt.plot(np.arange(1, len(test2.phi_components) + 1), test2.phi_components)
        plt.scatter(np.arange(1, len(test2.phi_components) + 1), test2.phi_components)
        plt.axhline(y=0.5, color="k")
        phi_max = test2.phi_components[-15:].max()
        phi_min = test2.phi_components[-15:].min()
        plt.axhspan(ymin=phi_min, ymax=phi_max, alpha=0.2, color="k")
        vec_relevant = np.arange(len(test2.phi_components)) * (
            (test2.phi_components > phi_max) | (test2.phi_components < phi_min)
        )
        pca_comp_kept = int(np.where(vec_relevant != np.arange(len(vec_relevant)))[0][0])
        pca_comp_kept = np.max([pca_comp_kept, pca_comp_kept2])

        plt.savefig(self.dir_root + "IMAGES/ThAr_PCA_variances" + ext + ".pdf")

        if pca_comp_kept > nb_pca_max_kept:
            pca_comp_kept = nb_pca_max_kept

        plt.figure(figsize=(15, 10))
        for j in range(pca_comp_kept):
            plt.subplot(pca_comp_kept, 2, 2 * j + 1)
            plt.scatter(jdb, test2.vec[:, j])
            plt.subplot(pca_comp_kept, 2, 2 * j + 2)
            plt.scatter((jdb - phase_mod) % 365.25, test2.vec[:, j])
        plt.subplots_adjust(top=0.95, bottom=0.07, left=0.07, right=0.95, hspace=0)
        plt.savefig(self.dir_root + "IMAGES/ThAr_PCA_vectors" + ext + ".pdf")

        #        to_be_fit = diff_ref/(epsilon+np.std(diff_ref,axis=0))
        #
        #        rcorr = np.zeros(len(wave))
        #        for j in range(pca_comp_kept):
        #            proxy1 = test2.vec[:,j]
        #            rslope1 = np.median((to_be_fit-np.mean(to_be_fit,axis=0))/((proxy1-np.mean(proxy1))[:,np.newaxis]),axis=0)
        #            rcorr1 = abs(rslope1*np.std(proxy1)/(epsilon+np.std(to_be_fit,axis=0)))
        #            rcorr = np.nanmax([rcorr1,rcorr],axis=0)
        #        rcorr[np.isnan(rcorr)] = 0
        #        rcorr_thorium_free = rcorr[int(myf.find_nearest(wave,wave_min_train)[0]):int(myf.find_nearest(wave,wave_max_train)[0])]
        #        rcorr_thorium = rcorr[int(myf.find_nearest(wave,3850)[0]):int(myf.find_nearest(wave,4050)[0])]
        #
        #        plt.figure()
        #        plt.hist(rcorr_thorium,label='contaminated region',bins=np.linspace(0,1,100),alpha=0.5)
        #        plt.hist(rcorr_thorium_free,bins=np.linspace(0,1,100),label='free region',alpha=0.5)
        #        plt.legend()
        #        plt.yscale('log')
        #        plt.xlabel(r'|$\mathcal{R}_{pearson}$|',fontsize=14)
        #        plt.savefig(self.dir_root+'IMAGES/ThAr_control_check'+ext+'.pdf')

        collection = myc.table(diff_ref.T)
        base_vec = np.vstack([np.ones(len(flux)), test2.vec[:, 0:pca_comp_kept].T])
        collection.fit_base(base_vec, num_sim=1)

        correction = np.zeros((len(wave), len(jdb)))
        correction = collection.coeff_fitted.dot(base_vec)
        correction = np.transpose(correction)

        correction_backup = correction.copy()

        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, correction[j], 0 * wave)
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[0]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                correction_backup[j] = test.y

        index_min_backup = int(
            myf.find_nearest(wave, myf.doppler_r(wave[0], berv.max() * 1000)[0])[0]
        )
        index_max_backup = int(
            myf.find_nearest(wave, myf.doppler_r(wave[-1], berv.min() * 1000)[0])[0]
        )
        correction_backup[:, 0:index_min_backup] = 0
        correction_backup[:, index_max_backup:] = 0
        index_hole_right = int(
            myf.find_nearest(wave, hole_right + 1)[0]
        )  # correct 1 angstrom band due to stange artefact at the border of the gap
        index_hole_left = int(
            myf.find_nearest(wave, hole_left - 1)[0]
        )  # correct 1 angstrom band due to stange artefact at the border of the gap
        correction_backup[:, index_hole_left : index_hole_right + 1] = 0

        diff2_backup = diff_backup - correction_backup

        new_conti = conti * (diff_backup + ref) / (epsilon + diff2_backup + ref)
        new_continuum = new_conti.copy()
        new_continuum[flux == 0] = conti[flux == 0]
        new_continuum[np.isnan(new_continuum)] = conti[np.isnan(new_continuum)]
        new_continuum[new_continuum == 0] = conti[new_continuum == 0]

        max_var = wave[np.std(correction, axis=0).argsort()[::-1]]
        max_var = max_var[(max_var < 4400)][0]

        wave_min = myf.find_nearest(wave, max_var - 25)[1]
        wave_max = myf.find_nearest(wave, max_var + 25)[1]

        idx_min = myf.find_nearest(wave, wave_min)[0]
        idx_max = myf.find_nearest(wave, wave_max)[0] + 1

        new_wave = wave[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))
        plt.axes([0.05, 0.55, 0.90, 0.40])
        ax = plt.gca()
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_ref)),
            diff_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.55, 0.01, 0.40])
        plt.colorbar(cax=cbaxes)

        plt.axes([0.05, 0.1, 0.90, 0.40], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff_ref)),
            diff2_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)

        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.1, 0.01, 0.40])
        plt.colorbar(cax=cbaxes2)

        plt.savefig(self.dir_root + "IMAGES/Correction_ThAr" + ext + ".png")

        correction_contam = diff_backup - diff2_backup
        to_be_saved = {"wave": wave, "correction_map": correction_contam}
        myf.pickle_dump(
            to_be_saved,
            open(self.dir_root + "CORRECTION_MAP/map_matching_contam" + ext + ".p", "wb"),
        )

        print("Computation of the new continua, wait ... \n")
        time.sleep(0.5)

        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_contam" + ext] = output
            file["matching_contam" + ext]["parameters"] = {
                "reference_spectrum": reference,
                "sub_dico_used": sub_dico,
                "nb_pca_component": pca_comp_kept,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.dico_actif = "matching_contam" + ext

        plt.show(block=False)

    # =============================================================================
    #  ACTIVITY CORRECTION
    # =============================================================================

    def yarara_correct_activity(
        self,
        sub_dico="matching_telluric",
        continuum="linear",
        wave_min=3900,
        wave_max=4400,
        smooth_corr=5,
        reference="median",
        rv_shift="none",
        proxy_corr=["CaII"],
    ):

        """
        Display the time-series spectra with proxies and its correlation

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        wave_min : Minimum x axis limit
        wave_max : Maximum x axis limit
        zoom : int-type, to improve the resolution of the 2D plot
        smooth_map = int-type, smooth the 2D plot by gaussian 2D convolution
        smooth_corr = smooth thecoefficient  ofcorrelation curve
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum usedin the difference
        berv_shift : True/False to move in terrestrial rest-frame
        proxy_corr : keyword  of the first proxies from RASSINE dictionnary to use in the correlation
        proxy_detrending : Degree of the polynomial fit to detrend the proxy

        cmap : cmap of the 2D plot
        dwin : window correction increase by dwin to slightly correct above around the peak of correlation


        """

        myf.print_box("\n---- RECIPE : CORRECTION ACTIVITY (CCF MOMENTS) ----\n")

        directory = self.directory

        zoom = self.zoom
        smooth_map = self.smooth_map
        cmap = self.cmap
        planet = self.planet
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100
        self.import_material()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        flux = []
        snr = []
        conti = []
        prox = []
        jdb = []
        rv = []

        self.import_table()
        for prox_name in proxy_corr:
            prox.append(np.array(self.table[prox_name]))
        proxy = np.array(prox)

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
            snr.append(file["parameters"]["SNR_5500"])
            conti.append(file[sub_dico]["continuum_" + continuum])
            flux.append(file["flux" + kw] / file[sub_dico]["continuum_" + continuum])
            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            try:
                rv.append(file["parameters"][rv_shift])
            except:
                rv.append(0)

        step = file[sub_dico]["parameters"]["step"]

        wave = np.array(wave)
        flux = np.array(flux)
        conti = np.array(conti)
        snr = np.array(snr)
        proxy = np.array(prox)
        jdb = np.array(jdb)
        rv = np.array(rv)
        mean_rv = np.mean(rv)
        rv = rv - mean_rv

        #        proxy = myc.tableXY(jdb,proxy)
        #        proxy.substract_polyfit(proxy_detrending)
        #        proxy = proxy.detrend_poly.y

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            print("[INFO] Reference spectrum : median")
            ref = np.median(flux, axis=0)
        elif reference == "master":
            print("[INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print("[INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = flux[reference]
        else:
            ref = 0 * np.median(flux, axis=0)

        if low_cmap is None:
            low_cmap = np.percentile(flux - ref, 2.5)
        if high_cmap is None:
            high_cmap = np.percentile(flux - ref, 97.5)

        diff = myf.smooth2d(flux - ref, smooth_map)
        diff_backup = diff.copy()

        if np.sum(rv) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, diff[j], 0 * wave)
                test.x = myf.doppler_r(test.x, rv[j] * 1000)[1]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                diff[j] = test.y

        collection = myc.table(diff.T)
        base_vec = np.vstack([np.ones(len(flux)), proxy])
        collection.fit_base(base_vec, num_sim=1)

        collection.coeff_fitted[:, 1] = myf.smooth(
            collection.coeff_fitted[:, 1], smooth_corr, shape="savgol"
        )

        correction = collection.coeff_fitted.dot(base_vec)
        correction = np.transpose(correction)

        correction_backup = correction.copy()
        if np.sum(rv) != 0:
            for j in tqdm(np.arange(len(flux))):
                test = myc.tableXY(wave, correction[j], 0 * wave)
                test.x = myf.doppler_r(test.x, rv[j] * 1000)[0]
                test.interpolate(new_grid=wave, method="cubic", replace=True, interpolate_x=False)
                correction_backup[j] = test.y

        index_min_backup = int(
            myf.find_nearest(wave, myf.doppler_r(wave[0], rv.max() * 1000)[0])[0]
        )
        index_max_backup = int(
            myf.find_nearest(wave, myf.doppler_r(wave[-1], rv.min() * 1000)[0])[0]
        )
        correction_backup[:, 0:index_min_backup] = 0
        correction_backup[:, index_max_backup:] = 0

        diff2_backup = diff_backup - correction_backup

        new_conti = conti * (diff_backup + ref) / (diff2_backup + ref + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[flux == 0] = conti[flux == 0]
        new_continuum = self.uncorrect_hole(new_continuum, conti)

        # plot end

        idx_min = 0
        idx_max = len(wave)

        if wave_min is not None:
            idx_min = myf.find_nearest(wave, wave_min)[0]
        if wave_max is not None:
            idx_max = myf.find_nearest(wave, wave_max)[0] + 1

        if (idx_min == 0) & (idx_max == 1):
            idx_max = myf.find_nearest(wave, np.min(wave) + 500)[0] + 1

        new_wave = wave[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))

        plt.axes([0.05, 0.66, 0.90, 0.25])
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff)),
            100 * diff_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
        ax1 = plt.colorbar(cax=cbaxes)
        ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff)),
            100 * diff2_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
        ax2 = plt.colorbar(cax=cbaxes2)
        ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff)),
            100 * correction_backup[:, int(idx_min) : int(idx_max)],
            zoom=zoom,
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
        ax3 = plt.colorbar(cax=cbaxes3)
        ax3.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.savefig(self.dir_root + "IMAGES/Correction_activity.png")

        correction_activity = correction_backup
        to_be_saved = {"wave": wave, "correction_map": correction_activity}
        myf.pickle_dump(
            to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_activity.p", "wb")
        )

        print("\nComputation of the new continua, wait ... \n")
        time.sleep(0.5)

        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_activity"] = output
            file["matching_activity"]["parameters"] = {
                "reference_spectrum": reference,
                "sub_dico_used": sub_dico,
                "proxy_used": proxy_corr,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.dico_actif = "matching_activity"

    # =============================================================================
    # SUPRESS VARIATION RELATIF TO MEDIAN-MAD SPECTRUM (COSMIC PEAK WITH VALUE > 1)
    # =============================================================================

    def yarara_correct_cosmics(
        self, sub_dico="matching_diff", continuum="linear", k_sigma=3, bypass_warning=True
    ):

        """
        Supress flux value outside k-sigma mad clipping

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)

        """

        myf.print_box("\n---- RECIPE : CORRECTION COSMICS ----\n")

        directory = self.directory
        planet = self.planet
        self.import_dico_tree()
        epsilon = 1e-12

        reduction_accepted = True

        if not bypass_warning:
            if "matching_smooth" in list(self.dico_tree["dico"]):
                myf.make_sound("Warning")
                answer = myf.sphinx(
                    " [WARNING] Launch that recipes will remove the smooth correction of the previous loop iteration. Do you want to purchase (y/n) ?",
                    rep=["y", "n"],
                )
                if answer == "n":
                    reduction_accepted = False

        if reduction_accepted:
            kw = "_planet" * planet
            if kw != "":
                print("\n---- PLANET ACTIVATED ----")

            if sub_dico is None:
                sub_dico = self.dico_actif
            print("---- DICO %s used ----" % (sub_dico))

            files = glob.glob(directory + "RASSI*.p")
            files = np.sort(files)

            all_flux = []
            conti = []
            all_flux_norm = []
            all_snr = []
            jdb = []

            for i, j in enumerate(files):
                file = pd.read_pickle(j)
                if not i:
                    grid = file["wave"]
                all_flux.append(file["flux" + kw])
                conti.append(file[sub_dico]["continuum_" + continuum])
                all_flux_norm.append(file["flux" + kw] / file[sub_dico]["continuum_" + continuum])
                all_snr.append(file["parameters"]["SNR_5500"])
                jdb.append(file["parameters"]["jdb"])

            step = file[sub_dico]["parameters"]["step"]
            all_flux = np.array(all_flux)
            conti = np.array(conti)
            all_flux_norm = np.array(all_flux_norm)
            all_snr = np.array(all_snr)
            jdb = np.array(jdb)

            med = np.median(all_flux_norm, axis=0)
            mad = 1.48 * np.median(abs(all_flux_norm - med), axis=0)
            all_flux_corrected = all_flux_norm.copy()
            level = (med + k_sigma * mad) * np.ones(len(jdb))[:, np.newaxis]
            mask = (all_flux_norm > 1) & (all_flux_norm > level)

            print(
                "\n [INFO] Percentage of cosmics detected with k-sigma %.0f : %.2f%% \n"
                % (k_sigma, 100 * np.sum(mask) / len(mask.T) / len(mask))
            )

            med_map = med * np.ones(len(jdb))[:, np.newaxis]

            plt.figure(figsize=(10, 10))
            plt.scatter(
                all_snr, np.sum(mask, axis=1) * 100 / len(mask.T), edgecolor="k", c=jdb, cmap="brg"
            )
            ax = plt.colorbar()
            plt.yscale("log")
            plt.ylim(0.001, 100)
            plt.xlabel("SNR", fontsize=13)
            plt.ylabel("Percent of the spectrum flagged as cosmics [%]", fontsize=13)
            plt.grid()
            ax.ax.set_ylabel("Jdb", fontsize=13)

            plt.figure(figsize=(20, 5))
            all_flux_corrected[mask] = med_map[mask]
            for j in range(len(jdb)):
                plt.plot(grid, all_flux_corrected[j] - 1.5, color="b", alpha=0.3)
                plt.plot(grid, all_flux_norm[j], color="k", alpha=0.3)
            plt.ylim(-2, 2)
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
            plt.ylabel(r"Flux normalised", fontsize=14)
            plt.savefig(self.dir_root + "IMAGES/Correction_cosmics.png")

            correction_cosmics = all_flux_norm - all_flux_corrected
            to_be_saved = {"wave": grid, "correction_map": correction_cosmics}
            myf.pickle_dump(
                to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_cosmics.p", "wb")
            )

            new_continuum = all_flux / (all_flux_corrected + epsilon)
            new_continuum[all_flux == 0] = conti[all_flux == 0]
            new_continuum[new_continuum != new_continuum] = conti[
                new_continuum != new_continuum
            ]  # to supress mystic nan appearing

            print("\nComputation of the new continua, wait ... \n")
            time.sleep(0.5)
            count_file = -1
            for j in tqdm(files):
                count_file += 1
                file = pd.read_pickle(j)
                output = {"continuum_" + continuum: new_continuum[count_file]}
                file["matching_cosmics"] = output
                file["matching_cosmics"]["parameters"] = {
                    "sub_dico_used": sub_dico,
                    "k_sigma": k_sigma,
                    "step": step + 1,
                }
                ras.save_pickle(j, file)

            self.dico_actif = "matching_cosmics"

    # =============================================================================
    # SUPRESS VARIATION RELATIF TO MEDIAN-MAD SPECTRUM (OUTLIERS CORRECTION + ECCENTRIC PLANET)
    # =============================================================================

    def yarara_correct_mad(
        self, sub_dico="matching_diff", continuum="linear", k_sigma=2, k_mad=2, n_iter=1, ext="0"
    ):

        """
        Supress flux value outside k-sigma mad clipping

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)

        """

        myf.print_box("\n---- RECIPE : CORRECTION MAD ----\n")

        directory = self.directory
        planet = self.planet

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        all_flux = []
        all_flux_std = []
        all_snr = []
        jdb = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                grid = file["wave"]

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            all_flux.append(f_norm)
            all_flux_std.append(f_norm_std)
            all_snr.append(file["parameters"]["SNR_5500"])
            jdb.append(file["parameters"]["jdb"])

        step = file[sub_dico]["parameters"]["step"]

        all_flux = np.array(all_flux)
        all_flux_std = np.array(all_flux_std)
        all_snr = np.array(all_snr)
        snr_max = all_snr.argmax()
        jdb = np.array(jdb)

        # plt.subplot(3,1,1)
        # for j in range(len(all_flux)):
        #    plt.plot(grid,all_flux[j])
        # ax = plt.gca()
        # plt.plot(grid,np.median(all_flux,axis=0),color='k',zorder=1000)

        all_flux[np.isnan(all_flux)] = 0

        all_flux2 = all_flux.copy()
        # all_flux2[np.isnan(all_flux2)] = 0

        med = np.median(all_flux2.copy(), axis=0).copy()
        mean = np.mean(all_flux2.copy(), axis=0).copy()
        sup = np.percentile(all_flux2.copy(), 84, axis=0).copy()
        inf = np.percentile(all_flux2.copy(), 16, axis=0).copy()
        ref = all_flux2[snr_max].copy()

        ok = "y"

        save = []
        count = 0
        while ok == "y":

            mad = 1.48 * np.median(
                abs(all_flux2 - np.median(all_flux2, axis=0)), axis=0
            )  # mad transformed in sigma
            mad[mad == 0] = 100
            counter_removed = []
            cum_curve = []
            for j in tqdm(range(len(all_flux2))):
                sigma = myc.tableXY(
                    grid, (abs(all_flux2[j] - med) - all_flux_std[j] * k_sigma) / mad
                )
                sigma.smooth(box_pts=6, shape="rectangular")
                # sigma.rolling(window=100,quantile=0.50)
                # sig = (sigma.y>(sigma.roll_Q1+3*sigma.roll_IQ))
                # sig = sigma.roll_Q1+3*sigma.roll_IQ

                # sigma.y *= -1
                # sigma.find_max(vicinity=5)
                # loc_min = sigma.index_max.copy()
                # sigma.y *= -1

                mask = sigma.y > k_mad

                # sigma.find_max(vicinity=5)
                # loc_max = np.array([sigma.y_max, sigma.index_max]).T
                # loc_max = loc_max[loc_max[:,0]>k_mad] # only keep sigma higher than k_sigma
                # loc_max = loc_max[:,-1]

                # diff = loc_max - loc_min[:,np.newaxis]
                # diff1 = diff.copy()
                # #diff2 = diff.copy()
                # diff1[diff1<0] = 1000 #arbitrary large value
                # #diff2[diff2>0] = -1000 #arbitrary small value
                # left = np.argmin(diff1,axis=1)
                # left = np.unique(left)
                # mask = np.zeros(len(grid)).astype('bool')
                # for k in range(len(left)-1):
                #     mask[int(sigma.index_max[left[k]]):int(sigma.index_max[left[k]+1])+1] = True

                # all_flux2[j][sigma.y>3] = med[sigma.y>3]

                all_flux2[j][mask] = med[mask]
                counter_removed.append(100 * np.sum(mask * (ref < 0.9)) / np.sum(ref < 0.9))
                cum_curve.append(100 * np.cumsum(mask * (ref < 0.9)) / np.sum(ref < 0.9))

            self.counter_mad_removed = np.array(counter_removed)
            self.cum_curves = np.array(cum_curve)
            self.cum_curves[self.cum_curves[:, -1] == 0, -1] = 1

            med2 = np.median(all_flux2, axis=0)
            mean2 = np.mean(all_flux2, axis=0)
            sup2 = np.percentile(all_flux2, 84, axis=0)
            inf2 = np.percentile(all_flux2, 16, axis=0)
            ref2 = all_flux2[snr_max].copy()

            save.append((mean - mean2).copy())

            if n_iter is None:
                plt.subplot(3, 1, 1)
                plt.plot(grid, med, color="k")
                plt.plot(grid, ref, color="k", alpha=0.4)
                plt.fill_between(grid, sup, y2=inf, alpha=0.5, color="b")
                ax = plt.gca()
                plt.subplot(3, 1, 2, sharex=ax, sharey=ax)
                plt.plot(grid, med2, color="k")
                plt.plot(grid, ref2, color="k", alpha=0.4)
                plt.fill_between(grid, sup2, y2=inf2, alpha=0.5, color="g")
                plt.subplot(3, 1, 3, sharex=ax)
                plt.plot(grid, ref - ref2, color="k", alpha=0.4)
                for k in range(len(save)):
                    plt.plot(grid, save[k])
                plt.axhline(y=0, color="r")

                plt.show(block=False)
                ok = myf.sphinx(
                    " Do you want to iterate one more time (y), quit (n) or save (s) ? (y/n/s)",
                    rep=["y", "n", "s"],
                )
                plt.close()
            else:
                if n_iter == 1:
                    ok = "s"
                else:
                    n_iter -= 1
                    ok = "y"

            if ok != "y":
                break
            else:
                count += 1
        if ok == "s":
            plt.figure(figsize=(23, 16))
            plt.subplot(2, 3, 1)
            plt.axhline(
                y=0.15,
                color="k",
                ls=":",
                label="rejection criterion  (%.0f)" % (sum(self.counter_mad_removed > 0.15)),
            )
            plt.legend()
            plt.scatter(jdb, self.counter_mad_removed, c=jdb, cmap="jet")
            plt.xlabel("Time", fontsize=13)
            plt.ylabel("Percent of the spectrum removed [%]", fontsize=13)
            ax = plt.colorbar()
            ax.ax.set_ylabel("Time")
            plt.subplot(2, 3, 4)
            plt.axhline(
                y=0.15,
                color="k",
                ls=":",
                label="rejection criterion (%.0f)" % (sum(self.counter_mad_removed > 0.15)),
            )
            plt.scatter(all_snr, self.counter_mad_removed, c=jdb, cmap="jet")
            plt.xlabel("SNR", fontsize=13)
            plt.ylabel("Percent of the spectrum removed [%]", fontsize=13)
            ax = plt.colorbar()
            ax.ax.set_ylabel("Time")

            jet = plt.get_cmap("jet")
            vmin = np.min(jdb)
            vmax = np.max(jdb)

            cNorm = mplcolors.Normalize(vmin=vmin, vmax=vmax)
            scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)

            plt.subplot(2, 3, 2)
            for j in range(len(jdb)):
                colorVal = scalarMap.to_rgba(jdb[j])
                plt.plot(grid[::500], self.cum_curves[j][::500], color=colorVal, alpha=0.5)
            plt.xlabel("Wavelength", fontsize=13)
            plt.ylabel("Cumulative of spectrum removed [%]", fontsize=13)

            plt.subplot(2, 3, 5)
            for j in range(len(jdb)):
                colorVal = scalarMap.to_rgba(jdb[j])
                plt.plot(
                    grid[::500],
                    self.cum_curves[j][::500] / self.cum_curves[j][-1] * 100,
                    color=colorVal,
                    alpha=0.3,
                )
            plt.xlabel("Wavelength", fontsize=13)
            plt.ylabel("Normalised cumulative spectrum removed [%]", fontsize=13)

            plt.subplot(2, 3, 3)
            for j in range(len(jdb)):
                plt.plot(grid[::500], self.cum_curves[j][::500], color="k", alpha=0.3)
            plt.xlabel("Wavelength", fontsize=13)
            plt.ylabel("Cumulative of spectrum removed [%]", fontsize=13)

            plt.subplot(2, 3, 6)
            for j in range(len(jdb)):
                colorVal = scalarMap.to_rgba(jdb[j])
                plt.plot(
                    grid[::500],
                    self.cum_curves[j][::500] / self.cum_curves[j][-1] * 100,
                    color="k",
                    alpha=0.3,
                )
            plt.xlabel("Wavelength", fontsize=13)
            plt.ylabel("Normalised cumulative spectrum removed [%]", fontsize=13)
            plt.subplots_adjust(left=0.07, right=0.97)
            plt.savefig(self.dir_root + "IMAGES/mad_statistics_iter_%s.png" % (ext))

            correction_mad = all_flux - all_flux2
            to_be_saved = {"wave": grid, "correction_map": correction_mad}
            myf.pickle_dump(
                to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_mad.p", "wb")
            )

            print("\nComputation of the new continua, wait ... \n")
            time.sleep(0.5)
            count_file = -1
            self.debug = (all_flux, all_flux2)

            for j in tqdm(files):
                count_file += 1
                file = pd.read_pickle(j)
                all_flux2[count_file][file["flux" + kw] == 0] = 1
                all_flux2[count_file][all_flux2[count_file] == 0] = 1
                new_flux = file["flux" + kw] / all_flux2[count_file]
                new_flux[(new_flux == 0) | (new_flux != new_flux)] = 1
                mask = yarara_artefact_suppressed(
                    file[sub_dico]["continuum_" + continuum],
                    new_flux,
                    larger_than=50,
                    lower_than=-50,
                )
                new_flux[mask] = file[sub_dico]["continuum_" + continuum][mask]
                output = {"continuum_" + continuum: new_flux}
                file["matching_mad"] = output
                file["matching_mad"]["parameters"] = {
                    "iteration": count + 1,
                    "sub_dico_used": sub_dico,
                    "k_sigma": k_sigma,
                    "k_mad": k_mad,
                    "step": step + 1,
                }
                ras.save_pickle(j, file)

            self.dico_actif = "matching_mad"

    # =============================================================================
    # CORRECTION OF FROG (GHOST AND STITCHING)
    # =============================================================================

    def yarara_produce_mask_background(
        self,
        sub_dico="matching_diff",
        wave_min=3800,
        wave_max=4100,
        wave_min_kernel=None,
        wave_max_kernel=5000,
    ):
        self.import_material()
        load = self.material
        ref = load["reference_spectrum"]

        i1 = myf.find_nearest(load.wave, wave_min)[0][0]
        i2 = myf.find_nearest(load.wave, wave_max)[0][0]
        save = []
        for j in tqdm(range(len(sts.table))):
            a = self.spectrum(num=j, norm=True)
            save.append(np.nanmean(a.y[i1:i2]))
        save = np.array(save)

        pos = np.where(save > (np.percentile(save, 75) + 1.5 * myf.IQ(save)))[0]
        neg = np.where(save < (np.percentile(save, 25) - 1.5 * myf.IQ(save)))[0]

        save_pos = a.y * 0
        for j in pos:
            a = self.spectrum(num=j, norm=True)
            save_pos += a.y

        save_neg = a.y * 0
        for j in neg:
            a = self.spectrum(num=j, norm=True)
            save_neg += a.y

        save_pos /= len(pos)
        save_neg /= len(neg)

        ratio = (save_pos / save_neg - 1) * 100
        kernel = myf.smooth(ratio, box_pts=100, shape="rectangular")

        if wave_min_kernel is not None:
            i = myf.find_nearest(load.wave, wave_min_kernel)[0][0]
            kernel[0:i] = 0

        if wave_max_kernel is not None:
            i = myf.find_nearest(load.wave, wave_max_kernel)[0][0]
            kernel[i:-1] = 0

        coeff = []
        for j in tqdm(range(len(sts.table))):
            a = self.spectrum(num=j, norm=True)
            coeff.append(np.nanmean((a.y / ref) * kernel))
        coeff = np.array(coeff)

        coeff -= np.nanmean(coeff)
        coeff /= np.nanmax(coeff) - np.nanmin(coeff)

    def yarara_produce_mask_empiric(
        self,
        sub_dico="matching_diff",
        index_contam=[6, 23],
        index_clean=[136, -1],
        wave_min=3800,
        wave_max=7000,
        reference="median",
        berv_shift="berv",
        method="median",
        excluded_mask=None,
        outliers=["+", "-"],
        min_cluster_size=10,
        extended=5,
        rcorr_min=0.4,
        substract_map=[],
    ):
        """Used to build a mask directly from the observation , as for instance for HARPN"""

        self.import_table()
        self.import_material()
        epsilon = 1e-12

        load = self.material
        grid = np.array(load["wave"])

        m, s, wave = self.yarara_map(
            sub_dico=sub_dico,
            wave_min=wave_min,
            wave_max=wave_max,
            Plot=False,
            berv_shift=berv_shift,
            reference=reference,
            substract_map=substract_map,
        )

        if method == "median":
            chunck_contam = np.nanmedian(m[index_contam[0] : index_contam[1], :], axis=0)
            chunck_clean = np.nanmedian(m[index_clean[0] : index_clean[1], :], axis=0)
        elif method == "std":
            chunck_contam = np.nanstd(m[index_contam[0] : index_contam[1], :], axis=0)
            chunck_clean = np.nanstd(m[index_clean[0] : index_clean[1], :], axis=0)

        mask = np.zeros(len(wave)).astype("bool")
        if "+" in outliers:
            mask2 = (
                chunck_contam > np.nanpercentile(chunck_contam, 75) + 1.5 * myf.IQ(chunck_contam)
            ) & (chunck_clean < np.nanpercentile(chunck_clean, 75) + 1.5 * myf.IQ(chunck_clean))
            mask = mask | mask2
        if "-" in outliers:
            mask2 = (
                chunck_contam < np.nanpercentile(chunck_contam, 25) - 1.5 * myf.IQ(chunck_contam)
            ) & (chunck_clean > np.nanpercentile(chunck_clean, 25) - 1.5 * myf.IQ(chunck_clean))
            mask = mask | mask2

        val, borders = myf.clustering(mask, 0.5, 0)
        val2 = np.array([np.product(v) for v in val])
        borders = borders[val2.astype("bool")]
        borders = myf.merge_borders(borders)
        new_borders = borders[borders[:, -1] > min_cluster_size]
        new_mask = myf.flat_clustering(len(wave), new_borders, extended=extended)
        new_mask = (new_mask != 0).astype("int")
        final_mask = np.zeros(len(grid)).astype("bool")
        i1 = int(myf.find_nearest(grid, wave_min)[0])
        i2 = int(myf.find_nearest(grid, wave_max)[0])
        final_mask[i1 : i2 + 1] = (new_mask).astype("bool")

        if excluded_mask is None:
            excluded_mask = np.zeros(len(final_mask))[i1 : i2 + 1]

        new_mask = final_mask[i1 : i2 + 1]
        new_mask = new_mask & (~(excluded_mask).astype("bool"))

        sub_m = m[:, new_mask.astype("bool")]
        sub_m = sub_m / myf.IQ(sub_m, axis=0)

        proxy = np.median(sub_m, axis=1)
        proxy_rms = np.std(proxy)

        rslope = np.median(
            (m - np.mean(m, axis=0)) / ((proxy - np.mean(proxy))[:, np.newaxis]), axis=0
        )
        t = myc.table(m)
        t.rms_w(1 / s**2, axis=0)
        # rcorr = rslope*np.std(proxy)/np.std(diff,axis=0) #old version unweighted
        rcorr = (
            rslope * proxy_rms / (t.rms + epsilon)
        )  # need good weighting of the proxy and the flux

        mask = rcorr > abs(rcorr_min)

        val, borders = myf.clustering(mask, 0.5, 0)
        val2 = np.array([np.product(v) for v in val])
        borders = borders[val2.astype("bool")]
        borders = myf.merge_borders(borders)
        new_borders = borders[borders[:, -1] > min_cluster_size]
        new_mask = myf.flat_clustering(len(wave), new_borders, extended=extended)
        new_mask = (new_mask != 0).astype("int")
        final_mask2 = np.zeros(len(grid)).astype("bool")
        i1 = int(myf.find_nearest(grid, wave_min)[0])
        i2 = int(myf.find_nearest(grid, wave_max)[0])
        final_mask2[i1 : i2 + 1] = (new_mask).astype("bool")

        return final_mask2

    # =============================================================================
    # CORRECTION OF FROG (GHOST AND STITCHING)
    # =============================================================================

    def yarara_produce_mask_contam(self, frog_file=root + "/Python/Material/Contam_HARPN.p"):

        """
        Creation of the stitching mask on the spectrum

        Parameters
        ----------
        frog_file : files containing the wavelength of the stitching
        """

        directory = self.directory
        kw = "_planet" * self.planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        myf.print_box("\n---- RECIPE : PRODUCTION CONTAM MASK ----\n")

        print("\n [INFO] FROG file used : %s" % (frog_file))
        self.import_table()
        self.import_material()
        load = self.material

        grid = np.array(load["wave"])

        # extract frog table
        frog_table = pd.read_pickle(frog_file)
        # stitching

        print("\n [INFO] Producing the contam mask...")

        wave_contam = np.hstack(frog_table["wave"])
        contam = np.hstack(frog_table["contam"])

        vec = myc.tableXY(wave_contam, contam)
        vec.order()
        vec.interpolate(new_grid=np.array(load["wave"]), method="linear")

        load["contam"] = vec.y.astype("int")
        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

    def yarara_produce_mask_telluric(self, telluric_tresh=0.001):

        """
        Creation of the stitching mask on the spectrum

        Parameters
        ----------
        frog_file : files containing the wavelength of the stitching
        """

        directory = self.directory
        kw = "_planet" * self.planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        myf.print_box("\n---- RECIPE : PRODUCTION TELLURIC MASK ----\n")

        self.import_table()
        self.import_material()
        load = self.material

        grid = np.array(load["wave"])

        if self.table["telluric_fwhm"][0] is None:
            fwhm = np.array([3.0] * len(self.table.jdb))
        else:
            try:
                fwhm = np.array(self.table["telluric_fwhm"] * 2.35)
            except:
                fwhm = np.array([3.0] * len(self.table.jdb))

        fwhm = np.nanpercentile(fwhm, 95)

        if fwhm > 5:
            print(
                Fore.YELLOW
                + "\n [WARNING] FWHM of tellurics larger than 5 km/s (%.1f), reduced to default value of 3 km/s"
                % (fwhm)
                + Fore.RESET
            )
            myf.make_sound("warning")

        print("\n [INFO] FWHM of tellurics : %.1f km/s" % (fwhm))

        # extract frog table

        model = pd.read_pickle(root + "/Python/Material/model_telluric.p")
        grid = model["wave"]
        spectre = model["flux_norm"]
        telluric = myc.tableXY(grid, spectre)
        telluric.find_min()

        all_min = np.array([telluric.x_min, telluric.y_min, telluric.index_min]).T
        all_min = all_min[1 - all_min[:, 1] > telluric_tresh]

        # stitching

        print("\n [INFO] Producing the telluric mask...")

        wave_stitching = np.hstack(all_min[:, 0])
        gap_stitching = np.hstack(all_min[:, 1])

        vec = myc.tableXY(wave_stitching, gap_stitching)
        vec.order()
        telluric = vec.x[vec.y != 0]

        grid = np.array(load["wave"])
        match_telluric = myf.match_nearest(grid, telluric)
        indext = match_telluric[:, 0].astype("int")

        wavet_delta = np.zeros(len(grid))
        wavet_delta[indext] = 1

        wavet = grid[indext]
        max_t = wavet * ((1 + 1.55e-8) * (1 + (fwhm) / 299792.458))
        min_t = wavet * ((1 + 1.55e-8) * (1 + (-fwhm) / 299792.458))

        mask_telluric = np.sum(
            (grid > min_t[:, np.newaxis]) & (grid < max_t[:, np.newaxis]), axis=0
        ).astype("bool")
        self.telluric_zones = mask_telluric

        load["telluric"] = mask_telluric.astype("int")
        load["telluric_delta"] = wavet_delta.astype("int")

        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

    def yarara_produce_mask_stitching(self, frog_file=root + "/Python/Material/Ghost_HARPS03.p"):

        """
        Creation of the stitching mask on the spectrum

        Parameters
        ----------
        frog_file : files containing the wavelength of the stitching
        """

        directory = self.directory
        kw = "_planet" * self.planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        myf.print_box("\n---- RECIPE : PRODUCTION STITCHING MASK ----\n")

        print("\n [INFO] FROG file used : %s" % (frog_file))
        self.import_table()
        self.import_material()
        load = self.material

        grid = np.array(load["wave"])

        berv = np.array(self.table["berv" + kw])
        berv_max = berv.max()
        berv_min = berv.min()

        # extract frog table
        frog_table = pd.read_pickle(frog_file)
        berv_file = self.yarara_get_berv_value(frog_table["jdb"])

        # stitching

        print("\n [INFO] Producing the stitching mask...")

        wave_stitching = np.hstack(frog_table["wave"])
        gap_stitching = np.hstack(frog_table["stitching"])

        vec = myc.tableXY(wave_stitching, gap_stitching)
        vec.order()
        stitching = vec.x[vec.y != 0]

        stitching_b0 = myf.doppler_r(stitching, 0 * berv_file * 1000)[0]
        # all_stitch = myf.doppler_r(stitching_b0, berv*1000)[0]

        match_stitching = myf.match_nearest(grid, stitching_b0)
        indext = match_stitching[:, 0].astype("int")

        wavet_delta = np.zeros(len(grid))
        wavet_delta[indext] = 1

        wavet = grid[indext]
        max_t = wavet * ((1 + 1.55e-8) * (1 + (berv_max - 0 * berv_file) / 299792.458))
        min_t = wavet * ((1 + 1.55e-8) * (1 + (berv_min - 0 * berv_file) / 299792.458))

        mask_stitching = np.sum(
            (grid > min_t[:, np.newaxis]) & (grid < max_t[:, np.newaxis]), axis=0
        ).astype("bool")
        self.stitching_zones = mask_stitching

        load["stitching"] = mask_stitching.astype("int")
        load["stitching_delta"] = wavet_delta.astype("int")

        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

    def yarara_produce_mask_frog(self, frog_file=root + "/Python/Material/Ghost_HARPS03.p"):

        """
        Correction of the stitching/ghost on the spectrum by PCA fitting

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum used in the difference
        extended : extension of the cluster size
        frog_file : files containing the wavelength of the stitching
        """

        myf.print_box("\n---- RECIPE : MASK GHOST/STITCHING/THAR WITH FROG ----\n")

        directory = self.directory
        kw = "_planet" * self.planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        self.import_table()
        self.import_material()
        load = self.material

        file_test = self.import_spectrum()
        grid = file_test["wave"]

        berv_max = self.table["berv" + kw].max()
        berv_min = self.table["berv" + kw].min()
        imin = (
            myf.find_nearest(grid, myf.doppler_r(grid[0], np.max(abs(self.table.berv)) * 1000)[0])[
                0
            ][0]
            + 1
        )
        imax = myf.find_nearest(
            grid, myf.doppler_r(grid[-1], np.max(abs(self.table.berv)) * 1000)[1]
        )[0][0]

        # extract frog table
        frog_table = pd.read_pickle(frog_file)
        berv_file = self.yarara_get_berv_value(frog_table["jdb"])

        # ghost
        for correction in ["stitching", "ghost_a", "ghost_b", "thar"]:
            if correction in frog_table.keys():
                if correction == "stitching":
                    print("\n [INFO] Producing the stitching mask...")

                    wave_stitching = np.hstack(frog_table["wave"])
                    gap_stitching = np.hstack(frog_table["stitching"])

                    vec = myc.tableXY(wave_stitching, gap_stitching)
                    vec.order()
                    stitching = vec.x[vec.y != 0]

                    stitching_b0 = myf.doppler_r(stitching, 0 * berv_file * 1000)[0]
                    # all_stitch = myf.doppler_r(stitching_b0, berv*1000)[0]

                    match_stitching = myf.match_nearest(grid, stitching_b0)
                    indext = match_stitching[:, 0].astype("int")

                    wavet_delta = np.zeros(len(grid))
                    wavet_delta[indext] = 1

                    wavet = grid[indext]
                    max_t = wavet * ((1 + 1.55e-8) * (1 + (berv_max - 0 * berv_file) / 299792.458))
                    min_t = wavet * ((1 + 1.55e-8) * (1 + (berv_min - 0 * berv_file) / 299792.458))

                    mask_stitching = np.sum(
                        (grid > min_t[:, np.newaxis]) & (grid < max_t[:, np.newaxis]), axis=0
                    ).astype("bool")
                    self.stitching_zones = mask_stitching

                    mask_stitching[0:imin] = 0
                    mask_stitching[imax:] = 0

                    load["stitching"] = mask_stitching.astype("int")
                    load["stitching_delta"] = wavet_delta.astype("int")
                else:
                    if correction == "ghost_a":
                        print("\n [INFO] Producing the ghost mask A...")
                    elif correction == "ghost_b":
                        print("\n [INFO] Producing the ghost mask B...")
                    elif correction == "thar":
                        print("\n [INFO] Producing the thar mask...")

                    contam = frog_table[correction]
                    mask = np.zeros(len(grid))
                    wave_s2d = []
                    order_s2d = []
                    for order in np.arange(len(contam)):
                        vec = myc.tableXY(
                            myf.doppler_r(frog_table["wave"][order], 0 * berv_file * 1000)[0],
                            contam[order],
                            0 * contam[order],
                        )
                        vec.order()
                        vec.y[0:2] = 0
                        vec.y[-2:] = 0
                        begin = int(myf.find_nearest(grid, vec.x[0])[0])
                        end = int(myf.find_nearest(grid, vec.x[-1])[0])
                        sub_grid = grid[begin:end]
                        vec.interpolate(new_grid=sub_grid, method="linear", interpolate_x=False)
                        model = np.array(
                            load["reference_spectrum"][begin:end]
                            * load["correction_factor"][begin:end]
                        )
                        model[model == 0] = 1
                        contam_cumu = vec.y / model
                        if sum(contam_cumu != 0) != 0:
                            mask[begin:end] += np.nanmean(contam_cumu[contam_cumu != 0]) * (
                                contam_cumu != 0
                            )
                            order_s2d.append((vec.y != 0) * (1 + order / len(contam) / 20))
                            wave_s2d.append(sub_grid)

                    mask[0:imin] = 0
                    mask[imax:] = 0
                    load[correction] = mask
            else:
                load[correction] = np.zeros(len(grid))

        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

    def yarara_correct_frog(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        correction="stitching",
        berv_shift=False,
        wave_min=3800,
        wave_max=3975,
        wave_min_train=3700,
        wave_max_train=6000,
        complete_analysis=False,
        reference="median",
        equal_weight=True,
        nb_pca_comp=10,
        pca_comp_kept=None,
        rcorr_min=0,
        treshold_contam=0.5,
        algo_pca="empca",
    ):

        """
        Correction of the stitching/ghost on the spectrum by PCA fitting

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum used in the difference
        extended : extension of the cluster size
        """

        myf.print_box("\n---- RECIPE : CORRECTION %s WITH FROG ----\n" % (correction.upper()))

        directory = self.directory
        self.import_table()
        self.import_material()
        load = self.material

        cmap = self.cmap
        planet = self.planet
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        epsilon = 1e-12

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        all_flux = []
        all_flux_std = []
        snr = []
        jdb = []
        conti = []
        berv = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                grid = file["wave"]
                hole_left = file["parameters"]["hole_left"]
                hole_right = file["parameters"]["hole_right"]
            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
            all_flux.append(f_norm)
            all_flux_std.append(f_norm_std)
            conti.append(c)
            jdb.append(file["parameters"]["jdb"])
            snr.append(file["parameters"]["SNR_5500"])
            if type(berv_shift) != np.ndarray:
                try:
                    berv.append(file["parameters"][berv_shift])
                except:
                    berv.append(0)
            else:
                berv = berv_shift

        step = file[sub_dico]["parameters"]["step"]

        all_flux = np.array(all_flux)
        all_flux_std = np.array(all_flux_std)
        conti = np.array(conti)
        jdb = np.array(jdb)
        snr = np.array(snr)
        berv = np.array(berv)

        if reference == "snr":
            ref = all_flux[snr.argmax()]
        elif reference == "median":
            print(" [INFO] Reference spectrum : median")
            ref = np.median(all_flux, axis=0)
        elif reference == "master":
            print(" [INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print(" [INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = all_flux[reference]
        else:
            ref = 0 * np.median(all_flux, axis=0)

        berv_max = self.table["berv" + kw].max()
        berv_min = self.table["berv" + kw].min()

        diff = all_flux - ref

        diff_backup = diff.copy()

        if np.sum(abs(berv)) != 0:
            for j in tqdm(np.arange(len(all_flux))):
                test = myc.tableXY(grid, diff[j], all_flux_std[j])
                test.x = myf.doppler_r(test.x, berv[j] * 1000)[1]
                test.interpolate(new_grid=grid, method="cubic", replace=True, interpolate_x=False)
                diff[j] = test.y
                all_flux_std[j] = test.yerr

        # extract frog table
        # frog_table = pd.read_pickle(frog_file)
        berv_file = 0  # self.yarara_get_berv_value(frog_table['jdb'])

        mask = np.array(load[correction])

        loc_ghost = mask != 0

        # mask[mask<treshold_contam] = 0
        val, borders = myf.clustering(loc_ghost, 0.5, 1)
        val = np.array([np.product(v) for v in val])
        borders = borders[val == 1]

        min_t = grid[borders[:, 0]] * (
            (1 + 1.55e-8) * (1 + (berv_min - 0 * berv_file) / 299792.458)
        )
        max_t = grid[borders[:, 1]] * (
            (1 + 1.55e-8) * (1 + (berv_max - 0 * berv_file) / 299792.458)
        )

        if (correction == "ghost_a") | (correction == "ghost_b"):
            for j in range(3):
                if np.sum(mask > treshold_contam) < 200:
                    print(
                        Fore.YELLOW
                        + " [WARNING] Not enough wavelength in the mask, treshold contamination reduced down to %.2f"
                        % (treshold_contam)
                        + Fore.RESET
                    )
                    treshold_contam *= 0.75

        mask_ghost = np.sum(
            (grid > min_t[:, np.newaxis]) & (grid < max_t[:, np.newaxis]), axis=0
        ).astype("bool")
        mask_ghost_extraction = (
            mask_ghost
            & (mask > treshold_contam)
            & (ref < 1)
            & (np.array(1 - load["activity_proxies"]).astype("bool"))
            & (grid < wave_max_train)
            & (grid > wave_min_train)
        )  # extract everywhere

        if correction == "stitching":
            self.stitching = mask_ghost
            self.stitching_extracted = mask_ghost_extraction
        elif correction == "ghost_a":
            self.ghost_a = mask_ghost
            self.ghost_a_extracted = mask_ghost_extraction
        elif correction == "ghost_b":
            self.ghost_b = mask_ghost
            self.ghost_b_extracted = mask_ghost_extraction
        elif correction == "thar":
            self.thar = mask_ghost
            self.thar_extracted = mask_ghost_extraction
        elif correction == "contam":
            self.contam = mask_ghost
            self.contam_extracted = mask_ghost_extraction

        # compute pca

        if correction == "stitching":
            print(" [INFO] Computation of PCA vectors for stitching correction...")
            diff_ref = diff[:, mask_ghost]
            subflux = diff[:, (mask_ghost) & (np.array(load["ghost_a"]) == 0)]
            subflux_std = all_flux_std[:, (mask_ghost) & (np.array(load["ghost_a"]) == 0)]
            lab = "Stitching"
            name = "stitching"
        elif correction == "ghost_a":
            print(" [INFO] Computation of PCA vectors for ghost correction...")
            diff_ref = diff[:, mask_ghost]
            subflux = diff[:, (np.array(load["stitching"]) == 0) & (mask_ghost_extraction)]
            subflux_std = all_flux_std[
                :, (np.array(load["stitching"]) == 0) & (mask_ghost_extraction)
            ]
            lab = "Ghost_a"
            name = "ghost_a"
        elif correction == "ghost_b":
            print(" [INFO] Computation of PCA vectors for ghost correction...")
            diff_ref = diff[:, mask_ghost]
            subflux = diff[:, (load["thar"] == 0) & (mask_ghost_extraction)]
            subflux_std = all_flux_std[:, (load["thar"] == 0) & (mask_ghost_extraction)]
            lab = "Ghost_b"
            name = "ghost_b"
        elif correction == "thar":
            print(" [INFO] Computation of PCA vectors for thar correction...")
            diff_ref = diff.copy()
            subflux = diff[:, mask_ghost_extraction]
            subflux_std = all_flux_std[:, mask_ghost_extraction]
            lab = "Thar"
            name = "thar"
        elif correction == "contam":
            print(" [INFO] Computation of PCA vectors for contam correction...")
            diff_ref = diff[:, mask_ghost]
            subflux = diff[:, mask_ghost_extraction]
            subflux_std = all_flux_std[:, mask_ghost_extraction]
            lab = "Contam"
            name = "contam"

        subflux_std = subflux_std[:, np.std(subflux, axis=0) != 0]
        subflux = subflux[:, np.std(subflux, axis=0) != 0]

        if not len(subflux[0]):
            subflux = diff[:, 0:10]
            subflux_std = all_flux_std[:, 0:10]

        plt.figure(2, figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.imshow(subflux, aspect="auto", vmin=-0.005, vmax=0.005)
        plt.title(lab + " lines")
        plt.xlabel("Pixels extracted", fontsize=14)
        plt.ylabel("Time", fontsize=14)
        ax = plt.gca()
        plt.subplot(1, 2, 2, sharex=ax, sharey=ax)
        plt.imshow(
            subflux / (epsilon + np.std(subflux, axis=0)), aspect="auto", vmin=-0.005, vmax=0.005
        )
        plt.title(lab + " lines equalized")
        plt.xlabel("Pixels extracted", fontsize=14)
        plt.ylabel("Time", fontsize=14)

        c = int(equal_weight)

        X_train = (subflux / ((1 - c) + epsilon + c * np.std(subflux, axis=0))).T
        X_train_std = (subflux_std / ((1 - c) + epsilon + c * np.std(subflux, axis=0))).T

        # myf.pickle_dump({'jdb':np.array(self.table.jdb),'ratio_flux':X_train,'ratio_flux_std':X_train_std},open(root+'/Python/datasets/telluri_cenB.p','wb'))

        test2 = myc.table(X_train)
        test2.WPCA(algo_pca, weight=1 / X_train_std**2, comp_max=nb_pca_comp)

        phase_mod = np.arange(365)[
            np.argmin(
                np.array(
                    [np.max((jdb - k) % 365.25) - np.min((jdb - k) % 365.25) for k in range(365)]
                )
            )
        ]

        plt.figure(4, figsize=(10, 14))
        plt.subplot(3, 1, 1)
        plt.xlabel("# PCA components", fontsize=13)
        plt.ylabel("Variance explained", fontsize=13)
        plt.plot(np.arange(1, len(test2.phi_components) + 1), test2.var_ratio)
        plt.scatter(np.arange(1, len(test2.phi_components) + 1), test2.var_ratio)
        plt.subplot(3, 1, 2)
        plt.xlabel("# PCA components", fontsize=13)
        plt.ylabel("Z score", fontsize=13)
        plt.plot(np.arange(1, len(test2.phi_components) + 1), test2.zscore_components)
        plt.scatter(np.arange(1, len(test2.phi_components) + 1), test2.zscore_components)
        z_max = test2.zscore_components[-5:].max()
        z_min = test2.zscore_components[-5:].min()
        vec_relevant = np.arange(len(test2.zscore_components)) * (
            (test2.zscore_components > z_max) | (test2.zscore_components < z_min)
        )
        plt.axhspan(ymin=z_min, ymax=z_max, alpha=0.2, color="k")
        pca_comp_kept2 = int(np.where(vec_relevant != np.arange(len(vec_relevant)))[0][0])
        plt.subplot(3, 1, 3)
        plt.xlabel("# PCA components", fontsize=13)
        plt.ylabel(r"$\Phi(0)$", fontsize=13)
        plt.plot(np.arange(1, len(test2.phi_components) + 1), test2.phi_components)
        plt.scatter(np.arange(1, len(test2.phi_components) + 1), test2.phi_components)
        plt.axhline(y=0.5, color="k")
        phi_max = test2.phi_components[-5:].max()
        phi_min = test2.phi_components[-5:].min()
        plt.axhspan(ymin=phi_min, ymax=phi_max, alpha=0.2, color="k")
        vec_relevant = np.arange(len(test2.phi_components)) * (
            (test2.phi_components > phi_max) | (test2.phi_components < phi_min)
        )
        if pca_comp_kept is None:
            pca_comp_kept = int(np.where(vec_relevant != np.arange(len(vec_relevant)))[0][0])
            pca_comp_kept = np.max([pca_comp_kept, pca_comp_kept2])

        plt.savefig(self.dir_root + "IMAGES/" + name + "_PCA_variances.pdf")

        plt.figure(figsize=(15, 10))
        for j in range(pca_comp_kept):
            if j == 0:
                plt.subplot(pca_comp_kept, 2, 2 * j + 1)
                ax = plt.gca()
            else:
                plt.subplot(pca_comp_kept, 2, 2 * j + 1, sharex=ax)
            plt.scatter(jdb, test2.vec[:, j])
            plt.subplot(pca_comp_kept, 2, 2 * j + 2)
            plt.scatter((jdb - phase_mod) % 365.25, test2.vec[:, j])
        plt.subplots_adjust(top=0.95, bottom=0.07, left=0.07, right=0.95, hspace=0)
        plt.savefig(self.dir_root + "IMAGES/" + name + "_PCA_vectors.pdf")

        if correction == "stitching":
            self.vec_pca_stitching = test2.vec[:, 0:pca_comp_kept]
        elif correction == "ghost_a":
            self.vec_pca_ghost_a = test2.vec[:, 0:pca_comp_kept]
        elif correction == "ghost_b":
            self.vec_pca_ghost_b = test2.vec[:, 0:pca_comp_kept]
        elif correction == "thar":
            self.vec_pca_thar = test2.vec[:, 0:pca_comp_kept]
        elif correction == "contam":
            self.vec_pca_contam = test2.vec[:, 0:pca_comp_kept]

        to_be_fit = diff / (np.std(diff, axis=0) + epsilon)

        rcorr = np.zeros(len(grid))
        for j in range(pca_comp_kept):
            proxy1 = test2.vec[:, j]
            rslope1 = np.median(
                (to_be_fit - np.mean(to_be_fit, axis=0))
                / ((proxy1 - np.mean(proxy1))[:, np.newaxis]),
                axis=0,
            )
            rcorr1 = abs(rslope1 * np.std(proxy1) / (np.std(to_be_fit, axis=0) + epsilon))
            rcorr = np.nanmax([rcorr1, rcorr], axis=0)
        rcorr[np.isnan(rcorr)] = 0

        val, borders = myf.clustering(mask_ghost, 0.5, 1)
        val = np.array([np.product(j) for j in val])
        borders = borders[val.astype("bool")]
        borders = myf.merge_borders(borders)
        flat_mask = myf.flat_clustering(len(grid), borders, extended=50).astype("bool")
        rcorr_free = rcorr[~flat_mask]
        rcorr_contaminated = rcorr[flat_mask]

        if correction == "thar":
            mask_ghost = np.ones(len(grid)).astype("bool")

        plt.figure(figsize=(8, 6))
        bins_contam, bins, dust = plt.hist(
            rcorr_contaminated,
            label="contaminated region",
            bins=np.linspace(0, 1, 100),
            alpha=0.5,
            density=True,
        )
        bins_control, bins, dust = plt.hist(
            rcorr_free, bins=np.linspace(0, 1, 100), label="free region", alpha=0.5, density=True
        )
        plt.yscale("log")
        plt.legend()
        bins = bins[0:-1] + np.diff(bins) * 0.5
        sum_a = np.sum(bins_contam[bins > 0.40])
        sum_b = np.sum(bins_control[bins > 0.40])
        crit = int(sum_a > (2 * sum_b))
        check = ["r", "g"][crit]  # three times more correlation than in the control group
        plt.xlabel(r"|$\mathcal{R}_{pearson}$|", fontsize=14, fontweight="bold", color=check)
        plt.title("Density", color=check)
        myf.plot_color_box(color=check)

        plt.savefig(self.dir_root + "IMAGES/" + name + "_control_check.pdf")
        print(" [INFO] %.0f versus %.0f" % (sum_a, sum_b))

        if crit:
            print(" [INFO] Control check sucessfully performed: %s" % (name))
        else:
            print(
                Fore.YELLOW
                + " [WARNING] Control check failed. Correction may be poorly performed for: %s"
                % (name)
                + Fore.RESET
            )

        diff_ref[np.isnan(diff_ref)] = 0

        idx_min = myf.find_nearest(grid, wave_min)[0]
        idx_max = myf.find_nearest(grid, wave_max)[0] + 1

        new_wave = grid[int(idx_min) : int(idx_max)]

        if complete_analysis:
            plt.figure(figsize=(18, 12))
            plt.subplot(pca_comp_kept // 2 + 1, 2, 1)
            myf.my_colormesh(
                new_wave,
                np.arange(len(diff)),
                diff[:, int(idx_min) : int(idx_max)],
                vmin=low_cmap / 100,
                vmax=high_cmap / 100,
                cmap=cmap,
            )
            ax = plt.gca()
            for nb_vec in tqdm(range(1, pca_comp_kept)):
                correction2 = np.zeros((len(grid), len(jdb)))
                collection = myc.table(diff_ref.T)
                base_vec = np.vstack([np.ones(len(diff)), test2.vec[:, 0:nb_vec].T])
                collection.fit_base(base_vec, num_sim=1)
                correction2[mask_ghost] = collection.coeff_fitted.dot(base_vec)
                correction2 = np.transpose(correction2)
                diff_ref2 = diff - correction2
                plt.subplot(pca_comp_kept // 2 + 1, 2, nb_vec + 1, sharex=ax, sharey=ax)
                plt.title("Vec PCA fitted = %0.f" % (nb_vec))
                myf.my_colormesh(
                    new_wave,
                    np.arange(len(diff)),
                    diff_ref2[:, int(idx_min) : int(idx_max)],
                    vmin=low_cmap / 100,
                    vmax=high_cmap / 100,
                    cmap=cmap,
                )
            plt.subplots_adjust(top=0.95, bottom=0.07, left=0.07, right=0.95, hspace=0.3)
            plt.subplot(pca_comp_kept // 2 + 1, 2, pca_comp_kept + 1, sharex=ax)
            plt.plot(new_wave, mask[int(idx_min) : int(idx_max)])
            plt.plot(new_wave, mask_ghost_extraction[int(idx_min) : int(idx_max)], color="k")
            if correction == "stitching":
                plt.plot(new_wave, ref[int(idx_min) : int(idx_max)], color="gray")
        else:
            correction = np.zeros((len(grid), len(jdb)))
            collection = myc.table(diff_ref.T)
            base_vec = np.vstack([np.ones(len(diff)), test2.vec[:, 0:pca_comp_kept].T])
            collection.fit_base(base_vec, num_sim=1)
            correction[mask_ghost] = collection.coeff_fitted.dot(base_vec)
            correction = np.transpose(correction)
            correction[:, rcorr < rcorr_min] = 0

            if np.sum(abs(berv)) != 0:
                for j in tqdm(np.arange(len(all_flux))):
                    test = myc.tableXY(grid, correction[j], 0 * grid)
                    test.x = myf.doppler_r(test.x, berv[j] * 1000)[0]
                    test.interpolate(
                        new_grid=grid, method="cubic", replace=True, interpolate_x=False
                    )
                    correction[j] = test.y

                index_min_backup = int(myf.find_nearest(grid, myf.doppler_r(grid[0], 30000)[0])[0])
                index_max_backup = int(
                    myf.find_nearest(grid, myf.doppler_r(grid[-1], -30000)[0])[0]
                )
                correction[:, 0 : index_min_backup * 2] = 0
                correction[:, index_max_backup * 2 :] = 0
                index_hole_right = int(
                    myf.find_nearest(grid, hole_right + 1)[0]
                )  # correct 1 angstrom band due to stange artefact at the border of the gap
                index_hole_left = int(
                    myf.find_nearest(grid, hole_left - 1)[0]
                )  # correct 1 angstrom band due to stange artefact at the border of the gap
                correction[:, index_hole_left : index_hole_right + 1] = 0

            diff_ref2 = diff_backup - correction

            new_conti = conti * (diff_backup + ref) / (diff_ref2 + ref + epsilon)
            new_continuum = new_conti.copy()
            new_continuum[all_flux == 0] = conti[all_flux == 0]
            new_continuum[new_continuum != new_continuum] = conti[
                new_continuum != new_continuum
            ]  # to supress mystic nan appearing
            new_continuum[np.isnan(new_continuum)] = conti[np.isnan(new_continuum)]
            new_continuum[new_continuum == 0] = conti[new_continuum == 0]
            new_continuum = self.uncorrect_hole(new_continuum, conti)

            # plot end

            if (name == "thar") | (name == "stitching"):
                max_var = grid[np.std(correction, axis=0).argsort()[::-1]]
                if name == "thar":
                    max_var = max_var[max_var < 4400][0]
                else:
                    max_var = max_var[max_var < 6700][0]
                wave_min = myf.find_nearest(grid, max_var - 15)[1]
                wave_max = myf.find_nearest(grid, max_var + 15)[1]

                idx_min = myf.find_nearest(grid, wave_min)[0]
                idx_max = myf.find_nearest(grid, wave_max)[0] + 1

            new_wave = grid[int(idx_min) : int(idx_max)]

            fig = plt.figure(figsize=(21, 9))

            plt.axes([0.05, 0.66, 0.90, 0.25])
            myf.my_colormesh(
                new_wave,
                np.arange(len(diff)),
                100 * diff_backup[:, int(idx_min) : int(idx_max)],
                vmin=low_cmap,
                vmax=high_cmap,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
            ax1 = plt.colorbar(cax=cbaxes)
            ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

            plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
            myf.my_colormesh(
                new_wave,
                np.arange(len(diff)),
                100 * diff_ref2[:, int(idx_min) : int(idx_max)],
                vmin=low_cmap,
                vmax=high_cmap,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
            ax2 = plt.colorbar(cax=cbaxes2)
            ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

            plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
            myf.my_colormesh(
                new_wave,
                np.arange(len(diff)),
                100 * diff_backup[:, int(idx_min) : int(idx_max)]
                - 100 * diff_ref2[:, int(idx_min) : int(idx_max)],
                vmin=low_cmap,
                vmax=high_cmap,
                cmap=cmap,
            )
            plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
            plt.ylabel("Spectra  indexes (time)", fontsize=14)
            plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
            plt.ylim(0, None)
            ax = plt.gca()
            cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
            ax3 = plt.colorbar(cax=cbaxes3)
            ax3.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

            plt.savefig(self.dir_root + "IMAGES/Correction_" + name + ".png")

            if name == "ghost_b":
                diff_backup = []
                self.import_dico_tree()
                sub = np.array(
                    self.dico_tree.loc[self.dico_tree["dico"] == "matching_ghost_a", "dico_used"]
                )[0]
                for i, j in enumerate(files):
                    file = pd.read_pickle(j)
                    f = file["flux" + kw]
                    f_std = file["flux_err"]
                    c = file[sub]["continuum_" + continuum]
                    c_std = file["continuum_err"]
                    f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)
                    diff_backup.append(f_norm - ref)
                diff_backup = np.array(diff_backup)

                fig = plt.figure(figsize=(21, 9))

                plt.axes([0.05, 0.66, 0.90, 0.25])
                myf.my_colormesh(
                    new_wave,
                    np.arange(len(diff)),
                    100 * diff_backup[:, int(idx_min) : int(idx_max)],
                    vmin=low_cmap,
                    vmax=high_cmap,
                    cmap=cmap,
                )
                plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
                plt.ylabel("Spectra  indexes (time)", fontsize=14)
                plt.ylim(0, None)
                ax = plt.gca()
                cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
                ax1 = plt.colorbar(cax=cbaxes)
                ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

                plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
                myf.my_colormesh(
                    new_wave,
                    np.arange(len(diff)),
                    100 * diff_ref2[:, int(idx_min) : int(idx_max)],
                    vmin=low_cmap,
                    vmax=high_cmap,
                    cmap=cmap,
                )
                plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
                plt.ylabel("Spectra  indexes (time)", fontsize=14)
                plt.ylim(0, None)
                ax = plt.gca()
                cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
                ax2 = plt.colorbar(cax=cbaxes2)
                ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

                plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
                myf.my_colormesh(
                    new_wave,
                    np.arange(len(diff)),
                    100 * diff_backup[:, int(idx_min) : int(idx_max)]
                    - 100 * diff_ref2[:, int(idx_min) : int(idx_max)],
                    vmin=low_cmap,
                    vmax=high_cmap,
                    cmap=cmap,
                )
                plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
                plt.ylabel("Spectra  indexes (time)", fontsize=14)
                plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
                plt.ylim(0, None)
                ax = plt.gca()
                cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
                ax3 = plt.colorbar(cax=cbaxes3)
                ax3.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

                plt.savefig(self.dir_root + "IMAGES/Correction_ghost.png")

            to_be_saved = {"wave": grid, "correction_map": correction}
            myf.pickle_dump(
                to_be_saved,
                open(self.dir_root + "CORRECTION_MAP/map_matching_" + name + ".p", "wb"),
            )

            print("\nComputation of the new continua, wait ... \n")
            time.sleep(0.5)
            i = -1
            for j in tqdm(files):
                i += 1
                file = pd.read_pickle(j)
                output = {"continuum_" + continuum: new_continuum[i]}
                file["matching_" + name] = output
                file["matching_" + name]["parameters"] = {
                    "reference_spectrum": reference,
                    "sub_dico_used": sub_dico,
                    "equal_weight": equal_weight,
                    "pca_comp_kept": pca_comp_kept,
                    "step": step + 1,
                }
                ras.save_pickle(j, file)

            self.yarara_analyse_summary()

            self.dico_actif = "matching_" + name

            plt.show(block=False)

    # =============================================================================
    # CORRECTION OF THE STITCHING
    # =============================================================================

    def yarara_correct_stitching(
        self, sub_dico="matching_diff", continuum="linear", reference="median", smooth_box=5
    ):

        """
        Correction of the stitching on the spectrum by sigmoid fitting on folded map

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum used in the difference
        extended : extension of the cluster size
        tresh_rcorr : minimum correlation coefficient to consider a line as ghost
        low : lowest cmap value
        file_stiching : files containing the wavelength of the stitching
        berv_file : berv of the wave matrix used to extract stitching position
        """

        myf.print_box("\n---- RECIPE : CORRECTION STITCHING ----\n")

        directory = self.directory
        self.import_table()
        self.import_material()
        load = self.material

        cmap = self.cmap
        planet = self.planet
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        epsilon = 1e-12

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        all_flux = []
        all_flux_std = []
        snr = []
        jdb = []
        conti = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                grid = file["wave"]

            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum]
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)

            all_flux.append(f_norm)
            all_flux_std.append(f_norm_std)
            conti.append(c)
            jdb.append(file["parameters"]["jdb"])
            snr.append(file["parameters"]["SNR_5500"])

        step = file[sub_dico]["parameters"]["step"]

        all_flux = np.array(all_flux)
        all_flux_std = np.array(all_flux_std)
        conti = np.array(conti)
        jdb = np.array(jdb)
        snr = np.array(snr)

        if reference == "snr":
            ref = all_flux[snr.argmax()]
        elif reference == "median":
            print("[INFO] Reference spectrum : median")
            ref = np.median(all_flux, axis=0)
        elif reference == "master":
            print("[INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print("[INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = all_flux[reference]
        else:
            ref = 0 * np.median(all_flux, axis=0)

        diff = all_flux - ref

        stitch = np.array(load["stitching_delta"])
        stitch = np.array(load["wave"])[stitch != 0]

        mask_stitching = np.array(load["stitching"]).astype("bool")

        # fit step function model
        berv = np.array(self.table["berv" + kw])

        (
            map_binned,
            std_map_binned,
            index,
            jdb_mod,
            jdb_binned,
            jdb_offset,
        ) = self.yarara_map_folded(
            diff[:, mask_stitching], std_map=all_flux_std[:, mask_stitching]
        )

        grid_stitch = grid[mask_stitching]
        berv_diff = 3e5 * (grid_stitch - stitch[:, np.newaxis]) / grid_stitch
        berv_diff[abs(berv_diff) > 30] = 999

        max_stitch_crossing = np.max(np.sum(berv_diff != 999, axis=0))
        loc_max = np.zeros((2 + max_stitch_crossing, len(grid_stitch)))
        loc_max[-1, :] = len(berv) - 1

        berv_diff = np.sort(berv_diff, axis=0)[0:max_stitch_crossing]

        stitch_wave = myf.doppler_r(stitch, berv * 1000)[0]
        stitch_time = (np.arange(len(stitch_wave)) * np.ones(len(stitch_wave[0]))[:, np.newaxis]).T
        stitch_wave = stitch_wave[index]
        bervs = berv[index]

        dist = abs(bervs - berv_diff[:, :, np.newaxis])
        arg = np.argmin(dist, axis=2)
        dist_abs = np.min(dist, axis=2)

        loc_max[0:max_stitch_crossing, :] = arg * (dist_abs < 666)
        loc_max = np.sort(loc_max, axis=0)

        norm = np.std(map_binned, axis=0)
        norm[norm == 0] = 1

        map_binned_s = myf.smooth2d(map_binned, smooth_box)

        maxi_t_idx = loc_max.astype("int")
        maxi_t = jdb_binned[maxi_t_idx]
        maxi_t[0] = np.min(jdb_mod)
        maxi_t[-1] = np.max(jdb_mod)

        map_model = np.zeros(np.shape(diff[:, mask_stitching]))

        indices = np.ones(len(map_binned_s[0])) * np.arange(len(map_binned_s))[:, np.newaxis]
        indices2 = np.ones(len(map_binned_s[0])) * (jdb_mod[:, np.newaxis])

        for j in range(len(maxi_t) - 1):
            mask = (indices >= maxi_t_idx[j]) & (indices <= maxi_t_idx[j + 1])
            mask2 = (indices2 >= maxi_t[j]) & (indices2 <= maxi_t[j + 1])
            map_masked = map_binned_s.copy()
            map_masked[~mask] = np.nan
            map_model += np.nanmedian(map_masked, axis=0) * (mask2.astype("int"))

        map_model_smooth = map_model.copy()

        correction = np.zeros(np.shape(diff))
        correction[:, mask_stitching] = map_model_smooth
        index_back = index.argsort()

        correction = correction[index_back]

        # diff_ref[np.isnan(diff_ref)] = 0

        diff_ref2 = diff - correction

        new_conti = conti * (diff + ref) / (diff_ref2 + ref + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[all_flux == 0] = conti[all_flux == 0]
        new_continuum[new_continuum != new_continuum] = conti[
            new_continuum != new_continuum
        ]  # to supress mystic nan appearing

        max_var = grid[np.std(correction, axis=0).argsort()[::-1]]
        max_var = max_var[(max_var < 6250) & (max_var > 4700)][0]

        wave_min = myf.find_nearest(grid, max_var - 10)[1]
        wave_max = myf.find_nearest(grid, max_var + 10)[1]

        idx_min = myf.find_nearest(grid, wave_min)[0]
        idx_max = myf.find_nearest(grid, wave_max)[0] + 1

        new_wave = grid[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))

        plt.axes([0.05, 0.66, 0.90, 0.25])
        plt.scatter(stitch_wave, stitch_time, color="k", marker=".", zorder=10)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff)),
            100 * diff[index, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
        ax1 = plt.colorbar(cax=cbaxes)
        ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
        plt.scatter(stitch_wave, stitch_time, color="k", marker=".", zorder=10)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff)),
            100 * diff_ref2[index, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
        ax2 = plt.colorbar(cax=cbaxes2)
        ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
        plt.scatter(stitch_wave, stitch_time, color="k", marker=".", zorder=10)
        myf.my_colormesh(
            new_wave,
            np.arange(len(diff)),
            100 * correction[index, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)
        plt.xlim(np.min(new_wave), np.max(new_wave))
        ax = plt.gca()
        cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
        ax3 = plt.colorbar(cax=cbaxes3)
        ax3.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.savefig(self.dir_root + "IMAGES/Correction_stitching.png")

        to_be_saved = {"wave": grid, "correction_map": correction}
        myf.pickle_dump(
            to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_stitching.p", "wb")
        )

        print("\nComputation of the new continua, wait ... \n")
        time.sleep(0.5)
        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_stitching"] = output
            file["matching_stitching"]["parameters"] = {
                "reference_spectrum": reference,
                "sub_dico_used": sub_dico,
                "smooth_box": smooth_box,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.yarara_analyse_summary()

        self.dico_actif = "matching_stitching"

    def yarara_correct_borders_pxl(self, pixels_to_reject=[2, 4095], min_shift=-30, max_shift=30):
        """Produce a brute mask to flag lines crossing pixels according to min-max shift

        Parameters
        ----------
        pixels_to_reject : List of pixels
        min_shift : min shist value in km/s
        max_shift : max shist value in km/s
        """

        myf.print_box("\n---- RECIPE : CREATE PIXELS BORDERS MASK ----\n")

        self.import_material()
        load = self.material

        wave = np.array(load["wave"])
        dwave = np.mean(np.diff(wave))
        pxl = self.yarara_get_pixels()
        orders = self.yarara_get_orders()

        pxl *= orders != 0

        pixels_rejected = np.array(pixels_to_reject)

        pxl[pxl == 0] = np.max(pxl) * 2

        dist = np.zeros(len(pxl)).astype("bool")
        for i in np.arange(np.shape(pxl)[1]):
            dist = dist | (np.min(abs(pxl[:, i] - pixels_rejected[:, np.newaxis]), axis=0) == 0)

        # idx1, dust, dist1 = myf.find_nearest(pixels_rejected,pxl[:,0])
        # idx2, dust, dist2 = myf.find_nearest(pixels_rejected,pxl[:,1])

        # dist = (dist1<=1)|(dist2<=1)

        f = np.where(dist == 1)[0]
        plt.figure()
        for i in np.arange(np.shape(pxl)[1]):
            plt.scatter(pxl[f, i], orders[f, i])

        val, cluster = myf.clustering(dist, 0.5, 1)
        val = np.array([np.product(v) for v in val])
        cluster = cluster[val.astype("bool")]

        left = np.round(wave[cluster[:, 0]] * min_shift / 3e5 / dwave, 0).astype("int")
        right = np.round(wave[cluster[:, 1]] * max_shift / 3e5 / dwave, 0).astype("int")
        # length = right-left+1

        # wave_flagged = wave[f]
        # left = myf.doppler_r(wave_flagged,min_shift*1000)[0]
        # right = myf.doppler_r(wave_flagged,max_shift*1000)[0]

        # idx_left = myf.find_nearest(wave,left)[0]
        # idx_right = myf.find_nearest(wave,right)[0]

        idx_left = cluster[:, 0] + left
        idx_right = cluster[:, 1] + right

        flag_region = np.zeros(len(wave)).astype("int")

        for l, r in zip(idx_left, idx_right):
            flag_region[l : r + 1] = 1

        load["borders_pxl"] = flag_region.astype("int")
        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

    # =============================================================================
    # CORRECTION OF MINOR GHOSTS
    # =============================================================================

    def yarara_correct_berv_signals(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        planet=False,
        reference="median",
        berv_shift="berv",
        fast=True,
    ):

        """
        Display the time-series spectra with proxies and its correlation

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum usedin the difference
        berv_shift : keyword column to use to move in terrestrial rest-frame
        cmap : cmap of the 2D plot
        low_cmap : vmin cmap colorbar
        high_cmap : vmax cmap colorbar

        """

        myf.print_box("\n---- RECIPE : CREATE GHOST BERV MASK ----\n")

        directory = self.directory
        self.import_material()
        load = self.material

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        smooth_map = self.smooth_map

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        flux = []
        err_flux = []
        snr = []
        jdb = []
        berv = []
        rv_shift = []

        epsilon = 1e-12

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                wave = file["wave"]
            f = file["flux" + kw]
            f_std = file["flux_err"]
            c = file[sub_dico]["continuum_" + continuum] + epsilon
            c_std = file["continuum_err"]
            f_norm, f_norm_std = myf.flux_norm_std(f, f_std, c, c_std)

            snr.append(file["parameters"]["SNR_5500"])
            flux.append(f_norm)
            err_flux.append(f_norm_std)

            try:
                jdb.append(file["parameters"]["jdb"])
            except:
                jdb.append(i)
            if type(berv_shift) != np.ndarray:
                try:
                    berv.append(file["parameters"][berv_shift])
                except:
                    berv.append(0)
            else:
                berv = berv_shift
            try:
                rv_shift.append(file["parameters"]["RV_shift"])
            except:
                rv_shift.append(0)

        wave = np.array(wave)
        flux = np.array(flux)
        err_flux = np.array(err_flux)
        snr = np.array(snr)
        jdb = np.array(jdb)
        berv = np.array(berv)
        rv_shift = np.array(rv_shift)
        mean_berv = np.mean(berv)
        berv = berv - mean_berv

        flux *= np.array(load["correction_factor"])
        err_flux *= np.array(load["correction_factor"])

        if reference == "snr":
            ref = flux[snr.argmax()]
        elif reference == "median":
            ref = np.median(flux, axis=0)
        elif reference == "master":
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            ref = flux[reference]
        else:
            ref = 0 * np.median(flux, axis=0)

        diff = myf.smooth2d(flux - ref, smooth_map)

        save = []
        shifted = diff.copy()
        if fast:
            scaling = np.arange(-1.5, 2.1, 0.5)
        else:
            scaling = np.arange(-1.5, 2.5, 0.1)
        for amp in tqdm(scaling):
            for j in np.arange(len(flux)):
                test = myc.tableXY(wave, diff[j], 0 * wave)
                test.x = myf.doppler_r(test.x, (berv[j] * amp - rv_shift[j]) * 1000)[1]
                test.interpolate(new_grid=wave, method="linear", replace=True, interpolate_x=False)
                shifted[j] = test.y
            save.append(np.median(shifted, axis=0))

        save = np.array(save)
        mad = myf.mad(save, axis=0)
        med = np.median(save, axis=0)

        sup3 = myf.smooth(med + 3 * mad, box_pts=10, shape="gaussian")
        inf3 = myf.smooth(med - 3 * mad, box_pts=10, shape="gaussian")

        sup5 = myf.smooth(med + 5 * mad, box_pts=10, shape="gaussian")
        inf5 = myf.smooth(med - 5 * mad, box_pts=10, shape="gaussian")

        save2 = 0 * save.copy()
        save2[save > sup5] = save[save > sup5]
        save2[save < inf5] = save[save < inf5]

        plt.figure()

        for j in range(len(scaling)):
            plt.plot(wave, abs(save2[j]), label="amp = %.2f" % (scaling[j]))
        plt.plot(wave, abs(sup3), color="k")
        plt.plot(wave, abs(inf3), color="k")
        plt.plot(wave, abs(sup5), color="r")
        plt.plot(wave, abs(inf5), color="r")

        mask = np.sum(save2, axis=0) != 0
        val, borders = myf.clustering(mask, 0.5, 1)
        val = np.array([np.product(j) for j in val])
        borders = borders[val.astype("bool")]
        borders = myf.merge_borders(borders)
        flat_mask = myf.flat_clustering(len(wave), borders)
        mask2 = flat_mask.copy().astype("float")
        cluster_level = []
        for k in range(len(borders)):
            sub = np.argmax(np.max(abs(save2)[:, borders[k, 0] : borders[k, 1] + 2], axis=1))
            mask2[borders[k, 0] : borders[k, 1] + 2] = scaling[sub]
            cluster_level.append(scaling[sub])
        cluster_level = np.array(cluster_level)

        counter = pd.DataFrame({"occurence": cluster_level})["occurence"].value_counts()
        level_kept = np.array(counter.index[0 : len(counter.index) // 5 + 1])
        print(
            "Amplitude of berv found relevant : " + " | ".join(["%.1f" % (a) for a in level_kept])
        )

        mask3 = -flat_mask.copy().astype("float")
        mask3 += (flat_mask + mask2) * np.in1d(mask2, level_kept).astype("int")

        ghost_detected = mask3 >= 1.5

        val, borders = myf.clustering(ghost_detected, 0.5, 1)
        val = np.array([np.product(j) for j in val])
        borders = borders[val.astype("bool")]
        borders[:, 1] = borders[:, 1] + 2
        new_borders = borders.copy()
        new_borders[:, 0] = myf.find_nearest(
            wave, myf.doppler_r(wave[borders[:, 0]], 1000 * (60 + 2 * mean_berv))[1]
        )[0]
        new_borders[:, 1] = myf.find_nearest(
            wave, myf.doppler_r(wave[borders[:, 0]], 1000 * (-60 + 2 * mean_berv))[1]
        )[0]
        new_borders[:, -1] = new_borders[:, 1] - new_borders[:, 0] - 1
        mask_ghost = myf.flat_clustering(len(wave), new_borders)
        mask_ghost = mask_ghost != 0

        load["ghost2"] = mask_ghost.astype("int")
        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

    # =============================================================================
    # BRUTAL SUPRESSION OF REGION WITH TOO MUCH VARIANCE
    # =============================================================================

    def yarara_correct_background(
        self, sub_dico="matching_mad", continuum="linear", reference="median", win_ang=1
    ):
        self.yarara_correct_smooth(
            sub_dico=sub_dico,
            continuum=continuum,
            reference=reference,
            wave_min=3900,
            wave_max=4100,
            window_ang=win_ang,
        )

    def yarara_correct_background_backup(
        self,
        sub_dico="matching_mad",
        continuum="linear",
        reference="median",
        win_roll=30,
        instrument="HARPN",
    ):
        """
        background correction based on solar kernel HARPN

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum used in the difference
        win_roll : window size of the rolling algorithm
        """

        myf.print_box("\n---- RECIPE : CORRECTION BACKGROUND ----\n")

        directory = self.directory

        cmap = self.cmap
        planet = self.planet
        low_cmap = self.low_cmap
        high_cmap = self.high_cmap

        self.import_table()
        self.import_material()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        all_flux = []
        snr = []
        jdb = []
        conti = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                grid = file["wave"]
            all_flux.append(file["flux" + kw] / file[sub_dico]["continuum_" + continuum])
            conti.append(file[sub_dico]["continuum_" + continuum])
            jdb.append(file["parameters"]["jdb"])
            snr.append(file["parameters"]["SNR_5500"])

        step = file[sub_dico]["parameters"]["step"]
        all_flux = np.array(all_flux)
        conti = np.array(conti)

        if reference == "snr":
            ref = all_flux[snr.argmax()]
        elif reference == "median":
            print("[INFO] Reference spectrum : median")
            ref = np.median(all_flux, axis=0)
        elif reference == "master":
            print("[INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print("[INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = all_flux[reference]
        else:
            ref = 0 * np.median(all_flux, axis=0)

        kernel_sun = np.genfromtxt(
            root + "/Python/Material/%s_background_kernel.txt" % (instrument)
        )

        kernel = myc.tableXY(kernel_sun[:, 0], kernel_sun[:, 1])
        kernel.interpolate(new_grid=grid, method="linear")
        kernel.y[kernel.x < kernel_sun[0, 0]] = 0
        kernel.y[kernel.x > kernel_sun[-1, 0]] = 0

        all_flux = all_flux / ref

        proxy = []
        for j in tqdm(range(len(self.table))):
            proxy.append(np.nanmean((all_flux[j] * kernel.y)))
        proxy = np.array(proxy)
        proxy -= np.nanmean(proxy)
        proxy /= np.nanmax(proxy) - np.nanmin(proxy)

        self.yarara_obs_info(kw=["proxy_background", proxy])

        plt.plot(proxy)

        print(" [INFO] Running telluric recipe to correct for the background...")

        self.yarara_correct_telluric_proxy(
            sub_dico=sub_dico,
            sub_dico_output="background",
            continuum=continuum,
            wave_min=3900,
            wave_max=4300,
            reference=reference,
            berv_shift="pouet",
            smooth_corr=3,
            proxies_corr=["proxy_background"],
            proxies_detrending=None,
            wave_min_correction=None,
            wave_max_correction=5000,
            min_r_corr=0.40,
            sigma_ext=5,
        )

        # self.time_variation()
        # sts.yarara_time_variations(ratio=True,reference=ref,wave_min=np.min(grid),wave_max=np.max(grid),proxy_corr=proxy,smooth_corr=win_roll,Plot=False)

    def yarara_correct_offset_database(
        self,
        reference_star,
        sub_dico="matching_diff",
        continuum="linear",
        instrument="HARPN",
        berv_shift="berv",
        wave_min=None,
        wave_max=None,
        extended=20,
        wave_min_plot=5800,
        wave_max_plot=6000,
        val_systematic_det=-0.03,
        val_systematic_corr=0,
        replace=False,
    ):
        """
        background based on a star YARARA reduced of similar type star

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum used in the difference
        win_roll : window size of the rolling algorithm
        """

        myf.print_box("\n---- RECIPE : CORRECTION OFFSET DATABASE ----\n")

        directory = self.directory

        cmap = self.cmap
        planet = self.planet
        low_cmap = self.low_cmap * 100
        high_cmap = self.high_cmap * 100

        self.import_table()
        self.import_material()
        self.import_telluric()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        all_flux = []
        snr = []
        jdb = []
        conti = []
        berv = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                grid = file["wave"]
            all_flux.append(file["flux" + kw] / file[sub_dico]["continuum_" + continuum])
            conti.append(file[sub_dico]["continuum_" + continuum])
            jdb.append(file["parameters"]["jdb"])
            snr.append(file["parameters"]["SNR_5500"])
            berv.append(file["parameters"]["berv"])

        step = file[sub_dico]["parameters"]["step"]
        berv = np.array(berv)
        all_flux = np.array(all_flux)
        conti = np.array(conti)

        tab = pd.read_pickle(
            root
            + "/Yarara/"
            + reference_star
            + "/data/s1d/"
            + instrument
            + "/WORKSPACE/Analyse_material.p"
        )
        ref = myc.tableXY(tab["wave"], tab["reference_spectrum"])
        ref.interpolate(new_grid=grid, replace=True)

        maps, maps_std, wave = self.yarara_map(
            wave_min=None,
            wave_max=None,
            sub_dico=sub_dico,
            reference=ref.y,
            berv_shift=berv_shift,
            Plot=False,
        )

        contam = np.percentile(maps, 95, axis=0)
        contam_med = np.percentile(maps, 50, axis=0)

        if wave_min is not None:
            contam[grid < wave_min] = 0
            contam_med[grid < wave_min] = 0

        if wave_max is not None:
            contam[grid > wave_max] = 0
            contam_med[grid > wave_max] = 0

        systematics = 1 + contam - self.telluric_model.y

        val = (systematics < val_systematic_det).astype("int")
        val, borders = myf.clustering(val, 0.5, 0.5)
        val = np.array([np.product(v) for v in val])
        borders = borders[val == 1, :]

        mask = myf.flat_clustering(len(wave), borders, extended=extended) != 0
        mask = ~mask

        contam_med[mask] = 0
        contam[mask] = 0
        contam_med[contam_med > val_systematic_corr] = 0

        # self.debug2 = (systematics, contam, contam_med, mask, wave)
        # test = myc.tableXY(contam,contam_med-contam)
        # test.clip(max=[-1e-6,None])
        # test.rm_outliers()

        # test.binned_scatter(6,Plot=False)
        # test.binned_data.fit_poly(2,Plot=False)

        correction = all_flux.copy()
        if berv_shift is not None:
            if np.sum(abs(berv)) != 0:
                for j in tqdm(np.arange(len(all_flux))):
                    test = myc.tableXY(wave, contam_med, 0 * wave)
                    test.x = myf.doppler_r(test.x, berv[j] * 1000)[0]
                    test.interpolate(
                        new_grid=wave, method="linear", replace=True, interpolate_x=False
                    )
                    correction[j] = test.y

        if replace:
            mask_correction = np.sum(correction, axis=0)
            mask_correction = mask_correction != 0
            self.debug2 = (all_flux, mask_correction)
            spectra_corrected = all_flux.copy()
            spectra_corrected[:, mask_correction.astype("bool")] = np.median(
                spectra_corrected, axis=0
            )[mask_correction.astype("bool")]
            correction = all_flux - spectra_corrected
        else:
            spectra_corrected = all_flux - correction

        self.mask_contam = (np.sum(correction, axis=0) != 0).astype("int")

        load["rejected"] = self.mask_contam
        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

        ref2 = np.median(spectra_corrected, axis=0)

        new_conti = conti * (all_flux) / (spectra_corrected + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[all_flux == 0] = conti[all_flux == 0]
        new_continuum[new_continuum != new_continuum] = conti[
            new_continuum != new_continuum
        ]  # to supress mystic nan appearing
        new_continuum[np.isnan(new_continuum)] = conti[np.isnan(new_continuum)]
        new_continuum[new_continuum == 0] = conti[new_continuum == 0]
        new_continuum = self.uncorrect_hole(new_continuum, conti)

        idx_min = myf.find_nearest(wave, wave_min_plot)[0]
        idx_max = myf.find_nearest(wave, wave_max_plot)[0]
        new_wave = wave[int(idx_min) : int(idx_max)]

        fig = plt.figure(figsize=(21, 9))

        plt.axes([0.05, 0.66, 0.90, 0.25])
        myf.my_colormesh(
            new_wave,
            np.arange(len(all_flux)),
            100 * (all_flux - ref2)[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes = fig.add_axes([0.95, 0.66, 0.01, 0.25])
        ax1 = plt.colorbar(cax=cbaxes)
        ax1.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.375, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(all_flux)),
            100 * (spectra_corrected - ref2)[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=False)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes2 = fig.add_axes([0.95, 0.375, 0.01, 0.25])
        ax2 = plt.colorbar(cax=cbaxes2)
        ax2.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.axes([0.05, 0.09, 0.90, 0.25], sharex=ax, sharey=ax)
        myf.my_colormesh(
            new_wave,
            np.arange(len(all_flux)),
            100 * correction[:, int(idx_min) : int(idx_max)],
            vmin=low_cmap,
            vmax=high_cmap,
            cmap=cmap,
        )
        plt.tick_params(direction="in", top=True, right=True, labelbottom=True)
        plt.ylabel("Spectra  indexes (time)", fontsize=14)
        plt.xlabel(r"Wavelength [$\AA$]", fontsize=14)
        plt.ylim(0, None)
        ax = plt.gca()
        cbaxes3 = fig.add_axes([0.95, 0.09, 0.01, 0.25])
        ax3 = plt.colorbar(cax=cbaxes3)
        ax3.ax.set_ylabel(r"$\Delta$ flux normalised [%]", fontsize=14)

        plt.savefig(self.dir_root + "IMAGES/Correction_database.png")

        to_be_saved = {"wave": grid, "correction_map": correction}
        myf.pickle_dump(
            to_be_saved, open(self.dir_root + "CORRECTION_MAP/map_matching_database.p", "wb")
        )

        print("\nComputation of the new continua, wait ... \n")
        time.sleep(0.5)
        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_database"] = output
            file["matching_database"]["parameters"] = {
                "reference_spectrum": ref.y,
                "sub_dico_used": sub_dico,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.yarara_analyse_summary()

        self.dico_actif = "matching_database"

        plt.show(block=False)

    def yarara_correct_brute(
        self,
        sub_dico="matching_mad",
        continuum="linear",
        reference="median",
        win_roll=1000,
        min_length=5,
        percent_removed=10,
        k_sigma=2,
        extended=10,
        ghost2="HARPS03",
        borders_pxl=False,
    ):

        """
        Brutal suppression of flux value with variance to high (final solution)

        Parameters
        ----------
        sub_dico : The sub_dictionnary used to  select the continuum
        continuum : The continuum to select (either linear or cubic)
        reference : 'median', 'snr' or 'master' to select the reference normalised spectrum used in the difference
        win_roll : window size of the rolling algorithm
        min_length : minimum cluster length to be flagged
        k_sigma : k_sigma of the rolling mad clipping
        extended : extension of the cluster size
        low : lowest cmap value
        high : highest cmap value
        cmap : cmap of the 2D plot
        """

        myf.print_box("\n---- RECIPE : CORRECTION BRUTE ----\n")

        directory = self.directory

        cmap = self.cmap
        planet = self.planet
        low_cmap = self.low_cmap
        high_cmap = self.high_cmap

        self.import_material()
        load = self.material

        epsilon = 1e-12

        kw = "_planet" * planet
        if kw != "":
            print("\n---- PLANET ACTIVATED ----")

        if sub_dico is None:
            sub_dico = self.dico_actif
        print("\n---- DICO %s used ----\n" % (sub_dico))

        files = glob.glob(directory + "RASSI*.p")
        files = np.sort(files)

        all_flux = []
        snr = []
        jdb = []
        conti = []

        for i, j in enumerate(files):
            file = pd.read_pickle(j)
            if not i:
                grid = file["wave"]
            all_flux.append(file["flux" + kw] / file[sub_dico]["continuum_" + continuum])
            conti.append(file[sub_dico]["continuum_" + continuum])
            jdb.append(file["parameters"]["jdb"])
            snr.append(file["parameters"]["SNR_5500"])

        step = file[sub_dico]["parameters"]["step"]
        all_flux = np.array(all_flux)
        conti = np.array(conti)

        if reference == "snr":
            ref = all_flux[snr.argmax()]
        elif reference == "median":
            print("[INFO] Reference spectrum : median")
            ref = np.median(all_flux, axis=0)
        elif reference == "master":
            print("[INFO] Reference spectrum : master")
            ref = np.array(load["reference_spectrum"])
        elif type(reference) == int:
            print("[INFO] Reference spectrum : spectrum %.0f" % (reference))
            ref = all_flux[reference]
        else:
            ref = 0 * np.median(all_flux, axis=0)

        all_flux = all_flux - ref
        metric = np.std(all_flux, axis=0)
        smoothed_med = np.ravel(
            pd.DataFrame(metric).rolling(win_roll, center=True, min_periods=1).quantile(0.5)
        )
        smoothed_mad = np.ravel(
            pd.DataFrame(abs(metric - smoothed_med))
            .rolling(win_roll, center=True, min_periods=1)
            .quantile(0.5)
        )
        mask = (metric - smoothed_med) > smoothed_mad * 1.48 * k_sigma

        clus = myf.clustering(mask, 0.5, 1)[0]
        clus = np.array([np.product(j) for j in clus])
        cluster = myf.clustering(mask, 0.5, 1)[-1]
        cluster = np.hstack([cluster, clus[:, np.newaxis]])
        cluster = cluster[cluster[:, 3] == 1]
        cluster = cluster[cluster[:, 2] >= min_length]

        cluster2 = cluster.copy()
        sum_mask = []
        all_flat = []
        for j in tqdm(range(200)):
            cluster2[:, 0] -= extended
            cluster2[:, 1] += extended
            flat_vec = myf.flat_clustering(len(grid), cluster2[:, 0:2])
            flat_vec = flat_vec >= 1
            all_flat.append(flat_vec)
            sum_mask.append(np.sum(flat_vec))
        sum_mask = 100 * np.array(sum_mask) / len(grid)
        all_flat = np.array(all_flat)

        loc = myf.find_nearest(sum_mask, np.arange(5, 26, 5))[0]

        plt.figure(figsize=(16, 16))

        plt.subplot(3, 1, 1)
        plt.plot(grid, metric - smoothed_med, color="k")
        plt.plot(grid, smoothed_mad * 1.48 * k_sigma, color="r")
        plt.ylim(0, 0.01)
        ax = plt.gca()

        plt.subplot(3, 1, 2, sharex=ax)
        for i, j, k in zip(["5%", "10%", "15%", "20%", "25%"], loc, [1, 1.05, 1.1, 1.15, 1.2]):
            plt.plot(grid, all_flat[j] * k, label=i)
        plt.legend()

        plt.subplot(3, 2, 5)
        b = myc.tableXY(np.arange(len(sum_mask)) * 5, sum_mask)
        b.null()
        b.plot()
        plt.xlabel("Extension of rejection zones", fontsize=14)
        plt.ylabel("Percent of the spectrum rejected [%]", fontsize=14)

        for j in loc:
            plt.axhline(y=b.y[j], color="k", ls=":")

        ax = plt.gca()
        plt.subplot(3, 2, 6, sharex=ax)
        b.diff(replace=False)
        b.deri.plot()
        for j in loc:
            plt.axhline(y=b.deri.y[j], color="k", ls=":")

        if percent_removed is None:
            percent_removed = myf.sphinx("Select the percentage of spectrum removed")

        percent_removed = int(percent_removed)

        loc_select = myf.find_nearest(sum_mask, percent_removed)[0]

        final_mask = np.ravel(all_flat[loc_select]).astype("bool")

        if borders_pxl:
            borders_pxl_mask = np.array(load["borders_pxl"]).astype("bool")
        else:
            borders_pxl_mask = np.zeros(len(final_mask)).astype("bool")

        if ghost2:
            g = pd.read_pickle(root + "/Python/Material/Ghost2_" + ghost2 + ".p")
            ghost = myc.tableXY(g["wave"], g["ghost2"], 0 * g["wave"])
            ghost.interpolate(new_grid=grid, replace=True, method="linear", interpolate_x=False)
            ghost_brute_mask = ghost.y.astype("bool")
        else:
            ghost_brute_mask = np.zeros(len(final_mask)).astype("bool")
        load["ghost2"] = ghost_brute_mask.astype("int")
        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

        final_mask = final_mask | ghost_brute_mask | borders_pxl_mask
        self.brute_mask = final_mask

        load["mask_brute"] = final_mask
        myf.pickle_dump(load, open(self.directory + "Analyse_material.p", "wb"))

        all_flux2 = all_flux.copy()
        all_flux2[:, final_mask] = 0

        plt.figure()
        plt.subplot(2, 1, 1)
        plt.imshow(all_flux, aspect="auto", vmin=low_cmap, vmax=high_cmap, cmap=cmap)
        ax = plt.gca()
        plt.subplot(2, 1, 2, sharex=ax, sharey=ax)
        plt.imshow(all_flux2, aspect="auto", vmin=low_cmap, vmax=high_cmap, cmap=cmap)
        ax = plt.gca()

        new_conti = conti * (all_flux + ref) / (all_flux2 + ref + epsilon)
        new_continuum = new_conti.copy()
        new_continuum[all_flux == 0] = conti[all_flux == 0]
        new_continuum[new_continuum == 0] = conti[new_continuum == 0]
        new_continuum[np.isnan(new_continuum)] = conti[np.isnan(new_continuum)]

        print("\nComputation of the new continua, wait ... \n")
        time.sleep(0.5)

        i = -1
        for j in tqdm(files):
            i += 1
            file = pd.read_pickle(j)
            output = {"continuum_" + continuum: new_continuum[i]}
            file["matching_brute"] = output
            file["matching_brute"]["parameters"] = {
                "reference_spectrum": reference,
                "sub_dico_used": sub_dico,
                "k_sigma": k_sigma,
                "rolling_window": win_roll,
                "minimum_length_cluster": min_length,
                "percentage_removed": percent_removed,
                "step": step + 1,
            }
            ras.save_pickle(j, file)

        self.dico_actif = "matching_brute"

    # =============================================================================
    # AIRMASS
    # =============================================================================

    def yarara_mad_treshold(self, vec_metric, label=None, tresh=0.15):
        metric = np.arange(0.01, 5, 0.01)
        curve = np.sum((vec_metric - metric[:, np.newaxis]) < 0, axis=1)
        label = label + "(%.0f)" % (curve[myf.find_nearest(metric, tresh)[0][0]])
        plt.plot(metric, curve, label=label)
        plt.title("Tresh : %.2f" % (tresh))
        plt.xscale("log")
        plt.xlabel("Percentage of the spectrum rejected [%]")
        plt.ylabel("Nb spectra (P<P_cont)")
        if label is not None:
            plt.legend(loc=2)
        plt.axvline(x=tresh, ls=":", color="k")
        plt.xlim(0.01, 5)
        plt.ylim(0, len(vec_metric))
        plt.grid()
        plt.subplots_adjust(left=0.06, right=0.95, bottom=0.10, top=0.94)

    def uncorrect_hole(self, conti, conti_ref, values_forbidden=[0, np.inf]):
        file_test = self.import_spectrum()
        wave = np.array(file_test["wave"])
        hl = file_test["parameters"]["hole_left"]
        hr = file_test["parameters"]["hole_right"]

        if hl != -99.9:
            i1 = int(myf.find_nearest(wave, hl)[0])
            i2 = int(myf.find_nearest(wave, hr)[0])
            conti[:, i1 - 1 : i2 + 2] = conti_ref[:, i1 - 1 : i2 + 2].copy()

        for l in values_forbidden:
            conti[conti == l] = conti_ref[conti == l].copy()

        return conti

    def yarara_airmass_plot(
        self,
        sub_dico="matching_diff",
        continuum="linear",
        color_code="airmass",
        vmax=None,
        vmin=None,
    ):
        directory = self.directory
        file = self.import_rassine_output(directory)
        time = []
        t = []
        snr = []
        wave = []
        conti = []
        texp = []
        seeing = []
        humidity = []
        airmass = []
        rhk = []
        for j in range(len(file)):
            time.append(file[j]["parameters"][color_code])
            airmass.append(file[j]["parameters"]["airmass"])
            t.append(file[j]["parameters"]["jdb"])
            snr.append(file[j]["parameters"]["SNR_5500"])
            rhk.append(file[j]["parameters"]["CaIIH"])
            texp.append(file[j]["parameters"]["texp"])
            seeing.append(file[j]["parameters"]["seeing"])
            # humidity.append(file[j]['parameters']['humidity'])
            humidity.append(file[j]["parameters"]["telluric_depth"])
            conti.append(file[j][sub_dico]["continuum_" + continuum])
            wave.append(file[j]["wave"])

        time = np.array(time)
        snr = np.array(snr)
        texp = np.array(texp)
        t = np.array(t)
        seeing = np.array(seeing)
        humidity = np.array(humidity)
        airmass = np.array(airmass)
        rhk = np.array(rhk)

        if vmin is None:
            vmin = time.min()
        if vmax is None:
            vmax = time.max()

        colors = plt.cm.brg((time - vmin) / (vmax - vmin))
        scale = 50 * ((humidity - humidity.min()) / (humidity.max() - humidity.min())) ** 2 + 50

        plt.figure(figsize=(14, 12))
        plt.subplot(3, 3, 1)
        plt.scatter(t, airmass, c=colors)
        plt.subplot(3, 3, 2)
        plt.scatter(t, seeing, c=colors)
        plt.subplot(3, 3, 3)
        plt.scatter(t, humidity, c=colors)
        plt.subplot(3, 1, 2)
        for j in range(len(file)):
            plt.plot(wave[j], conti[j] / snr[j] ** 2, color=colors[j])
        plt.subplot(3, 1, 3)
        cumu = []
        for j in range(len(file)):
            cumu.append(np.sum(conti[j] / texp[j]) / len(conti[j]))
            plt.plot(wave[j], conti[j] / texp[j], color=colors[j])

        plt.figure()
        plt.subplot(1, 2, 1)
        test = myc.tableXY(airmass, seeing)
        test.fit_poly2d(cumu - np.mean(cumu), Draw=True, cmap="brg", ax_label="Flux bolo")
        res = test.z_res
        plt.scatter(airmass, seeing, c=cumu, s=scale, cmap="brg", edgecolor="k")
        plt.xlabel("Airmass")
        plt.ylabel("Seeing")
        plt.subplot(1, 2, 2)
        test = myc.tableXY(humidity, rhk)
        test.fit_poly2d(res, Draw=True, cmap="brg", ax_label="Flux bolo residuals")
        plt.scatter(humidity, rhk, c=cumu, cmap="brg", edgecolor="k")
        plt.xlabel("Humidity")
        plt.ylabel("RHK")

    def statistic_mask_ccf(self, instrument="HARPS03", sub_dico="matching_mad"):
        path = self.dir_root.split("Yarara/")[0]
        files = glob.glob(path + "Yarara/*/data/s1d/" + instrument + "/WORKSPACE/Analyse_ccf.p")
        plt.figure(figsize=(18, 9))
        plt.subplots_adjust(left=0.09, right=0.94, wspace=0.4, top=0.95, hspace=0.4)
        for n, col in enumerate(["rv", "fwhm", "contrast"]):

            factor = {"rv": 1000, "contrast": 100, "fwhm": 1}[col]

            all_stars = []

            rvs_harps = myc.tableXY(np.array([0]), np.array([np.nan]))
            rvs_kit = myc.tableXY(np.array([0]), np.array([np.nan]))
            rvs_species = []

            harps_species = []
            harps_mask = []
            rms_harps = []
            rms_kit = []

            c = -1
            for f in files:
                c += 1
                star = f.split("Yarara/")[1].split("/")[0]
                f1 = pd.read_pickle(f)
                all_keys = list(f1.keys())[1:]
                ccf = np.array(all_keys)[np.array([i.split("_")[0] for i in all_keys]) == "CCF"]
                if len(ccf):
                    if (sub_dico in list(f1[ccf[0]].keys())) & (
                        sub_dico in list(f1[ccf[-1]].keys())
                    ):
                        print(star)
                        all_stars.append(star)
                        tab = f1[ccf[0]][sub_dico]["table"]
                        try:
                            std = tab[col + "_std"]
                        except:
                            std = np.ones(len(tab["jdb"]))
                        rv1 = myc.tableXY(tab["jdb"], tab[col] * factor, std * factor)

                        tab = f1[ccf[-1]][sub_dico]["table"]
                        try:
                            std = tab[col + "_std"]
                        except:
                            std = np.ones(len(tab["jdb"]))
                        rv2 = myc.tableXY(tab["jdb"], tab[col] * factor, std * factor)

                        rv1, rv2 = rv1.match_x(rv2)

                        mask_out = (myf.rm_outliers(rv1.y, m=2)[0]) & (
                            myf.rm_outliers(rv2.y, m=2)[0]
                        )
                        rv1.masked(mask_out)
                        rv2.masked(mask_out)
                        rv1.recenter(who="Y")
                        rv2.recenter(who="Y")

                        rv1.substract_polyfit(3, replace=False)
                        rv2.substract_polyfit(3, replace=False)

                        mask_out = (myf.rm_outliers(rv1.detrend_poly.y, m=2)[0]) & (
                            myf.rm_outliers(rv2.detrend_poly.y, m=2)[0]
                        )
                        rv1.detrend_poly.masked(mask_out)
                        rv2.detrend_poly.masked(mask_out)

                        rv1.rms_w()
                        rv2.rms_w()

                        rv1.detrend_poly.rms_w()
                        rv2.detrend_poly.rms_w()

                        rvs_harps.merge(rv1.detrend_poly)
                        rvs_kit.merge(rv2.detrend_poly)
                        rvs_species.append(
                            int(ccf[0] == "CCF_G2") * np.ones(len(rv1.detrend_poly.x))
                        )

                        harps_species.append(c * np.ones(len(rv1.detrend_poly.x)))

                        harps_mask.append(ccf[0])
                        rms_harps.append([rv1.rms, rv1.detrend_poly.rms])
                        rms_kit.append([rv2.rms, rv2.detrend_poly.rms])
            harps_species = np.hstack(harps_species)
            rvs_species = np.hstack(rvs_species)
            harps_mask = np.hstack(harps_mask)
            harps_mask = np.hstack(harps_mask)
            rms_harps = np.array(rms_harps)
            rms_kit = np.array(rms_kit)
            all_stars = np.array(all_stars)
            rvs_harps.supress_nan()
            rvs_kit.supress_nan()
            mask_out = (myf.rm_outliers(rvs_kit.y, m=2)[0]) & (
                myf.rm_outliers(rvs_harps.y, m=2)[0]
            )
            rvs_harps.masked(mask_out)
            rvs_kit.masked(mask_out)
            mask_out = (myf.rm_outliers(rvs_kit.yerr, m=2)[0]) & (
                myf.rm_outliers(rvs_harps.yerr, m=2)[0]
            )
            rvs_harps.masked(mask_out)
            rvs_kit.masked(mask_out)

            nb = np.array([sum(harps_species == i) for i in np.unique(harps_species)])

            m = harps_mask == "CCF_G2"
            for j in range(3):
                mas = ["G2", "K5", "M2"][j]
                plt.subplot(2, 3, n + 1 + 3 * j)
                rms = myc.tableXY(rms_harps[:, 1], rms_harps[:, 1] - rms_kit[:, 1])
                plt.scatter(rms.x, rms.y, alpha=0)
                plt.ylim(
                    np.nanpercentile(rms.y, 25) - 1.5 * myf.IQ(rms.y),
                    np.nanpercentile(rms.y, 75) + 1.5 * myf.IQ(rms.y),
                )
                plt.xlim(
                    np.nanpercentile(rms.x, 25) - 1.5 * myf.IQ(rms.x),
                    np.nanpercentile(rms.x, 75) + 1.5 * myf.IQ(rms.x),
                )
                # rms.myscatter(liste=all_stars,num=False)
                plt.scatter(rms.x[m], rms.y[m], c=nb[m])
                ax = plt.colorbar()
                ax.ax.set_ylabel("Nb nightly obs")

                plt.xlabel("rms %s [m/s]" % (mas), fontsize=14)
                plt.ylabel("rms %s - rms KITCAT [m/s]" % (mas), fontsize=14)
                plt.axhline(y=0, color="r")
                plt.title(col.upper())
                m = (1 - m).astype("bool")

            plt.savefig(
                path + "Yarara/Comparison_mask_Kitcat_generic_%s_%s.png" % (instrument, sub_dico)
            )

    def calib_ca2_proxy(self, proxy="Ha"):
        all_summary = np.sort(glob.glob(root + "/YARARA/*/data/s1d/*/WORKSPACE/Analyse_summary.p"))
        all_star_info = np.sort(glob.glob(root + "/YARARA/*/data/s1d/*/STAR_INFO/Stellar*.p"))

        root1 = [i.split("/WORKSPACE")[0] for i in all_summary]
        root2 = [i.split("/STAR_INFO")[0] for i in all_star_info]

        root = np.array(root1)[np.in1d(root1, root2)]

        ins = [i.split("/")[-1] for i in root]
        star = [i.split("/YARARA/")[1].split("/")[0] for i in root]

        df = pd.DataFrame(np.array([root, ins, star]).T, columns=["root", "ins", "star"])
        df = df.loc[df["ins"] != "INS_merged"].reset_index(drop=True)

        BV = []
        Teff = []
        N_obs = []
        r_pearson = []
        slope = []
        offset = []
        chi2 = []
        mean = []
        mean_ca2 = []
        for n in df["root"]:
            star_info = pd.read_pickle(glob.glob(n + "/STAR_INFO/Stellar_info*.p")[0])
            bv = star_info["BV"]["fixed"]
            try:
                teff = star_info["Teff"]["Gray"]
            except:
                teff = star_info["Teff"]["fixed"]

            table = pd.read_pickle(glob.glob(n + "/WORKSPACE/Analyse_summary.p")[0])
            v1 = myc.tableXY(table["jdb"], table["CaII"], table["CaII_std"])
            v2 = myc.tableXY(table["jdb"], table[proxy])

            if np.sum(abs(v1.y)) * np.sum(abs(v2.y)):
                calib = v2.corr(v1, Draw=False)
                calib.fit_poly(d=1)

                r_pearson.append(calib.r_pearson_w)
                slope.append(calib.poly_coefficient[0])
                offset.append(calib.poly_coefficient[1])
                chi2.append(calib.chi2)
            else:
                r_pearson.append(np.nan)
                slope.append(np.nan)
                offset.append(np.nan)
                chi2.append(np.nan)

            mean.append(np.mean(v2.y))
            mean_ca2.append(np.mean(v1.y))
            BV.append(bv)
            Teff.append(teff)
            N_obs.append(len(v1.x))

        df["bv"] = np.array(BV)
        df["teff"] = np.array(Teff)
        df["Nobs"] = np.array(N_obs)
        df["r_pearson"] = np.array(r_pearson)
        df["slope"] = np.array(slope)
        df["offset"] = np.array(offset)
        df["chi2"] = np.array(chi2)
        df["mean"] = np.array(mean)
        df["ca2"] = np.array(mean_ca2)

        fdf = df.drop(columns="root")
        fdf = fdf.dropna()
        fdf = fdf.loc[fdf["Nobs"] > 30]
        fdf = fdf.loc[myf.rm_outliers(fdf["chi2"])[0]]
        fdf = fdf.loc[myf.rm_outliers(fdf["slope"])[0]]
        fdf = fdf.loc[fdf["r_pearson"] > 0.4]

        fdf = fdf.sort_values(by="r_pearson")

        test = myc.table(fdf.drop(columns=["ins", "star"]))
        test.pairplot()

        plt.scatter(
            fdf.loc[fdf["ins"] == "HARPS03", "bv"],
            fdf.loc[fdf["ins"] == "HARPS03", "mean"],
            c=fdf.loc[fdf["ins"] == "HARPS03", "slope"],
        )

    def soap_import(
        self,
        sub_dico="matching_mad",
        path="/Users/cretignier/Downloads/plage_incl_90.0_prot_25.1_size_0.240_lat_0_R115000",
        col="intensity_tot",
        time=None,
        dt=1,
    ):

        self.yarara_analyse_summary(rm_old=True)
        self.import_table()
        self.import_material()

        load = self.material
        table = self.table

        f = np.sort(glob.glob(path + "/integrated_spectrum_R115000*.csv"))
        f = np.hstack([f[0:50][::-1], f[50:]])
        if time is None:
            time = np.arange(len(f), dt) + 50000
        for i, fname in enumerate(f):
            file = pd.read_csv(fname, sep="\t")[1:]
            a = self.import_spectrum(num=i)
            spectrum = file[col].astype("float64")
            spectrum /= np.nanpercentile(spectrum, 97.5)
            wave = file["wave"].astype("float64")
            a["flux"] = np.array(spectrum).astype("float64")
            a["flux_err"] = np.array(np.sqrt(spectrum) / 300).astype("float64")
            a["continuum_err"] = np.array(0 * spectrum).astype("float64")

            a["wave"] = np.array(wave).astype("float64")
            a[sub_dico]["continuum_linear"] = np.ones(len(spectrum))
            a["matching_diff"]["continuum_linear"] = np.ones(len(spectrum))
            a["parameters"]["jdb"] = time[i]
            a["parameters"]["dwave"] = 0.005
            a["parameters"]["RV_sys"] = 0
            a["parameters"]["RV_shift"] = 0
            a["parameters"]["RV_sec"] = 0
            a["parameters"]["rv_dace"] = 0
            a["parameters"]["hole_left"] = -99.9
            a["parameters"]["hole_right"] = -99.9
            myf.pickle_dump(a, open(table.filename[i], "wb"))

        material = pd.DataFrame(np.ones((len(wave), len(load.keys()))), columns=load.keys())
        material["wave"] = wave
        material["reference_spectrum"] = spectrum

        myf.pickle_dump(material, open(self.directory + "Analyse_material.p", "wb"))

        for j in np.arange(i + 1, len(table)):
            name = table.filename[j]
            name2 = name.replace("RAS", "temp_RAS")
            os.system("mv " + name + " " + name2)

        self.yarara_analyse_summary(rm_old=True)
        self.import_table()
